{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import mlflow\n",
    "import optuna\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "from omegaconf.omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "from model import DRNOnlyModel, UCCModel\n",
    "from dataset import MnistEncodedDataset\n",
    "from omegaconf import DictConfig\n",
    "from utils import get_or_create_experiment, parse_experiment_runs_to_optuna_study\n",
    "\n",
    "experiment_id = get_or_create_experiment(\"ucc-drn-distil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = mlflow.search_runs(experiment_ids=[experiment_id], output_format=\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'info': {'artifact_uri': 'file:///D:/UCC-DRN-Pytorch/mnist/mlruns/380446538050732248/313d1bb3d6f446a2ab697d5f41c4c580/artifacts',\n",
       "  'end_time': 1741185524741,\n",
       "  'experiment_id': '380446538050732248',\n",
       "  'lifecycle_stage': 'active',\n",
       "  'run_id': '313d1bb3d6f446a2ab697d5f41c4c580',\n",
       "  'run_name': 'brawny-bass-550',\n",
       "  'run_uuid': '313d1bb3d6f446a2ab697d5f41c4c580',\n",
       "  'start_time': 1741184857968,\n",
       "  'status': 'FAILED',\n",
       "  'user_id': 'guanyu'},\n",
       " 'data': {'metrics': {'eval_ucc_acc': 0.66125,\n",
       "   'eval_ucc_loss': 0.8382442593574524,\n",
       "   'total_loss': 0.8076022267341614,\n",
       "   'train_distilation_loss': 0.05767541006207466,\n",
       "   'train_ucc_acc': 0.6500000357627869,\n",
       "   'train_ucc_loss': 1.0575778484344482},\n",
       "  'params': {'hidden_q': '16',\n",
       "   'lr': '0.09165019656675866',\n",
       "   'num_bins': '17',\n",
       "   'num_layers': '1',\n",
       "   'num_nodes': '6'},\n",
       "  'tags': {'mlflow.log-model.history': '[{\"run_id\": \"313d1bb3d6f446a2ab697d5f41c4c580\", \"artifact_path\": \"best_model.pth\", \"utc_time_created\": \"2025-03-05 14:28:57.248725\", \"model_uuid\": \"e360705116fd4fe19c3fee85adc10984\", \"flavors\": {\"pytorch\": {\"model_data\": \"data\", \"pytorch_version\": \"2.6.0+cu118\", \"code\": null}, \"python_function\": {\"pickle_module_name\": \"mlflow.pytorch.pickle_module\", \"loader_module\": \"mlflow.pytorch\", \"python_version\": \"3.11.5\", \"data\": \"data\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}}}, {\"run_id\": \"313d1bb3d6f446a2ab697d5f41c4c580\", \"artifact_path\": \"best_model.pth\", \"utc_time_created\": \"2025-03-05 14:30:04.486076\", \"model_uuid\": \"5fdb00bb7fec4c6f8401bf824463610e\", \"flavors\": {\"pytorch\": {\"model_data\": \"data\", \"pytorch_version\": \"2.6.0+cu118\", \"code\": null}, \"python_function\": {\"pickle_module_name\": \"mlflow.pytorch.pickle_module\", \"loader_module\": \"mlflow.pytorch\", \"python_version\": \"3.11.5\", \"data\": \"data\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}}}, {\"run_id\": \"313d1bb3d6f446a2ab697d5f41c4c580\", \"artifact_path\": \"best_model.pth\", \"utc_time_created\": \"2025-03-05 14:31:12.404451\", \"model_uuid\": \"4e1b8e1c84da435c9b319dd4cac90e49\", \"flavors\": {\"pytorch\": {\"model_data\": \"data\", \"pytorch_version\": \"2.6.0+cu118\", \"code\": null}, \"python_function\": {\"pickle_module_name\": \"mlflow.pytorch.pickle_module\", \"loader_module\": \"mlflow.pytorch\", \"python_version\": \"3.11.5\", \"data\": \"data\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}}}, {\"run_id\": \"313d1bb3d6f446a2ab697d5f41c4c580\", \"artifact_path\": \"best_model.pth\", \"utc_time_created\": \"2025-03-05 14:36:04.599790\", \"model_uuid\": \"1bb61baf1d8f43d7ab93b4cd9f7aa2ab\", \"flavors\": {\"pytorch\": {\"model_data\": \"data\", \"pytorch_version\": \"2.6.0+cu118\", \"code\": null}, \"python_function\": {\"pickle_module_name\": \"mlflow.pytorch.pickle_module\", \"loader_module\": \"mlflow.pytorch\", \"python_version\": \"3.11.5\", \"data\": \"data\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}}}, {\"run_id\": \"313d1bb3d6f446a2ab697d5f41c4c580\", \"artifact_path\": \"best_model.pth\", \"utc_time_created\": \"2025-03-05 14:37:19.349994\", \"model_uuid\": \"b6089b1524514858ac4185230af6b38b\", \"flavors\": {\"pytorch\": {\"model_data\": \"data\", \"pytorch_version\": \"2.6.0+cu118\", \"code\": null}, \"python_function\": {\"pickle_module_name\": \"mlflow.pytorch.pickle_module\", \"loader_module\": \"mlflow.pytorch\", \"python_version\": \"3.11.5\", \"data\": \"data\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}}}, {\"run_id\": \"313d1bb3d6f446a2ab697d5f41c4c580\", \"artifact_path\": \"best_model.pth\", \"utc_time_created\": \"2025-03-05 14:38:32.684621\", \"model_uuid\": \"ad8653686aaa417aba509a1d60ba3ce2\", \"flavors\": {\"pytorch\": {\"model_data\": \"data\", \"pytorch_version\": \"2.6.0+cu118\", \"code\": null}, \"python_function\": {\"pickle_module_name\": \"mlflow.pytorch.pickle_module\", \"loader_module\": \"mlflow.pytorch\", \"python_version\": \"3.11.5\", \"data\": \"data\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}}}]',\n",
       "   'mlflow.runName': 'brawny-bass-550',\n",
       "   'mlflow.source.git.commit': '8e08619b27da277966831a200de755d2adc6ea6b',\n",
       "   'mlflow.source.name': '.\\\\train_distil.py',\n",
       "   'mlflow.source.type': 'LOCAL',\n",
       "   'mlflow.user': 'guanyu'}},\n",
       " 'inputs': {'dataset_inputs': []}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs[0].to_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in runs:\n",
    "    dictionary = run.to_dictionary()\n",
    "    run_name = dictionary[\"info\"][\"run_name\"]\n",
    "    params = dictionary[\"data\"][\"params\"]\n",
    "    if run_name == \"fortunate-mule-124\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"params.json\", \"r\") as file:\n",
    "    params_json = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bins\n",
      "{'type': 'int', 'range': [5, 30], 'aliases': ['model.drn.num_bins', 'args.num_bins', 'model.kde_model.num_bins']}\n",
      "lr\n",
      "{'type': 'float', 'range': [0.0001, 0.1], 'aliases': ['args.learning_rate']}\n",
      "hidden_q\n",
      "{'type': 'int', 'range': [4, 100], 'aliases': ['model.drn.hidden_q']}\n",
      "num_layers\n",
      "{'type': 'int', 'range': [1, 10], 'aliases': ['model.drn.num_layers']}\n",
      "num_nodes\n",
      "{'type': 'int', 'range': [1, 10], 'aliases': ['model.drn.num_nodes']}\n",
      "lr_multiplier\n",
      "{'type': 'float', 'range': [1, 2], 'aliases': ['args.lr_multiplier']}\n"
     ]
    }
   ],
   "source": [
    "for p,v in params_json.items():\n",
    "    print(p)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {p: (float(value) if p==\"lr\"else int(value)) for p, value in params.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "        cfg = compose(config_name=\"train_drn\")\n",
    "\n",
    "for p, values in params_json.items():\n",
    "    if p == \"lr_multiplier\":\n",
    "        break\n",
    "    for a in values[\"aliases\"]:\n",
    "        exec(f\"cfg.{a} = params['{p}']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parent_model(device):\n",
    "    model_path = \"outputs\\\\2024-03-01\\\\14-44-08\"\n",
    "    ucc_cfg = OmegaConf.load(os.path.join(model_path, \".hydra\\\\config.yaml\"))\n",
    "    model = UCCModel(ucc_cfg)\n",
    "    state_dict = torch.load(os.path.join(model_path, \"mnist_ucc_best.pth\"), weights_only=False)[\"model_state_dict\"]\n",
    "    model.load_state_dict(state_dict)\n",
    "    parent_model = model.to(device)\n",
    "    parent_model.eval()\n",
    "    return parent_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m\n\u001b[0;32m     37\u001b[0m     val_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     38\u001b[0m         val_dataset,\n\u001b[0;32m     39\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m     40\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[0;32m     41\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loader, val_loader\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(model, parent_model, val_loader, device) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTuple\u001b[49m[np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat32]:\n\u001b[0;32m     47\u001b[0m     T \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     48\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "def init_model_and_optimizer(args, model_cfg, device):\n",
    "    model = DRNOnlyModel(model_cfg).to(device)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def init_dataloader(args):\n",
    "    train_dataset_len = args.train_num_steps * args.batch_size\n",
    "    train_dataset = MnistEncodedDataset(\n",
    "        mode=\"train\",\n",
    "        num_instances=args.num_instances,\n",
    "        num_samples_per_class=args.num_samples_per_class,\n",
    "        digit_arr=list(range(args.ucc_end-args.ucc_start+1)),\n",
    "        ucc_start=args.ucc_start,\n",
    "        ucc_end=args.ucc_end,\n",
    "        length=train_dataset_len,\n",
    "    )\n",
    "    val_dataset_len = args.val_num_steps * args.batch_size\n",
    "    val_dataset = MnistEncodedDataset(\n",
    "        mode=\"val\",\n",
    "        num_instances=args.num_instances,\n",
    "        num_samples_per_class=args.num_samples_per_class,\n",
    "        digit_arr=list(range(args.ucc_end-args.ucc_start+1)),\n",
    "        ucc_start=args.ucc_start,\n",
    "        ucc_end=args.ucc_end,\n",
    "        length=val_dataset_len,\n",
    "    )\n",
    "    # create DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def evaluate(model, parent_model, val_loader, device) -> Tuple[np.float32, np.float32]:\n",
    "    T = 2\n",
    "    model.eval()\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_samples, batch_labels in val_loader:\n",
    "            batch_samples = batch_samples.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            ucc_logits = model(batch_samples)\n",
    "            # parent_logits = parent_model.ucc_classifier(parent_model.kde(\n",
    "            #     batch_samples, parent_model.num_nodes, parent_model.sigma))\n",
    "            ucc_loss = F.cross_entropy(ucc_logits, batch_labels)\n",
    "\n",
    "            # soft_targets = nn.functional.softmax(parent_logits / T, dim=-1)\n",
    "            # soft_prob = nn.functional.log_softmax(ucc_logits / T, dim=-1)\n",
    "            # soft_targets_loss = torch.sum(\n",
    "            #     soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "            # loss = 0.75*ucc_loss + 0.25*soft_targets_loss\n",
    "            # acculate accuracy\n",
    "            _, ucc_predicts = torch.max(ucc_logits, dim=1)\n",
    "            acc = torch.sum(\n",
    "                ucc_predicts == batch_labels).item() / len(batch_labels)\n",
    "            val_acc_list.append(acc)\n",
    "            val_loss_list.append(ucc_loss.item())\n",
    "    return np.mean(val_loss_list), np.mean(val_acc_list)\n",
    "\n",
    "\n",
    "def train(args, model, parent_model, optimizer, lr_scheduler, train_loader, val_loader, device):\n",
    "    # distillation temperature\n",
    "    T = 2\n",
    "    # output_dir = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir\n",
    "    parent_model.eval()\n",
    "    model.train()\n",
    "    step = 0\n",
    "    best_eval_acc = 0\n",
    "    patience = 100\n",
    "    train_logs = []\n",
    "    for batch_samples, batch_labels in tqdm(train_loader):\n",
    "        batch_samples = batch_samples.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if model.alpha==1:\n",
    "        #     ucc_logits = model(batch_samples, batch_labels)\n",
    "        #     loss:torch.Tensor = model.compute_loss(\n",
    "        #         labels=batch_labels,\n",
    "        #         output=ucc_logits\n",
    "        #     )\n",
    "        # else:\n",
    "        # original loss\n",
    "        ucc_logits = model(batch_samples)\n",
    "        ucc_loss = model.compute_loss(\n",
    "            outputs=ucc_logits,\n",
    "            labels=batch_labels,\n",
    "        )\n",
    "        # distillation loss\n",
    "        with torch.no_grad():\n",
    "            parent_logits = parent_model.ucc_classifier(parent_model.kde(\n",
    "                batch_samples, parent_model.num_nodes, parent_model.sigma))\n",
    "\n",
    "        # soft_targets = nn.functional.softmax(parent_logits / T, dim=-1)\n",
    "        # soft_prob = nn.functional.log_softmax(ucc_logits / T, dim=-1)\n",
    "        # soft_targets_loss = torch.sum(\n",
    "        #     soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "        # loss = 0.75*ucc_loss + 0.25*soft_targets_loss\n",
    "\n",
    "        ucc_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                _, pred = torch.max(ucc_logits, dim=1)\n",
    "                accuracy = torch.sum(\n",
    "                    pred.flatten() == batch_labels.flatten())/len(batch_labels)\n",
    "            train_logs.append({\n",
    "                \"train_ucc_loss\": ucc_loss.detach().item(),\n",
    "                # \"train_distilation_loss\": soft_targets_loss.detach().item(),\n",
    "                # \"total_loss\": loss.detach().item(),\n",
    "                \"train_ucc_acc\": float(accuracy)})\n",
    "\n",
    "        if step % args.save_interval == 0:\n",
    "            eval_loss, eval_acc = evaluate(\n",
    "                model, parent_model, val_loader, device)\n",
    "            print(\n",
    "                f\"step: {step}, eval loss: {eval_loss}, eval acc: {eval_acc}\")\n",
    "            # early stop\n",
    "            if eval_acc > best_eval_acc:\n",
    "                patience = 10\n",
    "                best_eval_acc = eval_acc\n",
    "                # save model\n",
    "                # save_path = os.path.join(output_dir, f\"{args.model_name}_best.pth\")\n",
    "                # put eval loss and acc in model state dict\n",
    "                # save_dict = {\n",
    "                #     \"model_state_dict\": model.state_dict(),\n",
    "                #     \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                #     \"eval_loss\": eval_loss,\n",
    "                #     \"eval_acc\": eval_acc,\n",
    "                #     \"step\": step,\n",
    "                # }\n",
    "                # maybe save optimizer state dict as well\n",
    "                # torch.save(save_dict, save_path)\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0:\n",
    "                break\n",
    "            model.train()\n",
    "    print(\"Training finished!!!\")\n",
    "    return {\n",
    "        \"drn_model\": model,\n",
    "        \"best_acc\": best_eval_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 5)\n",
      "(55, 55)\n",
      "(4, 55)\n",
      "x_train shape: torch.Size([50000, 1, 28, 28])\n",
      "50000 train samples\n",
      "10000 val samples\n",
      "x_train shape: torch.Size([50000, 10])\n",
      "50000 train samples\n",
      "10000 val samples\n",
      "x_train shape: torch.Size([50000, 1, 28, 28])\n",
      "50000 train samples\n",
      "10000 val samples\n",
      "x_train shape: torch.Size([50000, 10])\n",
      "50000 train samples\n",
      "10000 val samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1001/100000 [02:35<71:04:59,  2.58s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1000, eval loss: 0.8058490535616875, eval acc: 0.9890000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2001/100000 [04:59<67:21:12,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2000, eval loss: 0.7606072467565537, eval acc: 0.9980000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3001/100000 [07:25<66:29:30,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3000, eval loss: 0.7552320855855942, eval acc: 0.9990000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4001/100000 [09:48<66:05:58,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4000, eval loss: 0.7542347142100334, eval acc: 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5001/100000 [12:12<62:58:32,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5000, eval loss: 0.7536104601621628, eval acc: 0.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6001/100000 [14:36<63:30:12,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6000, eval loss: 0.7513154190778732, eval acc: 0.9982499999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7001/100000 [17:00<62:51:49,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7000, eval loss: 0.7496176418662072, eval acc: 0.9990000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8001/100000 [19:24<62:22:36,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8000, eval loss: 0.7519190725684166, eval acc: 0.99675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9001/100000 [21:49<61:03:56,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9000, eval loss: 0.7498063942790032, eval acc: 0.9990000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10001/100000 [24:12<60:20:12,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10000, eval loss: 0.749206371307373, eval acc: 0.9997499999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11001/100000 [26:36<62:19:50,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11000, eval loss: 0.7504173627495766, eval acc: 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12001/100000 [29:00<60:01:43,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 12000, eval loss: 0.7498601135611535, eval acc: 0.9984999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13001/100000 [31:24<59:13:31,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 13000, eval loss: 0.7549134087562561, eval acc: 0.99625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14001/100000 [33:53<59:24:58,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 14000, eval loss: 0.748899539411068, eval acc: 0.99925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15001/100000 [36:20<58:07:06,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 15000, eval loss: 0.7490520638227463, eval acc: 0.9990000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16001/100000 [38:45<57:58:09,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 16000, eval loss: 0.7486720624566078, eval acc: 0.9990000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17001/100000 [41:10<56:57:16,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 17000, eval loss: 0.7518499410152435, eval acc: 0.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18001/100000 [43:34<67:30:04,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 18000, eval loss: 0.7515738433599473, eval acc: 0.9980000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18144/100000 [43:55<3:18:09,  6.88it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m parent_model \u001b[38;5;241m=\u001b[39m load_parent_model(device)\n\u001b[0;32m      5\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m init_dataloader(args)\n\u001b[1;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 97\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, model, parent_model, optimizer, lr_scheduler, train_loader, val_loader, device)\u001b[0m\n\u001b[0;32m     87\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# if model.alpha==1:\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m#     ucc_logits = model(batch_samples, batch_labels)\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#     loss:torch.Tensor = model.compute_loss(\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# original loss\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m ucc_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m ucc_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_loss(\n\u001b[0;32m     99\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mucc_logits,\n\u001b[0;32m    100\u001b[0m     labels\u001b[38;5;241m=\u001b[39mbatch_labels,\n\u001b[0;32m    101\u001b[0m )\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# distillation loss\u001b[39;00m\n",
      "File \u001b[1;32md:\\UCC-DRN-Pytorch\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\UCC-DRN-Pytorch\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\UCC-DRN-Pytorch\\mnist\\model.py:364\u001b[0m, in \u001b[0;36mDRNOnlyModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    362\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkde(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_bins, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma)\n\u001b[0;32m    363\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_bins)\n\u001b[1;32m--> 364\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32md:\\UCC-DRN-Pytorch\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\UCC-DRN-Pytorch\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\UCC-DRN-Pytorch\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\UCC-DRN-Pytorch\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\UCC-DRN-Pytorch\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\UCC-DRN-Pytorch\\mnist\\..\\drn_pytorch\\drn.py:72\u001b[0m, in \u001b[0;36mDRN.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m---> 72\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\UCC-DRN-Pytorch\\mnist\\..\\drn_pytorch\\drn.py:117\u001b[0m, in \u001b[0;36mDRN._forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m logsum \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(logPw, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# logsum.size() --> (batch_size,out_features,out_bins)\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# 3. log of exp of bias terms: log(expB) = exponent_B\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m exponent_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbq \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms0 \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_bins \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlamq, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mba \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_bins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlama\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# exponent_B.size() --> (out_features,out_bins)\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# 4. add B to logsum\u001b[39;00m\n\u001b[0;32m    119\u001b[0m logsumB \u001b[38;5;241m=\u001b[39m logsum \u001b[38;5;241m+\u001b[39m exponent_B \u001b[38;5;66;03m# logsumB.size() --> (batch_size,out_features,out_bins)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = cfg.args\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "model, optimizer = init_model_and_optimizer(args, cfg, device)\n",
    "parent_model = load_parent_model(device)\n",
    "train_loader, val_loader = init_dataloader(args)\n",
    "output = train(args, model, parent_model, optimizer, None, train_loader, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
