{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f9e9c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using optimization to find the optimal mean and variance for normal initialization\n",
    "from copy import deepcopy\n",
    "from hydra import compose, initialize\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import optuna\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "from omegaconf.omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model_ori import UCCModel\n",
    "from dataset import MnistDataset\n",
    "from utils import get_or_create_experiment, parse_experiment_runs_to_optuna_study\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "cfg_name = \"train\"\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    cfg = compose(config_name=cfg_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfaffe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UCCModel(cfg=cfg)\n",
    "pytorch_state_dict_shape = {}\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "for key, value in state_dict.items():\n",
    "    pytorch_state_dict_shape[key] = value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b6a219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder.0.weight': torch.Size([16, 1, 3, 3]),\n",
       " 'encoder.0.bias': torch.Size([16]),\n",
       " 'encoder.1.blocks.0.conv1.weight': torch.Size([32, 16, 3, 3]),\n",
       " 'encoder.1.blocks.0.conv1.bias': torch.Size([32]),\n",
       " 'encoder.1.blocks.0.conv2.weight': torch.Size([32, 32, 3, 3]),\n",
       " 'encoder.1.blocks.0.conv2.bias': torch.Size([32]),\n",
       " 'encoder.1.blocks.0.skip_conv.weight': torch.Size([16, 16, 1, 1]),\n",
       " 'encoder.1.blocks.0.skip_conv.bias': torch.Size([16]),\n",
       " 'encoder.2.blocks.0.conv1.weight': torch.Size([64, 32, 3, 3]),\n",
       " 'encoder.2.blocks.0.conv1.bias': torch.Size([64]),\n",
       " 'encoder.2.blocks.0.conv2.weight': torch.Size([64, 64, 3, 3]),\n",
       " 'encoder.2.blocks.0.conv2.bias': torch.Size([64]),\n",
       " 'encoder.2.blocks.0.skip_conv.weight': torch.Size([32, 32, 1, 1]),\n",
       " 'encoder.2.blocks.0.skip_conv.bias': torch.Size([32]),\n",
       " 'encoder.3.blocks.0.conv1.weight': torch.Size([128, 64, 3, 3]),\n",
       " 'encoder.3.blocks.0.conv1.bias': torch.Size([128]),\n",
       " 'encoder.3.blocks.0.conv2.weight': torch.Size([128, 128, 3, 3]),\n",
       " 'encoder.3.blocks.0.conv2.bias': torch.Size([128]),\n",
       " 'encoder.3.blocks.0.skip_conv.weight': torch.Size([64, 64, 1, 1]),\n",
       " 'encoder.3.blocks.0.skip_conv.bias': torch.Size([64]),\n",
       " 'encoder.6.weight': torch.Size([10, 6272]),\n",
       " 'decoder.0.weight': torch.Size([6272, 10]),\n",
       " 'decoder.0.bias': torch.Size([6272]),\n",
       " 'decoder.3.blocks.0.conv1.weight': torch.Size([64, 128, 3, 3]),\n",
       " 'decoder.3.blocks.0.conv1.bias': torch.Size([64]),\n",
       " 'decoder.3.blocks.0.conv2.weight': torch.Size([64, 64, 3, 3]),\n",
       " 'decoder.3.blocks.0.conv2.bias': torch.Size([64]),\n",
       " 'decoder.3.blocks.0.skip_conv.weight': torch.Size([128, 128, 1, 1]),\n",
       " 'decoder.3.blocks.0.skip_conv.bias': torch.Size([128]),\n",
       " 'decoder.4.blocks.0.conv1.weight': torch.Size([32, 64, 3, 3]),\n",
       " 'decoder.4.blocks.0.conv1.bias': torch.Size([32]),\n",
       " 'decoder.4.blocks.0.conv2.weight': torch.Size([32, 32, 3, 3]),\n",
       " 'decoder.4.blocks.0.conv2.bias': torch.Size([32]),\n",
       " 'decoder.4.blocks.0.skip_conv.weight': torch.Size([64, 64, 1, 1]),\n",
       " 'decoder.4.blocks.0.skip_conv.bias': torch.Size([64]),\n",
       " 'decoder.5.blocks.0.conv1.weight': torch.Size([16, 32, 3, 3]),\n",
       " 'decoder.5.blocks.0.conv1.bias': torch.Size([16]),\n",
       " 'decoder.5.blocks.0.conv2.weight': torch.Size([16, 16, 3, 3]),\n",
       " 'decoder.5.blocks.0.conv2.bias': torch.Size([16]),\n",
       " 'decoder.5.blocks.0.skip_conv.weight': torch.Size([32, 32, 1, 1]),\n",
       " 'decoder.5.blocks.0.skip_conv.bias': torch.Size([32]),\n",
       " 'decoder.7.weight': torch.Size([1, 16, 3, 3]),\n",
       " 'decoder.7.bias': torch.Size([1]),\n",
       " 'ucc_classifier.0.weight': torch.Size([384, 110]),\n",
       " 'ucc_classifier.0.bias': torch.Size([384]),\n",
       " 'ucc_classifier.2.weight': torch.Size([192, 384]),\n",
       " 'ucc_classifier.2.bias': torch.Size([192]),\n",
       " 'ucc_classifier.4.weight': torch.Size([4, 192]),\n",
       " 'ucc_classifier.4.bias': torch.Size([4])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_state_dict_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f52efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = {\n",
    "    'encoder.0.weight': '/model_1/conv2d_1/kernel:0',\n",
    "    'encoder.0.bias': '/model_1/conv2d_1/bias:0',\n",
    "    'encoder.1.blocks.0.conv1.weight': '/model_1/conv2d_2/kernel:0',\n",
    "    'encoder.1.blocks.0.conv1.bias': '/model_1/conv2d_2/bias:0',\n",
    "    'encoder.1.blocks.0.conv2.weight': '/model_1/conv2d_3/kernel:0',\n",
    "    'encoder.1.blocks.0.conv2.bias': '/model_1/conv2d_3/bias:0',\n",
    "    'encoder.2.blocks.0.conv1.weight': '/model_1/conv2d_4/kernel:0',\n",
    "    'encoder.2.blocks.0.conv1.bias': '/model_1/conv2d_4/bias:0',\n",
    "    'encoder.2.blocks.0.conv2.weight': '/model_1/conv2d_5/kernel:0',\n",
    "    'encoder.2.blocks.0.conv2.bias': '/model_1/conv2d_5/bias:0',\n",
    "    'encoder.2.blocks.0.skip_conv.weight': '/model_1/conv2d_6/kernel:0',\n",
    "    'encoder.2.blocks.0.skip_conv.bias': '/model_1/conv2d_6/bias:0',\n",
    "    'encoder.3.blocks.0.conv1.weight': '/model_1/conv2d_7/kernel:0',\n",
    "    'encoder.3.blocks.0.conv1.bias': '/model_1/conv2d_7/bias:0',\n",
    "    'encoder.3.blocks.0.conv2.weight': '/model_1/conv2d_8/kernel:0',\n",
    "    'encoder.3.blocks.0.conv2.bias': '/model_1/conv2d_8/bias:0',\n",
    "    'encoder.3.blocks.0.skip_conv.weight': '/model_1/conv2d_9/kernel:0',\n",
    "    'encoder.3.blocks.0.skip_conv.bias': '/model_1/conv2d_9/bias:0',\n",
    "    'encoder.6.weight': '/model_1/fc_sigmoid/kernel:0',\n",
    "    'ucc_classifier.0.weight': '/fc_relu1/fc_relu1/kernel:0',\n",
    "    'ucc_classifier.0.bias': '/fc_relu1/fc_relu1/bias:0',\n",
    "    'ucc_classifier.2.weight': '/fc_relu2/fc_relu2/kernel:0',\n",
    "    'ucc_classifier.2.bias': '/fc_relu2/fc_relu2/bias:0',\n",
    "    'ucc_classifier.4.weight': '/fc_softmax/fc_softmax/kernel:0',\n",
    "    'ucc_classifier.4.bias': '/fc_softmax/fc_softmax/bias:0'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "state_dict = {\n",
    "    name: torch.tensor(\n",
    "        np.transpose(model_1_weights_ori[value], (3,2,0,1)) \n",
    "        if len(model_1_weights_ori[value].shape)==4 \n",
    "        else (np.transpose(model_1_weights_ori[value], (1,0)) if len(model_1_weights_ori[value].shape)==2 \n",
    "            else model_1_weights_ori[value])\n",
    "        ) \n",
    "    for name, value in name_map.items()\n",
    "}\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad17e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = {\n",
    "    'encoder.0.weight': '/model_1/conv2d_1/kernel:0',\n",
    "    'encoder.0.bias': '/model_1/conv2d_1/bias:0',\n",
    "    'encoder.1.blocks.0.conv1.weight': '/model_1/conv2d_2/kernel:0',\n",
    "    'encoder.1.blocks.0.conv1.bias': '/model_1/conv2d_2/bias:0',\n",
    "    'encoder.1.blocks.0.conv2.weight': '/model_1/conv2d_3/kernel:0',\n",
    "    'encoder.1.blocks.0.conv2.bias': '/model_1/conv2d_3/bias:0',\n",
    "    'encoder.2.blocks.0.conv1.weight': '/model_1/conv2d_4/kernel:0',\n",
    "    'encoder.2.blocks.0.conv1.bias': '/model_1/conv2d_4/bias:0',\n",
    "    'encoder.2.blocks.0.conv2.weight': '/model_1/conv2d_5/kernel:0',\n",
    "    'encoder.2.blocks.0.conv2.bias': '/model_1/conv2d_5/bias:0',\n",
    "    'encoder.2.blocks.0.skip_conv.weight': '/model_1/conv2d_6/kernel:0',\n",
    "    'encoder.2.blocks.0.skip_conv.bias': '/model_1/conv2d_6/bias:0',\n",
    "    'encoder.3.blocks.0.conv1.weight': '/model_1/conv2d_7/kernel:0',\n",
    "    'encoder.3.blocks.0.conv1.bias': '/model_1/conv2d_7/bias:0',\n",
    "    'encoder.3.blocks.0.conv2.weight': '/model_1/conv2d_8/kernel:0',\n",
    "    'encoder.3.blocks.0.conv2.bias': '/model_1/conv2d_8/bias:0',\n",
    "    'encoder.3.blocks.0.skip_conv.weight': '/model_1/conv2d_9/kernel:0',\n",
    "    'encoder.3.blocks.0.skip_conv.bias': '/model_1/conv2d_9/bias:0',\n",
    "    'encoder.6.weight': '/model_1/fc_sigmoid/kernel:0',\n",
    "    'decoder.0.weight': torch.Size([6272, 10]),\n",
    "    'decoder.0.bias': torch.Size([6272]),\n",
    "    'decoder.3.blocks.0.conv1.weight': torch.Size([64, 128, 3, 3]),\n",
    "    'decoder.3.blocks.0.conv1.bias': torch.Size([64]),\n",
    "    'decoder.3.blocks.0.conv2.weight': torch.Size([64, 64, 3, 3]),\n",
    "    'decoder.3.blocks.0.conv2.bias': torch.Size([64]),\n",
    "    'decoder.3.blocks.0.skip_conv.weight': torch.Size([64, 128, 1, 1]),\n",
    "    'decoder.3.blocks.0.skip_conv.bias': torch.Size([64]),\n",
    "    'decoder.4.blocks.0.conv1.weight': torch.Size([32, 64, 3, 3]),\n",
    "    'decoder.4.blocks.0.conv1.bias': torch.Size([32]),\n",
    "    'decoder.4.blocks.0.conv2.weight': torch.Size([32, 32, 3, 3]),\n",
    "    'decoder.4.blocks.0.conv2.bias': torch.Size([32]),\n",
    "    'decoder.4.blocks.0.skip_conv.weight': torch.Size([32, 64, 1, 1]),\n",
    "    'decoder.4.blocks.0.skip_conv.bias': torch.Size([32]),\n",
    "    'decoder.5.blocks.0.conv1.weight': torch.Size([16, 32, 3, 3]),\n",
    "    'decoder.5.blocks.0.conv1.bias': torch.Size([16]),\n",
    "    'decoder.5.blocks.0.conv2.weight': torch.Size([16, 16, 3, 3]),\n",
    "    'decoder.5.blocks.0.conv2.bias': torch.Size([16]),\n",
    "    'decoder.5.blocks.0.skip_conv.weight': torch.Size([16, 32, 1, 1]),\n",
    "    'decoder.5.blocks.0.skip_conv.bias': torch.Size([16]),\n",
    "    'decoder.7.weight': torch.Size([1, 16, 3, 3]),\n",
    "    'decoder.7.bias': torch.Size([1]),\n",
    "    'ucc_classifier.0.weight': torch.Size([384, 110]),\n",
    "    'ucc_classifier.0.bias': torch.Size([384]),\n",
    "    'ucc_classifier.2.weight': torch.Size([192, 384]),\n",
    "    'ucc_classifier.2.bias': torch.Size([192]),\n",
    "    'ucc_classifier.4.weight': torch.Size([4, 192]),\n",
    "    'ucc_classifier.4.bias': torch.Size([4])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "976af6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212a6b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fc_relu1/fc_relu1/bias:0\n",
      "/fc_relu1/fc_relu1/kernel:0\n",
      "/fc_relu2/fc_relu2/bias:0\n",
      "/fc_relu2/fc_relu2/kernel:0\n",
      "/fc_softmax/fc_softmax/bias:0\n",
      "/fc_softmax/fc_softmax/kernel:0\n",
      "/model_1/conv2d_1/bias:0\n",
      "/model_1/conv2d_1/kernel:0\n",
      "/model_1/conv2d_2/bias:0\n",
      "/model_1/conv2d_2/kernel:0\n",
      "/model_1/conv2d_3/bias:0\n",
      "/model_1/conv2d_3/kernel:0\n",
      "/model_1/conv2d_4/bias:0\n",
      "/model_1/conv2d_4/kernel:0\n",
      "/model_1/conv2d_5/bias:0\n",
      "/model_1/conv2d_5/kernel:0\n",
      "/model_1/conv2d_6/bias:0\n",
      "/model_1/conv2d_6/kernel:0\n",
      "/model_1/conv2d_7/bias:0\n",
      "/model_1/conv2d_7/kernel:0\n",
      "/model_1/conv2d_8/bias:0\n",
      "/model_1/conv2d_8/kernel:0\n",
      "/model_1/conv2d_9/bias:0\n",
      "/model_1/conv2d_9/kernel:0\n",
      "/model_1/fc_sigmoid/kernel:0\n",
      "/model_3/conv2d_1/bias:0\n",
      "/model_3/conv2d_1/kernel:0\n",
      "/model_3/conv2d_10/bias:0\n",
      "/model_3/conv2d_10/kernel:0\n",
      "/model_3/conv2d_11/bias:0\n",
      "/model_3/conv2d_11/kernel:0\n",
      "/model_3/conv2d_12/bias:0\n",
      "/model_3/conv2d_12/kernel:0\n",
      "/model_3/conv2d_13/bias:0\n",
      "/model_3/conv2d_13/kernel:0\n",
      "/model_3/conv2d_14/bias:0\n",
      "/model_3/conv2d_14/kernel:0\n",
      "/model_3/conv2d_15/bias:0\n",
      "/model_3/conv2d_15/kernel:0\n",
      "/model_3/conv2d_16/bias:0\n",
      "/model_3/conv2d_16/kernel:0\n",
      "/model_3/conv2d_17/bias:0\n",
      "/model_3/conv2d_17/kernel:0\n",
      "/model_3/conv2d_18/bias:0\n",
      "/model_3/conv2d_18/kernel:0\n",
      "/model_3/conv2d_2/bias:0\n",
      "/model_3/conv2d_2/kernel:0\n",
      "/model_3/conv2d_3/bias:0\n",
      "/model_3/conv2d_3/kernel:0\n",
      "/model_3/conv2d_4/bias:0\n",
      "/model_3/conv2d_4/kernel:0\n",
      "/model_3/conv2d_5/bias:0\n",
      "/model_3/conv2d_5/kernel:0\n",
      "/model_3/conv2d_6/bias:0\n",
      "/model_3/conv2d_6/kernel:0\n",
      "/model_3/conv2d_7/bias:0\n",
      "/model_3/conv2d_7/kernel:0\n",
      "/model_3/conv2d_8/bias:0\n",
      "/model_3/conv2d_8/kernel:0\n",
      "/model_3/conv2d_9/bias:0\n",
      "/model_3/conv2d_9/kernel:0\n",
      "/model_3/dense_1/bias:0\n",
      "/model_3/dense_1/kernel:0\n",
      "/model_3/fc_sigmoid/kernel:0\n"
     ]
    }
   ],
   "source": [
    "model_1_weights_ori = {}\n",
    "\n",
    "keys = []\n",
    "with h5py.File(\"model_weights__2019_09_05__18_43_15__0123456789__128000.h5\", 'r') as f: # open file\n",
    "    f.visit(keys.append) # append all keys to list\n",
    "    for key in keys:\n",
    "        if ':' in key: # contains data if ':' in key\n",
    "            # if \"model_1\" in key:\n",
    "            print(f[key].name)\n",
    "            model_1_weights_ori[f[key].name] = f[key][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9ab78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_shape = {k:v.shape for k, v in model_1_weights_ori.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_key_mapping = {\n",
    "    'ucc_classifier.0.weight': \"/fc_relu1/fc_relu1/kernel:0\",\n",
    "    'ucc_classifier.0.bias': \"/fc_relu1/fc_relu1/bias:0\",\n",
    "    'ucc_classifier.2.weight': \"/fc_relu2/fc_relu2/kernel:0\",\n",
    "    'ucc_classifier.2.bias':  \"/fc_relu2/fc_relu2/bias:0\",\n",
    "    'ucc_classifier.4.weight': \"/fc_softmax/fc_softmax/kernel:0\",\n",
    "    'ucc_classifier.4.bias': \"/fc_softmax/fc_softmax/bias:0\",\n",
    "    'encoder.0.weight':\"/model_1/conv2d_1/kernel:0\",\n",
    "    'encoder.0.bias':\"/model_1/conv2d_1/bias:0\",\n",
    "    'encoder.1.blocks.0.conv1.weight': \"/model_1/conv2d_2/kernel:0\",\n",
    "    'encoder.1.blocks.0.conv1.bias': \"/model_1/conv2d_2/bias:0\",\n",
    "    'encoder.1.blocks.0.conv2.weight': \"/model_1/conv2d_3/kernel:0\",\n",
    "    'encoder.1.blocks.0.conv2.bias': \"/model_1/conv2d_3/bias:0\",\n",
    "    \n",
    "    'encoder.1.blocks.0.skip_conv.weight': torch.Size([32, 16, 1, 1]),\n",
    "    'encoder.1.blocks.0.skip_conv.bias': torch.Size([32]),\n",
    "    \n",
    "    'encoder.2.blocks.0.conv1.weight': \"/model_1/conv2d_4/kernel:0\",\n",
    "    'encoder.2.blocks.0.conv1.bias': \"/model_1/conv2d_4/bias:0\",\n",
    "    'encoder.2.blocks.0.conv2.weight':\"/model_1/conv2d_5/kernel:0\",\n",
    "    'encoder.2.blocks.0.conv2.bias': \"/model_1/conv2d_6/bias:0\",\n",
    "    \n",
    "    'encoder.2.blocks.0.skip_conv.weight': torch.Size([64, 32, 1, 1]),\n",
    "    'encoder.2.blocks.0.skip_conv.bias': torch.Size([64]),\n",
    "    \n",
    "    'encoder.3.blocks.0.conv1.weight': \"/model_1/conv2d_7/kernel:0\",\n",
    "    'encoder.3.blocks.0.conv1.bias': \"/model_1/conv2d_7/bias:0\",\n",
    "    'encoder.3.blocks.0.conv2.weight': \"/model_1/conv2d_8/kernel:0\",\n",
    "    'encoder.3.blocks.0.conv2.bias': \"/model_1/conv2d_8/bias:0\",\n",
    "    \n",
    "    'encoder.3.blocks.0.skip_conv.weight': torch.Size([128, 64, 1, 1]),\n",
    "    'encoder.3.blocks.0.skip_conv.bias': torch.Size([128]),\n",
    "    \n",
    "    'encoder.6.weight': \"/model_1/fc_sigmoid/kernel:0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import UCCModel\n",
    "experiment_id = \"644081323448183645\"\n",
    "run_id = \"aa908ca173244746a47f5299a8b9cda8\"\n",
    "model = torch.load(f\"mlruns/{experiment_id}/{run_id}/artifacts/best_model/data/model.pth\", weights_only=False, map_location=\"mps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
