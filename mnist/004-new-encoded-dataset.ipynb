{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "739a7e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import UCCDRNModel\n",
    "\n",
    "experiment_id = \"189454739472380536\"\n",
    "run_id = \"98d0cca708cc4f5ba40314aa134af4cd\"\n",
    "\n",
    "model = torch.load(os.path.join(\"mlruns\", experiment_id, run_id, \"artifacts/best_model/data/model.pth\"), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c05255",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/mnist/splitted_mnist_dataset.npz\"\n",
    "splitted_dataset = np.load(data_dir)\n",
    "\n",
    "x_train = splitted_dataset[\"x_train\"]\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1) / 255\n",
    "x_train = (x_train - x_train.mean(dim=(2, 3), keepdim=True)) / x_train.std(\n",
    "    dim=(2, 3), keepdim=True\n",
    ")\n",
    "\n",
    "# elif self.mode ==\"val\":\n",
    "x_val = splitted_dataset[\"x_val\"]\n",
    "x_val = torch.tensor(x_val, dtype=torch.float32).unsqueeze(1) / 255\n",
    "x_val = (x_val - x_val.mean(dim=(2, 3), keepdim=True)) / x_val.std(\n",
    "    dim=(2, 3), keepdim=True\n",
    ")\n",
    "x_test = splitted_dataset[\"x_test\"]\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1) / 255\n",
    "x_test = (x_test - x_test.mean(dim=(2, 3), keepdim=True)) / x_test.std(\n",
    "    dim=(2, 3), keepdim=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f330cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "x_train_encoded = []\n",
    "x_val_encoded = []\n",
    "x_test_encoded = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    batches = int(len(x_train)/200)\n",
    "    for i in range(batches):\n",
    "        tensor = x_train[i*200: (i+1)*200]\n",
    "        tensor = tensor.to(torch.float32).to(device)\n",
    "        outputs = model.encoder(tensor)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        if len(x_train_encoded)==0:\n",
    "            x_train_encoded = outputs\n",
    "        else:\n",
    "            x_train_encoded = np.concat((x_train_encoded, outputs))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    batches = int(len(x_val)/200)\n",
    "    for i in range(batches):\n",
    "        tensor = x_val[i*200: (i+1)*200]\n",
    "        tensor = tensor.to(torch.float32).to(device)\n",
    "        outputs = model.encoder(tensor)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        if len(x_val_encoded)==0:\n",
    "            x_val_encoded = outputs\n",
    "        else:\n",
    "            x_val_encoded = np.concat((x_val_encoded, outputs))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    batches = int(len(x_test)/200)\n",
    "    for i in range(batches):\n",
    "        tensor = x_test[i*200: (i+1)*200]\n",
    "        tensor = tensor.to(torch.float32).to(device)\n",
    "        outputs = model.encoder(tensor)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        if len(x_test_encoded)==0:\n",
    "            x_test_encoded = outputs\n",
    "        else:\n",
    "            x_test_encoded = np.concat((x_test_encoded, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8fca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_dataset = dict(splitted_dataset)\n",
    "splitted_dataset[\"x_train\"] = x_train_encoded\n",
    "splitted_dataset[\"x_val\"] = x_val_encoded\n",
    "splitted_dataset[\"x_test\"] = x_test_encoded\n",
    "\n",
    "np.savez(\"../data/mnist/splitted_drn_mnist_encoded_dataset.npz\", **splitted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5ea760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DRNMnistEncodedDataset\n",
    "from hydra import compose, initialize\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "        cfg = compose(config_name=\"train_drn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61e637c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([50000, 1, 28, 28])\n",
      "50000 train samples\n",
      "10000 val samples\n",
      "tensor(0.3105)\n",
      "tensor(0.1325)\n",
      "10000 test samples\n",
      "dict_keys(['x_val', 'y_train', 'y_val', 'x_test', 'y_test', 'x_train'])\n",
      "x_train shape: torch.Size([50000, 10])\n",
      "50000 train samples\n",
      "10000 val samples\n"
     ]
    }
   ],
   "source": [
    "args = cfg.args\n",
    "train_dataset_len = args.train_num_steps * args.batch_size\n",
    "dataset = DRNMnistEncodedDataset(\n",
    "    mode=\"train\",\n",
    "    num_instances=args.num_instances,\n",
    "    num_samples_per_class=args.num_samples_per_class,\n",
    "    digit_arr=list(range(args.ucc_end-args.ucc_start+1)),\n",
    "    ucc_start=args.ucc_start,\n",
    "    ucc_end=args.ucc_end,\n",
    "    length=train_dataset_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5aa5911",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected integer T_mult >= 1, but got 1.2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCosineAnnealingWarmRestarts\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.00005\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/UCC-DRN-Pytorch/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:1751\u001b[0m, in \u001b[0;36mCosineAnnealingWarmRestarts.__init__\u001b[0;34m(self, optimizer, T_0, T_mult, eta_min, last_epoch, verbose)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected positive integer T_0, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mT_0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m T_mult \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(T_mult, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected integer T_mult >= 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mT_mult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eta_min, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m)):\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected float or int eta_min, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meta_min\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(eta_min)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1755\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected integer T_mult >= 1, but got 1.2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.nn.Linear(10,10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.01)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 100, 1.2, eta_min=0.00005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15a735f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n",
      "[0.01]\n",
      "[0.00975650616856839]\n",
      "[0.009049859547015364]\n",
      "[0.007949231630155056]\n",
      "[0.006562359547015364]\n",
      "[0.005025]\n",
      "[0.0034876404529846383]\n",
      "[0.002100768369844947]\n",
      "[0.001000140452984637]\n",
      "[0.0002934938314316112]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000, 10000, 10):\n",
    "    lr_scheduler.step(epoch)\n",
    "    print(lr_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f532290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_optimizer(experiment_id, run_id):\n",
    "    model = torch.load(f\"mlruns/{experiment_id}/{run_id}/artifacts/best_model/data/model.pth\", weights_only=False)\n",
    "    # old_optimizer = torch.load(f\"mlruns/{experiment_id}/{run_id}/artifacts/best_optimizer.pth/best_optimizer.pth\", weights_only=False)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer.load_state_dict(old_optimizer.state_dict())\n",
    "    with open(f\"mlruns/{experiment_id}/{run_id}/metrics/eval_ucc_acc\") as file:\n",
    "        lines = file.readlines()\n",
    "        step = int(lines[-1].split(\" \")[-1])\n",
    "    return model, optimizer, step\n",
    "experiment_id = \"189454739472380536\"\n",
    "run_id = \"98d0cca708cc4f5ba40314aa134af4cd\"\n",
    "model, optimizer, step = load_model_and_optimizer(experiment_id=experiment_id, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07abbb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drn= model.ucc_classifier\n",
    "\n",
    "\n",
    "input_layer = drn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c6803d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "ba = []\n",
    "bq = []\n",
    "lama = []\n",
    "lamq = []\n",
    "for layer in drn[:-1]:\n",
    "    weights.append(layer.W)\n",
    "    ba.append(layer.ba)\n",
    "    bq.append(layer.bq)\n",
    "    lama.append(layer.lama)\n",
    "    lamq.append(layer.lamq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2cba3db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 10])\n",
      "torch.Size([9, 9])\n",
      "torch.Size([9, 9])\n",
      "torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "statistics = {}\n",
    "\n",
    "statistics[\"w\"] = []\n",
    "for index, w in enumerate(weights):\n",
    "    print(w.shape)\n",
    "    stat_dict = {}\n",
    "    stat_dict[\"max\"] = w.max(dim=1)\n",
    "    stat_dict[\"min\"] = w.min(dim=1)\n",
    "    stat_dict[\"std\"] = w.std(dim=1)\n",
    "    stat_dict[\"mean\"] = w.mean(dim=1)\n",
    "    statistics[\"w\"].append(\n",
    "        stat_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "15c900bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': [{'max': torch.return_types.max(\n",
       "   values=tensor([4.1258, 5.4059, 2.6690, 4.6601, 2.5000, 2.7482, 3.9410, 3.4813, 5.1677],\n",
       "          device='mps:0', grad_fn=<MaxBackward0>),\n",
       "   indices=tensor([1, 6, 2, 7, 6, 6, 7, 6, 4], device='mps:0')),\n",
       "   'min': torch.return_types.min(\n",
       "   values=tensor([-6.5798, -6.8592,  0.1061,  0.0466, -3.2616, -1.2818, -1.8255,  0.2992,\n",
       "           -6.4203], device='mps:0', grad_fn=<MinBackward0>),\n",
       "   indices=tensor([0, 8, 4, 4, 4, 4, 1, 7, 1], device='mps:0')),\n",
       "   'std': tensor([3.9890, 3.7651, 0.8355, 1.1779, 1.7089, 1.3231, 1.9351, 1.1359, 4.3443],\n",
       "          device='mps:0', grad_fn=<StdBackward0>),\n",
       "   'mean': tensor([-1.9596, -0.5349,  1.0705,  1.6854,  1.0741,  1.2019,  1.5159,  1.5380,\n",
       "           -2.6157], device='mps:0', grad_fn=<MeanBackward1>)},\n",
       "  {'max': torch.return_types.max(\n",
       "   values=tensor([ 6.4013,  6.1084, -3.0711, -3.0662, 12.4177,  5.1276, 12.6260, -3.0465,\n",
       "           -2.9798], device='mps:0', grad_fn=<MaxBackward0>),\n",
       "   indices=tensor([7, 8, 0, 0, 8, 8, 8, 0, 0], device='mps:0')),\n",
       "   'min': torch.return_types.min(\n",
       "   values=tensor([-0.9235, -0.6433, -7.3660, -7.8648, -4.0930, -0.6594, -5.1103, -7.8235,\n",
       "           -7.7444], device='mps:0', grad_fn=<MinBackward0>),\n",
       "   indices=tensor([3, 2, 7, 7, 2, 3, 2, 5, 7], device='mps:0')),\n",
       "   'std': tensor([2.2311, 2.1388, 1.5189, 1.6201, 4.9136, 1.8457, 5.0831, 1.6531, 1.6854],\n",
       "          device='mps:0', grad_fn=<StdBackward0>),\n",
       "   'mean': tensor([ 1.5708,  1.4850, -5.1750, -5.3625,  1.0311,  1.6094,  0.8601, -5.4248,\n",
       "           -5.2792], device='mps:0', grad_fn=<MeanBackward1>)},\n",
       "  {'max': torch.return_types.max(\n",
       "   values=tensor([14.9139, 20.2508, 13.2938, 23.6696, 13.2212, 21.4626, 21.3013, 20.8055,\n",
       "           12.5884], device='mps:0', grad_fn=<MaxBackward0>),\n",
       "   indices=tensor([5, 3, 6, 3, 6, 2, 3, 3, 6], device='mps:0')),\n",
       "   'min': torch.return_types.min(\n",
       "   values=tensor([-16.1378,  -8.1616, -13.3004,  -8.4363, -12.9017,  -7.3004,  -9.2450,\n",
       "            -8.6934, -13.1116], device='mps:0', grad_fn=<MinBackward0>),\n",
       "   indices=tensor([2, 1, 3, 1, 3, 1, 1, 1, 3], device='mps:0')),\n",
       "   'std': tensor([13.3014, 13.3696, 10.8834, 15.1496, 10.6144, 13.9677, 13.9141, 13.6099,\n",
       "           10.6802], device='mps:0', grad_fn=<StdBackward0>),\n",
       "   'mean': tensor([-2.4213,  5.7875, -2.3713,  7.1146, -2.1902,  6.3938,  6.1250,  5.8014,\n",
       "           -2.2922], device='mps:0', grad_fn=<MeanBackward1>)},\n",
       "  {'max': torch.return_types.max(\n",
       "   values=tensor([27.2079], device='mps:0', grad_fn=<MaxBackward0>),\n",
       "   indices=tensor([0], device='mps:0')),\n",
       "   'min': torch.return_types.min(\n",
       "   values=tensor([-4.8557], device='mps:0', grad_fn=<MinBackward0>),\n",
       "   indices=tensor([7], device='mps:0')),\n",
       "   'std': tensor([12.9275], device='mps:0', grad_fn=<StdBackward0>),\n",
       "   'mean': tensor([5.7314], device='mps:0', grad_fn=<MeanBackward1>)}]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7a3da66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.879528816539086e-12)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.power(np.e, -27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a621d4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-12"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8854fe4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.0219],\n",
       "        [ 1.0226],\n",
       "        [-0.0360],\n",
       "        [ 0.4180],\n",
       "        [ 0.5661],\n",
       "        [-0.0155],\n",
       "        [ 0.4543],\n",
       "        [ 0.5150],\n",
       "        [ 0.8170]], device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer.lama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2173f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = drn[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7518491f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.7307],\n",
       "        [ 0.6836],\n",
       "        [-0.7373],\n",
       "        [-0.6748],\n",
       "        [-1.3735],\n",
       "        [ 0.7535],\n",
       "        [-1.4816],\n",
       "        [-1.0582],\n",
       "        [-0.3545]], device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_1.bq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781cd10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
