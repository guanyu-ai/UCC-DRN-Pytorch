{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ace83264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using optimization to find the optimal mean and variance for normal initialization\n",
    "from copy import deepcopy\n",
    "from hydra import compose, initialize\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from omegaconf.omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import UCCModel\n",
    "from dataset import Cifar10Dataset\n",
    "from utils import get_or_create_experiment, parse_experiment_runs_to_optuna_study\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "cfg_name = \"train_cifar10_ucc\"\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    cfg = compose(config_name=cfg_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f19f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def init_model_and_optimizer(args, model_cfg, device):\n",
    "    model = UCCModel(model_cfg).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=args.learning_rate)\n",
    "    return model, optimizer\n",
    "\n",
    "def load_model_and_optimizer(experiment_id, run_id):\n",
    "    model = torch.load(f\"mlruns/{experiment_id}/{run_id}/artifacts/best_model/data/model.pth\", weights=False)\n",
    "    optimizer = torch.load(f\"mlruns/{experiment_id}/{run_id}/artifacts/optimizer.pt\", weights=False)\n",
    "    return model, optimizer\n",
    "\n",
    "def init_dataloader(args):\n",
    "    train_dataset_len = args.train_num_steps * args.batch_size\n",
    "    train_dataset = Cifar10Dataset(\n",
    "        mode=\"train\",\n",
    "        num_instances=args.num_instances,\n",
    "        object_arr=list(range(10)),\n",
    "        ucc_start=args.ucc_start,\n",
    "        ucc_end=args.ucc_end,\n",
    "        length=train_dataset_len,\n",
    "    )\n",
    "    val_dataset_len = args.val_num_steps * args.batch_size\n",
    "    val_dataset = Cifar10Dataset(\n",
    "        mode=\"val\",\n",
    "        num_instances=args.num_instances,\n",
    "        object_arr=list(range(10)),\n",
    "        ucc_start=args.ucc_start,\n",
    "        ucc_end=args.ucc_end,\n",
    "        length=val_dataset_len,\n",
    "    )\n",
    "    # create dataloader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_ae_loss_list = []\n",
    "    val_ucc_loss_list = []\n",
    "    val_acc_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_samples, batch_labels in val_loader:\n",
    "            batch_samples = batch_samples.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            ucc_logits, reconstruction = model(batch_samples, return_reconstruction=True)\n",
    "\n",
    "            ucc_loss = F.cross_entropy(ucc_logits, batch_labels)\n",
    "            val_ucc_loss_list.append(ucc_loss.item())\n",
    "\n",
    "            ae_loss = F.mse_loss(batch_samples, reconstruction)\n",
    "            val_ae_loss_list.append(ae_loss.item())\n",
    "\n",
    "            # acculate accuracy\n",
    "            # _, batch_labels = torch.max(batch_labels, dim=1)\n",
    "            \n",
    "            _, ucc_predicts = torch.max(ucc_logits, dim=1)\n",
    "            acc = torch.sum(ucc_predicts == batch_labels).item() / len(batch_labels)\n",
    "            val_acc_list.append(acc)\n",
    "    return {\n",
    "                \"eval_ae_loss\": np.round(np.mean(val_ae_loss_list), 5),\n",
    "                \"eval_ucc_loss\": np.round(np.mean(val_ucc_loss_list), 5),\n",
    "                \"eval_ucc_acc\": np.round(np.mean(val_acc_list), 5)\n",
    "            }\n",
    "\n",
    "def train(args, model, optimizer, lr_scheduler, train_loader, val_loader, device, step=0):\n",
    "    print(\"training\")\n",
    "    # mlflow.pytorch.log_model(model, \"init_model\")\n",
    "    # output_dir = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir\n",
    "\n",
    "    model.train()\n",
    "    best_eval_acc = 0\n",
    "    if step == 0:\n",
    "        mlflow.pytorch.log_model(\n",
    "            model,\n",
    "            artifact_path = \"best_model\"\n",
    "        )\n",
    "    for batch_samples, batch_labels in tqdm(train_loader):\n",
    "        batch_samples = batch_samples.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        ucc_logits, reconstruction = model(batch_samples, return_reconstruction=True)\n",
    "        print(ucc_logits)\n",
    "        ucc_loss = F.cross_entropy(ucc_logits, batch_labels)\n",
    "        \n",
    "        print(batch_samples.shape)\n",
    "        print(reconstruction.shape)\n",
    "        print(batch_labels.shape)\n",
    "        ae_loss = F.mse_loss(batch_samples, reconstruction)\n",
    "        loss = (1-model.alpha)*ucc_loss + model.alpha*ae_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        if step%20 ==0:\n",
    "            with torch.no_grad():\n",
    "                metric_dict = {}\n",
    "                grad_log = {name: torch.mean(param.grad).cpu().item(\n",
    "                ) for name, param in model.named_parameters() if isinstance(param.grad, torch.Tensor)}\n",
    "                if step == 1000 and ae_loss.detach().item()>0.99:\n",
    "                    encoder_grad_log = [grad for name, grad in grad_log.items() if \"encoder\" in name and \"weight\" in name]\n",
    "                    if max(encoder_grad_log)<1e-9:\n",
    "                        break\n",
    "                mlflow.log_metrics(grad_log, step=step)\n",
    "                metric_dict[\"train_ae_loss\"] = np.round(ae_loss.detach().item(), 5)\n",
    "                _, pred = torch.max(ucc_logits, dim=1)\n",
    "                accuracy = torch.sum(pred.flatten() == batch_labels.flatten())/len(batch_labels)\n",
    "                metric_dict[\"train_ucc_loss\"] = np.round(ucc_loss.detach().item(), 5)\n",
    "                metric_dict[\"train_ucc_acc\"] = np.round(float(accuracy), 5)\n",
    "                metric_dict[\"loss\"] = np.round(float(loss), 5)\n",
    "                print(f\"Step {step}:\", metric_dict)\n",
    "            mlflow.log_metrics(metric_dict, step=step)\n",
    "\n",
    "        if step % args.save_interval == 0:\n",
    "            eval_metric_dict = evaluate(\n",
    "                model,\n",
    "                val_loader,\n",
    "                device)\n",
    "            print(f\"step: {step},\" + \",\".join([f\"{key}: {value}\"for key, value in eval_metric_dict.items()]))\n",
    "            mlflow.log_metrics(eval_metric_dict, step=step)\n",
    "            eval_acc = eval_metric_dict[\"eval_ucc_acc\"]\n",
    "            if eval_acc > best_eval_acc or eval_acc==1.0:\n",
    "                best_eval_acc = eval_acc\n",
    "                mlflow.log_metric(\"best_eval_acc\", best_eval_acc)\n",
    "                mlflow.pytorch.log_model(model, artifact_path=\"best_model\")\n",
    "                torch.save(optimizer, \"optimizer.pt\")\n",
    "                mlflow.log_artifact(\"optimizer.pt\")\n",
    "            if step == 200000:\n",
    "                break\n",
    "            model.train()\n",
    "\n",
    "    print(\"Training finished!!!\")\n",
    "    return best_eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bb185e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_channels': 3, 'input_shape': [28, 28, 1], 'kde_model': {'num_bins': 11, 'sigma': 0.1}, 'encoder': {'conv_input_channel': 3, 'conv_output_channel': 16, 'block1_output_channel': 321, 'block1_num_layer': 1, 'block2_output_channel': 64, 'block2_num_layer': 1, 'block3_output_channel': 128, 'block3_num_layer': 1, 'flatten_size': 8192, 'num_features': 10}, 'decoder': {'linear_size': 8192, 'reshape_size': [7, 7, 128], 'block1_output_channel': 64, 'block1_num_layer': 1, 'block2_output_channel': 32, 'block2_num_layer': 1, 'block3_output_channel': 16, 'block3_num_layer': 1, 'output_channel': 3}, 'ucc_classifier': {'classification_model': None, 'fc1_output_size': 384, 'fc2_output_size': 192, 'dropout_rate': 0.0, 'num_classes': 4}, 'loss': {'alpha': 0.5}}\n",
      "483580740574181263\n",
      "10000 val samples\n",
      "10000 val samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 01:37:30 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/09/22 01:37:35 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/09/22 01:37:36 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 01:37:40 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "d:\\UCC-DRN-Pytorch\\.venv\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "d:\\UCC-DRN-Pytorch\\.venv\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2835da68c874a029b93b3001c37d4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4830, 0.5170],\n",
      "        [0.4758, 0.5242],\n",
      "        [0.4904, 0.5096],\n",
      "        [0.4871, 0.5129],\n",
      "        [0.4889, 0.5111],\n",
      "        [0.4914, 0.5086],\n",
      "        [0.4976, 0.5024],\n",
      "        [0.4949, 0.5051],\n",
      "        [0.4965, 0.5035],\n",
      "        [0.4807, 0.5193],\n",
      "        [0.4981, 0.5019],\n",
      "        [0.4986, 0.5014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([12, 32, 3, 32, 32])\n",
      "torch.Size([12, 32, 3, 32, 32])\n",
      "torch.Size([12])\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m artifact_path = run.info.artifact_uri\n\u001b[32m     16\u001b[39m mlflow.pytorch.log_model(\n\u001b[32m     17\u001b[39m         model,\n\u001b[32m     18\u001b[39m         artifact_path = \u001b[33m\"\u001b[39m\u001b[33minit_model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m best_acc = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(args, model, optimizer, lr_scheduler, train_loader, val_loader, device, step)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(reconstruction.shape)\n\u001b[32m    105\u001b[39m \u001b[38;5;28mprint\u001b[39m(batch_labels.shape)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m ae_loss = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreconstruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m loss = (\u001b[32m1\u001b[39m-model.alpha)*ucc_loss + model.alpha*ae_loss\n\u001b[32m    109\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UCC-DRN-Pytorch\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3889\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction, weight)\u001b[39m\n\u001b[32m   3885\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3886\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid reduction mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expected one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33msum\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3887\u001b[39m         )\n\u001b[32m   3888\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3889\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3890\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3891\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"file:\\\\D:\\\\UCC-DRN-Pytorch\\\\cifar10\\\\mlruns\")\n",
    "\n",
    "run_name = \"cifar10-ucc\"\n",
    "experiment_id = get_or_create_experiment(experiment_name=run_name)\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "with mlflow.start_run(nested=True) as run:\n",
    "    print(cfg.model)\n",
    "    print(experiment_id)\n",
    "    cfg.args.learning_rate = 0.001\n",
    "    mlflow.log_dict(dict(OmegaConf.to_object(cfg)), \"config.yaml\")\n",
    "    args = cfg.args\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "    model, optimizer = init_model_and_optimizer(args, cfg, device)\n",
    "    train_loader, val_loader = init_dataloader(args)\n",
    "    artifact_path = run.info.artifact_uri\n",
    "    mlflow.pytorch.log_model(\n",
    "            model,\n",
    "            artifact_path = \"init_model\")\n",
    "    best_acc = train(args, model, optimizer, None,\n",
    "                    train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c21c91c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
