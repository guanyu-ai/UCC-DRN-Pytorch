{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7caec648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using optimization to find the optimal mean and variance for normal initialization\n",
    "from copy import deepcopy\n",
    "from hydra import compose, initialize\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from omegaconf.omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import UCCDRNModel\n",
    "from dataset import Cifar10Dataset\n",
    "from utils import get_or_create_experiment, parse_experiment_runs_to_optuna_study\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "cfg_name = \"train_cifar10_ucc_drn\"\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    cfg = compose(config_name=cfg_name)\n",
    "x = np.arange(-0.25,0.35,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def init_model_and_optimizer(args, model_cfg, device):\n",
    "    model = UCCDRNModel(model_cfg).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=args.learning_rate)\n",
    "    return model, optimizer\n",
    "\n",
    "def load_model_and_optimizer(experiment_id, run_id):\n",
    "    model = torch.load(f\"mlruns/{experiment_id}/{run_id}/artifacts/best_model/data/model.pth\", weights=False)\n",
    "    optimizer = torch.load(f\"mlruns/{experiment_id}/{run_id}/artifacts/optimizer.pt\", weights=False)\n",
    "    return model, optimizer\n",
    "\n",
    "def init_dataloader(args):\n",
    "    train_dataset_len = args.train_num_steps * args.batch_size\n",
    "    train_dataset = Cifar10Dataset(\n",
    "        mode=\"train\",\n",
    "        num_instances=args.num_instances,\n",
    "        num_samples_per_class=args.num_samples_per_class,\n",
    "        object_arr=list(range(10)),\n",
    "        ucc_start=args.ucc_start,\n",
    "        ucc_end=args.ucc_end,\n",
    "        length=train_dataset_len,\n",
    "    )\n",
    "    val_dataset_len = args.val_num_steps * args.batch_size\n",
    "    val_dataset = Cifar10Dataset(\n",
    "        mode=\"val\",\n",
    "        num_instances=args.num_instances,\n",
    "        num_samples_per_class=args.num_samples_per_class,\n",
    "        object_arr=list(range(10)),\n",
    "        ucc_start=args.ucc_start,\n",
    "        ucc_end=args.ucc_end,\n",
    "        length=val_dataset_len,\n",
    "    )\n",
    "    # create dataloader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_ae_loss_list = []\n",
    "    val_ucc_loss_list = []\n",
    "    val_acc_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_samples, batch_labels in val_loader:\n",
    "            batch_samples = batch_samples.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            ucc_logits, reconstruction = model(batch_samples, return_reconstruction=True)\n",
    "\n",
    "            ucc_loss = F.cross_entropy(ucc_logits, batch_labels)\n",
    "            val_ucc_loss_list.append(ucc_loss.item())\n",
    "\n",
    "            ae_loss = F.mse_loss(batch_samples, reconstruction)\n",
    "            val_ae_loss_list.append(ae_loss.item())\n",
    "\n",
    "            # acculate accuracy\n",
    "            # _, batch_labels = torch.max(batch_labels, dim=1)\n",
    "            \n",
    "            _, ucc_predicts = torch.max(ucc_logits, dim=1)\n",
    "            acc = torch.sum(ucc_predicts == batch_labels).item() / len(batch_labels)\n",
    "            val_acc_list.append(acc)\n",
    "    return {\n",
    "                \"eval_ae_loss\": np.round(np.mean(val_ae_loss_list), 5),\n",
    "                \"eval_ucc_loss\": np.round(np.mean(val_ucc_loss_list), 5),\n",
    "                \"eval_ucc_acc\": np.round(np.mean(val_acc_list), 5)\n",
    "            }\n",
    "\n",
    "def train(args, model, optimizer, lr_scheduler, train_loader, val_loader, device, step=0):\n",
    "    print(\"training\")\n",
    "    # mlflow.pytorch.log_model(model, \"init_model\")\n",
    "    # output_dir = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir\n",
    "\n",
    "    model.train()\n",
    "    best_eval_acc = 0\n",
    "    if step == 0:\n",
    "        mlflow.pytorch.log_model(\n",
    "            model,\n",
    "            artifact_path = \"best_model\"\n",
    "        )\n",
    "    for batch_samples, batch_labels in tqdm(train_loader):\n",
    "        batch_samples = batch_samples.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        ucc_logits, reconstruction = model(batch_samples, return_reconstruction=True)\n",
    "        ucc_loss = F.cross_entropy(ucc_logits, batch_labels)\n",
    "        ae_loss = F.mse_loss(batch_samples, reconstruction)\n",
    "        loss = (1-model.alpha)*ucc_loss + model.alpha*ae_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        if step%20 ==0:\n",
    "            with torch.no_grad():\n",
    "                metric_dict = {}\n",
    "                grad_log = {name: torch.mean(param.grad).cpu().item(\n",
    "                ) for name, param in model.named_parameters() if isinstance(param.grad, torch.Tensor)}\n",
    "                if step == 1000 and ae_loss.detach().item()>0.99:\n",
    "                    encoder_grad_log = [grad for name, grad in grad_log.items() if \"encoder\" in name and \"weight\" in name]\n",
    "                    if max(encoder_grad_log)<1e-9:\n",
    "                        break\n",
    "                mlflow.log_metrics(grad_log, step=step)\n",
    "                metric_dict[\"train_ae_loss\"] = np.round(ae_loss.detach().item(), 5)\n",
    "                _, pred = torch.max(ucc_logits, dim=1)\n",
    "                accuracy = torch.sum(pred.flatten() == batch_labels.flatten())/len(batch_labels)\n",
    "                metric_dict[\"train_ucc_loss\"] = np.round(ucc_loss.detach().item(), 5)\n",
    "                metric_dict[\"train_ucc_acc\"] = np.round(float(accuracy), 5)\n",
    "                metric_dict[\"loss\"] = np.round(float(loss), 5)\n",
    "                print(f\"Step {step}:\", metric_dict)\n",
    "            mlflow.log_metrics(metric_dict, step=step)\n",
    "\n",
    "        if step % args.save_interval == 0:\n",
    "            eval_metric_dict = evaluate(\n",
    "                model,\n",
    "                val_loader,\n",
    "                device)\n",
    "            print(f\"step: {step},\" + \",\".join([f\"{key}: {value}\"for key, value in eval_metric_dict.items()]))\n",
    "            mlflow.log_metrics(eval_metric_dict, step=step)\n",
    "            eval_acc = eval_metric_dict[\"eval_ucc_acc\"]\n",
    "            if eval_acc > best_eval_acc or eval_acc==1.0:\n",
    "                best_eval_acc = eval_acc\n",
    "                mlflow.log_metric(\"best_eval_acc\", best_eval_acc)\n",
    "                mlflow.pytorch.log_model(model, artifact_path=\"best_model\")\n",
    "                torch.save(optimizer, \"optimizer.pt\")\n",
    "                mlflow.log_artifact(\"optimizer.pt\")\n",
    "            if step == 200000:\n",
    "                break\n",
    "            model.train()\n",
    "\n",
    "    print(\"Training finished!!!\")\n",
    "    return best_eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b0a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.3, 'init_lower_bound': -0.0, 'output_bins': 4}\n",
      "259546648097171860\n",
      "(100, 11)\n",
      "(100, 100)\n",
      "(4, 100)\n",
      "10000 val samples\n",
      "object0:0, num_train:4000, num_val:1000\n",
      "object1:1, num_train:4000, num_val:1000\n",
      "object2:2, num_train:4000, num_val:1000\n",
      "object3:3, num_train:4000, num_val:1000\n",
      "object4:4, num_train:4000, num_val:1000\n",
      "object5:5, num_train:4000, num_val:1000\n",
      "object6:6, num_train:4000, num_val:1000\n",
      "object7:7, num_train:4000, num_val:1000\n",
      "object8:8, num_train:4000, num_val:1000\n",
      "object9:9, num_train:4000, num_val:1000\n",
      "10000 val samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 15:16:59 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object0:0, num_train:4000, num_val:1000\n",
      "object1:1, num_train:4000, num_val:1000\n",
      "object2:2, num_train:4000, num_val:1000\n",
      "object3:3, num_train:4000, num_val:1000\n",
      "object4:4, num_train:4000, num_val:1000\n",
      "object5:5, num_train:4000, num_val:1000\n",
      "object6:6, num_train:4000, num_val:1000\n",
      "object7:7, num_train:4000, num_val:1000\n",
      "object8:8, num_train:4000, num_val:1000\n",
      "object9:9, num_train:4000, num_val:1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 15:17:04 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/09/15 15:17:04 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/15 15:17:08 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "d:\\UCC-DRN-Pytorch\\.venv\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "d:\\UCC-DRN-Pytorch\\.venv\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     35\u001b[39m artifact_path = run.info.artifact_uri\n\u001b[32m     36\u001b[39m mlflow.pytorch.log_model(\n\u001b[32m     37\u001b[39m         model,\n\u001b[32m     38\u001b[39m         artifact_path = \u001b[33m\"\u001b[39m\u001b[33minit_model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m best_acc = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(args, model, optimizer, lr_scheduler, train_loader, val_loader, device, step)\u001b[39m\n\u001b[32m    103\u001b[39m ae_loss = F.mse_loss(batch_samples, reconstruction)\n\u001b[32m    104\u001b[39m loss = (\u001b[32m1\u001b[39m-model.alpha)*ucc_loss + model.alpha*ae_loss\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m optimizer.step()\n\u001b[32m    110\u001b[39m step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UCC-DRN-Pytorch\\.venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UCC-DRN-Pytorch\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UCC-DRN-Pytorch\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "mlflow.set_tracking_uri(\"file:\\\\D:\\\\UCC-DRN-Pytorch\\\\cifar10\\\\mlruns\")\n",
    "\n",
    "run_name = \"cifar10-ucc-drn-search-init\"\n",
    "experiment_id = get_or_create_experiment(experiment_name=run_name)\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "last_lower_bound = 0\n",
    "last_upper_bound = 0.3\n",
    "for lower_bound in x:\n",
    "    lower_bound = np.round(lower_bound, 5)\n",
    "    for upper_bound in x:\n",
    "        upper_bound = np.round(upper_bound, 5)\n",
    "        if lower_bound<last_lower_bound:\n",
    "            continue\n",
    "        if lower_bound==last_lower_bound and upper_bound<last_upper_bound:\n",
    "            continue\n",
    "        if lower_bound >= upper_bound:\n",
    "            continue\n",
    "        # print(lower_bound)\n",
    "        # print(upper_bound)\n",
    "        with mlflow.start_run(nested=True) as run:\n",
    "            cfg.model.drn.init_lower_bound = float(lower_bound)\n",
    "            cfg.model.drn.init_upper_bound = float(upper_bound)\n",
    "            mlflow.log_params({\n",
    "                \"init_W_lower_bound\": float(lower_bound),\n",
    "                \"init_W_upper_bound\": float(upper_bound)\n",
    "            })\n",
    "            print(cfg.model.drn)\n",
    "            print(experiment_id)\n",
    "            cfg.args.learning_rate = 0.001\n",
    "            mlflow.log_dict(dict(OmegaConf.to_object(cfg)), \"config.yaml\")\n",
    "            args = cfg.args\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "            model, optimizer = init_model_and_optimizer(args, cfg, device)\n",
    "            train_loader, val_loader = init_dataloader(args)\n",
    "            artifact_path = run.info.artifact_uri\n",
    "            mlflow.pytorch.log_model(\n",
    "                    model,\n",
    "                    artifact_path = \"init_model\")\n",
    "            best_acc = train(args, model, optimizer, None,\n",
    "                            train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba98b710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {'dataset': 'cifar10', 'model_dir': 'saved_models/', 'model_name': 'cifar10_ucc', 'num_instances': 32, 'ucc_start': 1, 'ucc_end': 4, 'batch_size': 15, 'num_samples_per_class': 5, 'num_workers': 4, 'learning_rate': 0.001, 'num_bins': 11, 'num_features': 10, 'train_num_steps': 100000, 'val_num_steps': 200, 'save_interval': 1000, 'seed': 22}, 'model': {'num_channels': 3, 'input_shape': [28, 28, 1], 'kde_model': {'num_bins': 11, 'sigma': 0.1}, 'encoder': {'conv_input_channel': 3, 'conv_output_channel': 16, 'block1_output_channel': 321, 'block1_num_layer': 1, 'block2_output_channel': 64, 'block2_num_layer': 1, 'block3_output_channel': 128, 'block3_num_layer': 1, 'flatten_size': 8192, 'num_features': 10}, 'decoder': {'linear_size': 8192, 'reshape_size': [7, 7, 128], 'block1_output_channel': 64, 'block1_num_layer': 1, 'block2_output_channel': 32, 'block2_num_layer': 1, 'block3_output_channel': 16, 'block3_num_layer': 1, 'output_channel': 3}, 'drn': {'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.3, 'init_lower_bound': -0.0, 'output_bins': 4}, 'ucc_classifier': 'None', 'loss': {'alpha': 0.5}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955adbff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
