{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fIQPGyJnOntm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\UCC-DRN-Pytorch\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# using optimization to find the optimal mean and variance for normal initialization\n",
        "from copy import deepcopy\n",
        "from hydra import compose, initialize\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import mlflow\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple\n",
        "from omegaconf.omegaconf import OmegaConf\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from model import UCCDRNModel\n",
        "from dataset import CamelyonDatasetSeparatedBin, CamelyonDataset\n",
        "from utils import get_or_create_experiment, parse_experiment_runs_to_optuna_study\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "cfg_name = \"train_camelyon_ucc_drn\"\n",
        "with initialize(version_base=None, config_path=\"../configs\"):\n",
        "    cfg = compose(config_name=cfg_name)\n",
        "x = np.arange(-0.25,0.35,0.05)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-2.50000000e-01, -2.00000000e-01, -1.50000000e-01, -1.00000000e-01,\n",
              "       -5.00000000e-02, -5.55111512e-17,  5.00000000e-02,  1.00000000e-01,\n",
              "        1.50000000e-01,  2.00000000e-01,  2.50000000e-01,  3.00000000e-01])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1jkA63IFRubM"
      },
      "outputs": [],
      "source": [
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def init_model_and_optimizer(args, model_cfg, device):\n",
        "    model = UCCDRNModel(model_cfg).to(device)\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=args.learning_rate)\n",
        "    return model, optimizer\n",
        "\n",
        "def load_model_and_optimizer(experiment_id, run_id):\n",
        "    model = torch.load(f\"mlruns/{experiment_id}/{run_id}/artifacts/best_model/data/model.pth\", weights=False)\n",
        "    optimizer = torch.load(f\"mlruns/{experiment_id}/{run_id}/artifacts/optimizer.pt\", weights=False)\n",
        "    return model, optimizer\n",
        "\n",
        "def init_dataloader(args):\n",
        "    train_dataset_len = args.train_num_steps * args.batch_size\n",
        "    train_dataset = CamelyonDataset(\n",
        "        mode=\"train\",\n",
        "        patch_size=args.patch_size,\n",
        "        num_instances=args.num_instances,\n",
        "        dataset_len = 200000*args.batch_size\n",
        "    )\n",
        "    val_dataset_len = args.val_num_steps * args.batch_size\n",
        "    val_dataset = CamelyonDataset(\n",
        "        mode=\"val\",\n",
        "        patch_size=args.patch_size,\n",
        "        num_instances=args.num_instances,\n",
        "    )\n",
        "    # create dataloader\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers,\n",
        "        shuffle=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers,\n",
        "        shuffle=False,\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def evaluate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    val_ae_loss_list = []\n",
        "    val_ucc_loss_list = []\n",
        "    val_acc_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch_samples, batch_labels in val_loader:\n",
        "            batch_samples = batch_samples.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            ucc_logits, reconstruction = model(batch_samples, return_reconstruction=True)\n",
        "\n",
        "            ucc_loss = F.cross_entropy(ucc_logits, batch_labels)\n",
        "            val_ucc_loss_list.append(ucc_loss.item())\n",
        "\n",
        "            ae_loss = F.mse_loss(batch_samples, reconstruction)\n",
        "            val_ae_loss_list.append(ae_loss.item())\n",
        "\n",
        "            # acculate accuracy\n",
        "            # _, batch_labels = torch.max(batch_labels, dim=1)\n",
        "            \n",
        "            _, ucc_predicts = torch.max(ucc_logits, dim=1)\n",
        "            acc = torch.sum(ucc_predicts == batch_labels).item() / len(batch_labels)\n",
        "            val_acc_list.append(acc)\n",
        "    return {\n",
        "                \"eval_ae_loss\": np.round(np.mean(val_ae_loss_list), 5),\n",
        "                \"eval_ucc_loss\": np.round(np.mean(val_ucc_loss_list), 5),\n",
        "                \"eval_ucc_acc\": np.round(np.mean(val_acc_list), 5)\n",
        "            }\n",
        "\n",
        "def train(args, model, optimizer, lr_scheduler, train_loader, val_loader, device, step=0):\n",
        "    print(\"training\")\n",
        "    # mlflow.pytorch.log_model(model, \"init_model\")\n",
        "    # output_dir = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir\n",
        "\n",
        "    model.train()\n",
        "    best_eval_acc = 0\n",
        "    if step == 0:\n",
        "        mlflow.pytorch.log_model(\n",
        "            model,\n",
        "            artifact_path = \"best_model\"\n",
        "        )\n",
        "    for batch_samples, batch_labels in train_loader:\n",
        "        batch_samples = batch_samples.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        ucc_logits, reconstruction = model(batch_samples, return_reconstruction=True)\n",
        "        ucc_loss = F.cross_entropy(ucc_logits, batch_labels)\n",
        "        ae_loss = F.mse_loss(batch_samples, reconstruction)\n",
        "        loss = (1-model.alpha)*ucc_loss + model.alpha*ae_loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        step += 1\n",
        "\n",
        "        if step%20 ==0:\n",
        "            with torch.no_grad():\n",
        "                metric_dict = {}\n",
        "                grad_log = {name: torch.mean(param.grad).cpu().item(\n",
        "                ) for name, param in model.named_parameters() if isinstance(param.grad, torch.Tensor)}\n",
        "                if step == 1000 and ae_loss.detach().item()>0.99:\n",
        "                    encoder_grad_log = [grad for name, grad in grad_log.items() if \"encoder\" in name and \"weight\" in name]\n",
        "                    if max(encoder_grad_log)<1e-9:\n",
        "                        break\n",
        "                mlflow.log_metrics(grad_log, step=step)\n",
        "                metric_dict[\"train_ae_loss\"] = np.round(ae_loss.detach().item(), 5)\n",
        "                _, pred = torch.max(ucc_logits, dim=1)\n",
        "                accuracy = torch.sum(pred.flatten() == batch_labels.flatten())/len(batch_labels)\n",
        "                metric_dict[\"train_ucc_loss\"] = np.round(ucc_loss.detach().item(), 5)\n",
        "                metric_dict[\"train_ucc_acc\"] = np.round(float(accuracy), 5)\n",
        "                metric_dict[\"loss\"] = np.round(float(loss), 5)\n",
        "                print(f\"Step {step}:\", metric_dict)\n",
        "            mlflow.log_metrics(metric_dict, step=step)\n",
        "\n",
        "        if step % args.save_interval == 0:\n",
        "            eval_metric_dict = evaluate(\n",
        "                model,\n",
        "                val_loader,\n",
        "                device)\n",
        "            print(f\"step: {step},\" + \",\".join([f\"{key}: {value}\"for key, value in eval_metric_dict.items()]))\n",
        "            mlflow.log_metrics(eval_metric_dict, step=step)\n",
        "            eval_acc = eval_metric_dict[\"eval_ucc_acc\"]\n",
        "            if eval_acc > best_eval_acc or eval_acc==1.0:\n",
        "                best_eval_acc = eval_acc\n",
        "                mlflow.log_metric(\"best_eval_acc\", best_eval_acc)\n",
        "                mlflow.pytorch.log_model(model, artifact_path=\"best_model\")\n",
        "                torch.save(optimizer, \"optimizer.pt\")\n",
        "                mlflow.log_artifact(\"optimizer.pt\")\n",
        "            if step == 200000:\n",
        "                break\n",
        "            model.train()\n",
        "\n",
        "    print(\"Training finished!!!\")\n",
        "    return best_eval_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2X5JTgp3qCLF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.3, 'init_lower_bound': -0.0, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 02:42:19 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 02:42:24 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 02:42:24 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 02:42:28 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00051, 'train_ucc_loss': 0.69157, 'train_ucc_acc': 0.53125, 'loss': 0.84604}\n",
            "Step 40: {'train_ae_loss': 1.00022, 'train_ucc_loss': 0.67581, 'train_ucc_acc': 0.625, 'loss': 0.83802}\n",
            "Step 60: {'train_ae_loss': 1.00017, 'train_ucc_loss': 0.7009, 'train_ucc_acc': 0.46875, 'loss': 0.85054}\n",
            "Step 80: {'train_ae_loss': 1.00009, 'train_ucc_loss': 0.70868, 'train_ucc_acc': 0.40625, 'loss': 0.85438}\n",
            "Step 100: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.68349, 'train_ucc_acc': 0.59375, 'loss': 0.84176}\n",
            "Step 120: {'train_ae_loss': 1.00008, 'train_ucc_loss': 0.70915, 'train_ucc_acc': 0.375, 'loss': 0.85461}\n",
            "Step 140: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69125, 'train_ucc_acc': 0.53125, 'loss': 0.84563}\n",
            "Step 160: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69128, 'train_ucc_acc': 0.53125, 'loss': 0.84567}\n",
            "Step 180: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.70636, 'train_ucc_acc': 0.375, 'loss': 0.8532}\n",
            "Step 200: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.70251, 'train_ucc_acc': 0.40625, 'loss': 0.85126}\n",
            "Step 220: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.68895, 'train_ucc_acc': 0.5625, 'loss': 0.84449}\n",
            "Step 240: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69612, 'train_ucc_acc': 0.46875, 'loss': 0.84807}\n",
            "Step 260: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.70435, 'train_ucc_acc': 0.34375, 'loss': 0.8522}\n",
            "Step 280: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69556, 'train_ucc_acc': 0.46875, 'loss': 0.84778}\n",
            "Step 300: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.6917, 'train_ucc_acc': 0.53125, 'loss': 0.84585}\n",
            "Step 320: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.7034, 'train_ucc_acc': 0.34375, 'loss': 0.85169}\n",
            "Step 340: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.70275, 'train_ucc_acc': 0.34375, 'loss': 0.85137}\n",
            "Step 360: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69715, 'train_ucc_acc': 0.4375, 'loss': 0.84857}\n",
            "Step 380: {'train_ae_loss': 0.99994, 'train_ucc_loss': 0.69562, 'train_ucc_acc': 0.46875, 'loss': 0.84778}\n",
            "Step 400: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69961, 'train_ucc_acc': 0.40625, 'loss': 0.84979}\n",
            "Step 420: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.68397, 'train_ucc_acc': 0.65625, 'loss': 0.84201}\n",
            "Step 440: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.6878, 'train_ucc_acc': 0.59375, 'loss': 0.84392}\n",
            "Step 460: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.70133, 'train_ucc_acc': 0.375, 'loss': 0.85068}\n",
            "Step 480: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.70029, 'train_ucc_acc': 0.375, 'loss': 0.85016}\n",
            "Step 500: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69336, 'train_ucc_acc': 0.5, 'loss': 0.84668}\n",
            "Step 520: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69217, 'train_ucc_acc': 0.53125, 'loss': 0.84608}\n",
            "Step 540: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69801, 'train_ucc_acc': 0.375, 'loss': 0.84903}\n",
            "Step 560: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69556, 'train_ucc_acc': 0.4375, 'loss': 0.84778}\n",
            "Step 580: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69441, 'train_ucc_acc': 0.46875, 'loss': 0.8472}\n",
            "Step 600: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69455, 'train_ucc_acc': 0.46875, 'loss': 0.84729}\n",
            "Step 620: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69788, 'train_ucc_acc': 0.34375, 'loss': 0.84895}\n",
            "Step 640: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69469, 'train_ucc_acc': 0.4375, 'loss': 0.84736}\n",
            "Step 660: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.6932, 'train_ucc_acc': 0.5, 'loss': 0.84661}\n",
            "Step 680: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69324, 'train_ucc_acc': 0.5, 'loss': 0.84665}\n",
            "Step 700: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69488, 'train_ucc_acc': 0.4375, 'loss': 0.84745}\n",
            "Step 720: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.69324, 'train_ucc_acc': 0.5, 'loss': 0.8466}\n",
            "Step 740: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69231, 'train_ucc_acc': 0.53125, 'loss': 0.84617}\n",
            "Step 760: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69763, 'train_ucc_acc': 0.34375, 'loss': 0.84881}\n",
            "Step 780: {'train_ae_loss': 0.99995, 'train_ucc_loss': 0.69553, 'train_ucc_acc': 0.40625, 'loss': 0.84774}\n",
            "Step 800: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69462, 'train_ucc_acc': 0.4375, 'loss': 0.84732}\n",
            "Step 820: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69055, 'train_ucc_acc': 0.59375, 'loss': 0.84529}\n",
            "Step 840: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69615, 'train_ucc_acc': 0.40625, 'loss': 0.84807}\n",
            "Step 860: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69323, 'train_ucc_acc': 0.5, 'loss': 0.84661}\n",
            "Step 880: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69498, 'train_ucc_acc': 0.4375, 'loss': 0.84749}\n",
            "Step 900: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69527, 'train_ucc_acc': 0.40625, 'loss': 0.84763}\n",
            "Step 920: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69071, 'train_ucc_acc': 0.625, 'loss': 0.84536}\n",
            "Step 940: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69396, 'train_ucc_acc': 0.46875, 'loss': 0.84697}\n",
            "Step 960: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69759, 'train_ucc_acc': 0.34375, 'loss': 0.8488}\n",
            "Step 980: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.68929, 'train_ucc_acc': 0.65625, 'loss': 0.84465}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.1, 'init_lower_bound': 0.05, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 02:51:00 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 02:51:05 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 02:51:05 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 02:51:10 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00054, 'train_ucc_loss': 0.6969, 'train_ucc_acc': 0.4375, 'loss': 0.84872}\n",
            "Step 40: {'train_ae_loss': 1.00023, 'train_ucc_loss': 0.69213, 'train_ucc_acc': 0.53125, 'loss': 0.84618}\n",
            "Step 60: {'train_ae_loss': 1.00014, 'train_ucc_loss': 0.69431, 'train_ucc_acc': 0.46875, 'loss': 0.84723}\n",
            "Step 80: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69602, 'train_ucc_acc': 0.375, 'loss': 0.84804}\n",
            "Step 100: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69478, 'train_ucc_acc': 0.40625, 'loss': 0.84742}\n",
            "Step 120: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69215, 'train_ucc_acc': 0.5625, 'loss': 0.84611}\n",
            "Step 140: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69411, 'train_ucc_acc': 0.4375, 'loss': 0.84706}\n",
            "Step 160: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69256, 'train_ucc_acc': 0.625, 'loss': 0.84629}\n",
            "Step 180: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69337, 'train_ucc_acc': 0.25, 'loss': 0.84671}\n",
            "Step 200: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.6927, 'train_ucc_acc': 0.5625, 'loss': 0.84636}\n",
            "Step 220: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69206, 'train_ucc_acc': 0.59375, 'loss': 0.84605}\n",
            "Step 240: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6938, 'train_ucc_acc': 0.4375, 'loss': 0.8469}\n",
            "Step 260: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69303, 'train_ucc_acc': 0.53125, 'loss': 0.84653}\n",
            "Step 280: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84659}\n",
            "Step 300: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69295, 'train_ucc_acc': 0.53125, 'loss': 0.84649}\n",
            "Step 320: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.6937, 'train_ucc_acc': 0.40625, 'loss': 0.84685}\n",
            "Step 340: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69311, 'train_ucc_acc': 0.53125, 'loss': 0.84657}\n",
            "Step 360: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69314, 'train_ucc_acc': 0.53125, 'loss': 0.84659}\n",
            "Step 380: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69305, 'train_ucc_acc': 0.65625, 'loss': 0.84654}\n",
            "Step 400: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69367, 'train_ucc_acc': 0.40625, 'loss': 0.84685}\n",
            "Step 420: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69278, 'train_ucc_acc': 0.5625, 'loss': 0.84641}\n",
            "Step 440: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69308, 'train_ucc_acc': 0.53125, 'loss': 0.84656}\n",
            "Step 460: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69323, 'train_ucc_acc': 0.46875, 'loss': 0.84663}\n",
            "Step 480: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69326, 'train_ucc_acc': 0.46875, 'loss': 0.84664}\n",
            "Step 500: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69333, 'train_ucc_acc': 0.4375, 'loss': 0.84668}\n",
            "Step 520: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69243, 'train_ucc_acc': 0.59375, 'loss': 0.84623}\n",
            "Step 540: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69188, 'train_ucc_acc': 0.59375, 'loss': 0.84594}\n",
            "Step 560: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69204, 'train_ucc_acc': 0.5625, 'loss': 0.84603}\n",
            "Step 580: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69155, 'train_ucc_acc': 0.59375, 'loss': 0.84577}\n",
            "Step 600: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69279, 'train_ucc_acc': 0.53125, 'loss': 0.8464}\n",
            "Step 620: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69294, 'train_ucc_acc': 0.59375, 'loss': 0.84647}\n",
            "Step 640: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69306, 'train_ucc_acc': 0.5625, 'loss': 0.84653}\n",
            "Step 660: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69283, 'train_ucc_acc': 0.53125, 'loss': 0.84643}\n",
            "Step 680: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.694, 'train_ucc_acc': 0.4375, 'loss': 0.847}\n",
            "Step 700: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69273, 'train_ucc_acc': 0.53125, 'loss': 0.84637}\n",
            "Step 720: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69117, 'train_ucc_acc': 0.625, 'loss': 0.8456}\n",
            "Step 740: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69631, 'train_ucc_acc': 0.34375, 'loss': 0.84816}\n",
            "Step 760: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69319, 'train_ucc_acc': 0.5, 'loss': 0.8466}\n",
            "Step 780: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69127, 'train_ucc_acc': 0.5625, 'loss': 0.84563}\n",
            "Step 800: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69536, 'train_ucc_acc': 0.4375, 'loss': 0.84769}\n",
            "Step 820: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69135, 'train_ucc_acc': 0.5625, 'loss': 0.84567}\n",
            "Step 840: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69077, 'train_ucc_acc': 0.59375, 'loss': 0.84539}\n",
            "Step 860: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69246, 'train_ucc_acc': 0.53125, 'loss': 0.84624}\n",
            "Step 880: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69646, 'train_ucc_acc': 0.40625, 'loss': 0.84823}\n",
            "Step 900: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69843, 'train_ucc_acc': 0.34375, 'loss': 0.84922}\n",
            "Step 920: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69241, 'train_ucc_acc': 0.53125, 'loss': 0.84622}\n",
            "Step 940: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69391, 'train_ucc_acc': 0.46875, 'loss': 0.84696}\n",
            "Step 960: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69323, 'train_ucc_acc': 0.5, 'loss': 0.84662}\n",
            "Step 980: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69237, 'train_ucc_acc': 0.53125, 'loss': 0.84619}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.15, 'init_lower_bound': 0.05, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:00:15 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:00:19 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:00:19 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:00:23 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00047, 'train_ucc_loss': 0.69559, 'train_ucc_acc': 0.5, 'loss': 0.84803}\n",
            "Step 40: {'train_ae_loss': 1.00019, 'train_ucc_loss': 0.68728, 'train_ucc_acc': 0.5625, 'loss': 0.84374}\n",
            "Step 60: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.70881, 'train_ucc_acc': 0.375, 'loss': 0.85444}\n",
            "Step 80: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.70086, 'train_ucc_acc': 0.4375, 'loss': 0.85046}\n",
            "Step 100: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.70219, 'train_ucc_acc': 0.40625, 'loss': 0.85109}\n",
            "Step 120: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69612, 'train_ucc_acc': 0.46875, 'loss': 0.84805}\n",
            "Step 140: {'train_ae_loss': 1.00008, 'train_ucc_loss': 0.69146, 'train_ucc_acc': 0.53125, 'loss': 0.84577}\n",
            "Step 160: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.68733, 'train_ucc_acc': 0.59375, 'loss': 0.84369}\n",
            "Step 180: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.6994, 'train_ucc_acc': 0.40625, 'loss': 0.84972}\n",
            "Step 200: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.70049, 'train_ucc_acc': 0.375, 'loss': 0.85025}\n",
            "Step 220: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.68685, 'train_ucc_acc': 0.625, 'loss': 0.84342}\n",
            "Step 240: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.69522, 'train_ucc_acc': 0.46875, 'loss': 0.84759}\n",
            "Step 260: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69826, 'train_ucc_acc': 0.40625, 'loss': 0.84914}\n",
            "Step 280: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69493, 'train_ucc_acc': 0.46875, 'loss': 0.84748}\n",
            "Step 300: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69212, 'train_ucc_acc': 0.53125, 'loss': 0.84605}\n",
            "Step 320: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69613, 'train_ucc_acc': 0.375, 'loss': 0.84806}\n",
            "Step 340: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69318, 'train_ucc_acc': 0.5, 'loss': 0.8466}\n",
            "Step 360: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69263, 'train_ucc_acc': 0.53125, 'loss': 0.84631}\n",
            "Step 380: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69371, 'train_ucc_acc': 0.46875, 'loss': 0.84688}\n",
            "Step 400: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69373, 'train_ucc_acc': 0.46875, 'loss': 0.84687}\n",
            "Step 420: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69381, 'train_ucc_acc': 0.46875, 'loss': 0.84689}\n",
            "Step 440: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69316, 'train_ucc_acc': 0.5, 'loss': 0.84657}\n",
            "Step 460: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69482, 'train_ucc_acc': 0.375, 'loss': 0.8474}\n",
            "Step 480: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84657}\n",
            "Step 500: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69388, 'train_ucc_acc': 0.375, 'loss': 0.84696}\n",
            "Step 520: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69313, 'train_ucc_acc': 0.53125, 'loss': 0.84657}\n",
            "Step 540: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69297, 'train_ucc_acc': 0.53125, 'loss': 0.84649}\n",
            "Step 560: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69318, 'train_ucc_acc': 0.5, 'loss': 0.84659}\n",
            "Step 580: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69572, 'train_ucc_acc': 0.375, 'loss': 0.84786}\n",
            "Step 600: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6905, 'train_ucc_acc': 0.6875, 'loss': 0.84525}\n",
            "Step 620: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69278, 'train_ucc_acc': 0.53125, 'loss': 0.84638}\n",
            "Step 640: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69487, 'train_ucc_acc': 0.375, 'loss': 0.84743}\n",
            "Step 660: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69194, 'train_ucc_acc': 0.59375, 'loss': 0.84598}\n",
            "Step 680: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69319, 'train_ucc_acc': 0.5, 'loss': 0.8466}\n",
            "Step 700: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69273, 'train_ucc_acc': 0.53125, 'loss': 0.84636}\n",
            "Step 720: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69342, 'train_ucc_acc': 0.46875, 'loss': 0.84672}\n",
            "Step 740: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69337, 'train_ucc_acc': 0.40625, 'loss': 0.84668}\n",
            "Step 760: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69361, 'train_ucc_acc': 0.34375, 'loss': 0.84681}\n",
            "Step 780: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69207, 'train_ucc_acc': 0.625, 'loss': 0.84604}\n",
            "Step 800: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69369, 'train_ucc_acc': 0.375, 'loss': 0.84684}\n",
            "Step 820: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69288, 'train_ucc_acc': 0.53125, 'loss': 0.84644}\n",
            "Step 840: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69278, 'train_ucc_acc': 0.53125, 'loss': 0.84641}\n",
            "Step 860: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69367, 'train_ucc_acc': 0.46875, 'loss': 0.84683}\n",
            "Step 880: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69629, 'train_ucc_acc': 0.375, 'loss': 0.84816}\n",
            "Step 900: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69322, 'train_ucc_acc': 0.5, 'loss': 0.84661}\n",
            "Step 920: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.696, 'train_ucc_acc': 0.375, 'loss': 0.84801}\n",
            "Step 940: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69319, 'train_ucc_acc': 0.5, 'loss': 0.84659}\n",
            "Step 960: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69251, 'train_ucc_acc': 0.53125, 'loss': 0.84625}\n",
            "Step 980: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69691, 'train_ucc_acc': 0.34375, 'loss': 0.84845}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.2, 'init_lower_bound': 0.05, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:09:45 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:09:49 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:09:49 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:09:53 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00041, 'train_ucc_loss': 0.69725, 'train_ucc_acc': 0.40625, 'loss': 0.84883}\n",
            "Step 40: {'train_ae_loss': 1.00015, 'train_ucc_loss': 0.69038, 'train_ucc_acc': 0.59375, 'loss': 0.84527}\n",
            "Step 60: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.6923, 'train_ucc_acc': 0.53125, 'loss': 0.84618}\n",
            "Step 80: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69757, 'train_ucc_acc': 0.375, 'loss': 0.8488}\n",
            "Step 100: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69628, 'train_ucc_acc': 0.375, 'loss': 0.84816}\n",
            "Step 120: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69157, 'train_ucc_acc': 0.5625, 'loss': 0.84579}\n",
            "Step 140: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69232, 'train_ucc_acc': 0.53125, 'loss': 0.84616}\n",
            "Step 160: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69067, 'train_ucc_acc': 0.59375, 'loss': 0.84536}\n",
            "Step 180: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69324, 'train_ucc_acc': 0.5, 'loss': 0.84663}\n",
            "Step 200: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69543, 'train_ucc_acc': 0.40625, 'loss': 0.84772}\n",
            "Step 220: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69126, 'train_ucc_acc': 0.59375, 'loss': 0.84563}\n",
            "Step 240: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69564, 'train_ucc_acc': 0.40625, 'loss': 0.84784}\n",
            "Step 260: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6974, 'train_ucc_acc': 0.34375, 'loss': 0.8487}\n",
            "Step 280: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69384, 'train_ucc_acc': 0.46875, 'loss': 0.84692}\n",
            "Step 300: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69775, 'train_ucc_acc': 0.28125, 'loss': 0.84888}\n",
            "Step 320: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69317, 'train_ucc_acc': 0.5, 'loss': 0.84658}\n",
            "Step 340: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69253, 'train_ucc_acc': 0.5625, 'loss': 0.84627}\n",
            "Step 360: {'train_ae_loss': 0.99995, 'train_ucc_loss': 0.69406, 'train_ucc_acc': 0.40625, 'loss': 0.847}\n",
            "Step 380: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69403, 'train_ucc_acc': 0.375, 'loss': 0.84701}\n",
            "Step 400: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69294, 'train_ucc_acc': 0.59375, 'loss': 0.84647}\n",
            "Step 420: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69232, 'train_ucc_acc': 0.59375, 'loss': 0.84616}\n",
            "Step 440: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69292, 'train_ucc_acc': 0.53125, 'loss': 0.84647}\n",
            "Step 460: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69269, 'train_ucc_acc': 0.59375, 'loss': 0.84634}\n",
            "Step 480: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84658}\n",
            "Step 500: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69256, 'train_ucc_acc': 0.59375, 'loss': 0.84629}\n",
            "Step 520: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69303, 'train_ucc_acc': 0.53125, 'loss': 0.8465}\n",
            "Step 540: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.69325, 'train_ucc_acc': 0.46875, 'loss': 0.84661}\n",
            "Step 560: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69322, 'train_ucc_acc': 0.46875, 'loss': 0.84662}\n",
            "Step 580: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.8466}\n",
            "Step 600: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69284, 'train_ucc_acc': 0.59375, 'loss': 0.84643}\n",
            "Step 620: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84658}\n",
            "Step 640: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84659}\n",
            "Step 660: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69401, 'train_ucc_acc': 0.4375, 'loss': 0.84701}\n",
            "Step 680: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.6936, 'train_ucc_acc': 0.46875, 'loss': 0.8468}\n",
            "Step 700: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.8466}\n",
            "Step 720: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69365, 'train_ucc_acc': 0.46875, 'loss': 0.84684}\n",
            "Step 740: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69317, 'train_ucc_acc': 0.5, 'loss': 0.8466}\n",
            "Step 760: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6942, 'train_ucc_acc': 0.4375, 'loss': 0.8471}\n",
            "Step 780: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69288, 'train_ucc_acc': 0.53125, 'loss': 0.84644}\n",
            "Step 800: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69255, 'train_ucc_acc': 0.59375, 'loss': 0.8463}\n",
            "Step 820: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69262, 'train_ucc_acc': 0.65625, 'loss': 0.84632}\n",
            "Step 840: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69319, 'train_ucc_acc': 0.3125, 'loss': 0.8466}\n",
            "Step 860: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84657}\n",
            "Step 880: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69276, 'train_ucc_acc': 0.53125, 'loss': 0.84638}\n",
            "Step 900: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.692, 'train_ucc_acc': 0.5625, 'loss': 0.84598}\n",
            "Step 920: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69318, 'train_ucc_acc': 0.5, 'loss': 0.84658}\n",
            "Step 940: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69371, 'train_ucc_acc': 0.46875, 'loss': 0.84684}\n",
            "Step 960: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69233, 'train_ucc_acc': 0.5625, 'loss': 0.84619}\n",
            "Step 980: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69254, 'train_ucc_acc': 0.5625, 'loss': 0.84625}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.25, 'init_lower_bound': 0.05, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:19:18 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:19:22 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:19:22 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:19:26 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00058, 'train_ucc_loss': 0.69515, 'train_ucc_acc': 0.5, 'loss': 0.84787}\n",
            "Step 40: {'train_ae_loss': 1.0002, 'train_ucc_loss': 0.70293, 'train_ucc_acc': 0.4375, 'loss': 0.85156}\n",
            "Step 60: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.68741, 'train_ucc_acc': 0.5625, 'loss': 0.84374}\n",
            "Step 80: {'train_ae_loss': 1.00011, 'train_ucc_loss': 0.68785, 'train_ucc_acc': 0.5625, 'loss': 0.84398}\n",
            "Step 100: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.68817, 'train_ucc_acc': 0.5625, 'loss': 0.84411}\n",
            "Step 120: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.70312, 'train_ucc_acc': 0.40625, 'loss': 0.85159}\n",
            "Step 140: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69132, 'train_ucc_acc': 0.53125, 'loss': 0.84567}\n",
            "Step 160: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.70145, 'train_ucc_acc': 0.40625, 'loss': 0.85073}\n",
            "Step 180: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69151, 'train_ucc_acc': 0.53125, 'loss': 0.84576}\n",
            "Step 200: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69772, 'train_ucc_acc': 0.4375, 'loss': 0.84887}\n",
            "Step 220: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69543, 'train_ucc_acc': 0.46875, 'loss': 0.84771}\n",
            "Step 240: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.6866, 'train_ucc_acc': 0.625, 'loss': 0.84331}\n",
            "Step 260: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69364, 'train_ucc_acc': 0.5, 'loss': 0.84684}\n",
            "Step 280: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69365, 'train_ucc_acc': 0.5, 'loss': 0.84685}\n",
            "Step 300: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69684, 'train_ucc_acc': 0.4375, 'loss': 0.84841}\n",
            "Step 320: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.6967, 'train_ucc_acc': 0.4375, 'loss': 0.84834}\n",
            "Step 340: {'train_ae_loss': 0.99994, 'train_ucc_loss': 0.6934, 'train_ucc_acc': 0.5, 'loss': 0.84667}\n",
            "Step 360: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69462, 'train_ucc_acc': 0.46875, 'loss': 0.84733}\n",
            "Step 380: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69104, 'train_ucc_acc': 0.5625, 'loss': 0.84553}\n",
            "Step 400: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.68911, 'train_ucc_acc': 0.625, 'loss': 0.84456}\n",
            "Step 420: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69101, 'train_ucc_acc': 0.5625, 'loss': 0.84552}\n",
            "Step 440: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69779, 'train_ucc_acc': 0.375, 'loss': 0.8489}\n",
            "Step 460: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69323, 'train_ucc_acc': 0.5, 'loss': 0.84663}\n",
            "Step 480: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69458, 'train_ucc_acc': 0.4375, 'loss': 0.8473}\n",
            "Step 500: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69228, 'train_ucc_acc': 0.5625, 'loss': 0.84615}\n",
            "Step 520: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69356, 'train_ucc_acc': 0.46875, 'loss': 0.84677}\n",
            "Step 540: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69366, 'train_ucc_acc': 0.46875, 'loss': 0.84685}\n",
            "Step 560: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69078, 'train_ucc_acc': 0.6875, 'loss': 0.84539}\n",
            "Step 580: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69391, 'train_ucc_acc': 0.4375, 'loss': 0.84695}\n",
            "Step 600: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69401, 'train_ucc_acc': 0.4375, 'loss': 0.84701}\n",
            "Step 620: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6932, 'train_ucc_acc': 0.5, 'loss': 0.8466}\n",
            "Step 640: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69655, 'train_ucc_acc': 0.375, 'loss': 0.84828}\n",
            "Step 660: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.6927, 'train_ucc_acc': 0.53125, 'loss': 0.84634}\n",
            "Step 680: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69351, 'train_ucc_acc': 0.4375, 'loss': 0.84676}\n",
            "Step 700: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69333, 'train_ucc_acc': 0.40625, 'loss': 0.84669}\n",
            "Step 720: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69285, 'train_ucc_acc': 0.5625, 'loss': 0.84644}\n",
            "Step 740: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.6929, 'train_ucc_acc': 0.53125, 'loss': 0.84646}\n",
            "Step 760: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69295, 'train_ucc_acc': 0.53125, 'loss': 0.84648}\n",
            "Step 780: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69323, 'train_ucc_acc': 0.4375, 'loss': 0.8466}\n",
            "Step 800: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69311, 'train_ucc_acc': 0.53125, 'loss': 0.84656}\n",
            "Step 820: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.6933, 'train_ucc_acc': 0.40625, 'loss': 0.84664}\n",
            "Step 840: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69326, 'train_ucc_acc': 0.40625, 'loss': 0.84663}\n",
            "Step 860: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6932, 'train_ucc_acc': 0.46875, 'loss': 0.8466}\n",
            "Step 880: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69317, 'train_ucc_acc': 0.46875, 'loss': 0.8466}\n",
            "Step 900: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69273, 'train_ucc_acc': 0.5625, 'loss': 0.84638}\n",
            "Step 920: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69316, 'train_ucc_acc': 0.5, 'loss': 0.8466}\n",
            "Step 940: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69361, 'train_ucc_acc': 0.40625, 'loss': 0.84681}\n",
            "Step 960: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84657}\n",
            "Step 980: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69329, 'train_ucc_acc': 0.375, 'loss': 0.84666}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.3, 'init_lower_bound': 0.05, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:28:54 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:28:58 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:28:58 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:29:03 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00075, 'train_ucc_loss': 0.69482, 'train_ucc_acc': 0.5, 'loss': 0.84778}\n",
            "Step 40: {'train_ae_loss': 1.00014, 'train_ucc_loss': 0.69122, 'train_ucc_acc': 0.53125, 'loss': 0.84568}\n",
            "Step 60: {'train_ae_loss': 1.00012, 'train_ucc_loss': 0.67846, 'train_ucc_acc': 0.65625, 'loss': 0.83929}\n",
            "Step 80: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.70664, 'train_ucc_acc': 0.375, 'loss': 0.85335}\n",
            "Step 100: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69132, 'train_ucc_acc': 0.53125, 'loss': 0.84568}\n",
            "Step 120: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69137, 'train_ucc_acc': 0.53125, 'loss': 0.84571}\n",
            "Step 140: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69395, 'train_ucc_acc': 0.5, 'loss': 0.84698}\n",
            "Step 160: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69157, 'train_ucc_acc': 0.53125, 'loss': 0.84581}\n",
            "Step 180: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69555, 'train_ucc_acc': 0.46875, 'loss': 0.84778}\n",
            "Step 200: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69182, 'train_ucc_acc': 0.53125, 'loss': 0.84592}\n",
            "Step 220: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69191, 'train_ucc_acc': 0.53125, 'loss': 0.84595}\n",
            "Step 240: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69203, 'train_ucc_acc': 0.53125, 'loss': 0.846}\n",
            "Step 260: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69746, 'train_ucc_acc': 0.40625, 'loss': 0.84874}\n",
            "Step 280: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69062, 'train_ucc_acc': 0.5625, 'loss': 0.84531}\n",
            "Step 300: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69689, 'train_ucc_acc': 0.40625, 'loss': 0.84845}\n",
            "Step 320: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69552, 'train_ucc_acc': 0.4375, 'loss': 0.84777}\n",
            "Step 340: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.68714, 'train_ucc_acc': 0.65625, 'loss': 0.84358}\n",
            "Step 360: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69648, 'train_ucc_acc': 0.4375, 'loss': 0.84823}\n",
            "Step 380: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.68769, 'train_ucc_acc': 0.625, 'loss': 0.84384}\n",
            "Step 400: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69074, 'train_ucc_acc': 0.5625, 'loss': 0.84537}\n",
            "Step 420: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69591, 'train_ucc_acc': 0.4375, 'loss': 0.84795}\n",
            "Step 440: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69688, 'train_ucc_acc': 0.40625, 'loss': 0.84845}\n",
            "Step 460: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6893, 'train_ucc_acc': 0.625, 'loss': 0.84465}\n",
            "Step 480: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69405, 'train_ucc_acc': 0.46875, 'loss': 0.84704}\n",
            "Step 500: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69456, 'train_ucc_acc': 0.4375, 'loss': 0.8473}\n",
            "Step 520: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69489, 'train_ucc_acc': 0.40625, 'loss': 0.84745}\n",
            "Step 540: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.69318, 'train_ucc_acc': 0.5, 'loss': 0.84657}\n",
            "Step 560: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69267, 'train_ucc_acc': 0.53125, 'loss': 0.84634}\n",
            "Step 580: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84659}\n",
            "Step 600: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69304, 'train_ucc_acc': 0.53125, 'loss': 0.84652}\n",
            "Step 620: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84657}\n",
            "Step 640: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69338, 'train_ucc_acc': 0.46875, 'loss': 0.84669}\n",
            "Step 660: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69334, 'train_ucc_acc': 0.4375, 'loss': 0.84667}\n",
            "Step 680: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69269, 'train_ucc_acc': 0.6875, 'loss': 0.84634}\n",
            "Step 700: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.6936, 'train_ucc_acc': 0.4375, 'loss': 0.8468}\n",
            "Step 720: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69243, 'train_ucc_acc': 0.59375, 'loss': 0.84621}\n",
            "Step 740: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69408, 'train_ucc_acc': 0.375, 'loss': 0.84705}\n",
            "Step 760: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69294, 'train_ucc_acc': 0.53125, 'loss': 0.84647}\n",
            "Step 780: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.6927, 'train_ucc_acc': 0.5625, 'loss': 0.84635}\n",
            "Step 800: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69293, 'train_ucc_acc': 0.53125, 'loss': 0.84647}\n",
            "Step 820: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69336, 'train_ucc_acc': 0.46875, 'loss': 0.84669}\n",
            "Step 840: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84657}\n",
            "Step 860: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69185, 'train_ucc_acc': 0.65625, 'loss': 0.84591}\n",
            "Step 880: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.6914, 'train_ucc_acc': 0.625, 'loss': 0.84571}\n",
            "Step 900: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69192, 'train_ucc_acc': 0.59375, 'loss': 0.84598}\n",
            "Step 920: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69255, 'train_ucc_acc': 0.5625, 'loss': 0.84628}\n",
            "Step 940: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69153, 'train_ucc_acc': 0.6875, 'loss': 0.84577}\n",
            "Step 960: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69431, 'train_ucc_acc': 0.375, 'loss': 0.84716}\n",
            "Step 980: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69286, 'train_ucc_acc': 0.53125, 'loss': 0.84644}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.15, 'init_lower_bound': 0.1, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:37:39 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:37:43 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:37:43 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:37:46 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00048, 'train_ucc_loss': 0.69134, 'train_ucc_acc': 0.53125, 'loss': 0.84591}\n",
            "Step 40: {'train_ae_loss': 1.00022, 'train_ucc_loss': 0.68189, 'train_ucc_acc': 0.59375, 'loss': 0.84106}\n",
            "Step 60: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.70018, 'train_ucc_acc': 0.46875, 'loss': 0.85011}\n",
            "Step 80: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69121, 'train_ucc_acc': 0.53125, 'loss': 0.84563}\n",
            "Step 100: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.68712, 'train_ucc_acc': 0.5625, 'loss': 0.84356}\n",
            "Step 120: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.70708, 'train_ucc_acc': 0.40625, 'loss': 0.85356}\n",
            "Step 140: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.68756, 'train_ucc_acc': 0.5625, 'loss': 0.84378}\n",
            "Step 160: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69455, 'train_ucc_acc': 0.5, 'loss': 0.84729}\n",
            "Step 180: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.70626, 'train_ucc_acc': 0.375, 'loss': 0.85313}\n",
            "Step 200: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69681, 'train_ucc_acc': 0.46875, 'loss': 0.84841}\n",
            "Step 220: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.70028, 'train_ucc_acc': 0.40625, 'loss': 0.85013}\n",
            "Step 240: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69366, 'train_ucc_acc': 0.5, 'loss': 0.84682}\n",
            "Step 260: {'train_ae_loss': 0.99994, 'train_ucc_loss': 0.67983, 'train_ucc_acc': 0.75, 'loss': 0.83989}\n",
            "Step 280: {'train_ae_loss': 0.99991, 'train_ucc_loss': 0.68729, 'train_ucc_acc': 0.625, 'loss': 0.8436}\n",
            "Step 300: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.68641, 'train_ucc_acc': 0.65625, 'loss': 0.84321}\n",
            "Step 320: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69934, 'train_ucc_acc': 0.375, 'loss': 0.84966}\n",
            "Step 340: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69336, 'train_ucc_acc': 0.5, 'loss': 0.84667}\n",
            "Step 360: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69209, 'train_ucc_acc': 0.53125, 'loss': 0.84605}\n",
            "Step 380: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.68999, 'train_ucc_acc': 0.59375, 'loss': 0.84498}\n",
            "Step 400: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69444, 'train_ucc_acc': 0.46875, 'loss': 0.84723}\n",
            "Step 420: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69573, 'train_ucc_acc': 0.40625, 'loss': 0.84788}\n",
            "Step 440: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69069, 'train_ucc_acc': 0.65625, 'loss': 0.84536}\n",
            "Step 460: {'train_ae_loss': 1.00008, 'train_ucc_loss': 0.69464, 'train_ucc_acc': 0.4375, 'loss': 0.84736}\n",
            "Step 480: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69178, 'train_ucc_acc': 0.5625, 'loss': 0.84588}\n",
            "Step 500: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69548, 'train_ucc_acc': 0.375, 'loss': 0.84776}\n",
            "Step 520: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69317, 'train_ucc_acc': 0.5, 'loss': 0.84659}\n",
            "Step 540: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69262, 'train_ucc_acc': 0.53125, 'loss': 0.84631}\n",
            "Step 560: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69107, 'train_ucc_acc': 0.59375, 'loss': 0.84554}\n",
            "Step 580: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69201, 'train_ucc_acc': 0.5625, 'loss': 0.84601}\n",
            "Step 600: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69412, 'train_ucc_acc': 0.40625, 'loss': 0.84707}\n",
            "Step 620: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69321, 'train_ucc_acc': 0.4375, 'loss': 0.84661}\n",
            "Step 640: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69353, 'train_ucc_acc': 0.40625, 'loss': 0.84676}\n",
            "Step 660: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69207, 'train_ucc_acc': 0.625, 'loss': 0.84605}\n",
            "Step 680: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69437, 'train_ucc_acc': 0.34375, 'loss': 0.8472}\n",
            "Step 700: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69337, 'train_ucc_acc': 0.4375, 'loss': 0.84667}\n",
            "Step 720: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69304, 'train_ucc_acc': 0.53125, 'loss': 0.84651}\n",
            "Step 740: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69295, 'train_ucc_acc': 0.53125, 'loss': 0.84647}\n",
            "Step 760: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.69284, 'train_ucc_acc': 0.53125, 'loss': 0.8464}\n",
            "Step 780: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69459, 'train_ucc_acc': 0.40625, 'loss': 0.84728}\n",
            "Step 800: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69317, 'train_ucc_acc': 0.5, 'loss': 0.84658}\n",
            "Step 820: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69677, 'train_ucc_acc': 0.1875, 'loss': 0.84838}\n",
            "Step 840: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69211, 'train_ucc_acc': 0.65625, 'loss': 0.84605}\n",
            "Step 860: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69417, 'train_ucc_acc': 0.4375, 'loss': 0.8471}\n",
            "Step 880: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69064, 'train_ucc_acc': 0.625, 'loss': 0.84531}\n",
            "Step 900: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69743, 'train_ucc_acc': 0.3125, 'loss': 0.8487}\n",
            "Step 920: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.69463, 'train_ucc_acc': 0.40625, 'loss': 0.84729}\n",
            "Step 940: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69449, 'train_ucc_acc': 0.40625, 'loss': 0.84724}\n",
            "Step 960: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69212, 'train_ucc_acc': 0.5625, 'loss': 0.84606}\n",
            "Step 980: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69193, 'train_ucc_acc': 0.59375, 'loss': 0.84596}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.2, 'init_lower_bound': 0.1, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:47:09 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:47:15 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:47:15 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:47:19 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00051, 'train_ucc_loss': 0.67505, 'train_ucc_acc': 0.65625, 'loss': 0.83778}\n",
            "Step 40: {'train_ae_loss': 1.00022, 'train_ucc_loss': 0.7104, 'train_ucc_acc': 0.375, 'loss': 0.85531}\n",
            "Step 60: {'train_ae_loss': 1.00012, 'train_ucc_loss': 0.69462, 'train_ucc_acc': 0.5, 'loss': 0.84737}\n",
            "Step 80: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.70068, 'train_ucc_acc': 0.4375, 'loss': 0.85037}\n",
            "Step 100: {'train_ae_loss': 1.0001, 'train_ucc_loss': 0.70512, 'train_ucc_acc': 0.375, 'loss': 0.85261}\n",
            "Step 120: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69392, 'train_ucc_acc': 0.5, 'loss': 0.84698}\n",
            "Step 140: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.68969, 'train_ucc_acc': 0.5625, 'loss': 0.84486}\n",
            "Step 160: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.69539, 'train_ucc_acc': 0.46875, 'loss': 0.84773}\n",
            "Step 180: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.6809, 'train_ucc_acc': 0.71875, 'loss': 0.84047}\n",
            "Step 200: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69715, 'train_ucc_acc': 0.4375, 'loss': 0.84857}\n",
            "Step 220: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69831, 'train_ucc_acc': 0.40625, 'loss': 0.84915}\n",
            "Step 240: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.70097, 'train_ucc_acc': 0.3125, 'loss': 0.85052}\n",
            "Step 260: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6914, 'train_ucc_acc': 0.5625, 'loss': 0.8457}\n",
            "Step 280: {'train_ae_loss': 1.00009, 'train_ucc_loss': 0.69405, 'train_ucc_acc': 0.46875, 'loss': 0.84707}\n",
            "Step 300: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69462, 'train_ucc_acc': 0.4375, 'loss': 0.84732}\n",
            "Step 320: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69367, 'train_ucc_acc': 0.46875, 'loss': 0.84684}\n",
            "Step 340: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69342, 'train_ucc_acc': 0.46875, 'loss': 0.84673}\n",
            "Step 360: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69462, 'train_ucc_acc': 0.375, 'loss': 0.84731}\n",
            "Step 380: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69316, 'train_ucc_acc': 0.5, 'loss': 0.84659}\n",
            "Step 400: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69205, 'train_ucc_acc': 0.59375, 'loss': 0.84603}\n",
            "Step 420: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6937, 'train_ucc_acc': 0.4375, 'loss': 0.84685}\n",
            "Step 440: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69318, 'train_ucc_acc': 0.46875, 'loss': 0.84659}\n",
            "Step 460: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69298, 'train_ucc_acc': 0.59375, 'loss': 0.8465}\n",
            "Step 480: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69344, 'train_ucc_acc': 0.46875, 'loss': 0.84674}\n",
            "Step 500: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69347, 'train_ucc_acc': 0.46875, 'loss': 0.84673}\n",
            "Step 520: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84658}\n",
            "Step 540: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.69294, 'train_ucc_acc': 0.5625, 'loss': 0.84645}\n",
            "Step 560: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69311, 'train_ucc_acc': 0.53125, 'loss': 0.84657}\n",
            "Step 580: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69327, 'train_ucc_acc': 0.4375, 'loss': 0.84663}\n",
            "Step 600: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69307, 'train_ucc_acc': 0.5625, 'loss': 0.84654}\n",
            "Step 620: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69319, 'train_ucc_acc': 0.4375, 'loss': 0.84661}\n",
            "Step 640: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69301, 'train_ucc_acc': 0.53125, 'loss': 0.84651}\n",
            "Step 660: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69459, 'train_ucc_acc': 0.34375, 'loss': 0.84731}\n",
            "Step 680: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69297, 'train_ucc_acc': 0.53125, 'loss': 0.84649}\n",
            "Step 700: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69292, 'train_ucc_acc': 0.53125, 'loss': 0.84646}\n",
            "Step 720: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84658}\n",
            "Step 740: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69306, 'train_ucc_acc': 0.53125, 'loss': 0.84654}\n",
            "Step 760: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69312, 'train_ucc_acc': 0.53125, 'loss': 0.84656}\n",
            "Step 780: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69371, 'train_ucc_acc': 0.40625, 'loss': 0.84686}\n",
            "Step 800: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69202, 'train_ucc_acc': 0.71875, 'loss': 0.84602}\n",
            "Step 820: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69308, 'train_ucc_acc': 0.53125, 'loss': 0.84653}\n",
            "Step 840: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69305, 'train_ucc_acc': 0.5625, 'loss': 0.84653}\n",
            "Step 860: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.69297, 'train_ucc_acc': 0.53125, 'loss': 0.84647}\n",
            "Step 880: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69334, 'train_ucc_acc': 0.46875, 'loss': 0.84669}\n",
            "Step 900: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69301, 'train_ucc_acc': 0.5625, 'loss': 0.84651}\n",
            "Step 920: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69277, 'train_ucc_acc': 0.625, 'loss': 0.84639}\n",
            "Step 940: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69374, 'train_ucc_acc': 0.40625, 'loss': 0.84686}\n",
            "Step 960: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69305, 'train_ucc_acc': 0.53125, 'loss': 0.84652}\n",
            "Step 980: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69285, 'train_ucc_acc': 0.53125, 'loss': 0.84642}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.25, 'init_lower_bound': 0.1, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:56:11 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:56:15 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 03:56:15 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 03:56:20 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.0008, 'train_ucc_loss': 0.69126, 'train_ucc_acc': 0.53125, 'loss': 0.84603}\n",
            "Step 40: {'train_ae_loss': 1.00037, 'train_ucc_loss': 0.6953, 'train_ucc_acc': 0.5, 'loss': 0.84784}\n",
            "Step 60: {'train_ae_loss': 1.0002, 'train_ucc_loss': 0.68358, 'train_ucc_acc': 0.59375, 'loss': 0.84189}\n",
            "Step 80: {'train_ae_loss': 1.00011, 'train_ucc_loss': 0.69819, 'train_ucc_acc': 0.46875, 'loss': 0.84915}\n",
            "Step 100: {'train_ae_loss': 1.00011, 'train_ucc_loss': 0.68155, 'train_ucc_acc': 0.625, 'loss': 0.84083}\n",
            "Step 120: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.71166, 'train_ucc_acc': 0.34375, 'loss': 0.85586}\n",
            "Step 140: {'train_ae_loss': 1.00009, 'train_ucc_loss': 0.68179, 'train_ucc_acc': 0.625, 'loss': 0.84094}\n",
            "Step 160: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69748, 'train_ucc_acc': 0.46875, 'loss': 0.84875}\n",
            "Step 180: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69426, 'train_ucc_acc': 0.5, 'loss': 0.84713}\n",
            "Step 200: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69694, 'train_ucc_acc': 0.46875, 'loss': 0.84848}\n",
            "Step 220: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69141, 'train_ucc_acc': 0.53125, 'loss': 0.84569}\n",
            "Step 240: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69938, 'train_ucc_acc': 0.4375, 'loss': 0.84971}\n",
            "Step 260: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69947, 'train_ucc_acc': 0.4375, 'loss': 0.84975}\n",
            "Step 280: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69635, 'train_ucc_acc': 0.46875, 'loss': 0.84819}\n",
            "Step 300: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.68801, 'train_ucc_acc': 0.59375, 'loss': 0.84401}\n",
            "Step 320: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69516, 'train_ucc_acc': 0.46875, 'loss': 0.84758}\n",
            "Step 340: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69815, 'train_ucc_acc': 0.40625, 'loss': 0.84908}\n",
            "Step 360: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69847, 'train_ucc_acc': 0.40625, 'loss': 0.84924}\n",
            "Step 380: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69501, 'train_ucc_acc': 0.46875, 'loss': 0.8475}\n",
            "Step 400: {'train_ae_loss': 0.99995, 'train_ucc_loss': 0.68544, 'train_ucc_acc': 0.6875, 'loss': 0.84269}\n",
            "Step 420: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69449, 'train_ucc_acc': 0.46875, 'loss': 0.84725}\n",
            "Step 440: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69527, 'train_ucc_acc': 0.4375, 'loss': 0.84765}\n",
            "Step 460: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69326, 'train_ucc_acc': 0.5, 'loss': 0.84664}\n",
            "Step 480: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69606, 'train_ucc_acc': 0.40625, 'loss': 0.84805}\n",
            "Step 500: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69327, 'train_ucc_acc': 0.5, 'loss': 0.84663}\n",
            "Step 520: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69119, 'train_ucc_acc': 0.5625, 'loss': 0.8456}\n",
            "Step 540: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.68886, 'train_ucc_acc': 0.625, 'loss': 0.84443}\n",
            "Step 560: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69461, 'train_ucc_acc': 0.46875, 'loss': 0.84731}\n",
            "Step 580: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.70289, 'train_ucc_acc': 0.28125, 'loss': 0.85145}\n",
            "Step 600: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.70112, 'train_ucc_acc': 0.3125, 'loss': 0.85055}\n",
            "Step 620: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69918, 'train_ucc_acc': 0.375, 'loss': 0.84959}\n",
            "Step 640: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69341, 'train_ucc_acc': 0.5, 'loss': 0.8467}\n",
            "Step 660: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69462, 'train_ucc_acc': 0.46875, 'loss': 0.84732}\n",
            "Step 680: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.68986, 'train_ucc_acc': 0.59375, 'loss': 0.84492}\n",
            "Step 700: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69557, 'train_ucc_acc': 0.4375, 'loss': 0.8478}\n",
            "Step 720: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69526, 'train_ucc_acc': 0.4375, 'loss': 0.84766}\n",
            "Step 740: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69171, 'train_ucc_acc': 0.5625, 'loss': 0.84584}\n",
            "Step 760: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69319, 'train_ucc_acc': 0.5, 'loss': 0.84662}\n",
            "Step 780: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69385, 'train_ucc_acc': 0.46875, 'loss': 0.84693}\n",
            "Step 800: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69674, 'train_ucc_acc': 0.34375, 'loss': 0.8484}\n",
            "Step 820: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69386, 'train_ucc_acc': 0.46875, 'loss': 0.84695}\n",
            "Step 840: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.68727, 'train_ucc_acc': 0.75, 'loss': 0.84363}\n",
            "Step 860: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.6886, 'train_ucc_acc': 0.75, 'loss': 0.84431}\n",
            "Step 880: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69144, 'train_ucc_acc': 0.59375, 'loss': 0.84571}\n",
            "Step 900: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69502, 'train_ucc_acc': 0.40625, 'loss': 0.84754}\n",
            "Step 920: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69098, 'train_ucc_acc': 0.59375, 'loss': 0.84547}\n",
            "Step 940: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69243, 'train_ucc_acc': 0.53125, 'loss': 0.84624}\n",
            "Step 960: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69008, 'train_ucc_acc': 0.625, 'loss': 0.84505}\n",
            "Step 980: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69327, 'train_ucc_acc': 0.5, 'loss': 0.84665}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.3, 'init_lower_bound': 0.1, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:05:13 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:05:18 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:05:18 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:05:22 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00066, 'train_ucc_loss': 0.68574, 'train_ucc_acc': 0.5625, 'loss': 0.8432}\n",
            "Step 40: {'train_ae_loss': 1.00025, 'train_ucc_loss': 0.73125, 'train_ucc_acc': 0.3125, 'loss': 0.86575}\n",
            "Step 60: {'train_ae_loss': 1.00018, 'train_ucc_loss': 0.70198, 'train_ucc_acc': 0.46875, 'loss': 0.85108}\n",
            "Step 80: {'train_ae_loss': 1.00011, 'train_ucc_loss': 0.68638, 'train_ucc_acc': 0.5625, 'loss': 0.84325}\n",
            "Step 100: {'train_ae_loss': 1.00009, 'train_ucc_loss': 0.7048, 'train_ucc_acc': 0.4375, 'loss': 0.85244}\n",
            "Step 120: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.6912, 'train_ucc_acc': 0.53125, 'loss': 0.84562}\n",
            "Step 140: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69518, 'train_ucc_acc': 0.5, 'loss': 0.84762}\n",
            "Step 160: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69466, 'train_ucc_acc': 0.5, 'loss': 0.84735}\n",
            "Step 180: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69446, 'train_ucc_acc': 0.5, 'loss': 0.84726}\n",
            "Step 200: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69727, 'train_ucc_acc': 0.46875, 'loss': 0.84865}\n",
            "Step 220: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69671, 'train_ucc_acc': 0.46875, 'loss': 0.84835}\n",
            "Step 240: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.70399, 'train_ucc_acc': 0.375, 'loss': 0.85202}\n",
            "Step 260: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69157, 'train_ucc_acc': 0.53125, 'loss': 0.8458}\n",
            "Step 280: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.6878, 'train_ucc_acc': 0.59375, 'loss': 0.8439}\n",
            "Step 300: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.6936, 'train_ucc_acc': 0.5, 'loss': 0.84681}\n",
            "Step 320: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69352, 'train_ucc_acc': 0.5, 'loss': 0.84677}\n",
            "Step 340: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.68829, 'train_ucc_acc': 0.59375, 'loss': 0.84416}\n",
            "Step 360: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69481, 'train_ucc_acc': 0.46875, 'loss': 0.84741}\n",
            "Step 380: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69939, 'train_ucc_acc': 0.34375, 'loss': 0.8497}\n",
            "Step 400: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6933, 'train_ucc_acc': 0.5, 'loss': 0.84665}\n",
            "Step 420: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69331, 'train_ucc_acc': 0.5, 'loss': 0.84667}\n",
            "Step 440: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69526, 'train_ucc_acc': 0.4375, 'loss': 0.84761}\n",
            "Step 460: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69487, 'train_ucc_acc': 0.4375, 'loss': 0.84744}\n",
            "Step 480: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69506, 'train_ucc_acc': 0.40625, 'loss': 0.84753}\n",
            "Step 500: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.6952, 'train_ucc_acc': 0.375, 'loss': 0.84762}\n",
            "Step 520: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69263, 'train_ucc_acc': 0.5625, 'loss': 0.8463}\n",
            "Step 540: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.6913, 'train_ucc_acc': 0.6875, 'loss': 0.84568}\n",
            "Step 560: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69553, 'train_ucc_acc': 0.34375, 'loss': 0.84776}\n",
            "Step 580: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69316, 'train_ucc_acc': 0.5, 'loss': 0.84658}\n",
            "Step 600: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69247, 'train_ucc_acc': 0.5625, 'loss': 0.84623}\n",
            "Step 620: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69405, 'train_ucc_acc': 0.4375, 'loss': 0.84703}\n",
            "Step 640: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69434, 'train_ucc_acc': 0.4375, 'loss': 0.84719}\n",
            "Step 660: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69429, 'train_ucc_acc': 0.40625, 'loss': 0.84715}\n",
            "Step 680: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69335, 'train_ucc_acc': 0.46875, 'loss': 0.84667}\n",
            "Step 700: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69331, 'train_ucc_acc': 0.46875, 'loss': 0.84665}\n",
            "Step 720: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.6938, 'train_ucc_acc': 0.4375, 'loss': 0.84693}\n",
            "Step 740: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.693, 'train_ucc_acc': 0.53125, 'loss': 0.84651}\n",
            "Step 760: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69324, 'train_ucc_acc': 0.4375, 'loss': 0.8466}\n",
            "Step 780: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69314, 'train_ucc_acc': 0.53125, 'loss': 0.84657}\n",
            "Step 800: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69314, 'train_ucc_acc': 0.53125, 'loss': 0.84657}\n",
            "Step 820: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69307, 'train_ucc_acc': 0.53125, 'loss': 0.84653}\n",
            "Step 840: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69267, 'train_ucc_acc': 0.65625, 'loss': 0.84634}\n",
            "Step 860: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69307, 'train_ucc_acc': 0.59375, 'loss': 0.84655}\n",
            "Step 880: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.375, 'loss': 0.84659}\n",
            "Step 900: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69325, 'train_ucc_acc': 0.40625, 'loss': 0.84663}\n",
            "Step 920: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.6932, 'train_ucc_acc': 0.4375, 'loss': 0.8466}\n",
            "Step 940: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84658}\n",
            "Step 960: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69301, 'train_ucc_acc': 0.5625, 'loss': 0.84651}\n",
            "Step 980: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69285, 'train_ucc_acc': 0.5625, 'loss': 0.84644}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.2, 'init_lower_bound': 0.15, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:14:19 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:14:26 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:14:26 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:14:30 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00064, 'train_ucc_loss': 0.68542, 'train_ucc_acc': 0.5625, 'loss': 0.84303}\n",
            "Step 40: {'train_ae_loss': 1.00023, 'train_ucc_loss': 0.67966, 'train_ucc_acc': 0.59375, 'loss': 0.83995}\n",
            "Step 60: {'train_ae_loss': 1.00013, 'train_ucc_loss': 0.69152, 'train_ucc_acc': 0.53125, 'loss': 0.84582}\n",
            "Step 80: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69634, 'train_ucc_acc': 0.5, 'loss': 0.84819}\n",
            "Step 100: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.6822, 'train_ucc_acc': 0.59375, 'loss': 0.8411}\n",
            "Step 120: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69546, 'train_ucc_acc': 0.5, 'loss': 0.84776}\n",
            "Step 140: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69904, 'train_ucc_acc': 0.46875, 'loss': 0.84953}\n",
            "Step 160: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69123, 'train_ucc_acc': 0.53125, 'loss': 0.84564}\n",
            "Step 180: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69739, 'train_ucc_acc': 0.46875, 'loss': 0.8487}\n",
            "Step 200: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69411, 'train_ucc_acc': 0.5, 'loss': 0.84705}\n",
            "Step 220: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69149, 'train_ucc_acc': 0.53125, 'loss': 0.84575}\n",
            "Step 240: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69156, 'train_ucc_acc': 0.53125, 'loss': 0.8458}\n",
            "Step 260: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69606, 'train_ucc_acc': 0.46875, 'loss': 0.84803}\n",
            "Step 280: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69596, 'train_ucc_acc': 0.46875, 'loss': 0.84801}\n",
            "Step 300: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69588, 'train_ucc_acc': 0.46875, 'loss': 0.84794}\n",
            "Step 320: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.68502, 'train_ucc_acc': 0.625, 'loss': 0.84251}\n",
            "Step 340: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.70045, 'train_ucc_acc': 0.40625, 'loss': 0.85023}\n",
            "Step 360: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69157, 'train_ucc_acc': 0.53125, 'loss': 0.8458}\n",
            "Step 380: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.70898, 'train_ucc_acc': 0.28125, 'loss': 0.85449}\n",
            "Step 400: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.68169, 'train_ucc_acc': 0.6875, 'loss': 0.84083}\n",
            "Step 420: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69541, 'train_ucc_acc': 0.46875, 'loss': 0.8477}\n",
            "Step 440: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69342, 'train_ucc_acc': 0.5, 'loss': 0.84673}\n",
            "Step 460: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69335, 'train_ucc_acc': 0.5, 'loss': 0.84669}\n",
            "Step 480: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69098, 'train_ucc_acc': 0.5625, 'loss': 0.8455}\n",
            "Step 500: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69225, 'train_ucc_acc': 0.53125, 'loss': 0.84614}\n",
            "Step 520: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.6915, 'train_ucc_acc': 0.5625, 'loss': 0.84575}\n",
            "Step 540: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69227, 'train_ucc_acc': 0.53125, 'loss': 0.84614}\n",
            "Step 560: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.68996, 'train_ucc_acc': 0.59375, 'loss': 0.84496}\n",
            "Step 580: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69452, 'train_ucc_acc': 0.46875, 'loss': 0.84726}\n",
            "Step 600: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69211, 'train_ucc_acc': 0.53125, 'loss': 0.84608}\n",
            "Step 620: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69471, 'train_ucc_acc': 0.46875, 'loss': 0.84736}\n",
            "Step 640: {'train_ae_loss': 0.99995, 'train_ucc_loss': 0.69719, 'train_ucc_acc': 0.40625, 'loss': 0.84857}\n",
            "Step 660: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69843, 'train_ucc_acc': 0.375, 'loss': 0.84922}\n",
            "Step 680: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.69579, 'train_ucc_acc': 0.4375, 'loss': 0.84787}\n",
            "Step 700: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69611, 'train_ucc_acc': 0.4375, 'loss': 0.84806}\n",
            "Step 720: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69349, 'train_ucc_acc': 0.5, 'loss': 0.84676}\n",
            "Step 740: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69661, 'train_ucc_acc': 0.4375, 'loss': 0.84829}\n",
            "Step 760: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69086, 'train_ucc_acc': 0.5625, 'loss': 0.84544}\n",
            "Step 780: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69152, 'train_ucc_acc': 0.5625, 'loss': 0.84575}\n",
            "Step 800: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.69319, 'train_ucc_acc': 0.5, 'loss': 0.84657}\n",
            "Step 820: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69317, 'train_ucc_acc': 0.5, 'loss': 0.84659}\n",
            "Step 840: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69456, 'train_ucc_acc': 0.34375, 'loss': 0.84728}\n",
            "Step 860: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.69293, 'train_ucc_acc': 0.5625, 'loss': 0.84644}\n",
            "Step 880: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69316, 'train_ucc_acc': 0.5, 'loss': 0.84657}\n",
            "Step 900: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69436, 'train_ucc_acc': 0.375, 'loss': 0.84719}\n",
            "Step 920: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69289, 'train_ucc_acc': 0.5625, 'loss': 0.84645}\n",
            "Step 940: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69292, 'train_ucc_acc': 0.53125, 'loss': 0.84646}\n",
            "Step 960: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69107, 'train_ucc_acc': 0.65625, 'loss': 0.84552}\n",
            "Step 980: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69272, 'train_ucc_acc': 0.53125, 'loss': 0.84635}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.25, 'init_lower_bound': 0.15, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:23:25 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:23:31 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:23:31 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:23:35 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00082, 'train_ucc_loss': 0.69221, 'train_ucc_acc': 0.53125, 'loss': 0.84651}\n",
            "Step 40: {'train_ae_loss': 1.00028, 'train_ucc_loss': 0.69797, 'train_ucc_acc': 0.5, 'loss': 0.84913}\n",
            "Step 60: {'train_ae_loss': 1.00023, 'train_ucc_loss': 0.69713, 'train_ucc_acc': 0.5, 'loss': 0.84868}\n",
            "Step 80: {'train_ae_loss': 1.00017, 'train_ucc_loss': 0.70192, 'train_ucc_acc': 0.46875, 'loss': 0.85105}\n",
            "Step 100: {'train_ae_loss': 1.00011, 'train_ucc_loss': 0.70619, 'train_ucc_acc': 0.4375, 'loss': 0.85315}\n",
            "Step 120: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.7099, 'train_ucc_acc': 0.40625, 'loss': 0.85498}\n",
            "Step 140: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.70864, 'train_ucc_acc': 0.40625, 'loss': 0.85434}\n",
            "Step 160: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.67466, 'train_ucc_acc': 0.65625, 'loss': 0.83736}\n",
            "Step 180: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.68355, 'train_ucc_acc': 0.59375, 'loss': 0.84179}\n",
            "Step 200: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69832, 'train_ucc_acc': 0.46875, 'loss': 0.84917}\n",
            "Step 220: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.68467, 'train_ucc_acc': 0.59375, 'loss': 0.84236}\n",
            "Step 240: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.68216, 'train_ucc_acc': 0.625, 'loss': 0.8411}\n",
            "Step 260: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69426, 'train_ucc_acc': 0.5, 'loss': 0.84713}\n",
            "Step 280: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69406, 'train_ucc_acc': 0.5, 'loss': 0.84704}\n",
            "Step 300: {'train_ae_loss': 1.00008, 'train_ucc_loss': 0.70404, 'train_ucc_acc': 0.375, 'loss': 0.85206}\n",
            "Step 320: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69155, 'train_ucc_acc': 0.53125, 'loss': 0.8458}\n",
            "Step 340: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69939, 'train_ucc_acc': 0.40625, 'loss': 0.84971}\n",
            "Step 360: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69838, 'train_ucc_acc': 0.40625, 'loss': 0.84918}\n",
            "Step 380: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69841, 'train_ucc_acc': 0.375, 'loss': 0.84921}\n",
            "Step 400: {'train_ae_loss': 0.99995, 'train_ucc_loss': 0.69571, 'train_ucc_acc': 0.4375, 'loss': 0.84783}\n",
            "Step 420: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69128, 'train_ucc_acc': 0.5625, 'loss': 0.84562}\n",
            "Step 440: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69726, 'train_ucc_acc': 0.375, 'loss': 0.84866}\n",
            "Step 460: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69328, 'train_ucc_acc': 0.5, 'loss': 0.84664}\n",
            "Step 480: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69209, 'train_ucc_acc': 0.53125, 'loss': 0.84605}\n",
            "Step 500: {'train_ae_loss': 0.99993, 'train_ucc_loss': 0.6895, 'train_ucc_acc': 0.59375, 'loss': 0.84471}\n",
            "Step 520: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69595, 'train_ucc_acc': 0.4375, 'loss': 0.84796}\n",
            "Step 540: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69342, 'train_ucc_acc': 0.5, 'loss': 0.84671}\n",
            "Step 560: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.68858, 'train_ucc_acc': 0.59375, 'loss': 0.84433}\n",
            "Step 580: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.6918, 'train_ucc_acc': 0.53125, 'loss': 0.84592}\n",
            "Step 600: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69469, 'train_ucc_acc': 0.46875, 'loss': 0.84734}\n",
            "Step 620: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69688, 'train_ucc_acc': 0.40625, 'loss': 0.84843}\n",
            "Step 640: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69132, 'train_ucc_acc': 0.5625, 'loss': 0.84566}\n",
            "Step 660: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69603, 'train_ucc_acc': 0.40625, 'loss': 0.84804}\n",
            "Step 680: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.68674, 'train_ucc_acc': 0.75, 'loss': 0.84338}\n",
            "Step 700: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69391, 'train_ucc_acc': 0.46875, 'loss': 0.84694}\n",
            "Step 720: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69401, 'train_ucc_acc': 0.46875, 'loss': 0.84701}\n",
            "Step 740: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6932, 'train_ucc_acc': 0.5, 'loss': 0.8466}\n",
            "Step 760: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69516, 'train_ucc_acc': 0.375, 'loss': 0.84757}\n",
            "Step 780: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69316, 'train_ucc_acc': 0.5, 'loss': 0.84658}\n",
            "Step 800: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.6928, 'train_ucc_acc': 0.53125, 'loss': 0.84639}\n",
            "Step 820: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6928, 'train_ucc_acc': 0.53125, 'loss': 0.8464}\n",
            "Step 840: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6956, 'train_ucc_acc': 0.375, 'loss': 0.8478}\n",
            "Step 860: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69491, 'train_ucc_acc': 0.40625, 'loss': 0.84746}\n",
            "Step 880: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69406, 'train_ucc_acc': 0.4375, 'loss': 0.84702}\n",
            "Step 900: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69346, 'train_ucc_acc': 0.4375, 'loss': 0.84673}\n",
            "Step 920: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69381, 'train_ucc_acc': 0.375, 'loss': 0.8469}\n",
            "Step 940: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69396, 'train_ucc_acc': 0.4375, 'loss': 0.84698}\n",
            "Step 960: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69386, 'train_ucc_acc': 0.4375, 'loss': 0.84692}\n",
            "Step 980: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69276, 'train_ucc_acc': 0.53125, 'loss': 0.84638}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.3, 'init_lower_bound': 0.15, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:32:29 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:32:34 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:32:35 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:32:39 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00116, 'train_ucc_loss': 0.71942, 'train_ucc_acc': 0.40625, 'loss': 0.86029}\n",
            "Step 40: {'train_ae_loss': 1.00034, 'train_ucc_loss': 0.71122, 'train_ucc_acc': 0.4375, 'loss': 0.85578}\n",
            "Step 60: {'train_ae_loss': 1.00011, 'train_ucc_loss': 0.69169, 'train_ucc_acc': 0.53125, 'loss': 0.8459}\n",
            "Step 80: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69702, 'train_ucc_acc': 0.5, 'loss': 0.84854}\n",
            "Step 100: {'train_ae_loss': 1.00012, 'train_ucc_loss': 0.69633, 'train_ucc_acc': 0.5, 'loss': 0.84823}\n",
            "Step 120: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.7003, 'train_ucc_acc': 0.46875, 'loss': 0.85018}\n",
            "Step 140: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.712, 'train_ucc_acc': 0.375, 'loss': 0.85603}\n",
            "Step 160: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.69502, 'train_ucc_acc': 0.5, 'loss': 0.84754}\n",
            "Step 180: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69487, 'train_ucc_acc': 0.5, 'loss': 0.84745}\n",
            "Step 200: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69122, 'train_ucc_acc': 0.53125, 'loss': 0.84563}\n",
            "Step 220: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.68134, 'train_ucc_acc': 0.625, 'loss': 0.8407}\n",
            "Step 240: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.69768, 'train_ucc_acc': 0.46875, 'loss': 0.84888}\n",
            "Step 260: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.68501, 'train_ucc_acc': 0.59375, 'loss': 0.84253}\n",
            "Step 280: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.70042, 'train_ucc_acc': 0.4375, 'loss': 0.85021}\n",
            "Step 300: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69419, 'train_ucc_acc': 0.5, 'loss': 0.84711}\n",
            "Step 320: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.68848, 'train_ucc_acc': 0.5625, 'loss': 0.84426}\n",
            "Step 340: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.68592, 'train_ucc_acc': 0.59375, 'loss': 0.84296}\n",
            "Step 360: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.68863, 'train_ucc_acc': 0.5625, 'loss': 0.84434}\n",
            "Step 380: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.68582, 'train_ucc_acc': 0.59375, 'loss': 0.84292}\n",
            "Step 400: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69957, 'train_ucc_acc': 0.4375, 'loss': 0.8498}\n",
            "Step 420: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.6888, 'train_ucc_acc': 0.5625, 'loss': 0.84439}\n",
            "Step 440: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69626, 'train_ucc_acc': 0.46875, 'loss': 0.84814}\n",
            "Step 460: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.68933, 'train_ucc_acc': 0.5625, 'loss': 0.84469}\n",
            "Step 480: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.70497, 'train_ucc_acc': 0.34375, 'loss': 0.8525}\n",
            "Step 500: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69382, 'train_ucc_acc': 0.5, 'loss': 0.84691}\n",
            "Step 520: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69159, 'train_ucc_acc': 0.53125, 'loss': 0.8458}\n",
            "Step 540: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.70023, 'train_ucc_acc': 0.40625, 'loss': 0.85012}\n",
            "Step 560: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69995, 'train_ucc_acc': 0.40625, 'loss': 0.84997}\n",
            "Step 580: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69728, 'train_ucc_acc': 0.4375, 'loss': 0.84864}\n",
            "Step 600: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69507, 'train_ucc_acc': 0.46875, 'loss': 0.84754}\n",
            "Step 620: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69644, 'train_ucc_acc': 0.4375, 'loss': 0.84822}\n",
            "Step 640: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.6991, 'train_ucc_acc': 0.375, 'loss': 0.84956}\n",
            "Step 660: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.68975, 'train_ucc_acc': 0.59375, 'loss': 0.84489}\n",
            "Step 680: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69567, 'train_ucc_acc': 0.4375, 'loss': 0.84783}\n",
            "Step 700: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69227, 'train_ucc_acc': 0.53125, 'loss': 0.84614}\n",
            "Step 720: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69234, 'train_ucc_acc': 0.53125, 'loss': 0.84617}\n",
            "Step 740: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69406, 'train_ucc_acc': 0.46875, 'loss': 0.84703}\n",
            "Step 760: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69551, 'train_ucc_acc': 0.40625, 'loss': 0.84776}\n",
            "Step 780: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.695, 'train_ucc_acc': 0.40625, 'loss': 0.84752}\n",
            "Step 800: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69274, 'train_ucc_acc': 0.53125, 'loss': 0.84638}\n",
            "Step 820: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69043, 'train_ucc_acc': 0.65625, 'loss': 0.84522}\n",
            "Step 840: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69265, 'train_ucc_acc': 0.53125, 'loss': 0.84633}\n",
            "Step 860: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69365, 'train_ucc_acc': 0.46875, 'loss': 0.84682}\n",
            "Step 880: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69193, 'train_ucc_acc': 0.59375, 'loss': 0.84596}\n",
            "Step 900: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69218, 'train_ucc_acc': 0.5625, 'loss': 0.84609}\n",
            "Step 920: {'train_ae_loss': 1.00008, 'train_ucc_loss': 0.69516, 'train_ucc_acc': 0.40625, 'loss': 0.84762}\n",
            "Step 940: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69324, 'train_ucc_acc': 0.5, 'loss': 0.84661}\n",
            "Step 960: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69618, 'train_ucc_acc': 0.375, 'loss': 0.84809}\n",
            "Step 980: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6908, 'train_ucc_acc': 0.59375, 'loss': 0.8454}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.25, 'init_lower_bound': 0.2, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:41:32 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:41:37 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:41:37 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:41:43 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00041, 'train_ucc_loss': 0.6858, 'train_ucc_acc': 0.5625, 'loss': 0.8431}\n",
            "Step 40: {'train_ae_loss': 1.00018, 'train_ucc_loss': 0.71348, 'train_ucc_acc': 0.40625, 'loss': 0.85683}\n",
            "Step 60: {'train_ae_loss': 1.00015, 'train_ucc_loss': 0.68631, 'train_ucc_acc': 0.5625, 'loss': 0.84323}\n",
            "Step 80: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.69125, 'train_ucc_acc': 0.53125, 'loss': 0.84566}\n",
            "Step 100: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.69939, 'train_ucc_acc': 0.46875, 'loss': 0.84973}\n",
            "Step 120: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.7021, 'train_ucc_acc': 0.4375, 'loss': 0.85107}\n",
            "Step 140: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69445, 'train_ucc_acc': 0.5, 'loss': 0.84725}\n",
            "Step 160: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.68839, 'train_ucc_acc': 0.5625, 'loss': 0.84422}\n",
            "Step 180: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69711, 'train_ucc_acc': 0.46875, 'loss': 0.84855}\n",
            "Step 200: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69397, 'train_ucc_acc': 0.5, 'loss': 0.84699}\n",
            "Step 220: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69857, 'train_ucc_acc': 0.4375, 'loss': 0.8493}\n",
            "Step 240: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.69833, 'train_ucc_acc': 0.4375, 'loss': 0.8492}\n",
            "Step 260: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69603, 'train_ucc_acc': 0.46875, 'loss': 0.84802}\n",
            "Step 280: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.70054, 'train_ucc_acc': 0.40625, 'loss': 0.85027}\n",
            "Step 300: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.70043, 'train_ucc_acc': 0.40625, 'loss': 0.85021}\n",
            "Step 320: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69579, 'train_ucc_acc': 0.46875, 'loss': 0.84788}\n",
            "Step 340: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.69362, 'train_ucc_acc': 0.5, 'loss': 0.84684}\n",
            "Step 360: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69173, 'train_ucc_acc': 0.53125, 'loss': 0.84588}\n",
            "Step 380: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.68166, 'train_ucc_acc': 0.71875, 'loss': 0.84084}\n",
            "Step 400: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69195, 'train_ucc_acc': 0.53125, 'loss': 0.84598}\n",
            "Step 420: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69336, 'train_ucc_acc': 0.5, 'loss': 0.8467}\n",
            "Step 440: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69211, 'train_ucc_acc': 0.53125, 'loss': 0.84605}\n",
            "Step 460: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69829, 'train_ucc_acc': 0.375, 'loss': 0.84916}\n",
            "Step 480: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.68884, 'train_ucc_acc': 0.625, 'loss': 0.84443}\n",
            "Step 500: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.6923, 'train_ucc_acc': 0.53125, 'loss': 0.84616}\n",
            "Step 520: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69017, 'train_ucc_acc': 0.59375, 'loss': 0.8451}\n",
            "Step 540: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69496, 'train_ucc_acc': 0.4375, 'loss': 0.84748}\n",
            "Step 560: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69014, 'train_ucc_acc': 0.65625, 'loss': 0.84508}\n",
            "Step 580: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69488, 'train_ucc_acc': 0.40625, 'loss': 0.84745}\n",
            "Step 600: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69074, 'train_ucc_acc': 0.65625, 'loss': 0.84537}\n",
            "Step 620: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69443, 'train_ucc_acc': 0.4375, 'loss': 0.84721}\n",
            "Step 640: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.695, 'train_ucc_acc': 0.40625, 'loss': 0.84751}\n",
            "Step 660: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69317, 'train_ucc_acc': 0.5, 'loss': 0.84658}\n",
            "Step 680: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69389, 'train_ucc_acc': 0.4375, 'loss': 0.84696}\n",
            "Step 700: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69508, 'train_ucc_acc': 0.34375, 'loss': 0.84755}\n",
            "Step 720: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69327, 'train_ucc_acc': 0.46875, 'loss': 0.84664}\n",
            "Step 740: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84657}\n",
            "Step 760: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84658}\n",
            "Step 780: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69288, 'train_ucc_acc': 0.625, 'loss': 0.84643}\n",
            "Step 800: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69272, 'train_ucc_acc': 0.5625, 'loss': 0.84636}\n",
            "Step 820: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69317, 'train_ucc_acc': 0.5, 'loss': 0.8466}\n",
            "Step 840: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69054, 'train_ucc_acc': 0.75, 'loss': 0.84527}\n",
            "Step 860: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69282, 'train_ucc_acc': 0.53125, 'loss': 0.84641}\n",
            "Step 880: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6944, 'train_ucc_acc': 0.40625, 'loss': 0.8472}\n",
            "Step 900: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69281, 'train_ucc_acc': 0.5625, 'loss': 0.8464}\n",
            "Step 920: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69207, 'train_ucc_acc': 0.625, 'loss': 0.84604}\n",
            "Step 940: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69318, 'train_ucc_acc': 0.5, 'loss': 0.84659}\n",
            "Step 960: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69198, 'train_ucc_acc': 0.5625, 'loss': 0.84599}\n",
            "Step 980: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69213, 'train_ucc_acc': 0.5625, 'loss': 0.84606}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.3, 'init_lower_bound': 0.2, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:50:35 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:50:40 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:50:40 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:50:44 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.0007, 'train_ucc_loss': 0.70339, 'train_ucc_acc': 0.46875, 'loss': 0.85204}\n",
            "Step 40: {'train_ae_loss': 1.0002, 'train_ucc_loss': 0.6862, 'train_ucc_acc': 0.5625, 'loss': 0.8432}\n",
            "Step 60: {'train_ae_loss': 1.0001, 'train_ucc_loss': 0.69134, 'train_ucc_acc': 0.53125, 'loss': 0.84572}\n",
            "Step 80: {'train_ae_loss': 1.00009, 'train_ucc_loss': 0.70548, 'train_ucc_acc': 0.4375, 'loss': 0.85279}\n",
            "Step 100: {'train_ae_loss': 1.00008, 'train_ucc_loss': 0.72173, 'train_ucc_acc': 0.3125, 'loss': 0.86091}\n",
            "Step 120: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.70327, 'train_ucc_acc': 0.4375, 'loss': 0.85166}\n",
            "Step 140: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.67251, 'train_ucc_acc': 0.6875, 'loss': 0.83627}\n",
            "Step 160: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.70195, 'train_ucc_acc': 0.4375, 'loss': 0.85099}\n",
            "Step 180: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.7154, 'train_ucc_acc': 0.3125, 'loss': 0.85771}\n",
            "Step 200: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.70423, 'train_ucc_acc': 0.40625, 'loss': 0.85214}\n",
            "Step 220: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69128, 'train_ucc_acc': 0.53125, 'loss': 0.84565}\n",
            "Step 240: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.70334, 'train_ucc_acc': 0.40625, 'loss': 0.8517}\n",
            "Step 260: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.68853, 'train_ucc_acc': 0.5625, 'loss': 0.84428}\n",
            "Step 280: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.70498, 'train_ucc_acc': 0.375, 'loss': 0.85252}\n",
            "Step 300: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69135, 'train_ucc_acc': 0.53125, 'loss': 0.8457}\n",
            "Step 320: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.68352, 'train_ucc_acc': 0.625, 'loss': 0.84178}\n",
            "Step 340: {'train_ae_loss': 1.00006, 'train_ucc_loss': 0.70101, 'train_ucc_acc': 0.40625, 'loss': 0.85054}\n",
            "Step 360: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.68755, 'train_ucc_acc': 0.59375, 'loss': 0.84379}\n",
            "Step 380: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69165, 'train_ucc_acc': 0.53125, 'loss': 0.84583}\n",
            "Step 400: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69167, 'train_ucc_acc': 0.53125, 'loss': 0.84585}\n",
            "Step 420: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.6937, 'train_ucc_acc': 0.5, 'loss': 0.84686}\n",
            "Step 440: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.70157, 'train_ucc_acc': 0.375, 'loss': 0.85079}\n",
            "Step 460: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69178, 'train_ucc_acc': 0.53125, 'loss': 0.8459}\n",
            "Step 480: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69517, 'train_ucc_acc': 0.46875, 'loss': 0.84759}\n",
            "Step 500: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69834, 'train_ucc_acc': 0.40625, 'loss': 0.84919}\n",
            "Step 520: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69345, 'train_ucc_acc': 0.5, 'loss': 0.84674}\n",
            "Step 540: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69484, 'train_ucc_acc': 0.46875, 'loss': 0.84743}\n",
            "Step 560: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.68646, 'train_ucc_acc': 0.65625, 'loss': 0.84322}\n",
            "Step 580: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.68817, 'train_ucc_acc': 0.625, 'loss': 0.84409}\n",
            "Step 600: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69329, 'train_ucc_acc': 0.5, 'loss': 0.84664}\n",
            "Step 620: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69032, 'train_ucc_acc': 0.59375, 'loss': 0.84517}\n",
            "Step 640: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69049, 'train_ucc_acc': 0.59375, 'loss': 0.84525}\n",
            "Step 660: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69064, 'train_ucc_acc': 0.59375, 'loss': 0.84532}\n",
            "Step 680: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69405, 'train_ucc_acc': 0.46875, 'loss': 0.84701}\n",
            "Step 700: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69265, 'train_ucc_acc': 0.53125, 'loss': 0.84634}\n",
            "Step 720: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69151, 'train_ucc_acc': 0.59375, 'loss': 0.84576}\n",
            "Step 740: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69226, 'train_ucc_acc': 0.5625, 'loss': 0.84615}\n",
            "Step 760: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69094, 'train_ucc_acc': 0.65625, 'loss': 0.84548}\n",
            "Step 780: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69271, 'train_ucc_acc': 0.53125, 'loss': 0.84637}\n",
            "Step 800: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.692, 'train_ucc_acc': 0.5625, 'loss': 0.846}\n",
            "Step 820: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.68751, 'train_ucc_acc': 0.71875, 'loss': 0.84377}\n",
            "Step 840: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69057, 'train_ucc_acc': 0.59375, 'loss': 0.84529}\n",
            "Step 860: {'train_ae_loss': 1.00005, 'train_ucc_loss': 0.69159, 'train_ucc_acc': 0.5625, 'loss': 0.84582}\n",
            "Step 880: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69238, 'train_ucc_acc': 0.53125, 'loss': 0.84621}\n",
            "Step 900: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69321, 'train_ucc_acc': 0.5, 'loss': 0.84659}\n",
            "Step 920: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.69432, 'train_ucc_acc': 0.4375, 'loss': 0.84716}\n",
            "Step 940: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69534, 'train_ucc_acc': 0.375, 'loss': 0.84768}\n",
            "Step 960: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69315, 'train_ucc_acc': 0.5, 'loss': 0.84657}\n",
            "Step 980: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69291, 'train_ucc_acc': 0.53125, 'loss': 0.84645}\n",
            "Training finished!!!\n",
            "{'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.3, 'init_lower_bound': 0.25, 'output_bins': 2}\n",
            "716864409634968403\n",
            "(100, 11)\n",
            "(100, 100)\n",
            "(2, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:59:39 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:59:44 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/09/09 04:59:44 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/09 04:59:49 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: {'train_ae_loss': 1.00096, 'train_ucc_loss': 0.68605, 'train_ucc_acc': 0.5625, 'loss': 0.8435}\n",
            "Step 40: {'train_ae_loss': 1.00036, 'train_ucc_loss': 0.67572, 'train_ucc_acc': 0.625, 'loss': 0.83804}\n",
            "Step 60: {'train_ae_loss': 1.00019, 'train_ucc_loss': 0.71452, 'train_ucc_acc': 0.375, 'loss': 0.85735}\n",
            "Step 80: {'train_ae_loss': 1.00011, 'train_ucc_loss': 0.71081, 'train_ucc_acc': 0.375, 'loss': 0.85546}\n",
            "Step 100: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.69443, 'train_ucc_acc': 0.5, 'loss': 0.84725}\n",
            "Step 120: {'train_ae_loss': 1.00008, 'train_ucc_loss': 0.69405, 'train_ucc_acc': 0.5, 'loss': 0.84707}\n",
            "Step 140: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.68339, 'train_ucc_acc': 0.625, 'loss': 0.84173}\n",
            "Step 160: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.69402, 'train_ucc_acc': 0.5, 'loss': 0.84704}\n",
            "Step 180: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.68673, 'train_ucc_acc': 0.59375, 'loss': 0.8434}\n",
            "Step 200: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69164, 'train_ucc_acc': 0.53125, 'loss': 0.84583}\n",
            "Step 220: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69694, 'train_ucc_acc': 0.4375, 'loss': 0.84848}\n",
            "Step 240: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.68955, 'train_ucc_acc': 0.59375, 'loss': 0.84479}\n",
            "Step 260: {'train_ae_loss': 1.00007, 'train_ucc_loss': 0.69225, 'train_ucc_acc': 0.53125, 'loss': 0.84616}\n",
            "Step 280: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69223, 'train_ucc_acc': 0.53125, 'loss': 0.84613}\n",
            "Step 300: {'train_ae_loss': 1.0, 'train_ucc_loss': 0.6876, 'train_ucc_acc': 0.6875, 'loss': 0.8438}\n",
            "Step 320: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69563, 'train_ucc_acc': 0.4375, 'loss': 0.84783}\n",
            "Step 340: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69592, 'train_ucc_acc': 0.4375, 'loss': 0.84798}\n",
            "Step 360: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.6954, 'train_ucc_acc': 0.4375, 'loss': 0.8477}\n",
            "Step 380: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69323, 'train_ucc_acc': 0.5, 'loss': 0.84661}\n",
            "Step 400: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69319, 'train_ucc_acc': 0.5, 'loss': 0.84661}\n",
            "Step 420: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69377, 'train_ucc_acc': 0.46875, 'loss': 0.84689}\n",
            "Step 440: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69279, 'train_ucc_acc': 0.53125, 'loss': 0.84641}\n",
            "Step 460: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69135, 'train_ucc_acc': 0.65625, 'loss': 0.84567}\n",
            "Step 480: {'train_ae_loss': 0.99999, 'train_ucc_loss': 0.69276, 'train_ucc_acc': 0.65625, 'loss': 0.84638}\n",
            "Step 500: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69194, 'train_ucc_acc': 0.65625, 'loss': 0.84597}\n",
            "Step 520: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69538, 'train_ucc_acc': 0.375, 'loss': 0.84769}\n",
            "Step 540: {'train_ae_loss': 0.99997, 'train_ucc_loss': 0.69349, 'train_ucc_acc': 0.46875, 'loss': 0.84673}\n",
            "Step 560: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69339, 'train_ucc_acc': 0.375, 'loss': 0.8467}\n",
            "Step 580: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69244, 'train_ucc_acc': 0.59375, 'loss': 0.84623}\n",
            "Step 600: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69173, 'train_ucc_acc': 0.5625, 'loss': 0.84588}\n",
            "Step 620: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69649, 'train_ucc_acc': 0.40625, 'loss': 0.84825}\n",
            "Step 640: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69324, 'train_ucc_acc': 0.5, 'loss': 0.84663}\n",
            "Step 660: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69176, 'train_ucc_acc': 0.5625, 'loss': 0.84589}\n",
            "Step 680: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69319, 'train_ucc_acc': 0.5, 'loss': 0.8466}\n",
            "Step 700: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69321, 'train_ucc_acc': 0.5, 'loss': 0.84661}\n",
            "Step 720: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69592, 'train_ucc_acc': 0.40625, 'loss': 0.84797}\n",
            "Step 740: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69502, 'train_ucc_acc': 0.4375, 'loss': 0.84752}\n",
            "Step 760: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69442, 'train_ucc_acc': 0.4375, 'loss': 0.84722}\n",
            "Step 780: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69455, 'train_ucc_acc': 0.4375, 'loss': 0.84729}\n",
            "Step 800: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69032, 'train_ucc_acc': 0.65625, 'loss': 0.84518}\n",
            "Step 820: {'train_ae_loss': 1.00002, 'train_ucc_loss': 0.69128, 'train_ucc_acc': 0.59375, 'loss': 0.84565}\n",
            "Step 840: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69167, 'train_ucc_acc': 0.5625, 'loss': 0.84585}\n",
            "Step 860: {'train_ae_loss': 0.99998, 'train_ucc_loss': 0.69016, 'train_ucc_acc': 0.625, 'loss': 0.84507}\n",
            "Step 880: {'train_ae_loss': 1.00003, 'train_ucc_loss': 0.69189, 'train_ucc_acc': 0.5625, 'loss': 0.84596}\n",
            "Step 900: {'train_ae_loss': 0.99996, 'train_ucc_loss': 0.69319, 'train_ucc_acc': 0.5, 'loss': 0.84657}\n",
            "Step 920: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.69389, 'train_ucc_acc': 0.46875, 'loss': 0.84695}\n",
            "Step 940: {'train_ae_loss': 1.00001, 'train_ucc_loss': 0.6893, 'train_ucc_acc': 0.6875, 'loss': 0.84465}\n",
            "Step 960: {'train_ae_loss': 0.99994, 'train_ucc_loss': 0.69239, 'train_ucc_acc': 0.53125, 'loss': 0.84616}\n",
            "Step 980: {'train_ae_loss': 1.00004, 'train_ucc_loss': 0.69183, 'train_ucc_acc': 0.5625, 'loss': 0.84593}\n",
            "Training finished!!!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "mlflow.set_tracking_uri(\"file:\\\\D:\\\\UCC-DRN-Pytorch\\\\camelyon\\\\mlruns\")\n",
        "\n",
        "run_name = \"camelyon-ucc-drn-search-init\"\n",
        "experiment_id = get_or_create_experiment(experiment_name=run_name)\n",
        "mlflow.set_experiment(experiment_id=experiment_id)\n",
        "last_lower_bound = 0\n",
        "last_upper_bound = 0.3\n",
        "for lower_bound in x:\n",
        "    lower_bound = np.round(lower_bound, 5)\n",
        "    for upper_bound in x:\n",
        "        upper_bound = np.round(upper_bound, 5)\n",
        "        if lower_bound<last_lower_bound:\n",
        "            continue\n",
        "        if lower_bound==last_lower_bound and upper_bound<last_upper_bound:\n",
        "            continue\n",
        "        if lower_bound >= upper_bound:\n",
        "            continue\n",
        "        # print(lower_bound)\n",
        "        # print(upper_bound)\n",
        "        with mlflow.start_run(nested=True) as run:\n",
        "            cfg.model.drn.init_lower_bound = float(lower_bound)\n",
        "            cfg.model.drn.init_upper_bound = float(upper_bound)\n",
        "            mlflow.log_params({\n",
        "                \"init_W_lower_bound\": float(lower_bound),\n",
        "                \"init_W_upper_bound\": float(upper_bound)\n",
        "            })\n",
        "            print(cfg.model.drn)\n",
        "            print(experiment_id)\n",
        "            cfg.args.learning_rate = 0.001\n",
        "            mlflow.log_dict(dict(OmegaConf.to_object(cfg)), \"config.yaml\")\n",
        "            args = cfg.args\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
        "            model, optimizer = init_model_and_optimizer(args, cfg, device)\n",
        "            train_loader, val_loader = init_dataloader(args)\n",
        "            artifact_path = run.info.artifact_uri\n",
        "            mlflow.pytorch.log_model(\n",
        "                    model,\n",
        "                    artifact_path = \"init_model\")\n",
        "            best_acc = train(args, model, optimizer, None,\n",
        "                            train_loader, val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.set_tracking_uri(\"file:///Users/tanguanyu/UCC-DRN-Pytorch/camelyon/mlruns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP-uF0b8vyWn"
      },
      "source": [
        "{'args': {'dataset': 'camelyon', 'model_dir': 'saved_models/', 'model_name': 'camelyon_ucc_drn', 'num_instances': 32, 'ucc_start': 1, 'ucc_end': 4, 'batch_size': 5, 'num_samples_per_class': 5, 'num_workers': 4, 'learning_rate': 0.0001, 'num_bins': 11, 'num_features': 10, 'train_num_steps': 100000, 'val_num_steps': 200, 'save_interval': 1000, 'patch_size': 32, 'seed': 22}, 'model': {'kde_model': {'num_bins': 11, 'sigma': 0.1}, 'num_channels': 3, 'encoder': {'conv_input_channel': 3, 'conv_output_channel': 16, 'block1_output_channel': 32, 'block1_num_layer': 1, 'block2_output_channel': 64, 'block2_num_layer': 1, 'block3_output_channel': 128, 'block3_num_layer': 1, 'flatten_size': 8192, 'num_features': 16}, 'decoder': {'linear_size': 8192, 'reshape_size': [128, 8, 8], 'block1_output_channel': 128, 'block1_num_layer': 1, 'block2_output_channel': 64, 'block2_num_layer': 1, 'block3_output_channel': 32, 'block3_num_layer': 1, 'output_channel': 3}, 'drn': {'num_bins': 11, 'hidden_q': 100, 'num_layers': 2, 'num_nodes': 9, 'init_method': 'uniform', 'init_upper_bound': 0.5, 'init_lower_bound': -0.5, 'output_bins': 4}, 'ucc_classifier': 'None', 'loss': {'alpha': 0.5}}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UExxHuIISwlh",
        "outputId": "670bda90-2541-459e-a298-3989979f04e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "39000\n",
            "training\n",
            "Step 39020: {'train_ae_loss': 0.6916, 'train_ucc_loss': 0.416, 'train_ucc_acc': 0.90625, 'loss': 0.5538}\n",
            "Step 39040: {'train_ae_loss': 0.67035, 'train_ucc_loss': 0.51921, 'train_ucc_acc': 0.8125, 'loss': 0.59478}\n",
            "Step 39060: {'train_ae_loss': 0.68061, 'train_ucc_loss': 0.41042, 'train_ucc_acc': 0.9375, 'loss': 0.54552}\n",
            "Step 39080: {'train_ae_loss': 0.67488, 'train_ucc_loss': 0.60488, 'train_ucc_acc': 0.6875, 'loss': 0.63988}\n",
            "Step 39100: {'train_ae_loss': 0.68034, 'train_ucc_loss': 0.57203, 'train_ucc_acc': 0.71875, 'loss': 0.62618}\n",
            "Step 39120: {'train_ae_loss': 0.68149, 'train_ucc_loss': 0.50473, 'train_ucc_acc': 0.8125, 'loss': 0.59311}\n",
            "Step 39140: {'train_ae_loss': 0.67443, 'train_ucc_loss': 0.44293, 'train_ucc_acc': 0.875, 'loss': 0.55868}\n",
            "Step 39160: {'train_ae_loss': 0.67169, 'train_ucc_loss': 0.48567, 'train_ucc_acc': 0.84375, 'loss': 0.57868}\n",
            "Step 39180: {'train_ae_loss': 0.68498, 'train_ucc_loss': 0.57264, 'train_ucc_acc': 0.71875, 'loss': 0.62881}\n",
            "Step 39200: {'train_ae_loss': 0.68728, 'train_ucc_loss': 0.43901, 'train_ucc_acc': 0.90625, 'loss': 0.56314}\n",
            "Step 39220: {'train_ae_loss': 0.67893, 'train_ucc_loss': 0.45625, 'train_ucc_acc': 0.875, 'loss': 0.56759}\n",
            "Step 39240: {'train_ae_loss': 0.67002, 'train_ucc_loss': 0.46782, 'train_ucc_acc': 0.84375, 'loss': 0.56892}\n",
            "Step 39260: {'train_ae_loss': 0.67555, 'train_ucc_loss': 0.53877, 'train_ucc_acc': 0.75, 'loss': 0.60716}\n",
            "Step 39280: {'train_ae_loss': 0.68003, 'train_ucc_loss': 0.51947, 'train_ucc_acc': 0.78125, 'loss': 0.59975}\n",
            "Step 39300: {'train_ae_loss': 0.67865, 'train_ucc_loss': 0.65149, 'train_ucc_acc': 0.65625, 'loss': 0.66507}\n",
            "Step 39320: {'train_ae_loss': 0.67624, 'train_ucc_loss': 0.45847, 'train_ucc_acc': 0.875, 'loss': 0.56735}\n",
            "Step 39340: {'train_ae_loss': 0.67955, 'train_ucc_loss': 0.45448, 'train_ucc_acc': 0.84375, 'loss': 0.56701}\n",
            "Step 39360: {'train_ae_loss': 0.68003, 'train_ucc_loss': 0.52777, 'train_ucc_acc': 0.75, 'loss': 0.6039}\n",
            "Step 39380: {'train_ae_loss': 0.67533, 'train_ucc_loss': 0.52179, 'train_ucc_acc': 0.78125, 'loss': 0.59856}\n",
            "Step 39400: {'train_ae_loss': 0.68642, 'train_ucc_loss': 0.54071, 'train_ucc_acc': 0.8125, 'loss': 0.61356}\n",
            "Step 39420: {'train_ae_loss': 0.68176, 'train_ucc_loss': 0.55822, 'train_ucc_acc': 0.75, 'loss': 0.61999}\n",
            "Step 39440: {'train_ae_loss': 0.68927, 'train_ucc_loss': 0.51463, 'train_ucc_acc': 0.78125, 'loss': 0.60195}\n",
            "Step 39460: {'train_ae_loss': 0.67631, 'train_ucc_loss': 0.42965, 'train_ucc_acc': 0.875, 'loss': 0.55298}\n",
            "Step 39480: {'train_ae_loss': 0.68087, 'train_ucc_loss': 0.48166, 'train_ucc_acc': 0.84375, 'loss': 0.58127}\n",
            "Step 39500: {'train_ae_loss': 0.6724, 'train_ucc_loss': 0.44018, 'train_ucc_acc': 0.875, 'loss': 0.55629}\n",
            "Step 39520: {'train_ae_loss': 0.69004, 'train_ucc_loss': 0.48538, 'train_ucc_acc': 0.8125, 'loss': 0.58771}\n",
            "Step 39540: {'train_ae_loss': 0.67067, 'train_ucc_loss': 0.49069, 'train_ucc_acc': 0.8125, 'loss': 0.58068}\n",
            "Step 39560: {'train_ae_loss': 0.67055, 'train_ucc_loss': 0.46634, 'train_ucc_acc': 0.8125, 'loss': 0.56844}\n",
            "Step 39580: {'train_ae_loss': 0.67556, 'train_ucc_loss': 0.46096, 'train_ucc_acc': 0.8125, 'loss': 0.56826}\n",
            "Step 39600: {'train_ae_loss': 0.68007, 'train_ucc_loss': 0.5302, 'train_ucc_acc': 0.78125, 'loss': 0.60513}\n",
            "Step 39620: {'train_ae_loss': 0.6849, 'train_ucc_loss': 0.42516, 'train_ucc_acc': 0.90625, 'loss': 0.55503}\n",
            "Step 39640: {'train_ae_loss': 0.66862, 'train_ucc_loss': 0.48929, 'train_ucc_acc': 0.8125, 'loss': 0.57895}\n",
            "Step 39660: {'train_ae_loss': 0.6833, 'train_ucc_loss': 0.54029, 'train_ucc_acc': 0.78125, 'loss': 0.61179}\n",
            "Step 39680: {'train_ae_loss': 0.69109, 'train_ucc_loss': 0.48129, 'train_ucc_acc': 0.84375, 'loss': 0.58619}\n",
            "Step 39700: {'train_ae_loss': 0.66732, 'train_ucc_loss': 0.41265, 'train_ucc_acc': 0.90625, 'loss': 0.53998}\n",
            "Step 39720: {'train_ae_loss': 0.68175, 'train_ucc_loss': 0.52474, 'train_ucc_acc': 0.8125, 'loss': 0.60324}\n",
            "Step 39740: {'train_ae_loss': 0.67934, 'train_ucc_loss': 0.41337, 'train_ucc_acc': 0.9375, 'loss': 0.54635}\n",
            "Step 39760: {'train_ae_loss': 0.69445, 'train_ucc_loss': 0.59743, 'train_ucc_acc': 0.6875, 'loss': 0.64594}\n",
            "Step 39780: {'train_ae_loss': 0.68315, 'train_ucc_loss': 0.48734, 'train_ucc_acc': 0.8125, 'loss': 0.58525}\n",
            "Step 39800: {'train_ae_loss': 0.68528, 'train_ucc_loss': 0.55931, 'train_ucc_acc': 0.71875, 'loss': 0.62229}\n",
            "Step 39820: {'train_ae_loss': 0.68333, 'train_ucc_loss': 0.51749, 'train_ucc_acc': 0.78125, 'loss': 0.60041}\n",
            "Step 39840: {'train_ae_loss': 0.69345, 'train_ucc_loss': 0.40207, 'train_ucc_acc': 0.90625, 'loss': 0.54776}\n",
            "Step 39860: {'train_ae_loss': 0.67274, 'train_ucc_loss': 0.43681, 'train_ucc_acc': 0.875, 'loss': 0.55477}\n",
            "Step 39880: {'train_ae_loss': 0.6815, 'train_ucc_loss': 0.51653, 'train_ucc_acc': 0.8125, 'loss': 0.59901}\n",
            "Step 39900: {'train_ae_loss': 0.6948, 'train_ucc_loss': 0.55942, 'train_ucc_acc': 0.75, 'loss': 0.62711}\n",
            "Step 39920: {'train_ae_loss': 0.67155, 'train_ucc_loss': 0.62006, 'train_ucc_acc': 0.65625, 'loss': 0.6458}\n",
            "Step 39940: {'train_ae_loss': 0.68089, 'train_ucc_loss': 0.50008, 'train_ucc_acc': 0.8125, 'loss': 0.59048}\n",
            "Step 39960: {'train_ae_loss': 0.66914, 'train_ucc_loss': 0.56496, 'train_ucc_acc': 0.78125, 'loss': 0.61705}\n",
            "Step 39980: {'train_ae_loss': 0.66959, 'train_ucc_loss': 0.52198, 'train_ucc_acc': 0.78125, 'loss': 0.59579}\n",
            "Step 40000: {'train_ae_loss': 0.69114, 'train_ucc_loss': 0.39495, 'train_ucc_acc': 0.9375, 'loss': 0.54305}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 13:12:18 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 40000,eval_ae_loss: 0.67765,eval_ucc_loss: 0.51457,eval_ucc_acc: 0.7959\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 13:12:28 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 40020: {'train_ae_loss': 0.68062, 'train_ucc_loss': 0.58808, 'train_ucc_acc': 0.75, 'loss': 0.63435}\n",
            "Step 40040: {'train_ae_loss': 0.69193, 'train_ucc_loss': 0.43797, 'train_ucc_acc': 0.875, 'loss': 0.56495}\n",
            "Step 40060: {'train_ae_loss': 0.66665, 'train_ucc_loss': 0.54457, 'train_ucc_acc': 0.8125, 'loss': 0.60561}\n",
            "Step 40080: {'train_ae_loss': 0.68385, 'train_ucc_loss': 0.52384, 'train_ucc_acc': 0.75, 'loss': 0.60384}\n",
            "Step 40100: {'train_ae_loss': 0.67599, 'train_ucc_loss': 0.59935, 'train_ucc_acc': 0.71875, 'loss': 0.63767}\n",
            "Step 40120: {'train_ae_loss': 0.6861, 'train_ucc_loss': 0.5231, 'train_ucc_acc': 0.75, 'loss': 0.6046}\n",
            "Step 40140: {'train_ae_loss': 0.67646, 'train_ucc_loss': 0.58163, 'train_ucc_acc': 0.71875, 'loss': 0.62904}\n",
            "Step 40160: {'train_ae_loss': 0.66883, 'train_ucc_loss': 0.52022, 'train_ucc_acc': 0.8125, 'loss': 0.59452}\n",
            "Step 40180: {'train_ae_loss': 0.6748, 'train_ucc_loss': 0.57497, 'train_ucc_acc': 0.6875, 'loss': 0.62488}\n",
            "Step 40200: {'train_ae_loss': 0.68534, 'train_ucc_loss': 0.46582, 'train_ucc_acc': 0.84375, 'loss': 0.57558}\n",
            "Step 40220: {'train_ae_loss': 0.67852, 'train_ucc_loss': 0.50547, 'train_ucc_acc': 0.8125, 'loss': 0.592}\n",
            "Step 40240: {'train_ae_loss': 0.67471, 'train_ucc_loss': 0.58876, 'train_ucc_acc': 0.6875, 'loss': 0.63173}\n",
            "Step 40260: {'train_ae_loss': 0.67595, 'train_ucc_loss': 0.51921, 'train_ucc_acc': 0.78125, 'loss': 0.59758}\n",
            "Step 40280: {'train_ae_loss': 0.65835, 'train_ucc_loss': 0.54353, 'train_ucc_acc': 0.75, 'loss': 0.60094}\n",
            "Step 40300: {'train_ae_loss': 0.67851, 'train_ucc_loss': 0.49872, 'train_ucc_acc': 0.8125, 'loss': 0.58861}\n",
            "Step 40320: {'train_ae_loss': 0.66951, 'train_ucc_loss': 0.50681, 'train_ucc_acc': 0.8125, 'loss': 0.58816}\n",
            "Step 40340: {'train_ae_loss': 0.67004, 'train_ucc_loss': 0.53858, 'train_ucc_acc': 0.78125, 'loss': 0.60431}\n",
            "Step 40360: {'train_ae_loss': 0.6565, 'train_ucc_loss': 0.49887, 'train_ucc_acc': 0.8125, 'loss': 0.57769}\n",
            "Step 40380: {'train_ae_loss': 0.66495, 'train_ucc_loss': 0.44691, 'train_ucc_acc': 0.875, 'loss': 0.55593}\n",
            "Step 40400: {'train_ae_loss': 0.665, 'train_ucc_loss': 0.51219, 'train_ucc_acc': 0.75, 'loss': 0.58859}\n",
            "Step 40420: {'train_ae_loss': 0.66979, 'train_ucc_loss': 0.40516, 'train_ucc_acc': 0.9375, 'loss': 0.53748}\n",
            "Step 40440: {'train_ae_loss': 0.68009, 'train_ucc_loss': 0.47497, 'train_ucc_acc': 0.8125, 'loss': 0.57753}\n",
            "Step 40460: {'train_ae_loss': 0.68156, 'train_ucc_loss': 0.44204, 'train_ucc_acc': 0.84375, 'loss': 0.5618}\n",
            "Step 40480: {'train_ae_loss': 0.71077, 'train_ucc_loss': 0.49724, 'train_ucc_acc': 0.84375, 'loss': 0.60401}\n",
            "Step 40500: {'train_ae_loss': 0.68109, 'train_ucc_loss': 0.48516, 'train_ucc_acc': 0.8125, 'loss': 0.58312}\n",
            "Step 40520: {'train_ae_loss': 0.68886, 'train_ucc_loss': 0.447, 'train_ucc_acc': 0.875, 'loss': 0.56793}\n",
            "Step 40540: {'train_ae_loss': 0.68231, 'train_ucc_loss': 0.59888, 'train_ucc_acc': 0.6875, 'loss': 0.6406}\n",
            "Step 40560: {'train_ae_loss': 0.66998, 'train_ucc_loss': 0.55368, 'train_ucc_acc': 0.78125, 'loss': 0.61183}\n",
            "Step 40580: {'train_ae_loss': 0.67669, 'train_ucc_loss': 0.51041, 'train_ucc_acc': 0.78125, 'loss': 0.59355}\n",
            "Step 40600: {'train_ae_loss': 0.67487, 'train_ucc_loss': 0.60454, 'train_ucc_acc': 0.6875, 'loss': 0.6397}\n",
            "Step 40620: {'train_ae_loss': 0.66241, 'train_ucc_loss': 0.75398, 'train_ucc_acc': 0.53125, 'loss': 0.70819}\n",
            "Step 40640: {'train_ae_loss': 0.68948, 'train_ucc_loss': 0.50122, 'train_ucc_acc': 0.8125, 'loss': 0.59535}\n",
            "Step 40660: {'train_ae_loss': 0.65939, 'train_ucc_loss': 0.62196, 'train_ucc_acc': 0.6875, 'loss': 0.64067}\n",
            "Step 40680: {'train_ae_loss': 0.67598, 'train_ucc_loss': 0.51041, 'train_ucc_acc': 0.78125, 'loss': 0.59319}\n",
            "Step 40700: {'train_ae_loss': 0.68311, 'train_ucc_loss': 0.5761, 'train_ucc_acc': 0.6875, 'loss': 0.62961}\n",
            "Step 40720: {'train_ae_loss': 0.67436, 'train_ucc_loss': 0.47404, 'train_ucc_acc': 0.84375, 'loss': 0.5742}\n",
            "Step 40740: {'train_ae_loss': 0.65857, 'train_ucc_loss': 0.53216, 'train_ucc_acc': 0.78125, 'loss': 0.59536}\n",
            "Step 40760: {'train_ae_loss': 0.669, 'train_ucc_loss': 0.50859, 'train_ucc_acc': 0.8125, 'loss': 0.58879}\n",
            "Step 40780: {'train_ae_loss': 0.66974, 'train_ucc_loss': 0.46087, 'train_ucc_acc': 0.8125, 'loss': 0.5653}\n",
            "Step 40800: {'train_ae_loss': 0.67402, 'train_ucc_loss': 0.36295, 'train_ucc_acc': 1.0, 'loss': 0.51848}\n",
            "Step 40820: {'train_ae_loss': 0.65721, 'train_ucc_loss': 0.47414, 'train_ucc_acc': 0.8125, 'loss': 0.56568}\n",
            "Step 40840: {'train_ae_loss': 0.66847, 'train_ucc_loss': 0.54325, 'train_ucc_acc': 0.78125, 'loss': 0.60586}\n",
            "Step 40860: {'train_ae_loss': 0.68497, 'train_ucc_loss': 0.43337, 'train_ucc_acc': 0.875, 'loss': 0.55917}\n",
            "Step 40880: {'train_ae_loss': 0.678, 'train_ucc_loss': 0.43058, 'train_ucc_acc': 0.90625, 'loss': 0.55429}\n",
            "Step 40900: {'train_ae_loss': 0.67692, 'train_ucc_loss': 0.57673, 'train_ucc_acc': 0.71875, 'loss': 0.62682}\n",
            "Step 40920: {'train_ae_loss': 0.68687, 'train_ucc_loss': 0.45977, 'train_ucc_acc': 0.875, 'loss': 0.57332}\n",
            "Step 40940: {'train_ae_loss': 0.66898, 'train_ucc_loss': 0.51511, 'train_ucc_acc': 0.78125, 'loss': 0.59205}\n",
            "Step 40960: {'train_ae_loss': 0.67818, 'train_ucc_loss': 0.50387, 'train_ucc_acc': 0.8125, 'loss': 0.59103}\n",
            "Step 40980: {'train_ae_loss': 0.67692, 'train_ucc_loss': 0.45001, 'train_ucc_acc': 0.84375, 'loss': 0.56346}\n",
            "Step 41000: {'train_ae_loss': 0.69218, 'train_ucc_loss': 0.40111, 'train_ucc_acc': 0.90625, 'loss': 0.54664}\n",
            "step: 41000,eval_ae_loss: 0.67748,eval_ucc_loss: 0.53992,eval_ucc_acc: 0.76367\n",
            "Step 41020: {'train_ae_loss': 0.67011, 'train_ucc_loss': 0.5195, 'train_ucc_acc': 0.78125, 'loss': 0.5948}\n",
            "Step 41040: {'train_ae_loss': 0.67729, 'train_ucc_loss': 0.54812, 'train_ucc_acc': 0.75, 'loss': 0.6127}\n",
            "Step 41060: {'train_ae_loss': 0.67898, 'train_ucc_loss': 0.52925, 'train_ucc_acc': 0.78125, 'loss': 0.60412}\n",
            "Step 41080: {'train_ae_loss': 0.66954, 'train_ucc_loss': 0.55532, 'train_ucc_acc': 0.8125, 'loss': 0.61243}\n",
            "Step 41100: {'train_ae_loss': 0.68744, 'train_ucc_loss': 0.47572, 'train_ucc_acc': 0.84375, 'loss': 0.58158}\n",
            "Step 41120: {'train_ae_loss': 0.68899, 'train_ucc_loss': 0.4854, 'train_ucc_acc': 0.84375, 'loss': 0.5872}\n",
            "Step 41140: {'train_ae_loss': 0.67982, 'train_ucc_loss': 0.45046, 'train_ucc_acc': 0.875, 'loss': 0.56514}\n",
            "Step 41160: {'train_ae_loss': 0.68322, 'train_ucc_loss': 0.52172, 'train_ucc_acc': 0.78125, 'loss': 0.60247}\n",
            "Step 41180: {'train_ae_loss': 0.67079, 'train_ucc_loss': 0.48411, 'train_ucc_acc': 0.8125, 'loss': 0.57745}\n",
            "Step 41200: {'train_ae_loss': 0.67927, 'train_ucc_loss': 0.50523, 'train_ucc_acc': 0.8125, 'loss': 0.59225}\n",
            "Step 41220: {'train_ae_loss': 0.67433, 'train_ucc_loss': 0.45356, 'train_ucc_acc': 0.84375, 'loss': 0.56395}\n",
            "Step 41240: {'train_ae_loss': 0.67921, 'train_ucc_loss': 0.54316, 'train_ucc_acc': 0.71875, 'loss': 0.61118}\n",
            "Step 41260: {'train_ae_loss': 0.65881, 'train_ucc_loss': 0.55739, 'train_ucc_acc': 0.75, 'loss': 0.6081}\n",
            "Step 41280: {'train_ae_loss': 0.67916, 'train_ucc_loss': 0.49901, 'train_ucc_acc': 0.8125, 'loss': 0.58908}\n",
            "Step 41300: {'train_ae_loss': 0.67784, 'train_ucc_loss': 0.41949, 'train_ucc_acc': 0.875, 'loss': 0.54866}\n",
            "Step 41320: {'train_ae_loss': 0.6578, 'train_ucc_loss': 0.47149, 'train_ucc_acc': 0.875, 'loss': 0.56465}\n",
            "Step 41340: {'train_ae_loss': 0.67488, 'train_ucc_loss': 0.48692, 'train_ucc_acc': 0.8125, 'loss': 0.5809}\n",
            "Step 41360: {'train_ae_loss': 0.68282, 'train_ucc_loss': 0.47847, 'train_ucc_acc': 0.84375, 'loss': 0.58065}\n",
            "Step 41380: {'train_ae_loss': 0.6558, 'train_ucc_loss': 0.60797, 'train_ucc_acc': 0.71875, 'loss': 0.63188}\n",
            "Step 41400: {'train_ae_loss': 0.68138, 'train_ucc_loss': 0.44807, 'train_ucc_acc': 0.84375, 'loss': 0.56472}\n",
            "Step 41420: {'train_ae_loss': 0.68115, 'train_ucc_loss': 0.56661, 'train_ucc_acc': 0.75, 'loss': 0.62388}\n",
            "Step 41440: {'train_ae_loss': 0.6701, 'train_ucc_loss': 0.58442, 'train_ucc_acc': 0.6875, 'loss': 0.62726}\n",
            "Step 41460: {'train_ae_loss': 0.65427, 'train_ucc_loss': 0.50743, 'train_ucc_acc': 0.78125, 'loss': 0.58085}\n",
            "Step 41480: {'train_ae_loss': 0.67481, 'train_ucc_loss': 0.54765, 'train_ucc_acc': 0.75, 'loss': 0.61123}\n",
            "Step 41500: {'train_ae_loss': 0.67028, 'train_ucc_loss': 0.46391, 'train_ucc_acc': 0.8125, 'loss': 0.5671}\n",
            "Step 41520: {'train_ae_loss': 0.66795, 'train_ucc_loss': 0.54517, 'train_ucc_acc': 0.78125, 'loss': 0.60656}\n",
            "Step 41540: {'train_ae_loss': 0.68012, 'train_ucc_loss': 0.51783, 'train_ucc_acc': 0.75, 'loss': 0.59897}\n",
            "Step 41560: {'train_ae_loss': 0.65738, 'train_ucc_loss': 0.42612, 'train_ucc_acc': 0.875, 'loss': 0.54175}\n",
            "Step 41580: {'train_ae_loss': 0.67016, 'train_ucc_loss': 0.6444, 'train_ucc_acc': 0.625, 'loss': 0.65728}\n",
            "Step 41600: {'train_ae_loss': 0.68199, 'train_ucc_loss': 0.48742, 'train_ucc_acc': 0.84375, 'loss': 0.58471}\n",
            "Step 41620: {'train_ae_loss': 0.65672, 'train_ucc_loss': 0.43663, 'train_ucc_acc': 0.90625, 'loss': 0.54667}\n",
            "Step 41640: {'train_ae_loss': 0.66017, 'train_ucc_loss': 0.501, 'train_ucc_acc': 0.78125, 'loss': 0.58059}\n",
            "Step 41660: {'train_ae_loss': 0.67785, 'train_ucc_loss': 0.54035, 'train_ucc_acc': 0.78125, 'loss': 0.6091}\n",
            "Step 41680: {'train_ae_loss': 0.68803, 'train_ucc_loss': 0.41853, 'train_ucc_acc': 0.90625, 'loss': 0.55328}\n",
            "Step 41700: {'train_ae_loss': 0.65614, 'train_ucc_loss': 0.40265, 'train_ucc_acc': 0.9375, 'loss': 0.52939}\n",
            "Step 41720: {'train_ae_loss': 0.67269, 'train_ucc_loss': 0.63075, 'train_ucc_acc': 0.625, 'loss': 0.65172}\n",
            "Step 41740: {'train_ae_loss': 0.66014, 'train_ucc_loss': 0.49077, 'train_ucc_acc': 0.8125, 'loss': 0.57546}\n",
            "Step 41760: {'train_ae_loss': 0.65913, 'train_ucc_loss': 0.45047, 'train_ucc_acc': 0.875, 'loss': 0.5548}\n",
            "Step 41780: {'train_ae_loss': 0.64317, 'train_ucc_loss': 0.54728, 'train_ucc_acc': 0.78125, 'loss': 0.59522}\n",
            "Step 41800: {'train_ae_loss': 0.66904, 'train_ucc_loss': 0.62336, 'train_ucc_acc': 0.65625, 'loss': 0.6462}\n",
            "Step 41820: {'train_ae_loss': 0.66891, 'train_ucc_loss': 0.44869, 'train_ucc_acc': 0.90625, 'loss': 0.5588}\n",
            "Step 41840: {'train_ae_loss': 0.67672, 'train_ucc_loss': 0.40137, 'train_ucc_acc': 0.9375, 'loss': 0.53905}\n",
            "Step 41860: {'train_ae_loss': 0.68044, 'train_ucc_loss': 0.4301, 'train_ucc_acc': 0.90625, 'loss': 0.55527}\n",
            "Step 41880: {'train_ae_loss': 0.66457, 'train_ucc_loss': 0.46732, 'train_ucc_acc': 0.84375, 'loss': 0.56595}\n",
            "Step 41900: {'train_ae_loss': 0.6815, 'train_ucc_loss': 0.60418, 'train_ucc_acc': 0.6875, 'loss': 0.64284}\n",
            "Step 41920: {'train_ae_loss': 0.66394, 'train_ucc_loss': 0.48313, 'train_ucc_acc': 0.8125, 'loss': 0.57354}\n",
            "Step 41940: {'train_ae_loss': 0.66338, 'train_ucc_loss': 0.47434, 'train_ucc_acc': 0.8125, 'loss': 0.56886}\n",
            "Step 41960: {'train_ae_loss': 0.66455, 'train_ucc_loss': 0.42172, 'train_ucc_acc': 0.90625, 'loss': 0.54314}\n",
            "Step 41980: {'train_ae_loss': 0.66045, 'train_ucc_loss': 0.53942, 'train_ucc_acc': 0.71875, 'loss': 0.59994}\n",
            "Step 42000: {'train_ae_loss': 0.66834, 'train_ucc_loss': 0.46598, 'train_ucc_acc': 0.875, 'loss': 0.56716}\n",
            "step: 42000,eval_ae_loss: 0.66134,eval_ucc_loss: 0.50888,eval_ucc_acc: 0.79395\n",
            "Step 42020: {'train_ae_loss': 0.67385, 'train_ucc_loss': 0.48336, 'train_ucc_acc': 0.8125, 'loss': 0.57861}\n",
            "Step 42040: {'train_ae_loss': 0.67059, 'train_ucc_loss': 0.42652, 'train_ucc_acc': 0.875, 'loss': 0.54855}\n",
            "Step 42060: {'train_ae_loss': 0.67526, 'train_ucc_loss': 0.46603, 'train_ucc_acc': 0.84375, 'loss': 0.57065}\n",
            "Step 42080: {'train_ae_loss': 0.66422, 'train_ucc_loss': 0.5168, 'train_ucc_acc': 0.8125, 'loss': 0.59051}\n",
            "Step 42100: {'train_ae_loss': 0.67314, 'train_ucc_loss': 0.57088, 'train_ucc_acc': 0.71875, 'loss': 0.62201}\n",
            "Step 42120: {'train_ae_loss': 0.67403, 'train_ucc_loss': 0.46711, 'train_ucc_acc': 0.8125, 'loss': 0.57057}\n",
            "Step 42140: {'train_ae_loss': 0.66018, 'train_ucc_loss': 0.51472, 'train_ucc_acc': 0.78125, 'loss': 0.58745}\n",
            "Step 42160: {'train_ae_loss': 0.65907, 'train_ucc_loss': 0.44546, 'train_ucc_acc': 0.90625, 'loss': 0.55226}\n",
            "Step 42180: {'train_ae_loss': 0.6868, 'train_ucc_loss': 0.55267, 'train_ucc_acc': 0.75, 'loss': 0.61974}\n",
            "Step 42200: {'train_ae_loss': 0.65863, 'train_ucc_loss': 0.49836, 'train_ucc_acc': 0.78125, 'loss': 0.57849}\n",
            "Step 42220: {'train_ae_loss': 0.66929, 'train_ucc_loss': 0.50932, 'train_ucc_acc': 0.8125, 'loss': 0.5893}\n",
            "Step 42240: {'train_ae_loss': 0.6833, 'train_ucc_loss': 0.49066, 'train_ucc_acc': 0.8125, 'loss': 0.58698}\n",
            "Step 42260: {'train_ae_loss': 0.65769, 'train_ucc_loss': 0.50084, 'train_ucc_acc': 0.8125, 'loss': 0.57926}\n",
            "Step 42280: {'train_ae_loss': 0.66934, 'train_ucc_loss': 0.45693, 'train_ucc_acc': 0.875, 'loss': 0.56313}\n",
            "Step 42300: {'train_ae_loss': 0.67323, 'train_ucc_loss': 0.45631, 'train_ucc_acc': 0.90625, 'loss': 0.56477}\n",
            "Step 42320: {'train_ae_loss': 0.68502, 'train_ucc_loss': 0.47399, 'train_ucc_acc': 0.84375, 'loss': 0.57951}\n",
            "Step 42340: {'train_ae_loss': 0.64177, 'train_ucc_loss': 0.49735, 'train_ucc_acc': 0.8125, 'loss': 0.56956}\n",
            "Step 42360: {'train_ae_loss': 0.68782, 'train_ucc_loss': 0.45098, 'train_ucc_acc': 0.875, 'loss': 0.5694}\n",
            "Step 42380: {'train_ae_loss': 0.68057, 'train_ucc_loss': 0.62627, 'train_ucc_acc': 0.65625, 'loss': 0.65342}\n",
            "Step 42400: {'train_ae_loss': 0.66695, 'train_ucc_loss': 0.4671, 'train_ucc_acc': 0.84375, 'loss': 0.56703}\n",
            "Step 42420: {'train_ae_loss': 0.67455, 'train_ucc_loss': 0.44257, 'train_ucc_acc': 0.875, 'loss': 0.55856}\n",
            "Step 42440: {'train_ae_loss': 0.66785, 'train_ucc_loss': 0.54607, 'train_ucc_acc': 0.78125, 'loss': 0.60696}\n",
            "Step 42460: {'train_ae_loss': 0.66393, 'train_ucc_loss': 0.4735, 'train_ucc_acc': 0.875, 'loss': 0.56871}\n",
            "Step 42480: {'train_ae_loss': 0.66635, 'train_ucc_loss': 0.39884, 'train_ucc_acc': 0.90625, 'loss': 0.53259}\n",
            "Step 42500: {'train_ae_loss': 0.67743, 'train_ucc_loss': 0.56972, 'train_ucc_acc': 0.6875, 'loss': 0.62357}\n",
            "Step 42520: {'train_ae_loss': 0.67616, 'train_ucc_loss': 0.52507, 'train_ucc_acc': 0.75, 'loss': 0.60062}\n",
            "Step 42540: {'train_ae_loss': 0.67635, 'train_ucc_loss': 0.46305, 'train_ucc_acc': 0.84375, 'loss': 0.5697}\n",
            "Step 42560: {'train_ae_loss': 0.67243, 'train_ucc_loss': 0.70559, 'train_ucc_acc': 0.5625, 'loss': 0.68901}\n",
            "Step 42580: {'train_ae_loss': 0.67742, 'train_ucc_loss': 0.48584, 'train_ucc_acc': 0.84375, 'loss': 0.58163}\n",
            "Step 42600: {'train_ae_loss': 0.65768, 'train_ucc_loss': 0.54832, 'train_ucc_acc': 0.78125, 'loss': 0.603}\n",
            "Step 42620: {'train_ae_loss': 0.66761, 'train_ucc_loss': 0.45596, 'train_ucc_acc': 0.875, 'loss': 0.56179}\n",
            "Step 42640: {'train_ae_loss': 0.66114, 'train_ucc_loss': 0.55589, 'train_ucc_acc': 0.71875, 'loss': 0.60851}\n",
            "Step 42660: {'train_ae_loss': 0.67815, 'train_ucc_loss': 0.48055, 'train_ucc_acc': 0.8125, 'loss': 0.57935}\n",
            "Step 42680: {'train_ae_loss': 0.65493, 'train_ucc_loss': 0.52716, 'train_ucc_acc': 0.78125, 'loss': 0.59105}\n",
            "Step 42700: {'train_ae_loss': 0.66629, 'train_ucc_loss': 0.43905, 'train_ucc_acc': 0.875, 'loss': 0.55267}\n",
            "Step 42720: {'train_ae_loss': 0.68099, 'train_ucc_loss': 0.55377, 'train_ucc_acc': 0.71875, 'loss': 0.61738}\n",
            "Step 42740: {'train_ae_loss': 0.6735, 'train_ucc_loss': 0.46172, 'train_ucc_acc': 0.875, 'loss': 0.56761}\n",
            "Step 42760: {'train_ae_loss': 0.67132, 'train_ucc_loss': 0.50175, 'train_ucc_acc': 0.78125, 'loss': 0.58654}\n",
            "Step 42780: {'train_ae_loss': 0.67078, 'train_ucc_loss': 0.46975, 'train_ucc_acc': 0.84375, 'loss': 0.57027}\n",
            "Step 42800: {'train_ae_loss': 0.66306, 'train_ucc_loss': 0.5647, 'train_ucc_acc': 0.75, 'loss': 0.61388}\n",
            "Step 42820: {'train_ae_loss': 0.66157, 'train_ucc_loss': 0.60262, 'train_ucc_acc': 0.6875, 'loss': 0.63209}\n",
            "Step 42840: {'train_ae_loss': 0.65775, 'train_ucc_loss': 0.56398, 'train_ucc_acc': 0.75, 'loss': 0.61087}\n",
            "Step 42860: {'train_ae_loss': 0.69054, 'train_ucc_loss': 0.52792, 'train_ucc_acc': 0.75, 'loss': 0.60923}\n",
            "Step 42880: {'train_ae_loss': 0.65869, 'train_ucc_loss': 0.4564, 'train_ucc_acc': 0.875, 'loss': 0.55754}\n",
            "Step 42900: {'train_ae_loss': 0.67808, 'train_ucc_loss': 0.50356, 'train_ucc_acc': 0.78125, 'loss': 0.59082}\n",
            "Step 42920: {'train_ae_loss': 0.67906, 'train_ucc_loss': 0.53658, 'train_ucc_acc': 0.78125, 'loss': 0.60782}\n",
            "Step 42940: {'train_ae_loss': 0.67147, 'train_ucc_loss': 0.49214, 'train_ucc_acc': 0.84375, 'loss': 0.5818}\n",
            "Step 42960: {'train_ae_loss': 0.67521, 'train_ucc_loss': 0.45425, 'train_ucc_acc': 0.84375, 'loss': 0.56473}\n",
            "Step 42980: {'train_ae_loss': 0.68106, 'train_ucc_loss': 0.47557, 'train_ucc_acc': 0.875, 'loss': 0.57831}\n",
            "Step 43000: {'train_ae_loss': 0.66851, 'train_ucc_loss': 0.46697, 'train_ucc_acc': 0.84375, 'loss': 0.56774}\n",
            "step: 43000,eval_ae_loss: 0.67133,eval_ucc_loss: 0.60222,eval_ucc_acc: 0.69531\n",
            "Step 43020: {'train_ae_loss': 0.65887, 'train_ucc_loss': 0.50162, 'train_ucc_acc': 0.75, 'loss': 0.58025}\n",
            "Step 43040: {'train_ae_loss': 0.67102, 'train_ucc_loss': 0.5464, 'train_ucc_acc': 0.71875, 'loss': 0.60871}\n",
            "Step 43060: {'train_ae_loss': 0.68747, 'train_ucc_loss': 0.5446, 'train_ucc_acc': 0.78125, 'loss': 0.61603}\n",
            "Step 43080: {'train_ae_loss': 0.66507, 'train_ucc_loss': 0.46331, 'train_ucc_acc': 0.875, 'loss': 0.56419}\n",
            "Step 43100: {'train_ae_loss': 0.65858, 'train_ucc_loss': 0.42356, 'train_ucc_acc': 0.90625, 'loss': 0.54107}\n",
            "Step 43120: {'train_ae_loss': 0.67203, 'train_ucc_loss': 0.48687, 'train_ucc_acc': 0.8125, 'loss': 0.57945}\n",
            "Step 43140: {'train_ae_loss': 0.66752, 'train_ucc_loss': 0.46953, 'train_ucc_acc': 0.84375, 'loss': 0.56852}\n",
            "Step 43160: {'train_ae_loss': 0.66811, 'train_ucc_loss': 0.55175, 'train_ucc_acc': 0.75, 'loss': 0.60993}\n",
            "Step 43180: {'train_ae_loss': 0.68269, 'train_ucc_loss': 0.41941, 'train_ucc_acc': 0.90625, 'loss': 0.55105}\n",
            "Step 43200: {'train_ae_loss': 0.68539, 'train_ucc_loss': 0.46554, 'train_ucc_acc': 0.8125, 'loss': 0.57546}\n",
            "Step 43220: {'train_ae_loss': 0.67784, 'train_ucc_loss': 0.42059, 'train_ucc_acc': 0.90625, 'loss': 0.54921}\n",
            "Step 43240: {'train_ae_loss': 0.68179, 'train_ucc_loss': 0.5039, 'train_ucc_acc': 0.8125, 'loss': 0.59284}\n",
            "Step 43260: {'train_ae_loss': 0.66489, 'train_ucc_loss': 0.51099, 'train_ucc_acc': 0.78125, 'loss': 0.58794}\n",
            "Step 43280: {'train_ae_loss': 0.69219, 'train_ucc_loss': 0.33991, 'train_ucc_acc': 1.0, 'loss': 0.51605}\n",
            "Step 43300: {'train_ae_loss': 0.66357, 'train_ucc_loss': 0.67346, 'train_ucc_acc': 0.625, 'loss': 0.66852}\n",
            "Step 43320: {'train_ae_loss': 0.67616, 'train_ucc_loss': 0.63525, 'train_ucc_acc': 0.65625, 'loss': 0.6557}\n",
            "Step 43340: {'train_ae_loss': 0.67313, 'train_ucc_loss': 0.46152, 'train_ucc_acc': 0.84375, 'loss': 0.56732}\n",
            "Step 43360: {'train_ae_loss': 0.69392, 'train_ucc_loss': 0.5202, 'train_ucc_acc': 0.8125, 'loss': 0.60706}\n",
            "Step 43380: {'train_ae_loss': 0.66923, 'train_ucc_loss': 0.42988, 'train_ucc_acc': 0.90625, 'loss': 0.54956}\n",
            "Step 43400: {'train_ae_loss': 0.66255, 'train_ucc_loss': 0.39397, 'train_ucc_acc': 0.90625, 'loss': 0.52826}\n",
            "Step 43420: {'train_ae_loss': 0.66594, 'train_ucc_loss': 0.50099, 'train_ucc_acc': 0.84375, 'loss': 0.58346}\n",
            "Step 43440: {'train_ae_loss': 0.67085, 'train_ucc_loss': 0.4939, 'train_ucc_acc': 0.8125, 'loss': 0.58238}\n",
            "Step 43460: {'train_ae_loss': 0.66821, 'train_ucc_loss': 0.45131, 'train_ucc_acc': 0.875, 'loss': 0.55976}\n",
            "Step 43480: {'train_ae_loss': 0.67637, 'train_ucc_loss': 0.44398, 'train_ucc_acc': 0.875, 'loss': 0.56017}\n",
            "Step 43500: {'train_ae_loss': 0.69263, 'train_ucc_loss': 0.4011, 'train_ucc_acc': 0.90625, 'loss': 0.54686}\n",
            "Step 43520: {'train_ae_loss': 0.67109, 'train_ucc_loss': 0.43345, 'train_ucc_acc': 0.90625, 'loss': 0.55227}\n",
            "Step 43540: {'train_ae_loss': 0.67468, 'train_ucc_loss': 0.46512, 'train_ucc_acc': 0.8125, 'loss': 0.5699}\n",
            "Step 43560: {'train_ae_loss': 0.66575, 'train_ucc_loss': 0.52749, 'train_ucc_acc': 0.75, 'loss': 0.59662}\n",
            "Step 43580: {'train_ae_loss': 0.68606, 'train_ucc_loss': 0.44291, 'train_ucc_acc': 0.90625, 'loss': 0.56448}\n",
            "Step 43600: {'train_ae_loss': 0.67237, 'train_ucc_loss': 0.5376, 'train_ucc_acc': 0.78125, 'loss': 0.60499}\n",
            "Step 43620: {'train_ae_loss': 0.67849, 'train_ucc_loss': 0.5703, 'train_ucc_acc': 0.71875, 'loss': 0.6244}\n",
            "Step 43640: {'train_ae_loss': 0.67484, 'train_ucc_loss': 0.48756, 'train_ucc_acc': 0.8125, 'loss': 0.5812}\n",
            "Step 43660: {'train_ae_loss': 0.67204, 'train_ucc_loss': 0.46709, 'train_ucc_acc': 0.8125, 'loss': 0.56956}\n",
            "Step 43680: {'train_ae_loss': 0.68104, 'train_ucc_loss': 0.52004, 'train_ucc_acc': 0.78125, 'loss': 0.60054}\n",
            "Step 43700: {'train_ae_loss': 0.69701, 'train_ucc_loss': 0.48919, 'train_ucc_acc': 0.84375, 'loss': 0.5931}\n",
            "Step 43720: {'train_ae_loss': 0.67802, 'train_ucc_loss': 0.41018, 'train_ucc_acc': 0.90625, 'loss': 0.5441}\n",
            "Step 43740: {'train_ae_loss': 0.6726, 'train_ucc_loss': 0.46543, 'train_ucc_acc': 0.84375, 'loss': 0.56901}\n",
            "Step 43760: {'train_ae_loss': 0.67587, 'train_ucc_loss': 0.4464, 'train_ucc_acc': 0.84375, 'loss': 0.56114}\n",
            "Step 43780: {'train_ae_loss': 0.68537, 'train_ucc_loss': 0.498, 'train_ucc_acc': 0.84375, 'loss': 0.59168}\n",
            "Step 43800: {'train_ae_loss': 0.6894, 'train_ucc_loss': 0.43012, 'train_ucc_acc': 0.875, 'loss': 0.55976}\n",
            "Step 43820: {'train_ae_loss': 0.67537, 'train_ucc_loss': 0.59214, 'train_ucc_acc': 0.71875, 'loss': 0.63375}\n",
            "Step 43840: {'train_ae_loss': 0.67174, 'train_ucc_loss': 0.5878, 'train_ucc_acc': 0.75, 'loss': 0.62977}\n",
            "Step 43860: {'train_ae_loss': 0.67798, 'train_ucc_loss': 0.38171, 'train_ucc_acc': 0.96875, 'loss': 0.52984}\n",
            "Step 43880: {'train_ae_loss': 0.68418, 'train_ucc_loss': 0.53119, 'train_ucc_acc': 0.75, 'loss': 0.60769}\n",
            "Step 43900: {'train_ae_loss': 0.70544, 'train_ucc_loss': 0.47949, 'train_ucc_acc': 0.8125, 'loss': 0.59247}\n",
            "Step 43920: {'train_ae_loss': 0.68658, 'train_ucc_loss': 0.53382, 'train_ucc_acc': 0.75, 'loss': 0.6102}\n",
            "Step 43940: {'train_ae_loss': 0.67899, 'train_ucc_loss': 0.39123, 'train_ucc_acc': 0.9375, 'loss': 0.53511}\n",
            "Step 43960: {'train_ae_loss': 0.6854, 'train_ucc_loss': 0.49999, 'train_ucc_acc': 0.78125, 'loss': 0.59269}\n",
            "Step 43980: {'train_ae_loss': 0.67214, 'train_ucc_loss': 0.51099, 'train_ucc_acc': 0.78125, 'loss': 0.59156}\n",
            "Step 44000: {'train_ae_loss': 0.67078, 'train_ucc_loss': 0.50776, 'train_ucc_acc': 0.8125, 'loss': 0.58927}\n",
            "step: 44000,eval_ae_loss: 0.66732,eval_ucc_loss: 0.5195,eval_ucc_acc: 0.78223\n",
            "Step 44020: {'train_ae_loss': 0.6719, 'train_ucc_loss': 0.50283, 'train_ucc_acc': 0.8125, 'loss': 0.58737}\n",
            "Step 44040: {'train_ae_loss': 0.67581, 'train_ucc_loss': 0.48884, 'train_ucc_acc': 0.8125, 'loss': 0.58232}\n",
            "Step 44060: {'train_ae_loss': 0.66617, 'train_ucc_loss': 0.5541, 'train_ucc_acc': 0.71875, 'loss': 0.61014}\n",
            "Step 44080: {'train_ae_loss': 0.68095, 'train_ucc_loss': 0.52757, 'train_ucc_acc': 0.8125, 'loss': 0.60426}\n",
            "Step 44100: {'train_ae_loss': 0.66145, 'train_ucc_loss': 0.41452, 'train_ucc_acc': 0.875, 'loss': 0.53799}\n",
            "Step 44120: {'train_ae_loss': 0.65154, 'train_ucc_loss': 0.60244, 'train_ucc_acc': 0.6875, 'loss': 0.62699}\n",
            "Step 44140: {'train_ae_loss': 0.66233, 'train_ucc_loss': 0.49532, 'train_ucc_acc': 0.8125, 'loss': 0.57883}\n",
            "Step 44160: {'train_ae_loss': 0.66719, 'train_ucc_loss': 0.59541, 'train_ucc_acc': 0.6875, 'loss': 0.6313}\n",
            "Step 44180: {'train_ae_loss': 0.68353, 'train_ucc_loss': 0.45255, 'train_ucc_acc': 0.84375, 'loss': 0.56804}\n",
            "Step 44200: {'train_ae_loss': 0.67526, 'train_ucc_loss': 0.3876, 'train_ucc_acc': 0.9375, 'loss': 0.53143}\n",
            "Step 44220: {'train_ae_loss': 0.67461, 'train_ucc_loss': 0.54908, 'train_ucc_acc': 0.75, 'loss': 0.61184}\n",
            "Step 44240: {'train_ae_loss': 0.67031, 'train_ucc_loss': 0.49778, 'train_ucc_acc': 0.8125, 'loss': 0.58405}\n",
            "Step 44260: {'train_ae_loss': 0.6859, 'train_ucc_loss': 0.51322, 'train_ucc_acc': 0.75, 'loss': 0.59956}\n",
            "Step 44280: {'train_ae_loss': 0.67271, 'train_ucc_loss': 0.53999, 'train_ucc_acc': 0.8125, 'loss': 0.60635}\n",
            "Step 44300: {'train_ae_loss': 0.69047, 'train_ucc_loss': 0.49798, 'train_ucc_acc': 0.8125, 'loss': 0.59423}\n",
            "Step 44320: {'train_ae_loss': 0.68314, 'train_ucc_loss': 0.43435, 'train_ucc_acc': 0.875, 'loss': 0.55875}\n",
            "Step 44340: {'train_ae_loss': 0.6806, 'train_ucc_loss': 0.45572, 'train_ucc_acc': 0.8125, 'loss': 0.56816}\n",
            "Step 44360: {'train_ae_loss': 0.67481, 'train_ucc_loss': 0.46201, 'train_ucc_acc': 0.8125, 'loss': 0.56841}\n",
            "Step 44380: {'train_ae_loss': 0.67544, 'train_ucc_loss': 0.56714, 'train_ucc_acc': 0.75, 'loss': 0.62129}\n",
            "Step 44400: {'train_ae_loss': 0.66986, 'train_ucc_loss': 0.48236, 'train_ucc_acc': 0.84375, 'loss': 0.57611}\n",
            "Step 44420: {'train_ae_loss': 0.6827, 'train_ucc_loss': 0.48194, 'train_ucc_acc': 0.78125, 'loss': 0.58232}\n",
            "Step 44440: {'train_ae_loss': 0.66387, 'train_ucc_loss': 0.48945, 'train_ucc_acc': 0.84375, 'loss': 0.57666}\n",
            "Step 44460: {'train_ae_loss': 0.64994, 'train_ucc_loss': 0.63335, 'train_ucc_acc': 0.65625, 'loss': 0.64164}\n",
            "Step 44480: {'train_ae_loss': 0.6597, 'train_ucc_loss': 0.40357, 'train_ucc_acc': 0.90625, 'loss': 0.53164}\n",
            "Step 44500: {'train_ae_loss': 0.66177, 'train_ucc_loss': 0.51186, 'train_ucc_acc': 0.78125, 'loss': 0.58682}\n",
            "Step 44520: {'train_ae_loss': 0.65351, 'train_ucc_loss': 0.44907, 'train_ucc_acc': 0.875, 'loss': 0.55129}\n",
            "Step 44540: {'train_ae_loss': 0.67818, 'train_ucc_loss': 0.43984, 'train_ucc_acc': 0.875, 'loss': 0.55901}\n",
            "Step 44560: {'train_ae_loss': 0.66622, 'train_ucc_loss': 0.52539, 'train_ucc_acc': 0.78125, 'loss': 0.5958}\n",
            "Step 44580: {'train_ae_loss': 0.67554, 'train_ucc_loss': 0.42445, 'train_ucc_acc': 0.90625, 'loss': 0.55}\n",
            "Step 44600: {'train_ae_loss': 0.66492, 'train_ucc_loss': 0.49274, 'train_ucc_acc': 0.8125, 'loss': 0.57883}\n",
            "Step 44620: {'train_ae_loss': 0.67434, 'train_ucc_loss': 0.382, 'train_ucc_acc': 0.90625, 'loss': 0.52817}\n",
            "Step 44640: {'train_ae_loss': 0.66814, 'train_ucc_loss': 0.46083, 'train_ucc_acc': 0.84375, 'loss': 0.56448}\n",
            "Step 44660: {'train_ae_loss': 0.67194, 'train_ucc_loss': 0.39529, 'train_ucc_acc': 0.90625, 'loss': 0.53361}\n",
            "Step 44680: {'train_ae_loss': 0.67896, 'train_ucc_loss': 0.45653, 'train_ucc_acc': 0.84375, 'loss': 0.56775}\n",
            "Step 44700: {'train_ae_loss': 0.65716, 'train_ucc_loss': 0.46884, 'train_ucc_acc': 0.84375, 'loss': 0.563}\n",
            "Step 44720: {'train_ae_loss': 0.6577, 'train_ucc_loss': 0.429, 'train_ucc_acc': 0.875, 'loss': 0.54335}\n",
            "Step 44740: {'train_ae_loss': 0.66589, 'train_ucc_loss': 0.55131, 'train_ucc_acc': 0.78125, 'loss': 0.6086}\n",
            "Step 44760: {'train_ae_loss': 0.65598, 'train_ucc_loss': 0.63436, 'train_ucc_acc': 0.65625, 'loss': 0.64517}\n",
            "Step 44780: {'train_ae_loss': 0.65624, 'train_ucc_loss': 0.53541, 'train_ucc_acc': 0.78125, 'loss': 0.59582}\n",
            "Step 44800: {'train_ae_loss': 0.67892, 'train_ucc_loss': 0.5007, 'train_ucc_acc': 0.8125, 'loss': 0.58981}\n",
            "Step 44820: {'train_ae_loss': 0.68446, 'train_ucc_loss': 0.41476, 'train_ucc_acc': 0.875, 'loss': 0.54961}\n",
            "Step 44840: {'train_ae_loss': 0.6655, 'train_ucc_loss': 0.61589, 'train_ucc_acc': 0.65625, 'loss': 0.6407}\n",
            "Step 44860: {'train_ae_loss': 0.66632, 'train_ucc_loss': 0.51175, 'train_ucc_acc': 0.8125, 'loss': 0.58904}\n",
            "Step 44880: {'train_ae_loss': 0.66626, 'train_ucc_loss': 0.50052, 'train_ucc_acc': 0.8125, 'loss': 0.58339}\n",
            "Step 44900: {'train_ae_loss': 0.67641, 'train_ucc_loss': 0.50237, 'train_ucc_acc': 0.8125, 'loss': 0.58939}\n",
            "Step 44920: {'train_ae_loss': 0.65955, 'train_ucc_loss': 0.59801, 'train_ucc_acc': 0.65625, 'loss': 0.62878}\n",
            "Step 44940: {'train_ae_loss': 0.66906, 'train_ucc_loss': 0.51254, 'train_ucc_acc': 0.75, 'loss': 0.5908}\n",
            "Step 44960: {'train_ae_loss': 0.66169, 'train_ucc_loss': 0.41702, 'train_ucc_acc': 0.90625, 'loss': 0.53936}\n",
            "Step 44980: {'train_ae_loss': 0.67225, 'train_ucc_loss': 0.42031, 'train_ucc_acc': 0.875, 'loss': 0.54628}\n",
            "Step 45000: {'train_ae_loss': 0.67533, 'train_ucc_loss': 0.48489, 'train_ucc_acc': 0.8125, 'loss': 0.58011}\n",
            "step: 45000,eval_ae_loss: 0.66015,eval_ucc_loss: 0.52578,eval_ucc_acc: 0.7793\n",
            "Step 45020: {'train_ae_loss': 0.68046, 'train_ucc_loss': 0.44991, 'train_ucc_acc': 0.84375, 'loss': 0.56518}\n",
            "Step 45040: {'train_ae_loss': 0.68504, 'train_ucc_loss': 0.48186, 'train_ucc_acc': 0.84375, 'loss': 0.58345}\n",
            "Step 45060: {'train_ae_loss': 0.67087, 'train_ucc_loss': 0.5437, 'train_ucc_acc': 0.71875, 'loss': 0.60729}\n",
            "Step 45080: {'train_ae_loss': 0.67282, 'train_ucc_loss': 0.45592, 'train_ucc_acc': 0.84375, 'loss': 0.56437}\n",
            "Step 45100: {'train_ae_loss': 0.66666, 'train_ucc_loss': 0.4899, 'train_ucc_acc': 0.84375, 'loss': 0.57828}\n",
            "Step 45120: {'train_ae_loss': 0.66551, 'train_ucc_loss': 0.475, 'train_ucc_acc': 0.8125, 'loss': 0.57025}\n",
            "Step 45140: {'train_ae_loss': 0.66514, 'train_ucc_loss': 0.42224, 'train_ucc_acc': 0.84375, 'loss': 0.54369}\n",
            "Step 45160: {'train_ae_loss': 0.6682, 'train_ucc_loss': 0.51057, 'train_ucc_acc': 0.78125, 'loss': 0.58938}\n",
            "Step 45180: {'train_ae_loss': 0.68373, 'train_ucc_loss': 0.44058, 'train_ucc_acc': 0.875, 'loss': 0.56215}\n",
            "Step 45200: {'train_ae_loss': 0.67756, 'train_ucc_loss': 0.4464, 'train_ucc_acc': 0.875, 'loss': 0.56198}\n",
            "Step 45220: {'train_ae_loss': 0.66629, 'train_ucc_loss': 0.54065, 'train_ucc_acc': 0.78125, 'loss': 0.60347}\n",
            "Step 45240: {'train_ae_loss': 0.6582, 'train_ucc_loss': 0.51572, 'train_ucc_acc': 0.8125, 'loss': 0.58696}\n",
            "Step 45260: {'train_ae_loss': 0.67228, 'train_ucc_loss': 0.49811, 'train_ucc_acc': 0.8125, 'loss': 0.58519}\n",
            "Step 45280: {'train_ae_loss': 0.66958, 'train_ucc_loss': 0.53, 'train_ucc_acc': 0.78125, 'loss': 0.59979}\n",
            "Step 45300: {'train_ae_loss': 0.66955, 'train_ucc_loss': 0.49237, 'train_ucc_acc': 0.75, 'loss': 0.58096}\n",
            "Step 45320: {'train_ae_loss': 0.66089, 'train_ucc_loss': 0.51991, 'train_ucc_acc': 0.8125, 'loss': 0.5904}\n",
            "Step 45340: {'train_ae_loss': 0.63952, 'train_ucc_loss': 0.53338, 'train_ucc_acc': 0.75, 'loss': 0.58645}\n",
            "Step 45360: {'train_ae_loss': 0.67244, 'train_ucc_loss': 0.48435, 'train_ucc_acc': 0.78125, 'loss': 0.57839}\n",
            "Step 45380: {'train_ae_loss': 0.66399, 'train_ucc_loss': 0.4818, 'train_ucc_acc': 0.84375, 'loss': 0.5729}\n",
            "Step 45400: {'train_ae_loss': 0.68659, 'train_ucc_loss': 0.53321, 'train_ucc_acc': 0.78125, 'loss': 0.6099}\n",
            "Step 45420: {'train_ae_loss': 0.66333, 'train_ucc_loss': 0.49962, 'train_ucc_acc': 0.8125, 'loss': 0.58147}\n",
            "Step 45440: {'train_ae_loss': 0.65917, 'train_ucc_loss': 0.43877, 'train_ucc_acc': 0.90625, 'loss': 0.54897}\n",
            "Step 45460: {'train_ae_loss': 0.66727, 'train_ucc_loss': 0.5164, 'train_ucc_acc': 0.78125, 'loss': 0.59183}\n",
            "Step 45480: {'train_ae_loss': 0.67294, 'train_ucc_loss': 0.48698, 'train_ucc_acc': 0.84375, 'loss': 0.57996}\n",
            "Step 45500: {'train_ae_loss': 0.67602, 'train_ucc_loss': 0.4786, 'train_ucc_acc': 0.8125, 'loss': 0.57731}\n",
            "Step 45520: {'train_ae_loss': 0.66618, 'train_ucc_loss': 0.4885, 'train_ucc_acc': 0.84375, 'loss': 0.57734}\n",
            "Step 45540: {'train_ae_loss': 0.67314, 'train_ucc_loss': 0.48231, 'train_ucc_acc': 0.8125, 'loss': 0.57772}\n",
            "Step 45560: {'train_ae_loss': 0.67651, 'train_ucc_loss': 0.44059, 'train_ucc_acc': 0.84375, 'loss': 0.55855}\n",
            "Step 45580: {'train_ae_loss': 0.68533, 'train_ucc_loss': 0.38576, 'train_ucc_acc': 0.9375, 'loss': 0.53554}\n",
            "Step 45600: {'train_ae_loss': 0.65923, 'train_ucc_loss': 0.43545, 'train_ucc_acc': 0.875, 'loss': 0.54734}\n",
            "Step 45620: {'train_ae_loss': 0.65754, 'train_ucc_loss': 0.5801, 'train_ucc_acc': 0.6875, 'loss': 0.61882}\n",
            "Step 45640: {'train_ae_loss': 0.6687, 'train_ucc_loss': 0.49383, 'train_ucc_acc': 0.78125, 'loss': 0.58126}\n",
            "Step 45660: {'train_ae_loss': 0.65551, 'train_ucc_loss': 0.50428, 'train_ucc_acc': 0.8125, 'loss': 0.5799}\n",
            "Step 45680: {'train_ae_loss': 0.68522, 'train_ucc_loss': 0.51858, 'train_ucc_acc': 0.78125, 'loss': 0.6019}\n",
            "Step 45700: {'train_ae_loss': 0.66185, 'train_ucc_loss': 0.41218, 'train_ucc_acc': 0.90625, 'loss': 0.53702}\n",
            "Step 45720: {'train_ae_loss': 0.65304, 'train_ucc_loss': 0.49646, 'train_ucc_acc': 0.8125, 'loss': 0.57475}\n",
            "Step 45740: {'train_ae_loss': 0.65648, 'train_ucc_loss': 0.53413, 'train_ucc_acc': 0.78125, 'loss': 0.5953}\n",
            "Step 45760: {'train_ae_loss': 0.66367, 'train_ucc_loss': 0.3476, 'train_ucc_acc': 1.0, 'loss': 0.50563}\n",
            "Step 45780: {'train_ae_loss': 0.66294, 'train_ucc_loss': 0.54364, 'train_ucc_acc': 0.75, 'loss': 0.60329}\n",
            "Step 45800: {'train_ae_loss': 0.66296, 'train_ucc_loss': 0.38056, 'train_ucc_acc': 0.9375, 'loss': 0.52176}\n",
            "Step 45820: {'train_ae_loss': 0.69591, 'train_ucc_loss': 0.50984, 'train_ucc_acc': 0.78125, 'loss': 0.60287}\n",
            "Step 45840: {'train_ae_loss': 0.68828, 'train_ucc_loss': 0.45152, 'train_ucc_acc': 0.875, 'loss': 0.5699}\n",
            "Step 45860: {'train_ae_loss': 0.65656, 'train_ucc_loss': 0.53945, 'train_ucc_acc': 0.78125, 'loss': 0.59801}\n",
            "Step 45880: {'train_ae_loss': 0.69108, 'train_ucc_loss': 0.57833, 'train_ucc_acc': 0.6875, 'loss': 0.6347}\n",
            "Step 45900: {'train_ae_loss': 0.66108, 'train_ucc_loss': 0.47862, 'train_ucc_acc': 0.8125, 'loss': 0.56985}\n",
            "Step 45920: {'train_ae_loss': 0.66538, 'train_ucc_loss': 0.58584, 'train_ucc_acc': 0.6875, 'loss': 0.62561}\n",
            "Step 45940: {'train_ae_loss': 0.669, 'train_ucc_loss': 0.46432, 'train_ucc_acc': 0.84375, 'loss': 0.56666}\n",
            "Step 45960: {'train_ae_loss': 0.67235, 'train_ucc_loss': 0.48298, 'train_ucc_acc': 0.78125, 'loss': 0.57766}\n",
            "Step 45980: {'train_ae_loss': 0.65676, 'train_ucc_loss': 0.65625, 'train_ucc_acc': 0.59375, 'loss': 0.65651}\n",
            "Step 46000: {'train_ae_loss': 0.66355, 'train_ucc_loss': 0.51375, 'train_ucc_acc': 0.8125, 'loss': 0.58865}\n",
            "step: 46000,eval_ae_loss: 0.65909,eval_ucc_loss: 0.54435,eval_ucc_acc: 0.76367\n",
            "Step 46020: {'train_ae_loss': 0.65894, 'train_ucc_loss': 0.52586, 'train_ucc_acc': 0.75, 'loss': 0.5924}\n",
            "Step 46040: {'train_ae_loss': 0.66667, 'train_ucc_loss': 0.50247, 'train_ucc_acc': 0.8125, 'loss': 0.58457}\n",
            "Step 46060: {'train_ae_loss': 0.66258, 'train_ucc_loss': 0.42165, 'train_ucc_acc': 0.875, 'loss': 0.54212}\n",
            "Step 46080: {'train_ae_loss': 0.67882, 'train_ucc_loss': 0.58697, 'train_ucc_acc': 0.71875, 'loss': 0.63289}\n",
            "Step 46100: {'train_ae_loss': 0.68612, 'train_ucc_loss': 0.46126, 'train_ucc_acc': 0.8125, 'loss': 0.57369}\n",
            "Step 46120: {'train_ae_loss': 0.67433, 'train_ucc_loss': 0.47901, 'train_ucc_acc': 0.84375, 'loss': 0.57667}\n",
            "Step 46140: {'train_ae_loss': 0.67493, 'train_ucc_loss': 0.61759, 'train_ucc_acc': 0.65625, 'loss': 0.64626}\n",
            "Step 46160: {'train_ae_loss': 0.68089, 'train_ucc_loss': 0.45244, 'train_ucc_acc': 0.875, 'loss': 0.56666}\n",
            "Step 46180: {'train_ae_loss': 0.66957, 'train_ucc_loss': 0.51534, 'train_ucc_acc': 0.75, 'loss': 0.59246}\n",
            "Step 46200: {'train_ae_loss': 0.67527, 'train_ucc_loss': 0.48176, 'train_ucc_acc': 0.8125, 'loss': 0.57851}\n",
            "Step 46220: {'train_ae_loss': 0.6839, 'train_ucc_loss': 0.48864, 'train_ucc_acc': 0.84375, 'loss': 0.58627}\n",
            "Step 46240: {'train_ae_loss': 0.6767, 'train_ucc_loss': 0.56643, 'train_ucc_acc': 0.71875, 'loss': 0.62156}\n",
            "Step 46260: {'train_ae_loss': 0.67248, 'train_ucc_loss': 0.46604, 'train_ucc_acc': 0.875, 'loss': 0.56926}\n",
            "Step 46280: {'train_ae_loss': 0.68221, 'train_ucc_loss': 0.4331, 'train_ucc_acc': 0.875, 'loss': 0.55766}\n",
            "Step 46300: {'train_ae_loss': 0.64761, 'train_ucc_loss': 0.50493, 'train_ucc_acc': 0.84375, 'loss': 0.57627}\n",
            "Step 46320: {'train_ae_loss': 0.66041, 'train_ucc_loss': 0.46852, 'train_ucc_acc': 0.8125, 'loss': 0.56446}\n",
            "Step 46340: {'train_ae_loss': 0.6716, 'train_ucc_loss': 0.57727, 'train_ucc_acc': 0.71875, 'loss': 0.62443}\n",
            "Step 46360: {'train_ae_loss': 0.68728, 'train_ucc_loss': 0.46475, 'train_ucc_acc': 0.84375, 'loss': 0.57601}\n",
            "Step 46380: {'train_ae_loss': 0.64756, 'train_ucc_loss': 0.50094, 'train_ucc_acc': 0.8125, 'loss': 0.57425}\n",
            "Step 46400: {'train_ae_loss': 0.65324, 'train_ucc_loss': 0.55717, 'train_ucc_acc': 0.78125, 'loss': 0.6052}\n",
            "Step 46420: {'train_ae_loss': 0.65834, 'train_ucc_loss': 0.43551, 'train_ucc_acc': 0.84375, 'loss': 0.54692}\n",
            "Step 46440: {'train_ae_loss': 0.68724, 'train_ucc_loss': 0.50516, 'train_ucc_acc': 0.8125, 'loss': 0.5962}\n",
            "Step 46460: {'train_ae_loss': 0.65949, 'train_ucc_loss': 0.50966, 'train_ucc_acc': 0.78125, 'loss': 0.58458}\n",
            "Step 46480: {'train_ae_loss': 0.68495, 'train_ucc_loss': 0.58907, 'train_ucc_acc': 0.71875, 'loss': 0.63701}\n",
            "Step 46500: {'train_ae_loss': 0.66262, 'train_ucc_loss': 0.45507, 'train_ucc_acc': 0.84375, 'loss': 0.55884}\n",
            "Step 46520: {'train_ae_loss': 0.67984, 'train_ucc_loss': 0.52855, 'train_ucc_acc': 0.78125, 'loss': 0.6042}\n",
            "Step 46540: {'train_ae_loss': 0.68312, 'train_ucc_loss': 0.42183, 'train_ucc_acc': 0.84375, 'loss': 0.55247}\n",
            "Step 46560: {'train_ae_loss': 0.65632, 'train_ucc_loss': 0.55819, 'train_ucc_acc': 0.75, 'loss': 0.60726}\n",
            "Step 46580: {'train_ae_loss': 0.66236, 'train_ucc_loss': 0.4233, 'train_ucc_acc': 0.90625, 'loss': 0.54283}\n",
            "Step 46600: {'train_ae_loss': 0.68395, 'train_ucc_loss': 0.62277, 'train_ucc_acc': 0.6875, 'loss': 0.65336}\n",
            "Step 46620: {'train_ae_loss': 0.67381, 'train_ucc_loss': 0.42755, 'train_ucc_acc': 0.875, 'loss': 0.55068}\n",
            "Step 46640: {'train_ae_loss': 0.68008, 'train_ucc_loss': 0.41737, 'train_ucc_acc': 0.90625, 'loss': 0.54873}\n",
            "Step 46660: {'train_ae_loss': 0.6598, 'train_ucc_loss': 0.42732, 'train_ucc_acc': 0.90625, 'loss': 0.54356}\n",
            "Step 46680: {'train_ae_loss': 0.68063, 'train_ucc_loss': 0.38099, 'train_ucc_acc': 0.9375, 'loss': 0.53081}\n",
            "Step 46700: {'train_ae_loss': 0.66315, 'train_ucc_loss': 0.48097, 'train_ucc_acc': 0.84375, 'loss': 0.57206}\n",
            "Step 46720: {'train_ae_loss': 0.66997, 'train_ucc_loss': 0.51786, 'train_ucc_acc': 0.75, 'loss': 0.59392}\n",
            "Step 46740: {'train_ae_loss': 0.67234, 'train_ucc_loss': 0.50539, 'train_ucc_acc': 0.8125, 'loss': 0.58886}\n",
            "Step 46760: {'train_ae_loss': 0.67389, 'train_ucc_loss': 0.51893, 'train_ucc_acc': 0.8125, 'loss': 0.59641}\n",
            "Step 46780: {'train_ae_loss': 0.6697, 'train_ucc_loss': 0.5008, 'train_ucc_acc': 0.8125, 'loss': 0.58525}\n",
            "Step 46800: {'train_ae_loss': 0.66831, 'train_ucc_loss': 0.49562, 'train_ucc_acc': 0.84375, 'loss': 0.58196}\n",
            "Step 46820: {'train_ae_loss': 0.67875, 'train_ucc_loss': 0.47633, 'train_ucc_acc': 0.8125, 'loss': 0.57754}\n",
            "Step 46840: {'train_ae_loss': 0.66964, 'train_ucc_loss': 0.47358, 'train_ucc_acc': 0.84375, 'loss': 0.57161}\n",
            "Step 46860: {'train_ae_loss': 0.67389, 'train_ucc_loss': 0.45711, 'train_ucc_acc': 0.875, 'loss': 0.5655}\n",
            "Step 46880: {'train_ae_loss': 0.66491, 'train_ucc_loss': 0.48818, 'train_ucc_acc': 0.8125, 'loss': 0.57655}\n",
            "Step 46900: {'train_ae_loss': 0.64906, 'train_ucc_loss': 0.46235, 'train_ucc_acc': 0.8125, 'loss': 0.55571}\n",
            "Step 46920: {'train_ae_loss': 0.66862, 'train_ucc_loss': 0.41954, 'train_ucc_acc': 0.84375, 'loss': 0.54408}\n",
            "Step 46940: {'train_ae_loss': 0.67375, 'train_ucc_loss': 0.4681, 'train_ucc_acc': 0.875, 'loss': 0.57092}\n",
            "Step 46960: {'train_ae_loss': 0.65956, 'train_ucc_loss': 0.54983, 'train_ucc_acc': 0.75, 'loss': 0.6047}\n",
            "Step 46980: {'train_ae_loss': 0.67417, 'train_ucc_loss': 0.51156, 'train_ucc_acc': 0.78125, 'loss': 0.59286}\n",
            "Step 47000: {'train_ae_loss': 0.6958, 'train_ucc_loss': 0.38491, 'train_ucc_acc': 0.9375, 'loss': 0.54035}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 14:12:40 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 47000,eval_ae_loss: 0.67212,eval_ucc_loss: 0.48501,eval_ucc_acc: 0.81836\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 14:12:45 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "d:\\UCC-DRN-Pytorch\\.venv\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  warnings.warn(\n",
            "d:\\UCC-DRN-Pytorch\\.venv\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 47020: {'train_ae_loss': 0.6738, 'train_ucc_loss': 0.43359, 'train_ucc_acc': 0.90625, 'loss': 0.55369}\n",
            "Step 47040: {'train_ae_loss': 0.66665, 'train_ucc_loss': 0.55604, 'train_ucc_acc': 0.71875, 'loss': 0.61135}\n",
            "Step 47060: {'train_ae_loss': 0.6795, 'train_ucc_loss': 0.47694, 'train_ucc_acc': 0.8125, 'loss': 0.57822}\n",
            "Step 47080: {'train_ae_loss': 0.68019, 'train_ucc_loss': 0.49749, 'train_ucc_acc': 0.8125, 'loss': 0.58884}\n",
            "Step 47100: {'train_ae_loss': 0.66919, 'train_ucc_loss': 0.51343, 'train_ucc_acc': 0.8125, 'loss': 0.59131}\n",
            "Step 47120: {'train_ae_loss': 0.69231, 'train_ucc_loss': 0.52063, 'train_ucc_acc': 0.78125, 'loss': 0.60647}\n",
            "Step 47140: {'train_ae_loss': 0.67332, 'train_ucc_loss': 0.53661, 'train_ucc_acc': 0.75, 'loss': 0.60496}\n",
            "Step 47160: {'train_ae_loss': 0.67374, 'train_ucc_loss': 0.47203, 'train_ucc_acc': 0.84375, 'loss': 0.57288}\n",
            "Step 47180: {'train_ae_loss': 0.67887, 'train_ucc_loss': 0.40317, 'train_ucc_acc': 0.90625, 'loss': 0.54102}\n",
            "Step 47200: {'train_ae_loss': 0.6851, 'train_ucc_loss': 0.43166, 'train_ucc_acc': 0.875, 'loss': 0.55838}\n",
            "Step 47220: {'train_ae_loss': 0.67469, 'train_ucc_loss': 0.53707, 'train_ucc_acc': 0.78125, 'loss': 0.60588}\n",
            "Step 47240: {'train_ae_loss': 0.68305, 'train_ucc_loss': 0.56427, 'train_ucc_acc': 0.75, 'loss': 0.62366}\n",
            "Step 47260: {'train_ae_loss': 0.66674, 'train_ucc_loss': 0.62147, 'train_ucc_acc': 0.65625, 'loss': 0.6441}\n",
            "Step 47280: {'train_ae_loss': 0.66368, 'train_ucc_loss': 0.52417, 'train_ucc_acc': 0.78125, 'loss': 0.59392}\n",
            "Step 47300: {'train_ae_loss': 0.67911, 'train_ucc_loss': 0.48963, 'train_ucc_acc': 0.8125, 'loss': 0.58437}\n",
            "Step 47320: {'train_ae_loss': 0.68129, 'train_ucc_loss': 0.45659, 'train_ucc_acc': 0.875, 'loss': 0.56894}\n",
            "Step 47340: {'train_ae_loss': 0.66405, 'train_ucc_loss': 0.45211, 'train_ucc_acc': 0.875, 'loss': 0.55808}\n",
            "Step 47360: {'train_ae_loss': 0.66563, 'train_ucc_loss': 0.45965, 'train_ucc_acc': 0.8125, 'loss': 0.56264}\n",
            "Step 47380: {'train_ae_loss': 0.67194, 'train_ucc_loss': 0.43008, 'train_ucc_acc': 0.875, 'loss': 0.55101}\n",
            "Step 47400: {'train_ae_loss': 0.67373, 'train_ucc_loss': 0.4948, 'train_ucc_acc': 0.8125, 'loss': 0.58426}\n",
            "Step 47420: {'train_ae_loss': 0.66168, 'train_ucc_loss': 0.58039, 'train_ucc_acc': 0.71875, 'loss': 0.62104}\n",
            "Step 47440: {'train_ae_loss': 0.66858, 'train_ucc_loss': 0.60236, 'train_ucc_acc': 0.65625, 'loss': 0.63547}\n",
            "Step 47460: {'train_ae_loss': 0.66855, 'train_ucc_loss': 0.54222, 'train_ucc_acc': 0.78125, 'loss': 0.60538}\n",
            "Step 47480: {'train_ae_loss': 0.67517, 'train_ucc_loss': 0.50919, 'train_ucc_acc': 0.78125, 'loss': 0.59218}\n",
            "Step 47500: {'train_ae_loss': 0.67513, 'train_ucc_loss': 0.43402, 'train_ucc_acc': 0.875, 'loss': 0.55458}\n",
            "Step 47520: {'train_ae_loss': 0.6807, 'train_ucc_loss': 0.50304, 'train_ucc_acc': 0.8125, 'loss': 0.59187}\n",
            "Step 47540: {'train_ae_loss': 0.66572, 'train_ucc_loss': 0.48824, 'train_ucc_acc': 0.8125, 'loss': 0.57698}\n",
            "Step 47560: {'train_ae_loss': 0.68392, 'train_ucc_loss': 0.41042, 'train_ucc_acc': 0.90625, 'loss': 0.54717}\n",
            "Step 47580: {'train_ae_loss': 0.68168, 'train_ucc_loss': 0.42219, 'train_ucc_acc': 0.875, 'loss': 0.55194}\n",
            "Step 47600: {'train_ae_loss': 0.68486, 'train_ucc_loss': 0.53212, 'train_ucc_acc': 0.78125, 'loss': 0.60849}\n",
            "Step 47620: {'train_ae_loss': 0.65716, 'train_ucc_loss': 0.44155, 'train_ucc_acc': 0.875, 'loss': 0.54936}\n",
            "Step 47640: {'train_ae_loss': 0.67479, 'train_ucc_loss': 0.41354, 'train_ucc_acc': 0.9375, 'loss': 0.54417}\n",
            "Step 47660: {'train_ae_loss': 0.65292, 'train_ucc_loss': 0.57457, 'train_ucc_acc': 0.71875, 'loss': 0.61375}\n",
            "Step 47680: {'train_ae_loss': 0.69787, 'train_ucc_loss': 0.36549, 'train_ucc_acc': 0.96875, 'loss': 0.53168}\n",
            "Step 47700: {'train_ae_loss': 0.66427, 'train_ucc_loss': 0.44007, 'train_ucc_acc': 0.875, 'loss': 0.55217}\n",
            "Step 47720: {'train_ae_loss': 0.67078, 'train_ucc_loss': 0.44223, 'train_ucc_acc': 0.875, 'loss': 0.5565}\n",
            "Step 47740: {'train_ae_loss': 0.66641, 'train_ucc_loss': 0.48789, 'train_ucc_acc': 0.84375, 'loss': 0.57715}\n",
            "Step 47760: {'train_ae_loss': 0.66933, 'train_ucc_loss': 0.54209, 'train_ucc_acc': 0.78125, 'loss': 0.60571}\n",
            "Step 47780: {'train_ae_loss': 0.66509, 'train_ucc_loss': 0.61961, 'train_ucc_acc': 0.6875, 'loss': 0.64235}\n",
            "Step 47800: {'train_ae_loss': 0.6782, 'train_ucc_loss': 0.44948, 'train_ucc_acc': 0.84375, 'loss': 0.56384}\n",
            "Step 47820: {'train_ae_loss': 0.67677, 'train_ucc_loss': 0.54044, 'train_ucc_acc': 0.78125, 'loss': 0.6086}\n",
            "Step 47840: {'train_ae_loss': 0.66072, 'train_ucc_loss': 0.40572, 'train_ucc_acc': 0.90625, 'loss': 0.53322}\n",
            "Step 47860: {'train_ae_loss': 0.68939, 'train_ucc_loss': 0.51498, 'train_ucc_acc': 0.8125, 'loss': 0.60219}\n",
            "Step 47880: {'train_ae_loss': 0.66569, 'train_ucc_loss': 0.44012, 'train_ucc_acc': 0.875, 'loss': 0.5529}\n",
            "Step 47900: {'train_ae_loss': 0.6794, 'train_ucc_loss': 0.42726, 'train_ucc_acc': 0.875, 'loss': 0.55333}\n",
            "Step 47920: {'train_ae_loss': 0.66762, 'train_ucc_loss': 0.52288, 'train_ucc_acc': 0.78125, 'loss': 0.59525}\n",
            "Step 47940: {'train_ae_loss': 0.68161, 'train_ucc_loss': 0.44827, 'train_ucc_acc': 0.875, 'loss': 0.56494}\n",
            "Step 47960: {'train_ae_loss': 0.66064, 'train_ucc_loss': 0.52343, 'train_ucc_acc': 0.78125, 'loss': 0.59204}\n",
            "Step 47980: {'train_ae_loss': 0.66819, 'train_ucc_loss': 0.35024, 'train_ucc_acc': 0.96875, 'loss': 0.50922}\n",
            "Step 48000: {'train_ae_loss': 0.68251, 'train_ucc_loss': 0.44828, 'train_ucc_acc': 0.84375, 'loss': 0.56539}\n",
            "step: 48000,eval_ae_loss: 0.654,eval_ucc_loss: 0.50112,eval_ucc_acc: 0.80859\n",
            "Step 48020: {'train_ae_loss': 0.67339, 'train_ucc_loss': 0.42175, 'train_ucc_acc': 0.90625, 'loss': 0.54757}\n",
            "Step 48040: {'train_ae_loss': 0.65897, 'train_ucc_loss': 0.43812, 'train_ucc_acc': 0.875, 'loss': 0.54854}\n",
            "Step 48060: {'train_ae_loss': 0.66058, 'train_ucc_loss': 0.49561, 'train_ucc_acc': 0.78125, 'loss': 0.5781}\n",
            "Step 48080: {'train_ae_loss': 0.68165, 'train_ucc_loss': 0.48895, 'train_ucc_acc': 0.8125, 'loss': 0.5853}\n",
            "Step 48100: {'train_ae_loss': 0.65607, 'train_ucc_loss': 0.47879, 'train_ucc_acc': 0.78125, 'loss': 0.56743}\n",
            "Step 48120: {'train_ae_loss': 0.67128, 'train_ucc_loss': 0.53499, 'train_ucc_acc': 0.78125, 'loss': 0.60314}\n",
            "Step 48140: {'train_ae_loss': 0.66973, 'train_ucc_loss': 0.59168, 'train_ucc_acc': 0.6875, 'loss': 0.63071}\n",
            "Step 48160: {'train_ae_loss': 0.68417, 'train_ucc_loss': 0.55392, 'train_ucc_acc': 0.75, 'loss': 0.61905}\n",
            "Step 48180: {'train_ae_loss': 0.66298, 'train_ucc_loss': 0.50438, 'train_ucc_acc': 0.8125, 'loss': 0.58368}\n",
            "Step 48200: {'train_ae_loss': 0.66666, 'train_ucc_loss': 0.49117, 'train_ucc_acc': 0.8125, 'loss': 0.57892}\n",
            "Step 48220: {'train_ae_loss': 0.66994, 'train_ucc_loss': 0.50995, 'train_ucc_acc': 0.8125, 'loss': 0.58995}\n",
            "Step 48240: {'train_ae_loss': 0.67321, 'train_ucc_loss': 0.45576, 'train_ucc_acc': 0.8125, 'loss': 0.56449}\n",
            "Step 48260: {'train_ae_loss': 0.66592, 'train_ucc_loss': 0.39988, 'train_ucc_acc': 0.90625, 'loss': 0.5329}\n",
            "Step 48280: {'train_ae_loss': 0.66845, 'train_ucc_loss': 0.42217, 'train_ucc_acc': 0.90625, 'loss': 0.54531}\n",
            "Step 48300: {'train_ae_loss': 0.68083, 'train_ucc_loss': 0.45022, 'train_ucc_acc': 0.875, 'loss': 0.56553}\n",
            "Step 48320: {'train_ae_loss': 0.67602, 'train_ucc_loss': 0.455, 'train_ucc_acc': 0.84375, 'loss': 0.56551}\n",
            "Step 48340: {'train_ae_loss': 0.68052, 'train_ucc_loss': 0.43566, 'train_ucc_acc': 0.84375, 'loss': 0.55809}\n",
            "Step 48360: {'train_ae_loss': 0.68811, 'train_ucc_loss': 0.44721, 'train_ucc_acc': 0.875, 'loss': 0.56766}\n",
            "Step 48380: {'train_ae_loss': 0.66122, 'train_ucc_loss': 0.53893, 'train_ucc_acc': 0.78125, 'loss': 0.60007}\n",
            "Step 48400: {'train_ae_loss': 0.6969, 'train_ucc_loss': 0.42573, 'train_ucc_acc': 0.90625, 'loss': 0.56132}\n",
            "Step 48420: {'train_ae_loss': 0.65621, 'train_ucc_loss': 0.42481, 'train_ucc_acc': 0.875, 'loss': 0.54051}\n",
            "Step 48440: {'train_ae_loss': 0.67058, 'train_ucc_loss': 0.49653, 'train_ucc_acc': 0.8125, 'loss': 0.58356}\n",
            "Step 48460: {'train_ae_loss': 0.64467, 'train_ucc_loss': 0.46653, 'train_ucc_acc': 0.84375, 'loss': 0.5556}\n",
            "Step 48480: {'train_ae_loss': 0.67195, 'train_ucc_loss': 0.48901, 'train_ucc_acc': 0.84375, 'loss': 0.58048}\n",
            "Step 48500: {'train_ae_loss': 0.65598, 'train_ucc_loss': 0.53276, 'train_ucc_acc': 0.75, 'loss': 0.59437}\n",
            "Step 48520: {'train_ae_loss': 0.6644, 'train_ucc_loss': 0.45521, 'train_ucc_acc': 0.8125, 'loss': 0.5598}\n",
            "Step 48540: {'train_ae_loss': 0.67844, 'train_ucc_loss': 0.45134, 'train_ucc_acc': 0.84375, 'loss': 0.56489}\n",
            "Step 48560: {'train_ae_loss': 0.67574, 'train_ucc_loss': 0.40673, 'train_ucc_acc': 0.875, 'loss': 0.54123}\n",
            "Step 48580: {'train_ae_loss': 0.67445, 'train_ucc_loss': 0.41997, 'train_ucc_acc': 0.90625, 'loss': 0.54721}\n",
            "Step 48600: {'train_ae_loss': 0.67446, 'train_ucc_loss': 0.4132, 'train_ucc_acc': 0.90625, 'loss': 0.54383}\n",
            "Step 48620: {'train_ae_loss': 0.66833, 'train_ucc_loss': 0.52969, 'train_ucc_acc': 0.75, 'loss': 0.59901}\n",
            "Step 48640: {'train_ae_loss': 0.67205, 'train_ucc_loss': 0.53667, 'train_ucc_acc': 0.75, 'loss': 0.60436}\n",
            "Step 48660: {'train_ae_loss': 0.66695, 'train_ucc_loss': 0.48215, 'train_ucc_acc': 0.8125, 'loss': 0.57455}\n",
            "Step 48680: {'train_ae_loss': 0.65031, 'train_ucc_loss': 0.55405, 'train_ucc_acc': 0.75, 'loss': 0.60218}\n",
            "Step 48700: {'train_ae_loss': 0.64358, 'train_ucc_loss': 0.49859, 'train_ucc_acc': 0.78125, 'loss': 0.57108}\n",
            "Step 48720: {'train_ae_loss': 0.67172, 'train_ucc_loss': 0.52559, 'train_ucc_acc': 0.75, 'loss': 0.59866}\n",
            "Step 48740: {'train_ae_loss': 0.65838, 'train_ucc_loss': 0.43251, 'train_ucc_acc': 0.875, 'loss': 0.54545}\n",
            "Step 48760: {'train_ae_loss': 0.66449, 'train_ucc_loss': 0.45981, 'train_ucc_acc': 0.84375, 'loss': 0.56215}\n",
            "Step 48780: {'train_ae_loss': 0.65204, 'train_ucc_loss': 0.46805, 'train_ucc_acc': 0.84375, 'loss': 0.56005}\n",
            "Step 48800: {'train_ae_loss': 0.66577, 'train_ucc_loss': 0.4767, 'train_ucc_acc': 0.84375, 'loss': 0.57124}\n",
            "Step 48820: {'train_ae_loss': 0.67783, 'train_ucc_loss': 0.48389, 'train_ucc_acc': 0.8125, 'loss': 0.58086}\n",
            "Step 48840: {'train_ae_loss': 0.67927, 'train_ucc_loss': 0.47634, 'train_ucc_acc': 0.8125, 'loss': 0.5778}\n",
            "Step 48860: {'train_ae_loss': 0.67331, 'train_ucc_loss': 0.45531, 'train_ucc_acc': 0.875, 'loss': 0.56431}\n",
            "Step 48880: {'train_ae_loss': 0.68387, 'train_ucc_loss': 0.46267, 'train_ucc_acc': 0.84375, 'loss': 0.57327}\n",
            "Step 48900: {'train_ae_loss': 0.67976, 'train_ucc_loss': 0.38149, 'train_ucc_acc': 0.9375, 'loss': 0.53063}\n",
            "Step 48920: {'train_ae_loss': 0.69055, 'train_ucc_loss': 0.43553, 'train_ucc_acc': 0.84375, 'loss': 0.56304}\n",
            "Step 48940: {'train_ae_loss': 0.66601, 'train_ucc_loss': 0.41806, 'train_ucc_acc': 0.90625, 'loss': 0.54204}\n",
            "Step 48960: {'train_ae_loss': 0.65987, 'train_ucc_loss': 0.51096, 'train_ucc_acc': 0.84375, 'loss': 0.58542}\n",
            "Step 48980: {'train_ae_loss': 0.65263, 'train_ucc_loss': 0.50515, 'train_ucc_acc': 0.78125, 'loss': 0.57889}\n",
            "Step 49000: {'train_ae_loss': 0.66991, 'train_ucc_loss': 0.40531, 'train_ucc_acc': 0.90625, 'loss': 0.53761}\n",
            "step: 49000,eval_ae_loss: 0.65509,eval_ucc_loss: 0.50445,eval_ucc_acc: 0.7998\n",
            "Step 49020: {'train_ae_loss': 0.64698, 'train_ucc_loss': 0.38883, 'train_ucc_acc': 0.9375, 'loss': 0.51791}\n",
            "Step 49040: {'train_ae_loss': 0.66726, 'train_ucc_loss': 0.44014, 'train_ucc_acc': 0.875, 'loss': 0.5537}\n",
            "Step 49060: {'train_ae_loss': 0.67016, 'train_ucc_loss': 0.42529, 'train_ucc_acc': 0.84375, 'loss': 0.54773}\n",
            "Step 49080: {'train_ae_loss': 0.67977, 'train_ucc_loss': 0.42163, 'train_ucc_acc': 0.90625, 'loss': 0.5507}\n",
            "Step 49100: {'train_ae_loss': 0.6714, 'train_ucc_loss': 0.44985, 'train_ucc_acc': 0.84375, 'loss': 0.56063}\n",
            "Step 49120: {'train_ae_loss': 0.65483, 'train_ucc_loss': 0.50564, 'train_ucc_acc': 0.78125, 'loss': 0.58023}\n",
            "Step 49140: {'train_ae_loss': 0.6675, 'train_ucc_loss': 0.546, 'train_ucc_acc': 0.75, 'loss': 0.60675}\n",
            "Step 49160: {'train_ae_loss': 0.66661, 'train_ucc_loss': 0.45617, 'train_ucc_acc': 0.875, 'loss': 0.56139}\n",
            "Step 49180: {'train_ae_loss': 0.67539, 'train_ucc_loss': 0.51732, 'train_ucc_acc': 0.78125, 'loss': 0.59636}\n",
            "Step 49200: {'train_ae_loss': 0.68326, 'train_ucc_loss': 0.42641, 'train_ucc_acc': 0.875, 'loss': 0.55483}\n",
            "Step 49220: {'train_ae_loss': 0.65751, 'train_ucc_loss': 0.45349, 'train_ucc_acc': 0.875, 'loss': 0.5555}\n",
            "Step 49240: {'train_ae_loss': 0.66359, 'train_ucc_loss': 0.56385, 'train_ucc_acc': 0.71875, 'loss': 0.61372}\n",
            "Step 49260: {'train_ae_loss': 0.66958, 'train_ucc_loss': 0.55482, 'train_ucc_acc': 0.75, 'loss': 0.6122}\n",
            "Step 49280: {'train_ae_loss': 0.66916, 'train_ucc_loss': 0.48983, 'train_ucc_acc': 0.8125, 'loss': 0.5795}\n",
            "Step 49300: {'train_ae_loss': 0.66124, 'train_ucc_loss': 0.54068, 'train_ucc_acc': 0.75, 'loss': 0.60096}\n",
            "Step 49320: {'train_ae_loss': 0.66304, 'train_ucc_loss': 0.39149, 'train_ucc_acc': 0.9375, 'loss': 0.52726}\n",
            "Step 49340: {'train_ae_loss': 0.66584, 'train_ucc_loss': 0.50397, 'train_ucc_acc': 0.78125, 'loss': 0.5849}\n",
            "Step 49360: {'train_ae_loss': 0.66765, 'train_ucc_loss': 0.49538, 'train_ucc_acc': 0.8125, 'loss': 0.58151}\n",
            "Step 49380: {'train_ae_loss': 0.67167, 'train_ucc_loss': 0.455, 'train_ucc_acc': 0.875, 'loss': 0.56333}\n",
            "Step 49400: {'train_ae_loss': 0.67213, 'train_ucc_loss': 0.47633, 'train_ucc_acc': 0.78125, 'loss': 0.57423}\n",
            "Step 49420: {'train_ae_loss': 0.65444, 'train_ucc_loss': 0.54967, 'train_ucc_acc': 0.75, 'loss': 0.60206}\n",
            "Step 49440: {'train_ae_loss': 0.6856, 'train_ucc_loss': 0.3868, 'train_ucc_acc': 0.9375, 'loss': 0.5362}\n",
            "Step 49460: {'train_ae_loss': 0.66591, 'train_ucc_loss': 0.51708, 'train_ucc_acc': 0.78125, 'loss': 0.59149}\n",
            "Step 49480: {'train_ae_loss': 0.66177, 'train_ucc_loss': 0.48797, 'train_ucc_acc': 0.8125, 'loss': 0.57487}\n",
            "Step 49500: {'train_ae_loss': 0.65745, 'train_ucc_loss': 0.56841, 'train_ucc_acc': 0.78125, 'loss': 0.61293}\n",
            "Step 49520: {'train_ae_loss': 0.66628, 'train_ucc_loss': 0.53983, 'train_ucc_acc': 0.75, 'loss': 0.60306}\n",
            "Step 49540: {'train_ae_loss': 0.67691, 'train_ucc_loss': 0.60869, 'train_ucc_acc': 0.6875, 'loss': 0.6428}\n",
            "Step 49560: {'train_ae_loss': 0.66589, 'train_ucc_loss': 0.38082, 'train_ucc_acc': 0.9375, 'loss': 0.52336}\n",
            "Step 49580: {'train_ae_loss': 0.67259, 'train_ucc_loss': 0.47666, 'train_ucc_acc': 0.8125, 'loss': 0.57462}\n",
            "Step 49600: {'train_ae_loss': 0.65898, 'train_ucc_loss': 0.4125, 'train_ucc_acc': 0.90625, 'loss': 0.53574}\n",
            "Step 49620: {'train_ae_loss': 0.6662, 'train_ucc_loss': 0.53246, 'train_ucc_acc': 0.78125, 'loss': 0.59933}\n",
            "Step 49640: {'train_ae_loss': 0.67126, 'train_ucc_loss': 0.4319, 'train_ucc_acc': 0.875, 'loss': 0.55158}\n",
            "Step 49660: {'train_ae_loss': 0.65401, 'train_ucc_loss': 0.56722, 'train_ucc_acc': 0.75, 'loss': 0.61062}\n",
            "Step 49680: {'train_ae_loss': 0.68289, 'train_ucc_loss': 0.3841, 'train_ucc_acc': 0.9375, 'loss': 0.53349}\n",
            "Step 49700: {'train_ae_loss': 0.66045, 'train_ucc_loss': 0.56553, 'train_ucc_acc': 0.75, 'loss': 0.61299}\n",
            "Step 49720: {'train_ae_loss': 0.67348, 'train_ucc_loss': 0.52512, 'train_ucc_acc': 0.75, 'loss': 0.5993}\n",
            "Step 49740: {'train_ae_loss': 0.654, 'train_ucc_loss': 0.52905, 'train_ucc_acc': 0.78125, 'loss': 0.59153}\n",
            "Step 49760: {'train_ae_loss': 0.67105, 'train_ucc_loss': 0.35997, 'train_ucc_acc': 0.96875, 'loss': 0.51551}\n",
            "Step 49780: {'train_ae_loss': 0.6797, 'train_ucc_loss': 0.47646, 'train_ucc_acc': 0.84375, 'loss': 0.57808}\n",
            "Step 49800: {'train_ae_loss': 0.65381, 'train_ucc_loss': 0.34783, 'train_ucc_acc': 0.9375, 'loss': 0.50082}\n",
            "Step 49820: {'train_ae_loss': 0.66712, 'train_ucc_loss': 0.5364, 'train_ucc_acc': 0.75, 'loss': 0.60176}\n",
            "Step 49840: {'train_ae_loss': 0.67624, 'train_ucc_loss': 0.44603, 'train_ucc_acc': 0.84375, 'loss': 0.56113}\n",
            "Step 49860: {'train_ae_loss': 0.67058, 'train_ucc_loss': 0.59298, 'train_ucc_acc': 0.6875, 'loss': 0.63178}\n",
            "Step 49880: {'train_ae_loss': 0.67614, 'train_ucc_loss': 0.53018, 'train_ucc_acc': 0.78125, 'loss': 0.60316}\n",
            "Step 49900: {'train_ae_loss': 0.67337, 'train_ucc_loss': 0.41821, 'train_ucc_acc': 0.90625, 'loss': 0.54579}\n",
            "Step 49920: {'train_ae_loss': 0.65099, 'train_ucc_loss': 0.59565, 'train_ucc_acc': 0.6875, 'loss': 0.62332}\n",
            "Step 49940: {'train_ae_loss': 0.67305, 'train_ucc_loss': 0.46457, 'train_ucc_acc': 0.875, 'loss': 0.56881}\n",
            "Step 49960: {'train_ae_loss': 0.68251, 'train_ucc_loss': 0.40363, 'train_ucc_acc': 0.875, 'loss': 0.54307}\n",
            "Step 49980: {'train_ae_loss': 0.66731, 'train_ucc_loss': 0.40125, 'train_ucc_acc': 0.90625, 'loss': 0.53428}\n",
            "Step 50000: {'train_ae_loss': 0.67623, 'train_ucc_loss': 0.4808, 'train_ucc_acc': 0.84375, 'loss': 0.57851}\n",
            "step: 50000,eval_ae_loss: 0.66113,eval_ucc_loss: 0.50735,eval_ucc_acc: 0.79492\n",
            "Step 50020: {'train_ae_loss': 0.66534, 'train_ucc_loss': 0.49161, 'train_ucc_acc': 0.84375, 'loss': 0.57847}\n",
            "Step 50040: {'train_ae_loss': 0.66351, 'train_ucc_loss': 0.45574, 'train_ucc_acc': 0.84375, 'loss': 0.55963}\n",
            "Step 50060: {'train_ae_loss': 0.66345, 'train_ucc_loss': 0.5091, 'train_ucc_acc': 0.75, 'loss': 0.58627}\n",
            "Step 50080: {'train_ae_loss': 0.67925, 'train_ucc_loss': 0.3701, 'train_ucc_acc': 0.96875, 'loss': 0.52468}\n",
            "Step 50100: {'train_ae_loss': 0.65602, 'train_ucc_loss': 0.50402, 'train_ucc_acc': 0.8125, 'loss': 0.58002}\n",
            "Step 50120: {'train_ae_loss': 0.66197, 'train_ucc_loss': 0.45081, 'train_ucc_acc': 0.875, 'loss': 0.55639}\n",
            "Step 50140: {'train_ae_loss': 0.68163, 'train_ucc_loss': 0.4596, 'train_ucc_acc': 0.8125, 'loss': 0.57061}\n",
            "Step 50160: {'train_ae_loss': 0.67251, 'train_ucc_loss': 0.51582, 'train_ucc_acc': 0.8125, 'loss': 0.59416}\n",
            "Step 50180: {'train_ae_loss': 0.66441, 'train_ucc_loss': 0.48176, 'train_ucc_acc': 0.84375, 'loss': 0.57309}\n",
            "Step 50200: {'train_ae_loss': 0.66202, 'train_ucc_loss': 0.43778, 'train_ucc_acc': 0.90625, 'loss': 0.5499}\n",
            "Step 50220: {'train_ae_loss': 0.66303, 'train_ucc_loss': 0.58688, 'train_ucc_acc': 0.71875, 'loss': 0.62496}\n",
            "Step 50240: {'train_ae_loss': 0.65577, 'train_ucc_loss': 0.44957, 'train_ucc_acc': 0.875, 'loss': 0.55267}\n",
            "Step 50260: {'train_ae_loss': 0.68111, 'train_ucc_loss': 0.52283, 'train_ucc_acc': 0.78125, 'loss': 0.60197}\n",
            "Step 50280: {'train_ae_loss': 0.68586, 'train_ucc_loss': 0.49758, 'train_ucc_acc': 0.8125, 'loss': 0.59172}\n",
            "Step 50300: {'train_ae_loss': 0.66527, 'train_ucc_loss': 0.38842, 'train_ucc_acc': 0.9375, 'loss': 0.52684}\n",
            "Step 50320: {'train_ae_loss': 0.66085, 'train_ucc_loss': 0.56919, 'train_ucc_acc': 0.71875, 'loss': 0.61502}\n",
            "Step 50340: {'train_ae_loss': 0.6683, 'train_ucc_loss': 0.50731, 'train_ucc_acc': 0.78125, 'loss': 0.58781}\n",
            "Step 50360: {'train_ae_loss': 0.68096, 'train_ucc_loss': 0.44744, 'train_ucc_acc': 0.84375, 'loss': 0.5642}\n",
            "Step 50380: {'train_ae_loss': 0.6662, 'train_ucc_loss': 0.4378, 'train_ucc_acc': 0.90625, 'loss': 0.552}\n",
            "Step 50400: {'train_ae_loss': 0.68525, 'train_ucc_loss': 0.44314, 'train_ucc_acc': 0.90625, 'loss': 0.56419}\n",
            "Step 50420: {'train_ae_loss': 0.65464, 'train_ucc_loss': 0.45551, 'train_ucc_acc': 0.875, 'loss': 0.55508}\n",
            "Step 50440: {'train_ae_loss': 0.6705, 'train_ucc_loss': 0.43995, 'train_ucc_acc': 0.875, 'loss': 0.55523}\n",
            "Step 50460: {'train_ae_loss': 0.65655, 'train_ucc_loss': 0.52735, 'train_ucc_acc': 0.75, 'loss': 0.59195}\n",
            "Step 50480: {'train_ae_loss': 0.66417, 'train_ucc_loss': 0.49931, 'train_ucc_acc': 0.78125, 'loss': 0.58174}\n",
            "Step 50500: {'train_ae_loss': 0.65729, 'train_ucc_loss': 0.4817, 'train_ucc_acc': 0.8125, 'loss': 0.5695}\n",
            "Step 50520: {'train_ae_loss': 0.68258, 'train_ucc_loss': 0.44224, 'train_ucc_acc': 0.84375, 'loss': 0.56241}\n",
            "Step 50540: {'train_ae_loss': 0.64777, 'train_ucc_loss': 0.45694, 'train_ucc_acc': 0.84375, 'loss': 0.55235}\n",
            "Step 50560: {'train_ae_loss': 0.66397, 'train_ucc_loss': 0.46048, 'train_ucc_acc': 0.875, 'loss': 0.56222}\n",
            "Step 50580: {'train_ae_loss': 0.67853, 'train_ucc_loss': 0.46982, 'train_ucc_acc': 0.84375, 'loss': 0.57417}\n",
            "Step 50600: {'train_ae_loss': 0.67287, 'train_ucc_loss': 0.45335, 'train_ucc_acc': 0.84375, 'loss': 0.56311}\n",
            "Step 50620: {'train_ae_loss': 0.67195, 'train_ucc_loss': 0.49867, 'train_ucc_acc': 0.8125, 'loss': 0.58531}\n",
            "Step 50640: {'train_ae_loss': 0.66488, 'train_ucc_loss': 0.58614, 'train_ucc_acc': 0.6875, 'loss': 0.62551}\n",
            "Step 50660: {'train_ae_loss': 0.65638, 'train_ucc_loss': 0.39902, 'train_ucc_acc': 0.90625, 'loss': 0.5277}\n",
            "Step 50680: {'train_ae_loss': 0.6744, 'train_ucc_loss': 0.40299, 'train_ucc_acc': 0.90625, 'loss': 0.53869}\n",
            "Step 50700: {'train_ae_loss': 0.66544, 'train_ucc_loss': 0.43286, 'train_ucc_acc': 0.90625, 'loss': 0.54915}\n",
            "Step 50720: {'train_ae_loss': 0.67784, 'train_ucc_loss': 0.38459, 'train_ucc_acc': 0.9375, 'loss': 0.53121}\n",
            "Step 50740: {'train_ae_loss': 0.66358, 'train_ucc_loss': 0.40701, 'train_ucc_acc': 0.9375, 'loss': 0.53529}\n",
            "Step 50760: {'train_ae_loss': 0.67288, 'train_ucc_loss': 0.46397, 'train_ucc_acc': 0.875, 'loss': 0.56843}\n",
            "Step 50780: {'train_ae_loss': 0.67174, 'train_ucc_loss': 0.4215, 'train_ucc_acc': 0.90625, 'loss': 0.54662}\n",
            "Step 50800: {'train_ae_loss': 0.65272, 'train_ucc_loss': 0.4763, 'train_ucc_acc': 0.84375, 'loss': 0.56451}\n",
            "Step 50820: {'train_ae_loss': 0.66537, 'train_ucc_loss': 0.45204, 'train_ucc_acc': 0.84375, 'loss': 0.55871}\n",
            "Step 50840: {'train_ae_loss': 0.67933, 'train_ucc_loss': 0.4814, 'train_ucc_acc': 0.8125, 'loss': 0.58037}\n",
            "Step 50860: {'train_ae_loss': 0.66804, 'train_ucc_loss': 0.43855, 'train_ucc_acc': 0.875, 'loss': 0.5533}\n",
            "Step 50880: {'train_ae_loss': 0.66574, 'train_ucc_loss': 0.46122, 'train_ucc_acc': 0.8125, 'loss': 0.56348}\n",
            "Step 50900: {'train_ae_loss': 0.66464, 'train_ucc_loss': 0.49016, 'train_ucc_acc': 0.84375, 'loss': 0.5774}\n",
            "Step 50920: {'train_ae_loss': 0.65755, 'train_ucc_loss': 0.52888, 'train_ucc_acc': 0.78125, 'loss': 0.59321}\n",
            "Step 50940: {'train_ae_loss': 0.65277, 'train_ucc_loss': 0.44994, 'train_ucc_acc': 0.875, 'loss': 0.55135}\n",
            "Step 50960: {'train_ae_loss': 0.66022, 'train_ucc_loss': 0.44926, 'train_ucc_acc': 0.875, 'loss': 0.55474}\n",
            "Step 50980: {'train_ae_loss': 0.6834, 'train_ucc_loss': 0.63983, 'train_ucc_acc': 0.625, 'loss': 0.66162}\n",
            "Step 51000: {'train_ae_loss': 0.66879, 'train_ucc_loss': 0.48904, 'train_ucc_acc': 0.8125, 'loss': 0.57892}\n",
            "step: 51000,eval_ae_loss: 0.66047,eval_ucc_loss: 0.51924,eval_ucc_acc: 0.78711\n",
            "Step 51020: {'train_ae_loss': 0.68411, 'train_ucc_loss': 0.51585, 'train_ucc_acc': 0.8125, 'loss': 0.59998}\n",
            "Step 51040: {'train_ae_loss': 0.6644, 'train_ucc_loss': 0.49595, 'train_ucc_acc': 0.78125, 'loss': 0.58018}\n",
            "Step 51060: {'train_ae_loss': 0.69124, 'train_ucc_loss': 0.45649, 'train_ucc_acc': 0.84375, 'loss': 0.57386}\n",
            "Step 51080: {'train_ae_loss': 0.67059, 'train_ucc_loss': 0.54128, 'train_ucc_acc': 0.75, 'loss': 0.60594}\n",
            "Step 51100: {'train_ae_loss': 0.64477, 'train_ucc_loss': 0.53576, 'train_ucc_acc': 0.75, 'loss': 0.59026}\n",
            "Step 51120: {'train_ae_loss': 0.67264, 'train_ucc_loss': 0.62178, 'train_ucc_acc': 0.65625, 'loss': 0.64721}\n",
            "Step 51140: {'train_ae_loss': 0.66825, 'train_ucc_loss': 0.59613, 'train_ucc_acc': 0.6875, 'loss': 0.63219}\n",
            "Step 51160: {'train_ae_loss': 0.65492, 'train_ucc_loss': 0.47611, 'train_ucc_acc': 0.84375, 'loss': 0.56551}\n",
            "Step 51180: {'train_ae_loss': 0.675, 'train_ucc_loss': 0.4251, 'train_ucc_acc': 0.875, 'loss': 0.55005}\n",
            "Step 51200: {'train_ae_loss': 0.66773, 'train_ucc_loss': 0.44404, 'train_ucc_acc': 0.875, 'loss': 0.55588}\n",
            "Step 51220: {'train_ae_loss': 0.6722, 'train_ucc_loss': 0.40396, 'train_ucc_acc': 0.9375, 'loss': 0.53808}\n",
            "Step 51240: {'train_ae_loss': 0.67309, 'train_ucc_loss': 0.44596, 'train_ucc_acc': 0.875, 'loss': 0.55952}\n",
            "Step 51260: {'train_ae_loss': 0.66541, 'train_ucc_loss': 0.50435, 'train_ucc_acc': 0.78125, 'loss': 0.58488}\n",
            "Step 51280: {'train_ae_loss': 0.6787, 'train_ucc_loss': 0.4079, 'train_ucc_acc': 0.90625, 'loss': 0.5433}\n",
            "Step 51300: {'train_ae_loss': 0.66454, 'train_ucc_loss': 0.46488, 'train_ucc_acc': 0.8125, 'loss': 0.56471}\n",
            "Step 51320: {'train_ae_loss': 0.67351, 'train_ucc_loss': 0.55658, 'train_ucc_acc': 0.75, 'loss': 0.61505}\n",
            "Step 51340: {'train_ae_loss': 0.67446, 'train_ucc_loss': 0.45177, 'train_ucc_acc': 0.84375, 'loss': 0.56312}\n",
            "Step 51360: {'train_ae_loss': 0.67047, 'train_ucc_loss': 0.40624, 'train_ucc_acc': 0.875, 'loss': 0.53835}\n",
            "Step 51380: {'train_ae_loss': 0.66914, 'train_ucc_loss': 0.44998, 'train_ucc_acc': 0.875, 'loss': 0.55956}\n",
            "Step 51400: {'train_ae_loss': 0.66749, 'train_ucc_loss': 0.4474, 'train_ucc_acc': 0.84375, 'loss': 0.55745}\n",
            "Step 51420: {'train_ae_loss': 0.69822, 'train_ucc_loss': 0.35875, 'train_ucc_acc': 0.96875, 'loss': 0.52848}\n",
            "Step 51440: {'train_ae_loss': 0.67066, 'train_ucc_loss': 0.51078, 'train_ucc_acc': 0.75, 'loss': 0.59072}\n",
            "Step 51460: {'train_ae_loss': 0.66468, 'train_ucc_loss': 0.47729, 'train_ucc_acc': 0.84375, 'loss': 0.57099}\n",
            "Step 51480: {'train_ae_loss': 0.65505, 'train_ucc_loss': 0.65862, 'train_ucc_acc': 0.625, 'loss': 0.65683}\n",
            "Step 51500: {'train_ae_loss': 0.65937, 'train_ucc_loss': 0.51088, 'train_ucc_acc': 0.8125, 'loss': 0.58513}\n",
            "Step 51520: {'train_ae_loss': 0.67097, 'train_ucc_loss': 0.49389, 'train_ucc_acc': 0.8125, 'loss': 0.58243}\n",
            "Step 51540: {'train_ae_loss': 0.66511, 'train_ucc_loss': 0.43007, 'train_ucc_acc': 0.875, 'loss': 0.54759}\n",
            "Step 51560: {'train_ae_loss': 0.6667, 'train_ucc_loss': 0.40883, 'train_ucc_acc': 0.9375, 'loss': 0.53776}\n",
            "Step 51580: {'train_ae_loss': 0.68344, 'train_ucc_loss': 0.46849, 'train_ucc_acc': 0.875, 'loss': 0.57597}\n",
            "Step 51600: {'train_ae_loss': 0.65518, 'train_ucc_loss': 0.44688, 'train_ucc_acc': 0.875, 'loss': 0.55103}\n",
            "Step 51620: {'train_ae_loss': 0.67374, 'train_ucc_loss': 0.48843, 'train_ucc_acc': 0.8125, 'loss': 0.58108}\n",
            "Step 51640: {'train_ae_loss': 0.6702, 'train_ucc_loss': 0.45882, 'train_ucc_acc': 0.84375, 'loss': 0.56451}\n",
            "Step 51660: {'train_ae_loss': 0.66613, 'train_ucc_loss': 0.43675, 'train_ucc_acc': 0.875, 'loss': 0.55144}\n",
            "Step 51680: {'train_ae_loss': 0.67874, 'train_ucc_loss': 0.43735, 'train_ucc_acc': 0.875, 'loss': 0.55805}\n",
            "Step 51700: {'train_ae_loss': 0.66096, 'train_ucc_loss': 0.49191, 'train_ucc_acc': 0.8125, 'loss': 0.57644}\n",
            "Step 51720: {'train_ae_loss': 0.67013, 'train_ucc_loss': 0.43269, 'train_ucc_acc': 0.84375, 'loss': 0.55141}\n",
            "Step 51740: {'train_ae_loss': 0.67577, 'train_ucc_loss': 0.5338, 'train_ucc_acc': 0.78125, 'loss': 0.60478}\n",
            "Step 51760: {'train_ae_loss': 0.64987, 'train_ucc_loss': 0.52403, 'train_ucc_acc': 0.78125, 'loss': 0.58695}\n",
            "Step 51780: {'train_ae_loss': 0.66677, 'train_ucc_loss': 0.50645, 'train_ucc_acc': 0.78125, 'loss': 0.58661}\n",
            "Step 51800: {'train_ae_loss': 0.67849, 'train_ucc_loss': 0.54288, 'train_ucc_acc': 0.6875, 'loss': 0.61069}\n",
            "Step 51820: {'train_ae_loss': 0.66042, 'train_ucc_loss': 0.44829, 'train_ucc_acc': 0.875, 'loss': 0.55436}\n",
            "Step 51840: {'train_ae_loss': 0.68676, 'train_ucc_loss': 0.40027, 'train_ucc_acc': 0.9375, 'loss': 0.54351}\n",
            "Step 51860: {'train_ae_loss': 0.66378, 'train_ucc_loss': 0.54623, 'train_ucc_acc': 0.75, 'loss': 0.605}\n",
            "Step 51880: {'train_ae_loss': 0.67237, 'train_ucc_loss': 0.41637, 'train_ucc_acc': 0.90625, 'loss': 0.54437}\n",
            "Step 51900: {'train_ae_loss': 0.66347, 'train_ucc_loss': 0.47435, 'train_ucc_acc': 0.84375, 'loss': 0.56891}\n",
            "Step 51920: {'train_ae_loss': 0.67353, 'train_ucc_loss': 0.37669, 'train_ucc_acc': 0.9375, 'loss': 0.52511}\n",
            "Step 51940: {'train_ae_loss': 0.66161, 'train_ucc_loss': 0.48351, 'train_ucc_acc': 0.84375, 'loss': 0.57256}\n",
            "Step 51960: {'train_ae_loss': 0.6533, 'train_ucc_loss': 0.45627, 'train_ucc_acc': 0.875, 'loss': 0.55478}\n",
            "Step 51980: {'train_ae_loss': 0.67005, 'train_ucc_loss': 0.35952, 'train_ucc_acc': 0.96875, 'loss': 0.51479}\n",
            "Step 52000: {'train_ae_loss': 0.67196, 'train_ucc_loss': 0.46815, 'train_ucc_acc': 0.8125, 'loss': 0.57006}\n",
            "step: 52000,eval_ae_loss: 0.65928,eval_ucc_loss: 0.50542,eval_ucc_acc: 0.79199\n",
            "Step 52020: {'train_ae_loss': 0.69345, 'train_ucc_loss': 0.4862, 'train_ucc_acc': 0.84375, 'loss': 0.58983}\n",
            "Step 52040: {'train_ae_loss': 0.67402, 'train_ucc_loss': 0.60018, 'train_ucc_acc': 0.6875, 'loss': 0.6371}\n",
            "Step 52060: {'train_ae_loss': 0.67117, 'train_ucc_loss': 0.40564, 'train_ucc_acc': 0.9375, 'loss': 0.53841}\n",
            "Step 52080: {'train_ae_loss': 0.67421, 'train_ucc_loss': 0.47553, 'train_ucc_acc': 0.78125, 'loss': 0.57487}\n",
            "Step 52100: {'train_ae_loss': 0.66485, 'train_ucc_loss': 0.70377, 'train_ucc_acc': 0.59375, 'loss': 0.68431}\n",
            "Step 52120: {'train_ae_loss': 0.66361, 'train_ucc_loss': 0.49281, 'train_ucc_acc': 0.78125, 'loss': 0.57821}\n",
            "Step 52140: {'train_ae_loss': 0.66991, 'train_ucc_loss': 0.5432, 'train_ucc_acc': 0.78125, 'loss': 0.60655}\n",
            "Step 52160: {'train_ae_loss': 0.6855, 'train_ucc_loss': 0.3433, 'train_ucc_acc': 1.0, 'loss': 0.5144}\n",
            "Step 52180: {'train_ae_loss': 0.69198, 'train_ucc_loss': 0.52824, 'train_ucc_acc': 0.8125, 'loss': 0.61011}\n",
            "Step 52200: {'train_ae_loss': 0.68238, 'train_ucc_loss': 0.55521, 'train_ucc_acc': 0.75, 'loss': 0.61879}\n",
            "Step 52220: {'train_ae_loss': 0.67233, 'train_ucc_loss': 0.47609, 'train_ucc_acc': 0.8125, 'loss': 0.57421}\n",
            "Step 52240: {'train_ae_loss': 0.67209, 'train_ucc_loss': 0.4228, 'train_ucc_acc': 0.875, 'loss': 0.54744}\n",
            "Step 52260: {'train_ae_loss': 0.65425, 'train_ucc_loss': 0.46064, 'train_ucc_acc': 0.84375, 'loss': 0.55745}\n",
            "Step 52280: {'train_ae_loss': 0.6711, 'train_ucc_loss': 0.53198, 'train_ucc_acc': 0.78125, 'loss': 0.60154}\n",
            "Step 52300: {'train_ae_loss': 0.65857, 'train_ucc_loss': 0.45689, 'train_ucc_acc': 0.875, 'loss': 0.55773}\n",
            "Step 52320: {'train_ae_loss': 0.66302, 'train_ucc_loss': 0.46317, 'train_ucc_acc': 0.8125, 'loss': 0.5631}\n",
            "Step 52340: {'train_ae_loss': 0.68084, 'train_ucc_loss': 0.43614, 'train_ucc_acc': 0.875, 'loss': 0.55849}\n",
            "Step 52360: {'train_ae_loss': 0.67713, 'train_ucc_loss': 0.5375, 'train_ucc_acc': 0.75, 'loss': 0.60732}\n",
            "Step 52380: {'train_ae_loss': 0.67045, 'train_ucc_loss': 0.48346, 'train_ucc_acc': 0.8125, 'loss': 0.57695}\n",
            "Step 52400: {'train_ae_loss': 0.67197, 'train_ucc_loss': 0.49253, 'train_ucc_acc': 0.8125, 'loss': 0.58225}\n",
            "Step 52420: {'train_ae_loss': 0.67985, 'train_ucc_loss': 0.56902, 'train_ucc_acc': 0.75, 'loss': 0.62444}\n",
            "Step 52440: {'train_ae_loss': 0.67277, 'train_ucc_loss': 0.3637, 'train_ucc_acc': 0.96875, 'loss': 0.51823}\n",
            "Step 52460: {'train_ae_loss': 0.64336, 'train_ucc_loss': 0.43764, 'train_ucc_acc': 0.90625, 'loss': 0.5405}\n",
            "Step 52480: {'train_ae_loss': 0.66261, 'train_ucc_loss': 0.48051, 'train_ucc_acc': 0.84375, 'loss': 0.57156}\n",
            "Step 52500: {'train_ae_loss': 0.6595, 'train_ucc_loss': 0.61316, 'train_ucc_acc': 0.6875, 'loss': 0.63633}\n",
            "Step 52520: {'train_ae_loss': 0.66922, 'train_ucc_loss': 0.578, 'train_ucc_acc': 0.6875, 'loss': 0.62361}\n",
            "Step 52540: {'train_ae_loss': 0.66385, 'train_ucc_loss': 0.46345, 'train_ucc_acc': 0.84375, 'loss': 0.56365}\n",
            "Step 52560: {'train_ae_loss': 0.66475, 'train_ucc_loss': 0.61649, 'train_ucc_acc': 0.6875, 'loss': 0.64062}\n",
            "Step 52580: {'train_ae_loss': 0.66941, 'train_ucc_loss': 0.41615, 'train_ucc_acc': 0.90625, 'loss': 0.54278}\n",
            "Step 52600: {'train_ae_loss': 0.65654, 'train_ucc_loss': 0.55557, 'train_ucc_acc': 0.78125, 'loss': 0.60606}\n",
            "Step 52620: {'train_ae_loss': 0.66473, 'train_ucc_loss': 0.51189, 'train_ucc_acc': 0.75, 'loss': 0.58831}\n",
            "Step 52640: {'train_ae_loss': 0.66466, 'train_ucc_loss': 0.58157, 'train_ucc_acc': 0.6875, 'loss': 0.62312}\n",
            "Step 52660: {'train_ae_loss': 0.67179, 'train_ucc_loss': 0.39459, 'train_ucc_acc': 0.90625, 'loss': 0.53319}\n",
            "Step 52680: {'train_ae_loss': 0.66791, 'train_ucc_loss': 0.48416, 'train_ucc_acc': 0.8125, 'loss': 0.57604}\n",
            "Step 52700: {'train_ae_loss': 0.68234, 'train_ucc_loss': 0.50194, 'train_ucc_acc': 0.8125, 'loss': 0.59214}\n",
            "Step 52720: {'train_ae_loss': 0.65523, 'train_ucc_loss': 0.53742, 'train_ucc_acc': 0.78125, 'loss': 0.59633}\n",
            "Step 52740: {'train_ae_loss': 0.6556, 'train_ucc_loss': 0.56433, 'train_ucc_acc': 0.75, 'loss': 0.60997}\n",
            "Step 52760: {'train_ae_loss': 0.65327, 'train_ucc_loss': 0.50068, 'train_ucc_acc': 0.84375, 'loss': 0.57697}\n",
            "Step 52780: {'train_ae_loss': 0.67162, 'train_ucc_loss': 0.41156, 'train_ucc_acc': 0.90625, 'loss': 0.54159}\n",
            "Step 52800: {'train_ae_loss': 0.67997, 'train_ucc_loss': 0.46424, 'train_ucc_acc': 0.8125, 'loss': 0.57211}\n",
            "Step 52820: {'train_ae_loss': 0.66765, 'train_ucc_loss': 0.48247, 'train_ucc_acc': 0.78125, 'loss': 0.57506}\n",
            "Step 52840: {'train_ae_loss': 0.68645, 'train_ucc_loss': 0.45607, 'train_ucc_acc': 0.84375, 'loss': 0.57126}\n",
            "Step 52860: {'train_ae_loss': 0.66126, 'train_ucc_loss': 0.53274, 'train_ucc_acc': 0.75, 'loss': 0.597}\n",
            "Step 52880: {'train_ae_loss': 0.65031, 'train_ucc_loss': 0.40049, 'train_ucc_acc': 0.90625, 'loss': 0.5254}\n",
            "Step 52900: {'train_ae_loss': 0.67513, 'train_ucc_loss': 0.37039, 'train_ucc_acc': 0.9375, 'loss': 0.52276}\n",
            "Step 52920: {'train_ae_loss': 0.65699, 'train_ucc_loss': 0.52975, 'train_ucc_acc': 0.75, 'loss': 0.59337}\n",
            "Step 52940: {'train_ae_loss': 0.67788, 'train_ucc_loss': 0.46536, 'train_ucc_acc': 0.8125, 'loss': 0.57162}\n",
            "Step 52960: {'train_ae_loss': 0.66392, 'train_ucc_loss': 0.49104, 'train_ucc_acc': 0.84375, 'loss': 0.57748}\n",
            "Step 52980: {'train_ae_loss': 0.6621, 'train_ucc_loss': 0.47884, 'train_ucc_acc': 0.84375, 'loss': 0.57047}\n",
            "Step 53000: {'train_ae_loss': 0.65384, 'train_ucc_loss': 0.50066, 'train_ucc_acc': 0.78125, 'loss': 0.57725}\n",
            "step: 53000,eval_ae_loss: 0.65527,eval_ucc_loss: 0.50369,eval_ucc_acc: 0.80566\n",
            "Step 53020: {'train_ae_loss': 0.67576, 'train_ucc_loss': 0.52299, 'train_ucc_acc': 0.75, 'loss': 0.59937}\n",
            "Step 53040: {'train_ae_loss': 0.65935, 'train_ucc_loss': 0.49909, 'train_ucc_acc': 0.78125, 'loss': 0.57922}\n",
            "Step 53060: {'train_ae_loss': 0.67549, 'train_ucc_loss': 0.60453, 'train_ucc_acc': 0.71875, 'loss': 0.64001}\n",
            "Step 53080: {'train_ae_loss': 0.66758, 'train_ucc_loss': 0.35942, 'train_ucc_acc': 0.96875, 'loss': 0.5135}\n",
            "Step 53100: {'train_ae_loss': 0.66532, 'train_ucc_loss': 0.42485, 'train_ucc_acc': 0.84375, 'loss': 0.54508}\n",
            "Step 53120: {'train_ae_loss': 0.67411, 'train_ucc_loss': 0.54458, 'train_ucc_acc': 0.78125, 'loss': 0.60935}\n",
            "Step 53140: {'train_ae_loss': 0.68272, 'train_ucc_loss': 0.49882, 'train_ucc_acc': 0.8125, 'loss': 0.59077}\n",
            "Step 53160: {'train_ae_loss': 0.65918, 'train_ucc_loss': 0.52362, 'train_ucc_acc': 0.71875, 'loss': 0.5914}\n",
            "Step 53180: {'train_ae_loss': 0.67803, 'train_ucc_loss': 0.42726, 'train_ucc_acc': 0.875, 'loss': 0.55264}\n",
            "Step 53200: {'train_ae_loss': 0.66547, 'train_ucc_loss': 0.42841, 'train_ucc_acc': 0.875, 'loss': 0.54694}\n",
            "Step 53220: {'train_ae_loss': 0.65219, 'train_ucc_loss': 0.3877, 'train_ucc_acc': 0.9375, 'loss': 0.51995}\n",
            "Step 53240: {'train_ae_loss': 0.68205, 'train_ucc_loss': 0.5314, 'train_ucc_acc': 0.78125, 'loss': 0.60672}\n",
            "Step 53260: {'train_ae_loss': 0.67335, 'train_ucc_loss': 0.4063, 'train_ucc_acc': 0.875, 'loss': 0.53982}\n",
            "Step 53280: {'train_ae_loss': 0.6732, 'train_ucc_loss': 0.54938, 'train_ucc_acc': 0.75, 'loss': 0.61129}\n",
            "Step 53300: {'train_ae_loss': 0.64575, 'train_ucc_loss': 0.4933, 'train_ucc_acc': 0.8125, 'loss': 0.56952}\n",
            "Step 53320: {'train_ae_loss': 0.67177, 'train_ucc_loss': 0.43029, 'train_ucc_acc': 0.875, 'loss': 0.55103}\n",
            "Step 53340: {'train_ae_loss': 0.67144, 'train_ucc_loss': 0.54252, 'train_ucc_acc': 0.78125, 'loss': 0.60698}\n",
            "Step 53360: {'train_ae_loss': 0.66326, 'train_ucc_loss': 0.52441, 'train_ucc_acc': 0.75, 'loss': 0.59384}\n",
            "Step 53380: {'train_ae_loss': 0.69381, 'train_ucc_loss': 0.54939, 'train_ucc_acc': 0.71875, 'loss': 0.6216}\n",
            "Step 53400: {'train_ae_loss': 0.66425, 'train_ucc_loss': 0.52106, 'train_ucc_acc': 0.78125, 'loss': 0.59266}\n",
            "Step 53420: {'train_ae_loss': 0.66759, 'train_ucc_loss': 0.45908, 'train_ucc_acc': 0.84375, 'loss': 0.56334}\n",
            "Step 53440: {'train_ae_loss': 0.66858, 'train_ucc_loss': 0.47967, 'train_ucc_acc': 0.84375, 'loss': 0.57412}\n",
            "Step 53460: {'train_ae_loss': 0.67317, 'train_ucc_loss': 0.56117, 'train_ucc_acc': 0.75, 'loss': 0.61717}\n",
            "Step 53480: {'train_ae_loss': 0.67922, 'train_ucc_loss': 0.37253, 'train_ucc_acc': 0.9375, 'loss': 0.52587}\n",
            "Step 53500: {'train_ae_loss': 0.65447, 'train_ucc_loss': 0.42412, 'train_ucc_acc': 0.90625, 'loss': 0.53929}\n",
            "Step 53520: {'train_ae_loss': 0.66597, 'train_ucc_loss': 0.4251, 'train_ucc_acc': 0.875, 'loss': 0.54554}\n",
            "Step 53540: {'train_ae_loss': 0.64831, 'train_ucc_loss': 0.45704, 'train_ucc_acc': 0.84375, 'loss': 0.55268}\n",
            "Step 53560: {'train_ae_loss': 0.68018, 'train_ucc_loss': 0.64418, 'train_ucc_acc': 0.65625, 'loss': 0.66218}\n",
            "Step 53580: {'train_ae_loss': 0.68803, 'train_ucc_loss': 0.44152, 'train_ucc_acc': 0.84375, 'loss': 0.56477}\n",
            "Step 53600: {'train_ae_loss': 0.67196, 'train_ucc_loss': 0.48621, 'train_ucc_acc': 0.84375, 'loss': 0.57908}\n",
            "Step 53620: {'train_ae_loss': 0.6705, 'train_ucc_loss': 0.47333, 'train_ucc_acc': 0.84375, 'loss': 0.57192}\n",
            "Step 53640: {'train_ae_loss': 0.65426, 'train_ucc_loss': 0.45051, 'train_ucc_acc': 0.875, 'loss': 0.55238}\n",
            "Step 53660: {'train_ae_loss': 0.65328, 'train_ucc_loss': 0.57152, 'train_ucc_acc': 0.71875, 'loss': 0.6124}\n",
            "Step 53680: {'train_ae_loss': 0.65775, 'train_ucc_loss': 0.49024, 'train_ucc_acc': 0.8125, 'loss': 0.574}\n",
            "Step 53700: {'train_ae_loss': 0.66029, 'train_ucc_loss': 0.50456, 'train_ucc_acc': 0.78125, 'loss': 0.58243}\n",
            "Step 53720: {'train_ae_loss': 0.67545, 'train_ucc_loss': 0.37423, 'train_ucc_acc': 0.9375, 'loss': 0.52484}\n",
            "Step 53740: {'train_ae_loss': 0.67151, 'train_ucc_loss': 0.4515, 'train_ucc_acc': 0.84375, 'loss': 0.5615}\n",
            "Step 53760: {'train_ae_loss': 0.66887, 'train_ucc_loss': 0.52962, 'train_ucc_acc': 0.8125, 'loss': 0.59924}\n",
            "Step 53780: {'train_ae_loss': 0.67549, 'train_ucc_loss': 0.43209, 'train_ucc_acc': 0.875, 'loss': 0.55379}\n",
            "Step 53800: {'train_ae_loss': 0.66351, 'train_ucc_loss': 0.45487, 'train_ucc_acc': 0.84375, 'loss': 0.55919}\n",
            "Step 53820: {'train_ae_loss': 0.67412, 'train_ucc_loss': 0.42691, 'train_ucc_acc': 0.84375, 'loss': 0.55052}\n",
            "Step 53840: {'train_ae_loss': 0.67307, 'train_ucc_loss': 0.59054, 'train_ucc_acc': 0.6875, 'loss': 0.6318}\n",
            "Step 53860: {'train_ae_loss': 0.6871, 'train_ucc_loss': 0.40472, 'train_ucc_acc': 0.90625, 'loss': 0.54591}\n",
            "Step 53880: {'train_ae_loss': 0.67113, 'train_ucc_loss': 0.5428, 'train_ucc_acc': 0.78125, 'loss': 0.60696}\n",
            "Step 53900: {'train_ae_loss': 0.66389, 'train_ucc_loss': 0.52195, 'train_ucc_acc': 0.78125, 'loss': 0.59292}\n",
            "Step 53920: {'train_ae_loss': 0.66383, 'train_ucc_loss': 0.45285, 'train_ucc_acc': 0.875, 'loss': 0.55834}\n",
            "Step 53940: {'train_ae_loss': 0.6723, 'train_ucc_loss': 0.40598, 'train_ucc_acc': 0.90625, 'loss': 0.53914}\n",
            "Step 53960: {'train_ae_loss': 0.65458, 'train_ucc_loss': 0.35095, 'train_ucc_acc': 0.96875, 'loss': 0.50277}\n",
            "Step 53980: {'train_ae_loss': 0.67104, 'train_ucc_loss': 0.52306, 'train_ucc_acc': 0.78125, 'loss': 0.59705}\n",
            "Step 54000: {'train_ae_loss': 0.65968, 'train_ucc_loss': 0.38331, 'train_ucc_acc': 0.9375, 'loss': 0.52149}\n",
            "step: 54000,eval_ae_loss: 0.65163,eval_ucc_loss: 0.49097,eval_ucc_acc: 0.81641\n",
            "Step 54020: {'train_ae_loss': 0.66524, 'train_ucc_loss': 0.41107, 'train_ucc_acc': 0.90625, 'loss': 0.53815}\n",
            "Step 54040: {'train_ae_loss': 0.66316, 'train_ucc_loss': 0.42806, 'train_ucc_acc': 0.90625, 'loss': 0.54561}\n",
            "Step 54060: {'train_ae_loss': 0.67316, 'train_ucc_loss': 0.42741, 'train_ucc_acc': 0.90625, 'loss': 0.55029}\n",
            "Step 54080: {'train_ae_loss': 0.6681, 'train_ucc_loss': 0.40263, 'train_ucc_acc': 0.90625, 'loss': 0.53536}\n",
            "Step 54100: {'train_ae_loss': 0.68031, 'train_ucc_loss': 0.43093, 'train_ucc_acc': 0.90625, 'loss': 0.55562}\n",
            "Step 54120: {'train_ae_loss': 0.66253, 'train_ucc_loss': 0.47285, 'train_ucc_acc': 0.8125, 'loss': 0.56769}\n",
            "Step 54140: {'train_ae_loss': 0.67555, 'train_ucc_loss': 0.45223, 'train_ucc_acc': 0.875, 'loss': 0.56389}\n",
            "Step 54160: {'train_ae_loss': 0.67083, 'train_ucc_loss': 0.49272, 'train_ucc_acc': 0.8125, 'loss': 0.58177}\n",
            "Step 54180: {'train_ae_loss': 0.67158, 'train_ucc_loss': 0.52783, 'train_ucc_acc': 0.78125, 'loss': 0.59971}\n",
            "Step 54200: {'train_ae_loss': 0.65565, 'train_ucc_loss': 0.48637, 'train_ucc_acc': 0.84375, 'loss': 0.57101}\n",
            "Step 54220: {'train_ae_loss': 0.65924, 'train_ucc_loss': 0.44236, 'train_ucc_acc': 0.84375, 'loss': 0.5508}\n",
            "Step 54240: {'train_ae_loss': 0.65149, 'train_ucc_loss': 0.46936, 'train_ucc_acc': 0.84375, 'loss': 0.56043}\n",
            "Step 54260: {'train_ae_loss': 0.66254, 'train_ucc_loss': 0.45346, 'train_ucc_acc': 0.84375, 'loss': 0.558}\n",
            "Step 54280: {'train_ae_loss': 0.69416, 'train_ucc_loss': 0.35872, 'train_ucc_acc': 0.96875, 'loss': 0.52644}\n",
            "Step 54300: {'train_ae_loss': 0.66275, 'train_ucc_loss': 0.59967, 'train_ucc_acc': 0.6875, 'loss': 0.63121}\n",
            "Step 54320: {'train_ae_loss': 0.66516, 'train_ucc_loss': 0.52516, 'train_ucc_acc': 0.78125, 'loss': 0.59516}\n",
            "Step 54340: {'train_ae_loss': 0.67382, 'train_ucc_loss': 0.53865, 'train_ucc_acc': 0.71875, 'loss': 0.60623}\n",
            "Step 54360: {'train_ae_loss': 0.6647, 'train_ucc_loss': 0.44435, 'train_ucc_acc': 0.84375, 'loss': 0.55452}\n",
            "Step 54380: {'train_ae_loss': 0.66165, 'train_ucc_loss': 0.49522, 'train_ucc_acc': 0.8125, 'loss': 0.57844}\n",
            "Step 54400: {'train_ae_loss': 0.65517, 'train_ucc_loss': 0.54471, 'train_ucc_acc': 0.75, 'loss': 0.59994}\n",
            "Step 54420: {'train_ae_loss': 0.65369, 'train_ucc_loss': 0.45, 'train_ucc_acc': 0.875, 'loss': 0.55184}\n",
            "Step 54440: {'train_ae_loss': 0.6654, 'train_ucc_loss': 0.57743, 'train_ucc_acc': 0.65625, 'loss': 0.62141}\n",
            "Step 54460: {'train_ae_loss': 0.66515, 'train_ucc_loss': 0.52526, 'train_ucc_acc': 0.78125, 'loss': 0.5952}\n",
            "Step 54480: {'train_ae_loss': 0.65787, 'train_ucc_loss': 0.48103, 'train_ucc_acc': 0.84375, 'loss': 0.56945}\n",
            "Step 54500: {'train_ae_loss': 0.67388, 'train_ucc_loss': 0.37527, 'train_ucc_acc': 0.9375, 'loss': 0.52458}\n",
            "Step 54520: {'train_ae_loss': 0.67133, 'train_ucc_loss': 0.4224, 'train_ucc_acc': 0.875, 'loss': 0.54686}\n",
            "Step 54540: {'train_ae_loss': 0.64994, 'train_ucc_loss': 0.45474, 'train_ucc_acc': 0.90625, 'loss': 0.55234}\n",
            "Step 54560: {'train_ae_loss': 0.67974, 'train_ucc_loss': 0.55928, 'train_ucc_acc': 0.71875, 'loss': 0.61951}\n",
            "Step 54580: {'train_ae_loss': 0.64363, 'train_ucc_loss': 0.60599, 'train_ucc_acc': 0.65625, 'loss': 0.62481}\n",
            "Step 54600: {'train_ae_loss': 0.66667, 'train_ucc_loss': 0.41424, 'train_ucc_acc': 0.90625, 'loss': 0.54045}\n",
            "Step 54620: {'train_ae_loss': 0.65908, 'train_ucc_loss': 0.51256, 'train_ucc_acc': 0.8125, 'loss': 0.58582}\n",
            "Step 54640: {'train_ae_loss': 0.68666, 'train_ucc_loss': 0.49926, 'train_ucc_acc': 0.8125, 'loss': 0.59296}\n",
            "Step 54660: {'train_ae_loss': 0.66796, 'train_ucc_loss': 0.49663, 'train_ucc_acc': 0.8125, 'loss': 0.58229}\n",
            "Step 54680: {'train_ae_loss': 0.67572, 'train_ucc_loss': 0.39962, 'train_ucc_acc': 0.90625, 'loss': 0.53767}\n",
            "Step 54700: {'train_ae_loss': 0.66114, 'train_ucc_loss': 0.40622, 'train_ucc_acc': 0.9375, 'loss': 0.53368}\n",
            "Step 54720: {'train_ae_loss': 0.66371, 'train_ucc_loss': 0.50932, 'train_ucc_acc': 0.78125, 'loss': 0.58651}\n",
            "Step 54740: {'train_ae_loss': 0.68125, 'train_ucc_loss': 0.41671, 'train_ucc_acc': 0.90625, 'loss': 0.54898}\n",
            "Step 54760: {'train_ae_loss': 0.6761, 'train_ucc_loss': 0.47832, 'train_ucc_acc': 0.8125, 'loss': 0.57721}\n",
            "Step 54780: {'train_ae_loss': 0.67123, 'train_ucc_loss': 0.50894, 'train_ucc_acc': 0.78125, 'loss': 0.59009}\n",
            "Step 54800: {'train_ae_loss': 0.67608, 'train_ucc_loss': 0.48526, 'train_ucc_acc': 0.8125, 'loss': 0.58067}\n",
            "Step 54820: {'train_ae_loss': 0.66142, 'train_ucc_loss': 0.55951, 'train_ucc_acc': 0.75, 'loss': 0.61047}\n",
            "Step 54840: {'train_ae_loss': 0.67285, 'train_ucc_loss': 0.509, 'train_ucc_acc': 0.78125, 'loss': 0.59092}\n",
            "Step 54860: {'train_ae_loss': 0.66585, 'train_ucc_loss': 0.49078, 'train_ucc_acc': 0.78125, 'loss': 0.57831}\n",
            "Step 54880: {'train_ae_loss': 0.67191, 'train_ucc_loss': 0.51379, 'train_ucc_acc': 0.78125, 'loss': 0.59285}\n",
            "Step 54900: {'train_ae_loss': 0.67966, 'train_ucc_loss': 0.44978, 'train_ucc_acc': 0.875, 'loss': 0.56472}\n",
            "Step 54920: {'train_ae_loss': 0.67902, 'train_ucc_loss': 0.45154, 'train_ucc_acc': 0.84375, 'loss': 0.56528}\n",
            "Step 54940: {'train_ae_loss': 0.64565, 'train_ucc_loss': 0.45715, 'train_ucc_acc': 0.875, 'loss': 0.5514}\n",
            "Step 54960: {'train_ae_loss': 0.67111, 'train_ucc_loss': 0.39607, 'train_ucc_acc': 0.90625, 'loss': 0.53359}\n",
            "Step 54980: {'train_ae_loss': 0.66426, 'train_ucc_loss': 0.56636, 'train_ucc_acc': 0.75, 'loss': 0.61531}\n",
            "Step 55000: {'train_ae_loss': 0.65728, 'train_ucc_loss': 0.46107, 'train_ucc_acc': 0.84375, 'loss': 0.55917}\n",
            "step: 55000,eval_ae_loss: 0.65949,eval_ucc_loss: 0.50632,eval_ucc_acc: 0.79785\n",
            "Step 55020: {'train_ae_loss': 0.67493, 'train_ucc_loss': 0.49801, 'train_ucc_acc': 0.84375, 'loss': 0.58647}\n",
            "Step 55040: {'train_ae_loss': 0.67623, 'train_ucc_loss': 0.50554, 'train_ucc_acc': 0.78125, 'loss': 0.59089}\n",
            "Step 55060: {'train_ae_loss': 0.65977, 'train_ucc_loss': 0.39672, 'train_ucc_acc': 0.9375, 'loss': 0.52825}\n",
            "Step 55080: {'train_ae_loss': 0.67178, 'train_ucc_loss': 0.53607, 'train_ucc_acc': 0.75, 'loss': 0.60393}\n",
            "Step 55100: {'train_ae_loss': 0.66264, 'train_ucc_loss': 0.5131, 'train_ucc_acc': 0.8125, 'loss': 0.58787}\n",
            "Step 55120: {'train_ae_loss': 0.69155, 'train_ucc_loss': 0.40239, 'train_ucc_acc': 0.90625, 'loss': 0.54697}\n",
            "Step 55140: {'train_ae_loss': 0.67336, 'train_ucc_loss': 0.45862, 'train_ucc_acc': 0.84375, 'loss': 0.56599}\n",
            "Step 55160: {'train_ae_loss': 0.66555, 'train_ucc_loss': 0.48634, 'train_ucc_acc': 0.84375, 'loss': 0.57595}\n",
            "Step 55180: {'train_ae_loss': 0.66387, 'train_ucc_loss': 0.42066, 'train_ucc_acc': 0.90625, 'loss': 0.54227}\n",
            "Step 55200: {'train_ae_loss': 0.67738, 'train_ucc_loss': 0.37002, 'train_ucc_acc': 0.9375, 'loss': 0.5237}\n",
            "Step 55220: {'train_ae_loss': 0.66191, 'train_ucc_loss': 0.43755, 'train_ucc_acc': 0.875, 'loss': 0.54973}\n",
            "Step 55240: {'train_ae_loss': 0.68023, 'train_ucc_loss': 0.54378, 'train_ucc_acc': 0.75, 'loss': 0.612}\n",
            "Step 55260: {'train_ae_loss': 0.68715, 'train_ucc_loss': 0.51682, 'train_ucc_acc': 0.8125, 'loss': 0.60199}\n",
            "Step 55280: {'train_ae_loss': 0.65644, 'train_ucc_loss': 0.32038, 'train_ucc_acc': 1.0, 'loss': 0.48841}\n",
            "Step 55300: {'train_ae_loss': 0.66347, 'train_ucc_loss': 0.3986, 'train_ucc_acc': 0.9375, 'loss': 0.53103}\n",
            "Step 55320: {'train_ae_loss': 0.69451, 'train_ucc_loss': 0.43224, 'train_ucc_acc': 0.90625, 'loss': 0.56337}\n",
            "Step 55340: {'train_ae_loss': 0.66475, 'train_ucc_loss': 0.51446, 'train_ucc_acc': 0.78125, 'loss': 0.5896}\n",
            "Step 55360: {'train_ae_loss': 0.6668, 'train_ucc_loss': 0.41161, 'train_ucc_acc': 0.90625, 'loss': 0.53921}\n",
            "Step 55380: {'train_ae_loss': 0.65852, 'train_ucc_loss': 0.50601, 'train_ucc_acc': 0.78125, 'loss': 0.58227}\n",
            "Step 55400: {'train_ae_loss': 0.67249, 'train_ucc_loss': 0.51503, 'train_ucc_acc': 0.78125, 'loss': 0.59376}\n",
            "Step 55420: {'train_ae_loss': 0.65885, 'train_ucc_loss': 0.44266, 'train_ucc_acc': 0.875, 'loss': 0.55075}\n",
            "Step 55440: {'train_ae_loss': 0.68463, 'train_ucc_loss': 0.48791, 'train_ucc_acc': 0.8125, 'loss': 0.58627}\n",
            "Step 55460: {'train_ae_loss': 0.67336, 'train_ucc_loss': 0.46694, 'train_ucc_acc': 0.8125, 'loss': 0.57015}\n",
            "Step 55480: {'train_ae_loss': 0.66175, 'train_ucc_loss': 0.51782, 'train_ucc_acc': 0.8125, 'loss': 0.58978}\n",
            "Step 55500: {'train_ae_loss': 0.6564, 'train_ucc_loss': 0.42237, 'train_ucc_acc': 0.90625, 'loss': 0.53938}\n",
            "Step 55520: {'train_ae_loss': 0.67871, 'train_ucc_loss': 0.49936, 'train_ucc_acc': 0.8125, 'loss': 0.58903}\n",
            "Step 55540: {'train_ae_loss': 0.6572, 'train_ucc_loss': 0.47224, 'train_ucc_acc': 0.84375, 'loss': 0.56472}\n",
            "Step 55560: {'train_ae_loss': 0.67451, 'train_ucc_loss': 0.47902, 'train_ucc_acc': 0.8125, 'loss': 0.57676}\n",
            "Step 55580: {'train_ae_loss': 0.66605, 'train_ucc_loss': 0.40179, 'train_ucc_acc': 0.90625, 'loss': 0.53392}\n",
            "Step 55600: {'train_ae_loss': 0.66408, 'train_ucc_loss': 0.35471, 'train_ucc_acc': 0.96875, 'loss': 0.5094}\n",
            "Step 55620: {'train_ae_loss': 0.67801, 'train_ucc_loss': 0.5294, 'train_ucc_acc': 0.78125, 'loss': 0.60371}\n",
            "Step 55640: {'train_ae_loss': 0.65593, 'train_ucc_loss': 0.49733, 'train_ucc_acc': 0.78125, 'loss': 0.57663}\n",
            "Step 55660: {'train_ae_loss': 0.6572, 'train_ucc_loss': 0.44363, 'train_ucc_acc': 0.875, 'loss': 0.55042}\n",
            "Step 55680: {'train_ae_loss': 0.68837, 'train_ucc_loss': 0.37861, 'train_ucc_acc': 0.9375, 'loss': 0.53349}\n",
            "Step 55700: {'train_ae_loss': 0.6641, 'train_ucc_loss': 0.37967, 'train_ucc_acc': 0.9375, 'loss': 0.52188}\n",
            "Step 55720: {'train_ae_loss': 0.65492, 'train_ucc_loss': 0.47471, 'train_ucc_acc': 0.84375, 'loss': 0.56482}\n",
            "Step 55740: {'train_ae_loss': 0.68702, 'train_ucc_loss': 0.46698, 'train_ucc_acc': 0.875, 'loss': 0.577}\n",
            "Step 55760: {'train_ae_loss': 0.67456, 'train_ucc_loss': 0.40842, 'train_ucc_acc': 0.90625, 'loss': 0.54149}\n",
            "Step 55780: {'train_ae_loss': 0.68249, 'train_ucc_loss': 0.48866, 'train_ucc_acc': 0.8125, 'loss': 0.58558}\n",
            "Step 55800: {'train_ae_loss': 0.65653, 'train_ucc_loss': 0.50858, 'train_ucc_acc': 0.8125, 'loss': 0.58255}\n",
            "Step 55820: {'train_ae_loss': 0.67218, 'train_ucc_loss': 0.48141, 'train_ucc_acc': 0.8125, 'loss': 0.5768}\n",
            "Step 55840: {'train_ae_loss': 0.68269, 'train_ucc_loss': 0.46437, 'train_ucc_acc': 0.8125, 'loss': 0.57353}\n",
            "Step 55860: {'train_ae_loss': 0.66044, 'train_ucc_loss': 0.38572, 'train_ucc_acc': 0.9375, 'loss': 0.52308}\n",
            "Step 55880: {'train_ae_loss': 0.67029, 'train_ucc_loss': 0.46939, 'train_ucc_acc': 0.8125, 'loss': 0.56984}\n",
            "Step 55900: {'train_ae_loss': 0.65773, 'train_ucc_loss': 0.48992, 'train_ucc_acc': 0.8125, 'loss': 0.57382}\n",
            "Step 55920: {'train_ae_loss': 0.67703, 'train_ucc_loss': 0.38763, 'train_ucc_acc': 0.90625, 'loss': 0.53233}\n",
            "Step 55940: {'train_ae_loss': 0.67986, 'train_ucc_loss': 0.47364, 'train_ucc_acc': 0.84375, 'loss': 0.57675}\n",
            "Step 55960: {'train_ae_loss': 0.67789, 'train_ucc_loss': 0.45711, 'train_ucc_acc': 0.84375, 'loss': 0.5675}\n",
            "Step 55980: {'train_ae_loss': 0.67283, 'train_ucc_loss': 0.47249, 'train_ucc_acc': 0.84375, 'loss': 0.57266}\n",
            "Step 56000: {'train_ae_loss': 0.66566, 'train_ucc_loss': 0.50891, 'train_ucc_acc': 0.78125, 'loss': 0.58728}\n",
            "step: 56000,eval_ae_loss: 0.66203,eval_ucc_loss: 0.49923,eval_ucc_acc: 0.80273\n",
            "Step 56020: {'train_ae_loss': 0.67774, 'train_ucc_loss': 0.45212, 'train_ucc_acc': 0.84375, 'loss': 0.56493}\n",
            "Step 56040: {'train_ae_loss': 0.66987, 'train_ucc_loss': 0.49577, 'train_ucc_acc': 0.8125, 'loss': 0.58282}\n",
            "Step 56060: {'train_ae_loss': 0.65946, 'train_ucc_loss': 0.46789, 'train_ucc_acc': 0.84375, 'loss': 0.56368}\n",
            "Step 56080: {'train_ae_loss': 0.65594, 'train_ucc_loss': 0.4885, 'train_ucc_acc': 0.8125, 'loss': 0.57222}\n",
            "Step 56100: {'train_ae_loss': 0.65868, 'train_ucc_loss': 0.39513, 'train_ucc_acc': 0.9375, 'loss': 0.5269}\n",
            "Step 56120: {'train_ae_loss': 0.66017, 'train_ucc_loss': 0.43322, 'train_ucc_acc': 0.875, 'loss': 0.5467}\n",
            "Step 56140: {'train_ae_loss': 0.67158, 'train_ucc_loss': 0.45064, 'train_ucc_acc': 0.8125, 'loss': 0.56111}\n",
            "Step 56160: {'train_ae_loss': 0.65319, 'train_ucc_loss': 0.49962, 'train_ucc_acc': 0.8125, 'loss': 0.5764}\n",
            "Step 56180: {'train_ae_loss': 0.67514, 'train_ucc_loss': 0.51755, 'train_ucc_acc': 0.78125, 'loss': 0.59634}\n",
            "Step 56200: {'train_ae_loss': 0.68897, 'train_ucc_loss': 0.4712, 'train_ucc_acc': 0.84375, 'loss': 0.58009}\n",
            "Step 56220: {'train_ae_loss': 0.66863, 'train_ucc_loss': 0.57019, 'train_ucc_acc': 0.71875, 'loss': 0.61941}\n",
            "Step 56240: {'train_ae_loss': 0.66905, 'train_ucc_loss': 0.4416, 'train_ucc_acc': 0.875, 'loss': 0.55532}\n",
            "Step 56260: {'train_ae_loss': 0.6653, 'train_ucc_loss': 0.44611, 'train_ucc_acc': 0.875, 'loss': 0.5557}\n",
            "Step 56280: {'train_ae_loss': 0.68312, 'train_ucc_loss': 0.41557, 'train_ucc_acc': 0.9375, 'loss': 0.54934}\n",
            "Step 56300: {'train_ae_loss': 0.66245, 'train_ucc_loss': 0.52249, 'train_ucc_acc': 0.75, 'loss': 0.59247}\n",
            "Step 56320: {'train_ae_loss': 0.68774, 'train_ucc_loss': 0.40094, 'train_ucc_acc': 0.875, 'loss': 0.54434}\n",
            "Step 56340: {'train_ae_loss': 0.64572, 'train_ucc_loss': 0.41052, 'train_ucc_acc': 0.90625, 'loss': 0.52812}\n",
            "Step 56360: {'train_ae_loss': 0.67637, 'train_ucc_loss': 0.48085, 'train_ucc_acc': 0.8125, 'loss': 0.57861}\n",
            "Step 56380: {'train_ae_loss': 0.65656, 'train_ucc_loss': 0.52361, 'train_ucc_acc': 0.78125, 'loss': 0.59008}\n",
            "Step 56400: {'train_ae_loss': 0.68895, 'train_ucc_loss': 0.46095, 'train_ucc_acc': 0.84375, 'loss': 0.57495}\n",
            "Step 56420: {'train_ae_loss': 0.67417, 'train_ucc_loss': 0.4425, 'train_ucc_acc': 0.875, 'loss': 0.55834}\n",
            "Step 56440: {'train_ae_loss': 0.67512, 'train_ucc_loss': 0.44294, 'train_ucc_acc': 0.875, 'loss': 0.55903}\n",
            "Step 56460: {'train_ae_loss': 0.66493, 'train_ucc_loss': 0.48956, 'train_ucc_acc': 0.8125, 'loss': 0.57725}\n",
            "Step 56480: {'train_ae_loss': 0.66649, 'train_ucc_loss': 0.42719, 'train_ucc_acc': 0.875, 'loss': 0.54684}\n",
            "Step 56500: {'train_ae_loss': 0.65855, 'train_ucc_loss': 0.48973, 'train_ucc_acc': 0.84375, 'loss': 0.57414}\n",
            "Step 56520: {'train_ae_loss': 0.66138, 'train_ucc_loss': 0.46098, 'train_ucc_acc': 0.8125, 'loss': 0.56118}\n",
            "Step 56540: {'train_ae_loss': 0.66939, 'train_ucc_loss': 0.61398, 'train_ucc_acc': 0.6875, 'loss': 0.64168}\n",
            "Step 56560: {'train_ae_loss': 0.66595, 'train_ucc_loss': 0.485, 'train_ucc_acc': 0.8125, 'loss': 0.57548}\n",
            "Step 56580: {'train_ae_loss': 0.66172, 'train_ucc_loss': 0.54527, 'train_ucc_acc': 0.71875, 'loss': 0.6035}\n",
            "Step 56600: {'train_ae_loss': 0.66197, 'train_ucc_loss': 0.48248, 'train_ucc_acc': 0.8125, 'loss': 0.57222}\n",
            "Step 56620: {'train_ae_loss': 0.65729, 'train_ucc_loss': 0.46226, 'train_ucc_acc': 0.8125, 'loss': 0.55977}\n",
            "Step 56640: {'train_ae_loss': 0.66978, 'train_ucc_loss': 0.33177, 'train_ucc_acc': 1.0, 'loss': 0.50077}\n",
            "Step 56660: {'train_ae_loss': 0.66704, 'train_ucc_loss': 0.45523, 'train_ucc_acc': 0.84375, 'loss': 0.56114}\n",
            "Step 56680: {'train_ae_loss': 0.66839, 'train_ucc_loss': 0.4677, 'train_ucc_acc': 0.8125, 'loss': 0.56805}\n",
            "Step 56700: {'train_ae_loss': 0.69236, 'train_ucc_loss': 0.38898, 'train_ucc_acc': 0.9375, 'loss': 0.54067}\n",
            "Step 56720: {'train_ae_loss': 0.65995, 'train_ucc_loss': 0.42537, 'train_ucc_acc': 0.875, 'loss': 0.54266}\n",
            "Step 56740: {'train_ae_loss': 0.66457, 'train_ucc_loss': 0.57904, 'train_ucc_acc': 0.71875, 'loss': 0.62181}\n",
            "Step 56760: {'train_ae_loss': 0.68171, 'train_ucc_loss': 0.4093, 'train_ucc_acc': 0.875, 'loss': 0.54551}\n",
            "Step 56780: {'train_ae_loss': 0.65382, 'train_ucc_loss': 0.48629, 'train_ucc_acc': 0.8125, 'loss': 0.57005}\n",
            "Step 56800: {'train_ae_loss': 0.67417, 'train_ucc_loss': 0.4909, 'train_ucc_acc': 0.8125, 'loss': 0.58253}\n",
            "Step 56820: {'train_ae_loss': 0.65497, 'train_ucc_loss': 0.48583, 'train_ucc_acc': 0.84375, 'loss': 0.5704}\n",
            "Step 56840: {'train_ae_loss': 0.66803, 'train_ucc_loss': 0.57344, 'train_ucc_acc': 0.75, 'loss': 0.62073}\n",
            "Step 56860: {'train_ae_loss': 0.68489, 'train_ucc_loss': 0.44997, 'train_ucc_acc': 0.875, 'loss': 0.56743}\n",
            "Step 56880: {'train_ae_loss': 0.65922, 'train_ucc_loss': 0.44392, 'train_ucc_acc': 0.84375, 'loss': 0.55157}\n",
            "Step 56900: {'train_ae_loss': 0.66476, 'train_ucc_loss': 0.50942, 'train_ucc_acc': 0.75, 'loss': 0.58709}\n",
            "Step 56920: {'train_ae_loss': 0.67857, 'train_ucc_loss': 0.51665, 'train_ucc_acc': 0.78125, 'loss': 0.59761}\n",
            "Step 56940: {'train_ae_loss': 0.65474, 'train_ucc_loss': 0.4276, 'train_ucc_acc': 0.875, 'loss': 0.54117}\n",
            "Step 56960: {'train_ae_loss': 0.68747, 'train_ucc_loss': 0.44115, 'train_ucc_acc': 0.875, 'loss': 0.56431}\n",
            "Step 56980: {'train_ae_loss': 0.68937, 'train_ucc_loss': 0.45455, 'train_ucc_acc': 0.84375, 'loss': 0.57196}\n",
            "Step 57000: {'train_ae_loss': 0.65933, 'train_ucc_loss': 0.50308, 'train_ucc_acc': 0.8125, 'loss': 0.5812}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 15:39:58 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 57000,eval_ae_loss: 0.65756,eval_ucc_loss: 0.48668,eval_ucc_acc: 0.82129\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 15:40:05 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 57020: {'train_ae_loss': 0.67498, 'train_ucc_loss': 0.5025, 'train_ucc_acc': 0.8125, 'loss': 0.58874}\n",
            "Step 57040: {'train_ae_loss': 0.66216, 'train_ucc_loss': 0.4637, 'train_ucc_acc': 0.84375, 'loss': 0.56293}\n",
            "Step 57060: {'train_ae_loss': 0.67636, 'train_ucc_loss': 0.35434, 'train_ucc_acc': 0.96875, 'loss': 0.51535}\n",
            "Step 57080: {'train_ae_loss': 0.65444, 'train_ucc_loss': 0.53257, 'train_ucc_acc': 0.78125, 'loss': 0.59351}\n",
            "Step 57100: {'train_ae_loss': 0.66404, 'train_ucc_loss': 0.52725, 'train_ucc_acc': 0.8125, 'loss': 0.59565}\n",
            "Step 57120: {'train_ae_loss': 0.66024, 'train_ucc_loss': 0.41045, 'train_ucc_acc': 0.875, 'loss': 0.53535}\n",
            "Step 57140: {'train_ae_loss': 0.65892, 'train_ucc_loss': 0.42048, 'train_ucc_acc': 0.90625, 'loss': 0.5397}\n",
            "Step 57160: {'train_ae_loss': 0.66403, 'train_ucc_loss': 0.47228, 'train_ucc_acc': 0.84375, 'loss': 0.56815}\n",
            "Step 57180: {'train_ae_loss': 0.65695, 'train_ucc_loss': 0.45466, 'train_ucc_acc': 0.84375, 'loss': 0.55581}\n",
            "Step 57200: {'train_ae_loss': 0.65761, 'train_ucc_loss': 0.43954, 'train_ucc_acc': 0.875, 'loss': 0.54857}\n",
            "Step 57220: {'train_ae_loss': 0.67543, 'train_ucc_loss': 0.36861, 'train_ucc_acc': 0.9375, 'loss': 0.52202}\n",
            "Step 57240: {'train_ae_loss': 0.67323, 'train_ucc_loss': 0.55291, 'train_ucc_acc': 0.75, 'loss': 0.61307}\n",
            "Step 57260: {'train_ae_loss': 0.6617, 'train_ucc_loss': 0.51384, 'train_ucc_acc': 0.8125, 'loss': 0.58777}\n",
            "Step 57280: {'train_ae_loss': 0.66552, 'train_ucc_loss': 0.51748, 'train_ucc_acc': 0.78125, 'loss': 0.5915}\n",
            "Step 57300: {'train_ae_loss': 0.66408, 'train_ucc_loss': 0.5738, 'train_ucc_acc': 0.6875, 'loss': 0.61894}\n",
            "Step 57320: {'train_ae_loss': 0.66889, 'train_ucc_loss': 0.6371, 'train_ucc_acc': 0.6875, 'loss': 0.653}\n",
            "Step 57340: {'train_ae_loss': 0.67096, 'train_ucc_loss': 0.50862, 'train_ucc_acc': 0.8125, 'loss': 0.58979}\n",
            "Step 57360: {'train_ae_loss': 0.65896, 'train_ucc_loss': 0.47036, 'train_ucc_acc': 0.84375, 'loss': 0.56466}\n",
            "Step 57380: {'train_ae_loss': 0.68061, 'train_ucc_loss': 0.48011, 'train_ucc_acc': 0.84375, 'loss': 0.58036}\n",
            "Step 57400: {'train_ae_loss': 0.66093, 'train_ucc_loss': 0.46256, 'train_ucc_acc': 0.84375, 'loss': 0.56175}\n",
            "Step 57420: {'train_ae_loss': 0.66017, 'train_ucc_loss': 0.35836, 'train_ucc_acc': 0.96875, 'loss': 0.50926}\n",
            "Step 57440: {'train_ae_loss': 0.66514, 'train_ucc_loss': 0.43803, 'train_ucc_acc': 0.875, 'loss': 0.55158}\n",
            "Step 57460: {'train_ae_loss': 0.66073, 'train_ucc_loss': 0.46658, 'train_ucc_acc': 0.84375, 'loss': 0.56366}\n",
            "Step 57480: {'train_ae_loss': 0.64986, 'train_ucc_loss': 0.44089, 'train_ucc_acc': 0.84375, 'loss': 0.54537}\n",
            "Step 57500: {'train_ae_loss': 0.67291, 'train_ucc_loss': 0.62608, 'train_ucc_acc': 0.6875, 'loss': 0.6495}\n",
            "Step 57520: {'train_ae_loss': 0.68607, 'train_ucc_loss': 0.54237, 'train_ucc_acc': 0.78125, 'loss': 0.61422}\n",
            "Step 57540: {'train_ae_loss': 0.67413, 'train_ucc_loss': 0.40755, 'train_ucc_acc': 0.9375, 'loss': 0.54084}\n",
            "Step 57560: {'train_ae_loss': 0.66709, 'train_ucc_loss': 0.47716, 'train_ucc_acc': 0.8125, 'loss': 0.57212}\n",
            "Step 57580: {'train_ae_loss': 0.67528, 'train_ucc_loss': 0.46235, 'train_ucc_acc': 0.84375, 'loss': 0.56881}\n",
            "Step 57600: {'train_ae_loss': 0.67131, 'train_ucc_loss': 0.57613, 'train_ucc_acc': 0.6875, 'loss': 0.62372}\n",
            "Step 57620: {'train_ae_loss': 0.66699, 'train_ucc_loss': 0.41115, 'train_ucc_acc': 0.90625, 'loss': 0.53907}\n",
            "Step 57640: {'train_ae_loss': 0.67087, 'train_ucc_loss': 0.49534, 'train_ucc_acc': 0.8125, 'loss': 0.58311}\n",
            "Step 57660: {'train_ae_loss': 0.67128, 'train_ucc_loss': 0.5962, 'train_ucc_acc': 0.6875, 'loss': 0.63374}\n",
            "Step 57680: {'train_ae_loss': 0.65939, 'train_ucc_loss': 0.43139, 'train_ucc_acc': 0.875, 'loss': 0.54539}\n",
            "Step 57700: {'train_ae_loss': 0.66055, 'train_ucc_loss': 0.42107, 'train_ucc_acc': 0.90625, 'loss': 0.54081}\n",
            "Step 57720: {'train_ae_loss': 0.66575, 'train_ucc_loss': 0.42567, 'train_ucc_acc': 0.875, 'loss': 0.54571}\n",
            "Step 57740: {'train_ae_loss': 0.6734, 'train_ucc_loss': 0.44206, 'train_ucc_acc': 0.875, 'loss': 0.55773}\n",
            "Step 57760: {'train_ae_loss': 0.66557, 'train_ucc_loss': 0.49444, 'train_ucc_acc': 0.78125, 'loss': 0.58}\n",
            "Step 57780: {'train_ae_loss': 0.67018, 'train_ucc_loss': 0.48098, 'train_ucc_acc': 0.8125, 'loss': 0.57558}\n",
            "Step 57800: {'train_ae_loss': 0.65928, 'train_ucc_loss': 0.37149, 'train_ucc_acc': 0.9375, 'loss': 0.51539}\n",
            "Step 57820: {'train_ae_loss': 0.66039, 'train_ucc_loss': 0.43466, 'train_ucc_acc': 0.84375, 'loss': 0.54753}\n",
            "Step 57840: {'train_ae_loss': 0.65503, 'train_ucc_loss': 0.47384, 'train_ucc_acc': 0.84375, 'loss': 0.56444}\n",
            "Step 57860: {'train_ae_loss': 0.65283, 'train_ucc_loss': 0.48303, 'train_ucc_acc': 0.84375, 'loss': 0.56793}\n",
            "Step 57880: {'train_ae_loss': 0.65272, 'train_ucc_loss': 0.5862, 'train_ucc_acc': 0.6875, 'loss': 0.61946}\n",
            "Step 57900: {'train_ae_loss': 0.66435, 'train_ucc_loss': 0.47135, 'train_ucc_acc': 0.8125, 'loss': 0.56785}\n",
            "Step 57920: {'train_ae_loss': 0.66895, 'train_ucc_loss': 0.37009, 'train_ucc_acc': 0.96875, 'loss': 0.51952}\n",
            "Step 57940: {'train_ae_loss': 0.65979, 'train_ucc_loss': 0.62905, 'train_ucc_acc': 0.6875, 'loss': 0.64442}\n",
            "Step 57960: {'train_ae_loss': 0.66724, 'train_ucc_loss': 0.45846, 'train_ucc_acc': 0.84375, 'loss': 0.56285}\n",
            "Step 57980: {'train_ae_loss': 0.67452, 'train_ucc_loss': 0.45512, 'train_ucc_acc': 0.84375, 'loss': 0.56482}\n",
            "Step 58000: {'train_ae_loss': 0.65781, 'train_ucc_loss': 0.44847, 'train_ucc_acc': 0.875, 'loss': 0.55314}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 15:48:57 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 58000,eval_ae_loss: 0.65993,eval_ucc_loss: 0.47575,eval_ucc_acc: 0.8291\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 15:49:01 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 58020: {'train_ae_loss': 0.6619, 'train_ucc_loss': 0.47368, 'train_ucc_acc': 0.8125, 'loss': 0.56779}\n",
            "Step 58040: {'train_ae_loss': 0.67721, 'train_ucc_loss': 0.41553, 'train_ucc_acc': 0.90625, 'loss': 0.54637}\n",
            "Step 58060: {'train_ae_loss': 0.67979, 'train_ucc_loss': 0.40768, 'train_ucc_acc': 0.90625, 'loss': 0.54374}\n",
            "Step 58080: {'train_ae_loss': 0.67458, 'train_ucc_loss': 0.51544, 'train_ucc_acc': 0.78125, 'loss': 0.59501}\n",
            "Step 58100: {'train_ae_loss': 0.67934, 'train_ucc_loss': 0.53396, 'train_ucc_acc': 0.78125, 'loss': 0.60665}\n",
            "Step 58120: {'train_ae_loss': 0.6563, 'train_ucc_loss': 0.39593, 'train_ucc_acc': 0.9375, 'loss': 0.52611}\n",
            "Step 58140: {'train_ae_loss': 0.66173, 'train_ucc_loss': 0.43314, 'train_ucc_acc': 0.875, 'loss': 0.54744}\n",
            "Step 58160: {'train_ae_loss': 0.65617, 'train_ucc_loss': 0.43561, 'train_ucc_acc': 0.875, 'loss': 0.54589}\n",
            "Step 58180: {'train_ae_loss': 0.66719, 'train_ucc_loss': 0.41501, 'train_ucc_acc': 0.90625, 'loss': 0.5411}\n",
            "Step 58200: {'train_ae_loss': 0.67337, 'train_ucc_loss': 0.4537, 'train_ucc_acc': 0.875, 'loss': 0.56353}\n",
            "Step 58220: {'train_ae_loss': 0.66488, 'train_ucc_loss': 0.50566, 'train_ucc_acc': 0.8125, 'loss': 0.58527}\n",
            "Step 58240: {'train_ae_loss': 0.6648, 'train_ucc_loss': 0.42879, 'train_ucc_acc': 0.90625, 'loss': 0.54679}\n",
            "Step 58260: {'train_ae_loss': 0.65301, 'train_ucc_loss': 0.433, 'train_ucc_acc': 0.875, 'loss': 0.54301}\n",
            "Step 58280: {'train_ae_loss': 0.67314, 'train_ucc_loss': 0.47613, 'train_ucc_acc': 0.84375, 'loss': 0.57463}\n",
            "Step 58300: {'train_ae_loss': 0.66821, 'train_ucc_loss': 0.47275, 'train_ucc_acc': 0.84375, 'loss': 0.57048}\n",
            "Step 58320: {'train_ae_loss': 0.67826, 'train_ucc_loss': 0.46017, 'train_ucc_acc': 0.84375, 'loss': 0.56921}\n",
            "Step 58340: {'train_ae_loss': 0.66191, 'train_ucc_loss': 0.38646, 'train_ucc_acc': 0.9375, 'loss': 0.52419}\n",
            "Step 58360: {'train_ae_loss': 0.66726, 'train_ucc_loss': 0.52167, 'train_ucc_acc': 0.78125, 'loss': 0.59446}\n",
            "Step 58380: {'train_ae_loss': 0.68388, 'train_ucc_loss': 0.38227, 'train_ucc_acc': 0.9375, 'loss': 0.53308}\n",
            "Step 58400: {'train_ae_loss': 0.67476, 'train_ucc_loss': 0.57962, 'train_ucc_acc': 0.75, 'loss': 0.62719}\n",
            "Step 58420: {'train_ae_loss': 0.67537, 'train_ucc_loss': 0.44179, 'train_ucc_acc': 0.875, 'loss': 0.55858}\n",
            "Step 58440: {'train_ae_loss': 0.67298, 'train_ucc_loss': 0.48033, 'train_ucc_acc': 0.78125, 'loss': 0.57666}\n",
            "Step 58460: {'train_ae_loss': 0.66476, 'train_ucc_loss': 0.47309, 'train_ucc_acc': 0.8125, 'loss': 0.56892}\n",
            "Step 58480: {'train_ae_loss': 0.66171, 'train_ucc_loss': 0.54489, 'train_ucc_acc': 0.75, 'loss': 0.6033}\n",
            "Step 58500: {'train_ae_loss': 0.67558, 'train_ucc_loss': 0.50708, 'train_ucc_acc': 0.8125, 'loss': 0.59133}\n",
            "Step 58520: {'train_ae_loss': 0.6619, 'train_ucc_loss': 0.38428, 'train_ucc_acc': 0.9375, 'loss': 0.52309}\n",
            "Step 58540: {'train_ae_loss': 0.68629, 'train_ucc_loss': 0.41766, 'train_ucc_acc': 0.90625, 'loss': 0.55197}\n",
            "Step 58560: {'train_ae_loss': 0.67874, 'train_ucc_loss': 0.44031, 'train_ucc_acc': 0.875, 'loss': 0.55953}\n",
            "Step 58580: {'train_ae_loss': 0.65402, 'train_ucc_loss': 0.47317, 'train_ucc_acc': 0.84375, 'loss': 0.5636}\n",
            "Step 58600: {'train_ae_loss': 0.67171, 'train_ucc_loss': 0.61195, 'train_ucc_acc': 0.6875, 'loss': 0.64183}\n",
            "Step 58620: {'train_ae_loss': 0.66576, 'train_ucc_loss': 0.42243, 'train_ucc_acc': 0.90625, 'loss': 0.54409}\n",
            "Step 58640: {'train_ae_loss': 0.66931, 'train_ucc_loss': 0.46919, 'train_ucc_acc': 0.84375, 'loss': 0.56925}\n",
            "Step 58660: {'train_ae_loss': 0.65317, 'train_ucc_loss': 0.49417, 'train_ucc_acc': 0.8125, 'loss': 0.57367}\n",
            "Step 58680: {'train_ae_loss': 0.65399, 'train_ucc_loss': 0.49946, 'train_ucc_acc': 0.78125, 'loss': 0.57672}\n",
            "Step 58700: {'train_ae_loss': 0.67306, 'train_ucc_loss': 0.47505, 'train_ucc_acc': 0.8125, 'loss': 0.57406}\n",
            "Step 58720: {'train_ae_loss': 0.67795, 'train_ucc_loss': 0.40859, 'train_ucc_acc': 0.90625, 'loss': 0.54327}\n",
            "Step 58740: {'train_ae_loss': 0.67357, 'train_ucc_loss': 0.4324, 'train_ucc_acc': 0.875, 'loss': 0.55298}\n",
            "Step 58760: {'train_ae_loss': 0.65374, 'train_ucc_loss': 0.47423, 'train_ucc_acc': 0.8125, 'loss': 0.56399}\n",
            "Step 58780: {'train_ae_loss': 0.65337, 'train_ucc_loss': 0.41395, 'train_ucc_acc': 0.90625, 'loss': 0.53366}\n",
            "Step 58800: {'train_ae_loss': 0.66344, 'train_ucc_loss': 0.53563, 'train_ucc_acc': 0.78125, 'loss': 0.59953}\n",
            "Step 58820: {'train_ae_loss': 0.68071, 'train_ucc_loss': 0.4916, 'train_ucc_acc': 0.84375, 'loss': 0.58616}\n",
            "Step 58840: {'train_ae_loss': 0.67988, 'train_ucc_loss': 0.41015, 'train_ucc_acc': 0.90625, 'loss': 0.54501}\n",
            "Step 58860: {'train_ae_loss': 0.68556, 'train_ucc_loss': 0.45529, 'train_ucc_acc': 0.8125, 'loss': 0.57043}\n",
            "Step 58880: {'train_ae_loss': 0.66865, 'train_ucc_loss': 0.45887, 'train_ucc_acc': 0.84375, 'loss': 0.56376}\n",
            "Step 58900: {'train_ae_loss': 0.66011, 'train_ucc_loss': 0.42197, 'train_ucc_acc': 0.90625, 'loss': 0.54104}\n",
            "Step 58920: {'train_ae_loss': 0.66653, 'train_ucc_loss': 0.3566, 'train_ucc_acc': 0.96875, 'loss': 0.51157}\n",
            "Step 58940: {'train_ae_loss': 0.68211, 'train_ucc_loss': 0.46274, 'train_ucc_acc': 0.875, 'loss': 0.57242}\n",
            "Step 58960: {'train_ae_loss': 0.69401, 'train_ucc_loss': 0.32328, 'train_ucc_acc': 1.0, 'loss': 0.50865}\n",
            "Step 58980: {'train_ae_loss': 0.68086, 'train_ucc_loss': 0.36831, 'train_ucc_acc': 0.9375, 'loss': 0.52459}\n",
            "Step 59000: {'train_ae_loss': 0.67873, 'train_ucc_loss': 0.51367, 'train_ucc_acc': 0.78125, 'loss': 0.5962}\n",
            "step: 59000,eval_ae_loss: 0.65302,eval_ucc_loss: 0.50475,eval_ucc_acc: 0.79883\n",
            "Step 59020: {'train_ae_loss': 0.68163, 'train_ucc_loss': 0.59207, 'train_ucc_acc': 0.6875, 'loss': 0.63685}\n",
            "Step 59040: {'train_ae_loss': 0.6683, 'train_ucc_loss': 0.46142, 'train_ucc_acc': 0.84375, 'loss': 0.56486}\n",
            "Step 59060: {'train_ae_loss': 0.67443, 'train_ucc_loss': 0.55604, 'train_ucc_acc': 0.75, 'loss': 0.61524}\n",
            "Step 59080: {'train_ae_loss': 0.6709, 'train_ucc_loss': 0.47596, 'train_ucc_acc': 0.8125, 'loss': 0.57343}\n",
            "Step 59100: {'train_ae_loss': 0.67363, 'train_ucc_loss': 0.39905, 'train_ucc_acc': 0.90625, 'loss': 0.53634}\n",
            "Step 59120: {'train_ae_loss': 0.67521, 'train_ucc_loss': 0.4708, 'train_ucc_acc': 0.8125, 'loss': 0.573}\n",
            "Step 59140: {'train_ae_loss': 0.66659, 'train_ucc_loss': 0.49315, 'train_ucc_acc': 0.8125, 'loss': 0.57987}\n",
            "Step 59160: {'train_ae_loss': 0.67128, 'train_ucc_loss': 0.45805, 'train_ucc_acc': 0.84375, 'loss': 0.56467}\n",
            "Step 59180: {'train_ae_loss': 0.66561, 'train_ucc_loss': 0.3889, 'train_ucc_acc': 0.9375, 'loss': 0.52725}\n",
            "Step 59200: {'train_ae_loss': 0.67118, 'train_ucc_loss': 0.44712, 'train_ucc_acc': 0.84375, 'loss': 0.55915}\n",
            "Step 59220: {'train_ae_loss': 0.68103, 'train_ucc_loss': 0.3927, 'train_ucc_acc': 0.9375, 'loss': 0.53686}\n",
            "Step 59240: {'train_ae_loss': 0.67378, 'train_ucc_loss': 0.4273, 'train_ucc_acc': 0.875, 'loss': 0.55054}\n",
            "Step 59260: {'train_ae_loss': 0.67829, 'train_ucc_loss': 0.55151, 'train_ucc_acc': 0.75, 'loss': 0.6149}\n",
            "Step 59280: {'train_ae_loss': 0.70535, 'train_ucc_loss': 0.55439, 'train_ucc_acc': 0.78125, 'loss': 0.62987}\n",
            "Step 59300: {'train_ae_loss': 0.65747, 'train_ucc_loss': 0.47097, 'train_ucc_acc': 0.84375, 'loss': 0.56422}\n",
            "Step 59320: {'train_ae_loss': 0.66642, 'train_ucc_loss': 0.42198, 'train_ucc_acc': 0.875, 'loss': 0.5442}\n",
            "Step 59340: {'train_ae_loss': 0.66545, 'train_ucc_loss': 0.65969, 'train_ucc_acc': 0.59375, 'loss': 0.66257}\n",
            "Step 59360: {'train_ae_loss': 0.6677, 'train_ucc_loss': 0.45073, 'train_ucc_acc': 0.84375, 'loss': 0.55921}\n",
            "Step 59380: {'train_ae_loss': 0.66504, 'train_ucc_loss': 0.40882, 'train_ucc_acc': 0.90625, 'loss': 0.53693}\n",
            "Step 59400: {'train_ae_loss': 0.6641, 'train_ucc_loss': 0.42924, 'train_ucc_acc': 0.90625, 'loss': 0.54667}\n",
            "Step 59420: {'train_ae_loss': 0.6804, 'train_ucc_loss': 0.4812, 'train_ucc_acc': 0.84375, 'loss': 0.5808}\n",
            "Step 59440: {'train_ae_loss': 0.66689, 'train_ucc_loss': 0.46471, 'train_ucc_acc': 0.84375, 'loss': 0.5658}\n",
            "Step 59460: {'train_ae_loss': 0.65879, 'train_ucc_loss': 0.40512, 'train_ucc_acc': 0.90625, 'loss': 0.53196}\n",
            "Step 59480: {'train_ae_loss': 0.67967, 'train_ucc_loss': 0.37561, 'train_ucc_acc': 0.9375, 'loss': 0.52764}\n",
            "Step 59500: {'train_ae_loss': 0.6577, 'train_ucc_loss': 0.41291, 'train_ucc_acc': 0.90625, 'loss': 0.5353}\n",
            "Step 59520: {'train_ae_loss': 0.66844, 'train_ucc_loss': 0.41507, 'train_ucc_acc': 0.90625, 'loss': 0.54175}\n",
            "Step 59540: {'train_ae_loss': 0.6543, 'train_ucc_loss': 0.38756, 'train_ucc_acc': 0.9375, 'loss': 0.52093}\n",
            "Step 59560: {'train_ae_loss': 0.66528, 'train_ucc_loss': 0.46852, 'train_ucc_acc': 0.84375, 'loss': 0.5669}\n",
            "Step 59580: {'train_ae_loss': 0.67084, 'train_ucc_loss': 0.41899, 'train_ucc_acc': 0.875, 'loss': 0.54492}\n",
            "Step 59600: {'train_ae_loss': 0.66976, 'train_ucc_loss': 0.50852, 'train_ucc_acc': 0.8125, 'loss': 0.58914}\n",
            "Step 59620: {'train_ae_loss': 0.66869, 'train_ucc_loss': 0.46078, 'train_ucc_acc': 0.84375, 'loss': 0.56473}\n",
            "Step 59640: {'train_ae_loss': 0.67325, 'train_ucc_loss': 0.46383, 'train_ucc_acc': 0.84375, 'loss': 0.56854}\n",
            "Step 59660: {'train_ae_loss': 0.65615, 'train_ucc_loss': 0.4577, 'train_ucc_acc': 0.84375, 'loss': 0.55692}\n",
            "Step 59680: {'train_ae_loss': 0.67677, 'train_ucc_loss': 0.40839, 'train_ucc_acc': 0.90625, 'loss': 0.54258}\n",
            "Step 59700: {'train_ae_loss': 0.67001, 'train_ucc_loss': 0.49974, 'train_ucc_acc': 0.8125, 'loss': 0.58488}\n",
            "Step 59720: {'train_ae_loss': 0.66105, 'train_ucc_loss': 0.48429, 'train_ucc_acc': 0.84375, 'loss': 0.57267}\n",
            "Step 59740: {'train_ae_loss': 0.6645, 'train_ucc_loss': 0.53192, 'train_ucc_acc': 0.78125, 'loss': 0.59821}\n",
            "Step 59760: {'train_ae_loss': 0.6646, 'train_ucc_loss': 0.41295, 'train_ucc_acc': 0.90625, 'loss': 0.53878}\n",
            "Step 59780: {'train_ae_loss': 0.66889, 'train_ucc_loss': 0.61927, 'train_ucc_acc': 0.6875, 'loss': 0.64408}\n",
            "Step 59800: {'train_ae_loss': 0.68172, 'train_ucc_loss': 0.47315, 'train_ucc_acc': 0.8125, 'loss': 0.57744}\n",
            "Step 59820: {'train_ae_loss': 0.67083, 'train_ucc_loss': 0.45007, 'train_ucc_acc': 0.875, 'loss': 0.56045}\n",
            "Step 59840: {'train_ae_loss': 0.66919, 'train_ucc_loss': 0.5783, 'train_ucc_acc': 0.6875, 'loss': 0.62374}\n",
            "Step 59860: {'train_ae_loss': 0.67453, 'train_ucc_loss': 0.47686, 'train_ucc_acc': 0.78125, 'loss': 0.57569}\n",
            "Step 59880: {'train_ae_loss': 0.67974, 'train_ucc_loss': 0.52374, 'train_ucc_acc': 0.78125, 'loss': 0.60174}\n",
            "Step 59900: {'train_ae_loss': 0.67871, 'train_ucc_loss': 0.43137, 'train_ucc_acc': 0.90625, 'loss': 0.55504}\n",
            "Step 59920: {'train_ae_loss': 0.65874, 'train_ucc_loss': 0.47658, 'train_ucc_acc': 0.8125, 'loss': 0.56766}\n",
            "Step 59940: {'train_ae_loss': 0.67424, 'train_ucc_loss': 0.43024, 'train_ucc_acc': 0.875, 'loss': 0.55224}\n",
            "Step 59960: {'train_ae_loss': 0.65216, 'train_ucc_loss': 0.46231, 'train_ucc_acc': 0.84375, 'loss': 0.55724}\n",
            "Step 59980: {'train_ae_loss': 0.65832, 'train_ucc_loss': 0.49904, 'train_ucc_acc': 0.8125, 'loss': 0.57868}\n",
            "Step 60000: {'train_ae_loss': 0.64678, 'train_ucc_loss': 0.47567, 'train_ucc_acc': 0.84375, 'loss': 0.56123}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 16:06:22 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 60000,eval_ae_loss: 0.6566,eval_ucc_loss: 0.47432,eval_ucc_acc: 0.83301\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 16:06:26 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 60020: {'train_ae_loss': 0.66913, 'train_ucc_loss': 0.51844, 'train_ucc_acc': 0.78125, 'loss': 0.59379}\n",
            "Step 60040: {'train_ae_loss': 0.67265, 'train_ucc_loss': 0.43698, 'train_ucc_acc': 0.875, 'loss': 0.55481}\n",
            "Step 60060: {'train_ae_loss': 0.65742, 'train_ucc_loss': 0.66087, 'train_ucc_acc': 0.625, 'loss': 0.65915}\n",
            "Step 60080: {'train_ae_loss': 0.67394, 'train_ucc_loss': 0.50116, 'train_ucc_acc': 0.78125, 'loss': 0.58755}\n",
            "Step 60100: {'train_ae_loss': 0.68681, 'train_ucc_loss': 0.46515, 'train_ucc_acc': 0.84375, 'loss': 0.57598}\n",
            "Step 60120: {'train_ae_loss': 0.64501, 'train_ucc_loss': 0.56014, 'train_ucc_acc': 0.78125, 'loss': 0.60257}\n",
            "Step 60140: {'train_ae_loss': 0.66805, 'train_ucc_loss': 0.59564, 'train_ucc_acc': 0.6875, 'loss': 0.63184}\n",
            "Step 60160: {'train_ae_loss': 0.6702, 'train_ucc_loss': 0.45997, 'train_ucc_acc': 0.84375, 'loss': 0.56509}\n",
            "Step 60180: {'train_ae_loss': 0.6676, 'train_ucc_loss': 0.47314, 'train_ucc_acc': 0.8125, 'loss': 0.57037}\n",
            "Step 60200: {'train_ae_loss': 0.65397, 'train_ucc_loss': 0.51131, 'train_ucc_acc': 0.78125, 'loss': 0.58264}\n",
            "Step 60220: {'train_ae_loss': 0.66318, 'train_ucc_loss': 0.60728, 'train_ucc_acc': 0.6875, 'loss': 0.63523}\n",
            "Step 60240: {'train_ae_loss': 0.66577, 'train_ucc_loss': 0.45519, 'train_ucc_acc': 0.84375, 'loss': 0.56048}\n",
            "Step 60260: {'train_ae_loss': 0.66479, 'train_ucc_loss': 0.47544, 'train_ucc_acc': 0.84375, 'loss': 0.57012}\n",
            "Step 60280: {'train_ae_loss': 0.67251, 'train_ucc_loss': 0.3964, 'train_ucc_acc': 0.9375, 'loss': 0.53446}\n",
            "Step 60300: {'train_ae_loss': 0.66564, 'train_ucc_loss': 0.52026, 'train_ucc_acc': 0.78125, 'loss': 0.59295}\n",
            "Step 60320: {'train_ae_loss': 0.67354, 'train_ucc_loss': 0.52411, 'train_ucc_acc': 0.75, 'loss': 0.59882}\n",
            "Step 60340: {'train_ae_loss': 0.66776, 'train_ucc_loss': 0.41396, 'train_ucc_acc': 0.90625, 'loss': 0.54086}\n",
            "Step 60360: {'train_ae_loss': 0.65045, 'train_ucc_loss': 0.4688, 'train_ucc_acc': 0.84375, 'loss': 0.55963}\n",
            "Step 60380: {'train_ae_loss': 0.64251, 'train_ucc_loss': 0.40621, 'train_ucc_acc': 0.90625, 'loss': 0.52436}\n",
            "Step 60400: {'train_ae_loss': 0.66391, 'train_ucc_loss': 0.48445, 'train_ucc_acc': 0.8125, 'loss': 0.57418}\n",
            "Step 60420: {'train_ae_loss': 0.67374, 'train_ucc_loss': 0.39699, 'train_ucc_acc': 0.90625, 'loss': 0.53536}\n",
            "Step 60440: {'train_ae_loss': 0.68804, 'train_ucc_loss': 0.48273, 'train_ucc_acc': 0.78125, 'loss': 0.58539}\n",
            "Step 60460: {'train_ae_loss': 0.67469, 'train_ucc_loss': 0.49247, 'train_ucc_acc': 0.78125, 'loss': 0.58358}\n",
            "Step 60480: {'train_ae_loss': 0.67361, 'train_ucc_loss': 0.62581, 'train_ucc_acc': 0.6875, 'loss': 0.64971}\n",
            "Step 60500: {'train_ae_loss': 0.65905, 'train_ucc_loss': 0.37858, 'train_ucc_acc': 0.9375, 'loss': 0.51882}\n",
            "Step 60520: {'train_ae_loss': 0.67943, 'train_ucc_loss': 0.46748, 'train_ucc_acc': 0.84375, 'loss': 0.57345}\n",
            "Step 60540: {'train_ae_loss': 0.66781, 'train_ucc_loss': 0.4145, 'train_ucc_acc': 0.90625, 'loss': 0.54116}\n",
            "Step 60560: {'train_ae_loss': 0.6647, 'train_ucc_loss': 0.49341, 'train_ucc_acc': 0.78125, 'loss': 0.57906}\n",
            "Step 60580: {'train_ae_loss': 0.65564, 'train_ucc_loss': 0.37758, 'train_ucc_acc': 0.9375, 'loss': 0.51661}\n",
            "Step 60600: {'train_ae_loss': 0.6677, 'train_ucc_loss': 0.54117, 'train_ucc_acc': 0.75, 'loss': 0.60443}\n",
            "Step 60620: {'train_ae_loss': 0.6576, 'train_ucc_loss': 0.50122, 'train_ucc_acc': 0.8125, 'loss': 0.57941}\n",
            "Step 60640: {'train_ae_loss': 0.69064, 'train_ucc_loss': 0.41032, 'train_ucc_acc': 0.9375, 'loss': 0.55048}\n",
            "Step 60660: {'train_ae_loss': 0.66271, 'train_ucc_loss': 0.45079, 'train_ucc_acc': 0.84375, 'loss': 0.55675}\n",
            "Step 60680: {'train_ae_loss': 0.6601, 'train_ucc_loss': 0.39516, 'train_ucc_acc': 0.90625, 'loss': 0.52763}\n",
            "Step 60700: {'train_ae_loss': 0.67759, 'train_ucc_loss': 0.48314, 'train_ucc_acc': 0.84375, 'loss': 0.58037}\n",
            "Step 60720: {'train_ae_loss': 0.67178, 'train_ucc_loss': 0.37282, 'train_ucc_acc': 0.96875, 'loss': 0.5223}\n",
            "Step 60740: {'train_ae_loss': 0.67078, 'train_ucc_loss': 0.48677, 'train_ucc_acc': 0.8125, 'loss': 0.57877}\n",
            "Step 60760: {'train_ae_loss': 0.66254, 'train_ucc_loss': 0.47871, 'train_ucc_acc': 0.84375, 'loss': 0.57062}\n",
            "Step 60780: {'train_ae_loss': 0.67032, 'train_ucc_loss': 0.45476, 'train_ucc_acc': 0.875, 'loss': 0.56254}\n",
            "Step 60800: {'train_ae_loss': 0.65219, 'train_ucc_loss': 0.48421, 'train_ucc_acc': 0.84375, 'loss': 0.5682}\n",
            "Step 60820: {'train_ae_loss': 0.66743, 'train_ucc_loss': 0.49694, 'train_ucc_acc': 0.78125, 'loss': 0.58219}\n",
            "Step 60840: {'train_ae_loss': 0.65594, 'train_ucc_loss': 0.41017, 'train_ucc_acc': 0.90625, 'loss': 0.53306}\n",
            "Step 60860: {'train_ae_loss': 0.66825, 'train_ucc_loss': 0.53702, 'train_ucc_acc': 0.78125, 'loss': 0.60264}\n",
            "Step 60880: {'train_ae_loss': 0.66723, 'train_ucc_loss': 0.53825, 'train_ucc_acc': 0.78125, 'loss': 0.60274}\n",
            "Step 60900: {'train_ae_loss': 0.64604, 'train_ucc_loss': 0.53289, 'train_ucc_acc': 0.78125, 'loss': 0.58947}\n",
            "Step 60920: {'train_ae_loss': 0.66959, 'train_ucc_loss': 0.39632, 'train_ucc_acc': 0.90625, 'loss': 0.53295}\n",
            "Step 60940: {'train_ae_loss': 0.66626, 'train_ucc_loss': 0.45275, 'train_ucc_acc': 0.84375, 'loss': 0.5595}\n",
            "Step 60960: {'train_ae_loss': 0.6722, 'train_ucc_loss': 0.43752, 'train_ucc_acc': 0.875, 'loss': 0.55486}\n",
            "Step 60980: {'train_ae_loss': 0.66401, 'train_ucc_loss': 0.51416, 'train_ucc_acc': 0.78125, 'loss': 0.58909}\n",
            "Step 61000: {'train_ae_loss': 0.67233, 'train_ucc_loss': 0.43284, 'train_ucc_acc': 0.875, 'loss': 0.55259}\n",
            "step: 61000,eval_ae_loss: 0.65414,eval_ucc_loss: 0.50842,eval_ucc_acc: 0.79492\n",
            "Step 61020: {'train_ae_loss': 0.6425, 'train_ucc_loss': 0.53183, 'train_ucc_acc': 0.75, 'loss': 0.58717}\n",
            "Step 61040: {'train_ae_loss': 0.67548, 'train_ucc_loss': 0.38674, 'train_ucc_acc': 0.9375, 'loss': 0.53111}\n",
            "Step 61060: {'train_ae_loss': 0.66814, 'train_ucc_loss': 0.4389, 'train_ucc_acc': 0.875, 'loss': 0.55352}\n",
            "Step 61080: {'train_ae_loss': 0.65666, 'train_ucc_loss': 0.46333, 'train_ucc_acc': 0.875, 'loss': 0.56}\n",
            "Step 61100: {'train_ae_loss': 0.67131, 'train_ucc_loss': 0.40651, 'train_ucc_acc': 0.90625, 'loss': 0.53891}\n",
            "Step 61120: {'train_ae_loss': 0.67751, 'train_ucc_loss': 0.45225, 'train_ucc_acc': 0.875, 'loss': 0.56488}\n",
            "Step 61140: {'train_ae_loss': 0.67635, 'train_ucc_loss': 0.44325, 'train_ucc_acc': 0.875, 'loss': 0.5598}\n",
            "Step 61160: {'train_ae_loss': 0.66108, 'train_ucc_loss': 0.50092, 'train_ucc_acc': 0.8125, 'loss': 0.581}\n",
            "Step 61180: {'train_ae_loss': 0.66435, 'train_ucc_loss': 0.46918, 'train_ucc_acc': 0.84375, 'loss': 0.56676}\n",
            "Step 61200: {'train_ae_loss': 0.64395, 'train_ucc_loss': 0.43017, 'train_ucc_acc': 0.875, 'loss': 0.53706}\n",
            "Step 61220: {'train_ae_loss': 0.67053, 'train_ucc_loss': 0.59634, 'train_ucc_acc': 0.65625, 'loss': 0.63344}\n",
            "Step 61240: {'train_ae_loss': 0.66191, 'train_ucc_loss': 0.46256, 'train_ucc_acc': 0.84375, 'loss': 0.56223}\n",
            "Step 61260: {'train_ae_loss': 0.67381, 'train_ucc_loss': 0.41016, 'train_ucc_acc': 0.90625, 'loss': 0.54199}\n",
            "Step 61280: {'train_ae_loss': 0.64386, 'train_ucc_loss': 0.34956, 'train_ucc_acc': 0.96875, 'loss': 0.49671}\n",
            "Step 61300: {'train_ae_loss': 0.6676, 'train_ucc_loss': 0.43668, 'train_ucc_acc': 0.875, 'loss': 0.55214}\n",
            "Step 61320: {'train_ae_loss': 0.67486, 'train_ucc_loss': 0.53095, 'train_ucc_acc': 0.78125, 'loss': 0.6029}\n",
            "Step 61340: {'train_ae_loss': 0.67587, 'train_ucc_loss': 0.46102, 'train_ucc_acc': 0.875, 'loss': 0.56845}\n",
            "Step 61360: {'train_ae_loss': 0.67277, 'train_ucc_loss': 0.43007, 'train_ucc_acc': 0.875, 'loss': 0.55142}\n",
            "Step 61380: {'train_ae_loss': 0.67349, 'train_ucc_loss': 0.37881, 'train_ucc_acc': 0.9375, 'loss': 0.52615}\n",
            "Step 61400: {'train_ae_loss': 0.65377, 'train_ucc_loss': 0.57562, 'train_ucc_acc': 0.71875, 'loss': 0.6147}\n",
            "Step 61420: {'train_ae_loss': 0.67426, 'train_ucc_loss': 0.45601, 'train_ucc_acc': 0.875, 'loss': 0.56514}\n",
            "Step 61440: {'train_ae_loss': 0.66704, 'train_ucc_loss': 0.49172, 'train_ucc_acc': 0.8125, 'loss': 0.57938}\n",
            "Step 61460: {'train_ae_loss': 0.68507, 'train_ucc_loss': 0.46307, 'train_ucc_acc': 0.84375, 'loss': 0.57407}\n",
            "Step 61480: {'train_ae_loss': 0.64851, 'train_ucc_loss': 0.58619, 'train_ucc_acc': 0.75, 'loss': 0.61735}\n",
            "Step 61500: {'train_ae_loss': 0.64087, 'train_ucc_loss': 0.44455, 'train_ucc_acc': 0.875, 'loss': 0.54271}\n",
            "Step 61520: {'train_ae_loss': 0.6514, 'train_ucc_loss': 0.44946, 'train_ucc_acc': 0.875, 'loss': 0.55043}\n",
            "Step 61540: {'train_ae_loss': 0.6708, 'train_ucc_loss': 0.40874, 'train_ucc_acc': 0.90625, 'loss': 0.53977}\n",
            "Step 61560: {'train_ae_loss': 0.66613, 'train_ucc_loss': 0.50909, 'train_ucc_acc': 0.78125, 'loss': 0.58761}\n",
            "Step 61580: {'train_ae_loss': 0.66646, 'train_ucc_loss': 0.44758, 'train_ucc_acc': 0.875, 'loss': 0.55702}\n",
            "Step 61600: {'train_ae_loss': 0.67345, 'train_ucc_loss': 0.52954, 'train_ucc_acc': 0.75, 'loss': 0.6015}\n",
            "Step 61620: {'train_ae_loss': 0.66176, 'train_ucc_loss': 0.52126, 'train_ucc_acc': 0.75, 'loss': 0.59151}\n",
            "Step 61640: {'train_ae_loss': 0.66508, 'train_ucc_loss': 0.38456, 'train_ucc_acc': 0.9375, 'loss': 0.52482}\n",
            "Step 61660: {'train_ae_loss': 0.66917, 'train_ucc_loss': 0.43259, 'train_ucc_acc': 0.875, 'loss': 0.55088}\n",
            "Step 61680: {'train_ae_loss': 0.66655, 'train_ucc_loss': 0.42662, 'train_ucc_acc': 0.90625, 'loss': 0.54658}\n",
            "Step 61700: {'train_ae_loss': 0.66829, 'train_ucc_loss': 0.54008, 'train_ucc_acc': 0.78125, 'loss': 0.60419}\n",
            "Step 61720: {'train_ae_loss': 0.65768, 'train_ucc_loss': 0.5033, 'train_ucc_acc': 0.78125, 'loss': 0.58049}\n",
            "Step 61740: {'train_ae_loss': 0.6652, 'train_ucc_loss': 0.48827, 'train_ucc_acc': 0.8125, 'loss': 0.57674}\n",
            "Step 61760: {'train_ae_loss': 0.65709, 'train_ucc_loss': 0.55149, 'train_ucc_acc': 0.78125, 'loss': 0.60429}\n",
            "Step 61780: {'train_ae_loss': 0.67004, 'train_ucc_loss': 0.38843, 'train_ucc_acc': 0.9375, 'loss': 0.52924}\n",
            "Step 61800: {'train_ae_loss': 0.66953, 'train_ucc_loss': 0.57147, 'train_ucc_acc': 0.71875, 'loss': 0.6205}\n",
            "Step 61820: {'train_ae_loss': 0.65865, 'train_ucc_loss': 0.5198, 'train_ucc_acc': 0.8125, 'loss': 0.58922}\n",
            "Step 61840: {'train_ae_loss': 0.6666, 'train_ucc_loss': 0.48216, 'train_ucc_acc': 0.8125, 'loss': 0.57438}\n",
            "Step 61860: {'train_ae_loss': 0.66676, 'train_ucc_loss': 0.55467, 'train_ucc_acc': 0.71875, 'loss': 0.61071}\n",
            "Step 61880: {'train_ae_loss': 0.6987, 'train_ucc_loss': 0.54021, 'train_ucc_acc': 0.75, 'loss': 0.61946}\n",
            "Step 61900: {'train_ae_loss': 0.69431, 'train_ucc_loss': 0.36121, 'train_ucc_acc': 0.96875, 'loss': 0.52776}\n",
            "Step 61920: {'train_ae_loss': 0.67281, 'train_ucc_loss': 0.52553, 'train_ucc_acc': 0.78125, 'loss': 0.59917}\n",
            "Step 61940: {'train_ae_loss': 0.66696, 'train_ucc_loss': 0.45508, 'train_ucc_acc': 0.875, 'loss': 0.56102}\n",
            "Step 61960: {'train_ae_loss': 0.665, 'train_ucc_loss': 0.4714, 'train_ucc_acc': 0.8125, 'loss': 0.5682}\n",
            "Step 61980: {'train_ae_loss': 0.6434, 'train_ucc_loss': 0.36398, 'train_ucc_acc': 0.9375, 'loss': 0.50369}\n",
            "Step 62000: {'train_ae_loss': 0.66138, 'train_ucc_loss': 0.48201, 'train_ucc_acc': 0.8125, 'loss': 0.5717}\n",
            "step: 62000,eval_ae_loss: 0.65475,eval_ucc_loss: 0.52157,eval_ucc_acc: 0.78418\n",
            "Step 62020: {'train_ae_loss': 0.6469, 'train_ucc_loss': 0.46108, 'train_ucc_acc': 0.84375, 'loss': 0.55399}\n",
            "Step 62040: {'train_ae_loss': 0.66591, 'train_ucc_loss': 0.49944, 'train_ucc_acc': 0.8125, 'loss': 0.58267}\n",
            "Step 62060: {'train_ae_loss': 0.66519, 'train_ucc_loss': 0.39481, 'train_ucc_acc': 0.9375, 'loss': 0.53}\n",
            "Step 62080: {'train_ae_loss': 0.66409, 'train_ucc_loss': 0.52853, 'train_ucc_acc': 0.75, 'loss': 0.59631}\n",
            "Step 62100: {'train_ae_loss': 0.67356, 'train_ucc_loss': 0.43715, 'train_ucc_acc': 0.875, 'loss': 0.55536}\n",
            "Step 62120: {'train_ae_loss': 0.66689, 'train_ucc_loss': 0.4699, 'train_ucc_acc': 0.875, 'loss': 0.56839}\n",
            "Step 62140: {'train_ae_loss': 0.66565, 'train_ucc_loss': 0.4332, 'train_ucc_acc': 0.84375, 'loss': 0.54942}\n",
            "Step 62160: {'train_ae_loss': 0.66463, 'train_ucc_loss': 0.51761, 'train_ucc_acc': 0.78125, 'loss': 0.59112}\n",
            "Step 62180: {'train_ae_loss': 0.66145, 'train_ucc_loss': 0.47492, 'train_ucc_acc': 0.84375, 'loss': 0.56819}\n",
            "Step 62200: {'train_ae_loss': 0.67778, 'train_ucc_loss': 0.50627, 'train_ucc_acc': 0.8125, 'loss': 0.59202}\n",
            "Step 62220: {'train_ae_loss': 0.67718, 'train_ucc_loss': 0.52876, 'train_ucc_acc': 0.75, 'loss': 0.60297}\n",
            "Step 62240: {'train_ae_loss': 0.66431, 'train_ucc_loss': 0.41765, 'train_ucc_acc': 0.875, 'loss': 0.54098}\n",
            "Step 62260: {'train_ae_loss': 0.6553, 'train_ucc_loss': 0.43608, 'train_ucc_acc': 0.875, 'loss': 0.54569}\n",
            "Step 62280: {'train_ae_loss': 0.67833, 'train_ucc_loss': 0.49395, 'train_ucc_acc': 0.8125, 'loss': 0.58614}\n",
            "Step 62300: {'train_ae_loss': 0.64903, 'train_ucc_loss': 0.44854, 'train_ucc_acc': 0.875, 'loss': 0.54878}\n",
            "Step 62320: {'train_ae_loss': 0.66753, 'train_ucc_loss': 0.42435, 'train_ucc_acc': 0.90625, 'loss': 0.54594}\n",
            "Step 62340: {'train_ae_loss': 0.66244, 'train_ucc_loss': 0.43827, 'train_ucc_acc': 0.875, 'loss': 0.55035}\n",
            "Step 62360: {'train_ae_loss': 0.66442, 'train_ucc_loss': 0.51315, 'train_ucc_acc': 0.78125, 'loss': 0.58878}\n",
            "Step 62380: {'train_ae_loss': 0.6803, 'train_ucc_loss': 0.38012, 'train_ucc_acc': 0.9375, 'loss': 0.53021}\n",
            "Step 62400: {'train_ae_loss': 0.65749, 'train_ucc_loss': 0.38317, 'train_ucc_acc': 0.9375, 'loss': 0.52033}\n",
            "Step 62420: {'train_ae_loss': 0.6629, 'train_ucc_loss': 0.49178, 'train_ucc_acc': 0.8125, 'loss': 0.57734}\n",
            "Step 62440: {'train_ae_loss': 0.65584, 'train_ucc_loss': 0.42909, 'train_ucc_acc': 0.875, 'loss': 0.54246}\n",
            "Step 62460: {'train_ae_loss': 0.64788, 'train_ucc_loss': 0.44803, 'train_ucc_acc': 0.84375, 'loss': 0.54796}\n",
            "Step 62480: {'train_ae_loss': 0.67861, 'train_ucc_loss': 0.43756, 'train_ucc_acc': 0.875, 'loss': 0.55808}\n",
            "Step 62500: {'train_ae_loss': 0.6586, 'train_ucc_loss': 0.42847, 'train_ucc_acc': 0.90625, 'loss': 0.54353}\n",
            "Step 62520: {'train_ae_loss': 0.67611, 'train_ucc_loss': 0.38451, 'train_ucc_acc': 0.90625, 'loss': 0.53031}\n",
            "Step 62540: {'train_ae_loss': 0.66327, 'train_ucc_loss': 0.32456, 'train_ucc_acc': 1.0, 'loss': 0.49392}\n",
            "Step 62560: {'train_ae_loss': 0.66302, 'train_ucc_loss': 0.60012, 'train_ucc_acc': 0.6875, 'loss': 0.63157}\n",
            "Step 62580: {'train_ae_loss': 0.6726, 'train_ucc_loss': 0.58267, 'train_ucc_acc': 0.71875, 'loss': 0.62763}\n",
            "Step 62600: {'train_ae_loss': 0.66269, 'train_ucc_loss': 0.47608, 'train_ucc_acc': 0.78125, 'loss': 0.56938}\n",
            "Step 62620: {'train_ae_loss': 0.66274, 'train_ucc_loss': 0.56764, 'train_ucc_acc': 0.78125, 'loss': 0.61519}\n",
            "Step 62640: {'train_ae_loss': 0.677, 'train_ucc_loss': 0.32357, 'train_ucc_acc': 1.0, 'loss': 0.50029}\n",
            "Step 62660: {'train_ae_loss': 0.69366, 'train_ucc_loss': 0.40908, 'train_ucc_acc': 0.90625, 'loss': 0.55137}\n",
            "Step 62680: {'train_ae_loss': 0.67679, 'train_ucc_loss': 0.39233, 'train_ucc_acc': 0.90625, 'loss': 0.53456}\n",
            "Step 62700: {'train_ae_loss': 0.66145, 'train_ucc_loss': 0.55983, 'train_ucc_acc': 0.75, 'loss': 0.61064}\n",
            "Step 62720: {'train_ae_loss': 0.65618, 'train_ucc_loss': 0.45387, 'train_ucc_acc': 0.84375, 'loss': 0.55502}\n",
            "Step 62740: {'train_ae_loss': 0.67474, 'train_ucc_loss': 0.63813, 'train_ucc_acc': 0.6875, 'loss': 0.65643}\n",
            "Step 62760: {'train_ae_loss': 0.66468, 'train_ucc_loss': 0.50991, 'train_ucc_acc': 0.8125, 'loss': 0.5873}\n",
            "Step 62780: {'train_ae_loss': 0.68026, 'train_ucc_loss': 0.44289, 'train_ucc_acc': 0.875, 'loss': 0.56157}\n",
            "Step 62800: {'train_ae_loss': 0.66296, 'train_ucc_loss': 0.4929, 'train_ucc_acc': 0.8125, 'loss': 0.57793}\n",
            "Step 62820: {'train_ae_loss': 0.70088, 'train_ucc_loss': 0.39813, 'train_ucc_acc': 0.90625, 'loss': 0.5495}\n",
            "Step 62840: {'train_ae_loss': 0.68399, 'train_ucc_loss': 0.39725, 'train_ucc_acc': 0.90625, 'loss': 0.54062}\n",
            "Step 62860: {'train_ae_loss': 0.66037, 'train_ucc_loss': 0.43911, 'train_ucc_acc': 0.84375, 'loss': 0.54974}\n",
            "Step 62880: {'train_ae_loss': 0.65717, 'train_ucc_loss': 0.49056, 'train_ucc_acc': 0.78125, 'loss': 0.57387}\n",
            "Step 62900: {'train_ae_loss': 0.65252, 'train_ucc_loss': 0.47706, 'train_ucc_acc': 0.84375, 'loss': 0.56479}\n",
            "Step 62920: {'train_ae_loss': 0.66121, 'train_ucc_loss': 0.46361, 'train_ucc_acc': 0.84375, 'loss': 0.56241}\n",
            "Step 62940: {'train_ae_loss': 0.66045, 'train_ucc_loss': 0.43682, 'train_ucc_acc': 0.875, 'loss': 0.54863}\n",
            "Step 62960: {'train_ae_loss': 0.6658, 'train_ucc_loss': 0.42156, 'train_ucc_acc': 0.84375, 'loss': 0.54368}\n",
            "Step 62980: {'train_ae_loss': 0.68792, 'train_ucc_loss': 0.46047, 'train_ucc_acc': 0.84375, 'loss': 0.57419}\n",
            "Step 63000: {'train_ae_loss': 0.64632, 'train_ucc_loss': 0.47781, 'train_ucc_acc': 0.8125, 'loss': 0.56206}\n",
            "step: 63000,eval_ae_loss: 0.65307,eval_ucc_loss: 0.51244,eval_ucc_acc: 0.79492\n",
            "Step 63020: {'train_ae_loss': 0.65728, 'train_ucc_loss': 0.45063, 'train_ucc_acc': 0.875, 'loss': 0.55395}\n",
            "Step 63040: {'train_ae_loss': 0.66785, 'train_ucc_loss': 0.40228, 'train_ucc_acc': 0.90625, 'loss': 0.53507}\n",
            "Step 63060: {'train_ae_loss': 0.67182, 'train_ucc_loss': 0.4893, 'train_ucc_acc': 0.8125, 'loss': 0.58056}\n",
            "Step 63080: {'train_ae_loss': 0.69445, 'train_ucc_loss': 0.46999, 'train_ucc_acc': 0.84375, 'loss': 0.58222}\n",
            "Step 63100: {'train_ae_loss': 0.66781, 'train_ucc_loss': 0.56087, 'train_ucc_acc': 0.71875, 'loss': 0.61434}\n",
            "Step 63120: {'train_ae_loss': 0.6401, 'train_ucc_loss': 0.61859, 'train_ucc_acc': 0.65625, 'loss': 0.62934}\n",
            "Step 63140: {'train_ae_loss': 0.66505, 'train_ucc_loss': 0.51027, 'train_ucc_acc': 0.8125, 'loss': 0.58766}\n",
            "Step 63160: {'train_ae_loss': 0.65595, 'train_ucc_loss': 0.55257, 'train_ucc_acc': 0.71875, 'loss': 0.60426}\n",
            "Step 63180: {'train_ae_loss': 0.66659, 'train_ucc_loss': 0.54483, 'train_ucc_acc': 0.75, 'loss': 0.60571}\n",
            "Step 63200: {'train_ae_loss': 0.66874, 'train_ucc_loss': 0.42679, 'train_ucc_acc': 0.875, 'loss': 0.54776}\n",
            "Step 63220: {'train_ae_loss': 0.66367, 'train_ucc_loss': 0.38474, 'train_ucc_acc': 0.9375, 'loss': 0.5242}\n",
            "Step 63240: {'train_ae_loss': 0.66511, 'train_ucc_loss': 0.48142, 'train_ucc_acc': 0.84375, 'loss': 0.57327}\n",
            "Step 63260: {'train_ae_loss': 0.67584, 'train_ucc_loss': 0.42912, 'train_ucc_acc': 0.875, 'loss': 0.55248}\n",
            "Step 63280: {'train_ae_loss': 0.67906, 'train_ucc_loss': 0.39621, 'train_ucc_acc': 0.9375, 'loss': 0.53764}\n",
            "Step 63300: {'train_ae_loss': 0.67863, 'train_ucc_loss': 0.41332, 'train_ucc_acc': 0.90625, 'loss': 0.54597}\n",
            "Step 63320: {'train_ae_loss': 0.64903, 'train_ucc_loss': 0.58461, 'train_ucc_acc': 0.6875, 'loss': 0.61682}\n",
            "Step 63340: {'train_ae_loss': 0.66726, 'train_ucc_loss': 0.52246, 'train_ucc_acc': 0.78125, 'loss': 0.59486}\n",
            "Step 63360: {'train_ae_loss': 0.66398, 'train_ucc_loss': 0.46334, 'train_ucc_acc': 0.84375, 'loss': 0.56366}\n",
            "Step 63380: {'train_ae_loss': 0.66488, 'train_ucc_loss': 0.46226, 'train_ucc_acc': 0.84375, 'loss': 0.56357}\n",
            "Step 63400: {'train_ae_loss': 0.65813, 'train_ucc_loss': 0.44207, 'train_ucc_acc': 0.875, 'loss': 0.5501}\n",
            "Step 63420: {'train_ae_loss': 0.65804, 'train_ucc_loss': 0.38259, 'train_ucc_acc': 0.9375, 'loss': 0.52031}\n",
            "Step 63440: {'train_ae_loss': 0.68503, 'train_ucc_loss': 0.42845, 'train_ucc_acc': 0.875, 'loss': 0.55674}\n",
            "Step 63460: {'train_ae_loss': 0.66506, 'train_ucc_loss': 0.42457, 'train_ucc_acc': 0.875, 'loss': 0.54482}\n",
            "Step 63480: {'train_ae_loss': 0.66553, 'train_ucc_loss': 0.44704, 'train_ucc_acc': 0.84375, 'loss': 0.55629}\n",
            "Step 63500: {'train_ae_loss': 0.65309, 'train_ucc_loss': 0.51993, 'train_ucc_acc': 0.8125, 'loss': 0.58651}\n",
            "Step 63520: {'train_ae_loss': 0.67313, 'train_ucc_loss': 0.39438, 'train_ucc_acc': 0.9375, 'loss': 0.53376}\n",
            "Step 63540: {'train_ae_loss': 0.67063, 'train_ucc_loss': 0.49023, 'train_ucc_acc': 0.84375, 'loss': 0.58043}\n",
            "Step 63560: {'train_ae_loss': 0.66553, 'train_ucc_loss': 0.43331, 'train_ucc_acc': 0.84375, 'loss': 0.54942}\n",
            "Step 63580: {'train_ae_loss': 0.66293, 'train_ucc_loss': 0.49046, 'train_ucc_acc': 0.84375, 'loss': 0.57669}\n",
            "Step 63600: {'train_ae_loss': 0.65152, 'train_ucc_loss': 0.51692, 'train_ucc_acc': 0.8125, 'loss': 0.58422}\n",
            "Step 63620: {'train_ae_loss': 0.66258, 'train_ucc_loss': 0.64176, 'train_ucc_acc': 0.65625, 'loss': 0.65217}\n",
            "Step 63640: {'train_ae_loss': 0.67581, 'train_ucc_loss': 0.5016, 'train_ucc_acc': 0.8125, 'loss': 0.5887}\n",
            "Step 63660: {'train_ae_loss': 0.65925, 'train_ucc_loss': 0.49442, 'train_ucc_acc': 0.8125, 'loss': 0.57684}\n",
            "Step 63680: {'train_ae_loss': 0.69023, 'train_ucc_loss': 0.46101, 'train_ucc_acc': 0.84375, 'loss': 0.57562}\n",
            "Step 63700: {'train_ae_loss': 0.67967, 'train_ucc_loss': 0.47861, 'train_ucc_acc': 0.78125, 'loss': 0.57914}\n",
            "Step 63720: {'train_ae_loss': 0.67033, 'train_ucc_loss': 0.46373, 'train_ucc_acc': 0.84375, 'loss': 0.56703}\n",
            "Step 63740: {'train_ae_loss': 0.67994, 'train_ucc_loss': 0.41321, 'train_ucc_acc': 0.9375, 'loss': 0.54657}\n",
            "Step 63760: {'train_ae_loss': 0.67682, 'train_ucc_loss': 0.48718, 'train_ucc_acc': 0.8125, 'loss': 0.582}\n",
            "Step 63780: {'train_ae_loss': 0.66431, 'train_ucc_loss': 0.464, 'train_ucc_acc': 0.84375, 'loss': 0.56415}\n",
            "Step 63800: {'train_ae_loss': 0.69806, 'train_ucc_loss': 0.45466, 'train_ucc_acc': 0.84375, 'loss': 0.57636}\n",
            "Step 63820: {'train_ae_loss': 0.68264, 'train_ucc_loss': 0.44146, 'train_ucc_acc': 0.875, 'loss': 0.56205}\n",
            "Step 63840: {'train_ae_loss': 0.67101, 'train_ucc_loss': 0.47768, 'train_ucc_acc': 0.8125, 'loss': 0.57435}\n",
            "Step 63860: {'train_ae_loss': 0.67502, 'train_ucc_loss': 0.39601, 'train_ucc_acc': 0.90625, 'loss': 0.53551}\n",
            "Step 63880: {'train_ae_loss': 0.66624, 'train_ucc_loss': 0.44189, 'train_ucc_acc': 0.875, 'loss': 0.55407}\n",
            "Step 63900: {'train_ae_loss': 0.64962, 'train_ucc_loss': 0.4559, 'train_ucc_acc': 0.84375, 'loss': 0.55276}\n",
            "Step 63920: {'train_ae_loss': 0.66865, 'train_ucc_loss': 0.48662, 'train_ucc_acc': 0.78125, 'loss': 0.57763}\n",
            "Step 63940: {'train_ae_loss': 0.67992, 'train_ucc_loss': 0.46166, 'train_ucc_acc': 0.84375, 'loss': 0.57079}\n",
            "Step 63960: {'train_ae_loss': 0.65761, 'train_ucc_loss': 0.44538, 'train_ucc_acc': 0.875, 'loss': 0.55149}\n",
            "Step 63980: {'train_ae_loss': 0.66971, 'train_ucc_loss': 0.37146, 'train_ucc_acc': 0.9375, 'loss': 0.52058}\n",
            "Step 64000: {'train_ae_loss': 0.66865, 'train_ucc_loss': 0.5791, 'train_ucc_acc': 0.71875, 'loss': 0.62387}\n",
            "step: 64000,eval_ae_loss: 0.65985,eval_ucc_loss: 0.49745,eval_ucc_acc: 0.80371\n",
            "Step 64020: {'train_ae_loss': 0.6656, 'train_ucc_loss': 0.4623, 'train_ucc_acc': 0.84375, 'loss': 0.56395}\n",
            "Step 64040: {'train_ae_loss': 0.67893, 'train_ucc_loss': 0.45089, 'train_ucc_acc': 0.84375, 'loss': 0.56491}\n",
            "Step 64060: {'train_ae_loss': 0.66132, 'train_ucc_loss': 0.38595, 'train_ucc_acc': 0.9375, 'loss': 0.52363}\n",
            "Step 64080: {'train_ae_loss': 0.67229, 'train_ucc_loss': 0.53761, 'train_ucc_acc': 0.75, 'loss': 0.60495}\n",
            "Step 64100: {'train_ae_loss': 0.66874, 'train_ucc_loss': 0.41498, 'train_ucc_acc': 0.875, 'loss': 0.54186}\n",
            "Step 64120: {'train_ae_loss': 0.64696, 'train_ucc_loss': 0.49107, 'train_ucc_acc': 0.8125, 'loss': 0.56901}\n",
            "Step 64140: {'train_ae_loss': 0.68315, 'train_ucc_loss': 0.47382, 'train_ucc_acc': 0.84375, 'loss': 0.57849}\n",
            "Step 64160: {'train_ae_loss': 0.67522, 'train_ucc_loss': 0.4767, 'train_ucc_acc': 0.84375, 'loss': 0.57596}\n",
            "Step 64180: {'train_ae_loss': 0.66597, 'train_ucc_loss': 0.54819, 'train_ucc_acc': 0.75, 'loss': 0.60708}\n",
            "Step 64200: {'train_ae_loss': 0.66423, 'train_ucc_loss': 0.44564, 'train_ucc_acc': 0.875, 'loss': 0.55493}\n",
            "Step 64220: {'train_ae_loss': 0.66604, 'train_ucc_loss': 0.44599, 'train_ucc_acc': 0.875, 'loss': 0.55602}\n",
            "Step 64240: {'train_ae_loss': 0.64742, 'train_ucc_loss': 0.50656, 'train_ucc_acc': 0.8125, 'loss': 0.57699}\n",
            "Step 64260: {'train_ae_loss': 0.67358, 'train_ucc_loss': 0.478, 'train_ucc_acc': 0.84375, 'loss': 0.57579}\n",
            "Step 64280: {'train_ae_loss': 0.66624, 'train_ucc_loss': 0.45057, 'train_ucc_acc': 0.84375, 'loss': 0.5584}\n",
            "Step 64300: {'train_ae_loss': 0.66957, 'train_ucc_loss': 0.4412, 'train_ucc_acc': 0.84375, 'loss': 0.55538}\n",
            "Step 64320: {'train_ae_loss': 0.66032, 'train_ucc_loss': 0.58037, 'train_ucc_acc': 0.6875, 'loss': 0.62035}\n",
            "Step 64340: {'train_ae_loss': 0.66866, 'train_ucc_loss': 0.45945, 'train_ucc_acc': 0.84375, 'loss': 0.56406}\n",
            "Step 64360: {'train_ae_loss': 0.6721, 'train_ucc_loss': 0.53032, 'train_ucc_acc': 0.78125, 'loss': 0.60121}\n",
            "Step 64380: {'train_ae_loss': 0.66845, 'train_ucc_loss': 0.50476, 'train_ucc_acc': 0.8125, 'loss': 0.58661}\n",
            "Step 64400: {'train_ae_loss': 0.67049, 'train_ucc_loss': 0.51834, 'train_ucc_acc': 0.75, 'loss': 0.59441}\n",
            "Step 64420: {'train_ae_loss': 0.68124, 'train_ucc_loss': 0.41509, 'train_ucc_acc': 0.875, 'loss': 0.54817}\n",
            "Step 64440: {'train_ae_loss': 0.66343, 'train_ucc_loss': 0.42243, 'train_ucc_acc': 0.875, 'loss': 0.54293}\n",
            "Step 64460: {'train_ae_loss': 0.63885, 'train_ucc_loss': 0.46045, 'train_ucc_acc': 0.84375, 'loss': 0.54965}\n",
            "Step 64480: {'train_ae_loss': 0.67627, 'train_ucc_loss': 0.48078, 'train_ucc_acc': 0.84375, 'loss': 0.57852}\n",
            "Step 64500: {'train_ae_loss': 0.66975, 'train_ucc_loss': 0.45554, 'train_ucc_acc': 0.84375, 'loss': 0.56265}\n",
            "Step 64520: {'train_ae_loss': 0.66595, 'train_ucc_loss': 0.37852, 'train_ucc_acc': 0.9375, 'loss': 0.52223}\n",
            "Step 64540: {'train_ae_loss': 0.66316, 'train_ucc_loss': 0.53793, 'train_ucc_acc': 0.78125, 'loss': 0.60054}\n",
            "Step 64560: {'train_ae_loss': 0.67371, 'train_ucc_loss': 0.50129, 'train_ucc_acc': 0.8125, 'loss': 0.5875}\n",
            "Step 64580: {'train_ae_loss': 0.67882, 'train_ucc_loss': 0.5463, 'train_ucc_acc': 0.71875, 'loss': 0.61256}\n",
            "Step 64600: {'train_ae_loss': 0.66342, 'train_ucc_loss': 0.43554, 'train_ucc_acc': 0.84375, 'loss': 0.54948}\n",
            "Step 64620: {'train_ae_loss': 0.68363, 'train_ucc_loss': 0.51104, 'train_ucc_acc': 0.8125, 'loss': 0.59733}\n",
            "Step 64640: {'train_ae_loss': 0.68322, 'train_ucc_loss': 0.42846, 'train_ucc_acc': 0.875, 'loss': 0.55584}\n",
            "Step 64660: {'train_ae_loss': 0.67558, 'train_ucc_loss': 0.60026, 'train_ucc_acc': 0.71875, 'loss': 0.63792}\n",
            "Step 64680: {'train_ae_loss': 0.66964, 'train_ucc_loss': 0.49768, 'train_ucc_acc': 0.8125, 'loss': 0.58366}\n",
            "Step 64700: {'train_ae_loss': 0.683, 'train_ucc_loss': 0.54725, 'train_ucc_acc': 0.75, 'loss': 0.61512}\n",
            "Step 64720: {'train_ae_loss': 0.67638, 'train_ucc_loss': 0.48989, 'train_ucc_acc': 0.8125, 'loss': 0.58313}\n",
            "Step 64740: {'train_ae_loss': 0.6754, 'train_ucc_loss': 0.47966, 'train_ucc_acc': 0.84375, 'loss': 0.57753}\n",
            "Step 64760: {'train_ae_loss': 0.67462, 'train_ucc_loss': 0.46514, 'train_ucc_acc': 0.875, 'loss': 0.56988}\n",
            "Step 64780: {'train_ae_loss': 0.66321, 'train_ucc_loss': 0.38486, 'train_ucc_acc': 0.9375, 'loss': 0.52404}\n",
            "Step 64800: {'train_ae_loss': 0.68823, 'train_ucc_loss': 0.43638, 'train_ucc_acc': 0.875, 'loss': 0.5623}\n",
            "Step 64820: {'train_ae_loss': 0.68608, 'train_ucc_loss': 0.5474, 'train_ucc_acc': 0.75, 'loss': 0.61674}\n",
            "Step 64840: {'train_ae_loss': 0.66847, 'train_ucc_loss': 0.41215, 'train_ucc_acc': 0.90625, 'loss': 0.54031}\n",
            "Step 64860: {'train_ae_loss': 0.67041, 'train_ucc_loss': 0.3789, 'train_ucc_acc': 0.9375, 'loss': 0.52465}\n",
            "Step 64880: {'train_ae_loss': 0.67254, 'train_ucc_loss': 0.40104, 'train_ucc_acc': 0.90625, 'loss': 0.53679}\n",
            "Step 64900: {'train_ae_loss': 0.66256, 'train_ucc_loss': 0.43977, 'train_ucc_acc': 0.875, 'loss': 0.55116}\n",
            "Step 64920: {'train_ae_loss': 0.65634, 'train_ucc_loss': 0.45303, 'train_ucc_acc': 0.8125, 'loss': 0.55468}\n",
            "Step 64940: {'train_ae_loss': 0.66764, 'train_ucc_loss': 0.44278, 'train_ucc_acc': 0.875, 'loss': 0.55521}\n",
            "Step 64960: {'train_ae_loss': 0.66187, 'train_ucc_loss': 0.47697, 'train_ucc_acc': 0.84375, 'loss': 0.56942}\n",
            "Step 64980: {'train_ae_loss': 0.664, 'train_ucc_loss': 0.46547, 'train_ucc_acc': 0.84375, 'loss': 0.56474}\n",
            "Step 65000: {'train_ae_loss': 0.67397, 'train_ucc_loss': 0.45877, 'train_ucc_acc': 0.875, 'loss': 0.56637}\n",
            "step: 65000,eval_ae_loss: 0.66543,eval_ucc_loss: 0.50155,eval_ucc_acc: 0.80273\n",
            "Step 65020: {'train_ae_loss': 0.68098, 'train_ucc_loss': 0.49005, 'train_ucc_acc': 0.84375, 'loss': 0.58552}\n",
            "Step 65040: {'train_ae_loss': 0.68407, 'train_ucc_loss': 0.50283, 'train_ucc_acc': 0.78125, 'loss': 0.59345}\n",
            "Step 65060: {'train_ae_loss': 0.67806, 'train_ucc_loss': 0.49869, 'train_ucc_acc': 0.8125, 'loss': 0.58838}\n",
            "Step 65080: {'train_ae_loss': 0.66009, 'train_ucc_loss': 0.38523, 'train_ucc_acc': 0.9375, 'loss': 0.52266}\n",
            "Step 65100: {'train_ae_loss': 0.66641, 'train_ucc_loss': 0.4197, 'train_ucc_acc': 0.875, 'loss': 0.54305}\n",
            "Step 65120: {'train_ae_loss': 0.67439, 'train_ucc_loss': 0.60172, 'train_ucc_acc': 0.65625, 'loss': 0.63805}\n",
            "Step 65140: {'train_ae_loss': 0.68174, 'train_ucc_loss': 0.47253, 'train_ucc_acc': 0.8125, 'loss': 0.57713}\n",
            "Step 65160: {'train_ae_loss': 0.65918, 'train_ucc_loss': 0.61059, 'train_ucc_acc': 0.6875, 'loss': 0.63488}\n",
            "Step 65180: {'train_ae_loss': 0.66094, 'train_ucc_loss': 0.55127, 'train_ucc_acc': 0.78125, 'loss': 0.60611}\n",
            "Step 65200: {'train_ae_loss': 0.64652, 'train_ucc_loss': 0.49172, 'train_ucc_acc': 0.8125, 'loss': 0.56912}\n",
            "Step 65220: {'train_ae_loss': 0.65439, 'train_ucc_loss': 0.48398, 'train_ucc_acc': 0.8125, 'loss': 0.56919}\n",
            "Step 65240: {'train_ae_loss': 0.67635, 'train_ucc_loss': 0.41099, 'train_ucc_acc': 0.90625, 'loss': 0.54367}\n",
            "Step 65260: {'train_ae_loss': 0.66842, 'train_ucc_loss': 0.47614, 'train_ucc_acc': 0.8125, 'loss': 0.57228}\n",
            "Step 65280: {'train_ae_loss': 0.67243, 'train_ucc_loss': 0.42321, 'train_ucc_acc': 0.875, 'loss': 0.54782}\n",
            "Step 65300: {'train_ae_loss': 0.68009, 'train_ucc_loss': 0.53072, 'train_ucc_acc': 0.75, 'loss': 0.6054}\n",
            "Step 65320: {'train_ae_loss': 0.67565, 'train_ucc_loss': 0.48684, 'train_ucc_acc': 0.78125, 'loss': 0.58125}\n",
            "Step 65340: {'train_ae_loss': 0.66291, 'train_ucc_loss': 0.46207, 'train_ucc_acc': 0.84375, 'loss': 0.56249}\n",
            "Step 65360: {'train_ae_loss': 0.65599, 'train_ucc_loss': 0.49853, 'train_ucc_acc': 0.75, 'loss': 0.57726}\n",
            "Step 65380: {'train_ae_loss': 0.65959, 'train_ucc_loss': 0.5032, 'train_ucc_acc': 0.78125, 'loss': 0.58139}\n",
            "Step 65400: {'train_ae_loss': 0.68685, 'train_ucc_loss': 0.49636, 'train_ucc_acc': 0.8125, 'loss': 0.5916}\n",
            "Step 65420: {'train_ae_loss': 0.64397, 'train_ucc_loss': 0.3954, 'train_ucc_acc': 0.90625, 'loss': 0.51969}\n",
            "Step 65440: {'train_ae_loss': 0.67662, 'train_ucc_loss': 0.49199, 'train_ucc_acc': 0.8125, 'loss': 0.5843}\n",
            "Step 65460: {'train_ae_loss': 0.65387, 'train_ucc_loss': 0.55201, 'train_ucc_acc': 0.75, 'loss': 0.60294}\n",
            "Step 65480: {'train_ae_loss': 0.6843, 'train_ucc_loss': 0.58149, 'train_ucc_acc': 0.6875, 'loss': 0.63289}\n",
            "Step 65500: {'train_ae_loss': 0.66745, 'train_ucc_loss': 0.39413, 'train_ucc_acc': 0.9375, 'loss': 0.53079}\n",
            "Step 65520: {'train_ae_loss': 0.66583, 'train_ucc_loss': 0.41335, 'train_ucc_acc': 0.90625, 'loss': 0.53959}\n",
            "Step 65540: {'train_ae_loss': 0.67233, 'train_ucc_loss': 0.47281, 'train_ucc_acc': 0.84375, 'loss': 0.57257}\n",
            "Step 65560: {'train_ae_loss': 0.67646, 'train_ucc_loss': 0.40937, 'train_ucc_acc': 0.90625, 'loss': 0.54292}\n",
            "Step 65580: {'train_ae_loss': 0.66684, 'train_ucc_loss': 0.44311, 'train_ucc_acc': 0.875, 'loss': 0.55497}\n",
            "Step 65600: {'train_ae_loss': 0.65746, 'train_ucc_loss': 0.50796, 'train_ucc_acc': 0.8125, 'loss': 0.58271}\n",
            "Step 65620: {'train_ae_loss': 0.65553, 'train_ucc_loss': 0.54356, 'train_ucc_acc': 0.75, 'loss': 0.59954}\n",
            "Step 65640: {'train_ae_loss': 0.66764, 'train_ucc_loss': 0.4364, 'train_ucc_acc': 0.875, 'loss': 0.55202}\n",
            "Step 65660: {'train_ae_loss': 0.64644, 'train_ucc_loss': 0.50429, 'train_ucc_acc': 0.78125, 'loss': 0.57536}\n",
            "Step 65680: {'train_ae_loss': 0.66987, 'train_ucc_loss': 0.70729, 'train_ucc_acc': 0.625, 'loss': 0.68858}\n",
            "Step 65700: {'train_ae_loss': 0.6748, 'train_ucc_loss': 0.36699, 'train_ucc_acc': 0.9375, 'loss': 0.52089}\n",
            "Step 65720: {'train_ae_loss': 0.67691, 'train_ucc_loss': 0.46641, 'train_ucc_acc': 0.84375, 'loss': 0.57166}\n",
            "Step 65740: {'train_ae_loss': 0.66002, 'train_ucc_loss': 0.42227, 'train_ucc_acc': 0.90625, 'loss': 0.54115}\n",
            "Step 65760: {'train_ae_loss': 0.6693, 'train_ucc_loss': 0.52766, 'train_ucc_acc': 0.78125, 'loss': 0.59848}\n",
            "Step 65780: {'train_ae_loss': 0.65131, 'train_ucc_loss': 0.43523, 'train_ucc_acc': 0.875, 'loss': 0.54327}\n",
            "Step 65800: {'train_ae_loss': 0.66869, 'train_ucc_loss': 0.35637, 'train_ucc_acc': 0.9375, 'loss': 0.51253}\n",
            "Step 65820: {'train_ae_loss': 0.66544, 'train_ucc_loss': 0.48346, 'train_ucc_acc': 0.84375, 'loss': 0.57445}\n",
            "Step 65840: {'train_ae_loss': 0.66252, 'train_ucc_loss': 0.44845, 'train_ucc_acc': 0.875, 'loss': 0.55548}\n",
            "Step 65860: {'train_ae_loss': 0.66025, 'train_ucc_loss': 0.49497, 'train_ucc_acc': 0.8125, 'loss': 0.57761}\n",
            "Step 65880: {'train_ae_loss': 0.67903, 'train_ucc_loss': 0.47291, 'train_ucc_acc': 0.8125, 'loss': 0.57597}\n",
            "Step 65900: {'train_ae_loss': 0.68432, 'train_ucc_loss': 0.38176, 'train_ucc_acc': 0.9375, 'loss': 0.53304}\n",
            "Step 65920: {'train_ae_loss': 0.65529, 'train_ucc_loss': 0.40179, 'train_ucc_acc': 0.90625, 'loss': 0.52854}\n",
            "Step 65940: {'train_ae_loss': 0.66019, 'train_ucc_loss': 0.5325, 'train_ucc_acc': 0.78125, 'loss': 0.59635}\n",
            "Step 65960: {'train_ae_loss': 0.67388, 'train_ucc_loss': 0.38892, 'train_ucc_acc': 0.90625, 'loss': 0.5314}\n",
            "Step 65980: {'train_ae_loss': 0.67025, 'train_ucc_loss': 0.52181, 'train_ucc_acc': 0.78125, 'loss': 0.59603}\n",
            "Step 66000: {'train_ae_loss': 0.66479, 'train_ucc_loss': 0.46769, 'train_ucc_acc': 0.84375, 'loss': 0.56624}\n",
            "step: 66000,eval_ae_loss: 0.65865,eval_ucc_loss: 0.48025,eval_ucc_acc: 0.83008\n",
            "Step 66020: {'train_ae_loss': 0.66514, 'train_ucc_loss': 0.38585, 'train_ucc_acc': 0.9375, 'loss': 0.52549}\n",
            "Step 66040: {'train_ae_loss': 0.68198, 'train_ucc_loss': 0.43168, 'train_ucc_acc': 0.875, 'loss': 0.55683}\n",
            "Step 66060: {'train_ae_loss': 0.67556, 'train_ucc_loss': 0.48288, 'train_ucc_acc': 0.8125, 'loss': 0.57922}\n",
            "Step 66080: {'train_ae_loss': 0.6636, 'train_ucc_loss': 0.49853, 'train_ucc_acc': 0.8125, 'loss': 0.58107}\n",
            "Step 66100: {'train_ae_loss': 0.67315, 'train_ucc_loss': 0.44229, 'train_ucc_acc': 0.875, 'loss': 0.55772}\n",
            "Step 66120: {'train_ae_loss': 0.66642, 'train_ucc_loss': 0.40494, 'train_ucc_acc': 0.90625, 'loss': 0.53568}\n",
            "Step 66140: {'train_ae_loss': 0.67433, 'train_ucc_loss': 0.42628, 'train_ucc_acc': 0.875, 'loss': 0.5503}\n",
            "Step 66160: {'train_ae_loss': 0.6671, 'train_ucc_loss': 0.404, 'train_ucc_acc': 0.90625, 'loss': 0.53555}\n",
            "Step 66180: {'train_ae_loss': 0.66901, 'train_ucc_loss': 0.3985, 'train_ucc_acc': 0.90625, 'loss': 0.53376}\n",
            "Step 66200: {'train_ae_loss': 0.65657, 'train_ucc_loss': 0.52186, 'train_ucc_acc': 0.8125, 'loss': 0.58922}\n",
            "Step 66220: {'train_ae_loss': 0.65712, 'train_ucc_loss': 0.46677, 'train_ucc_acc': 0.84375, 'loss': 0.56195}\n",
            "Step 66240: {'train_ae_loss': 0.67123, 'train_ucc_loss': 0.42706, 'train_ucc_acc': 0.875, 'loss': 0.54914}\n",
            "Step 66260: {'train_ae_loss': 0.66679, 'train_ucc_loss': 0.42314, 'train_ucc_acc': 0.875, 'loss': 0.54496}\n",
            "Step 66280: {'train_ae_loss': 0.67, 'train_ucc_loss': 0.49372, 'train_ucc_acc': 0.8125, 'loss': 0.58186}\n",
            "Step 66300: {'train_ae_loss': 0.67343, 'train_ucc_loss': 0.4366, 'train_ucc_acc': 0.84375, 'loss': 0.55502}\n",
            "Step 66320: {'train_ae_loss': 0.66627, 'train_ucc_loss': 0.44026, 'train_ucc_acc': 0.875, 'loss': 0.55327}\n",
            "Step 66340: {'train_ae_loss': 0.66473, 'train_ucc_loss': 0.4159, 'train_ucc_acc': 0.90625, 'loss': 0.54032}\n",
            "Step 66360: {'train_ae_loss': 0.672, 'train_ucc_loss': 0.58466, 'train_ucc_acc': 0.6875, 'loss': 0.62833}\n",
            "Step 66380: {'train_ae_loss': 0.69454, 'train_ucc_loss': 0.45244, 'train_ucc_acc': 0.84375, 'loss': 0.57349}\n",
            "Step 66400: {'train_ae_loss': 0.6793, 'train_ucc_loss': 0.3776, 'train_ucc_acc': 0.9375, 'loss': 0.52845}\n",
            "Step 66420: {'train_ae_loss': 0.66552, 'train_ucc_loss': 0.45484, 'train_ucc_acc': 0.84375, 'loss': 0.56018}\n",
            "Step 66440: {'train_ae_loss': 0.6746, 'train_ucc_loss': 0.5809, 'train_ucc_acc': 0.71875, 'loss': 0.62775}\n",
            "Step 66460: {'train_ae_loss': 0.66368, 'train_ucc_loss': 0.40575, 'train_ucc_acc': 0.90625, 'loss': 0.53472}\n",
            "Step 66480: {'train_ae_loss': 0.66246, 'train_ucc_loss': 0.49789, 'train_ucc_acc': 0.8125, 'loss': 0.58018}\n",
            "Step 66500: {'train_ae_loss': 0.64966, 'train_ucc_loss': 0.5249, 'train_ucc_acc': 0.78125, 'loss': 0.58728}\n",
            "Step 66520: {'train_ae_loss': 0.66578, 'train_ucc_loss': 0.47365, 'train_ucc_acc': 0.84375, 'loss': 0.56971}\n",
            "Step 66540: {'train_ae_loss': 0.68771, 'train_ucc_loss': 0.61889, 'train_ucc_acc': 0.65625, 'loss': 0.6533}\n",
            "Step 66560: {'train_ae_loss': 0.67848, 'train_ucc_loss': 0.54359, 'train_ucc_acc': 0.75, 'loss': 0.61103}\n",
            "Step 66580: {'train_ae_loss': 0.66412, 'train_ucc_loss': 0.49404, 'train_ucc_acc': 0.78125, 'loss': 0.57908}\n",
            "Step 66600: {'train_ae_loss': 0.6553, 'train_ucc_loss': 0.40571, 'train_ucc_acc': 0.90625, 'loss': 0.5305}\n",
            "Step 66620: {'train_ae_loss': 0.66589, 'train_ucc_loss': 0.61524, 'train_ucc_acc': 0.6875, 'loss': 0.64057}\n",
            "Step 66640: {'train_ae_loss': 0.67053, 'train_ucc_loss': 0.45586, 'train_ucc_acc': 0.84375, 'loss': 0.5632}\n",
            "Step 66660: {'train_ae_loss': 0.67193, 'train_ucc_loss': 0.42293, 'train_ucc_acc': 0.875, 'loss': 0.54743}\n",
            "Step 66680: {'train_ae_loss': 0.66251, 'train_ucc_loss': 0.4752, 'train_ucc_acc': 0.84375, 'loss': 0.56886}\n",
            "Step 66700: {'train_ae_loss': 0.66735, 'train_ucc_loss': 0.50987, 'train_ucc_acc': 0.78125, 'loss': 0.58861}\n",
            "Step 66720: {'train_ae_loss': 0.66438, 'train_ucc_loss': 0.41943, 'train_ucc_acc': 0.875, 'loss': 0.5419}\n",
            "Step 66740: {'train_ae_loss': 0.66921, 'train_ucc_loss': 0.44678, 'train_ucc_acc': 0.875, 'loss': 0.558}\n",
            "Step 66760: {'train_ae_loss': 0.66008, 'train_ucc_loss': 0.50368, 'train_ucc_acc': 0.8125, 'loss': 0.58188}\n",
            "Step 66780: {'train_ae_loss': 0.67029, 'train_ucc_loss': 0.48545, 'train_ucc_acc': 0.8125, 'loss': 0.57787}\n",
            "Step 66800: {'train_ae_loss': 0.65849, 'train_ucc_loss': 0.48804, 'train_ucc_acc': 0.8125, 'loss': 0.57327}\n",
            "Step 66820: {'train_ae_loss': 0.65976, 'train_ucc_loss': 0.59664, 'train_ucc_acc': 0.71875, 'loss': 0.6282}\n",
            "Step 66840: {'train_ae_loss': 0.65473, 'train_ucc_loss': 0.46699, 'train_ucc_acc': 0.84375, 'loss': 0.56086}\n",
            "Step 66860: {'train_ae_loss': 0.6639, 'train_ucc_loss': 0.5141, 'train_ucc_acc': 0.8125, 'loss': 0.589}\n",
            "Step 66880: {'train_ae_loss': 0.65542, 'train_ucc_loss': 0.42043, 'train_ucc_acc': 0.90625, 'loss': 0.53792}\n",
            "Step 66900: {'train_ae_loss': 0.6638, 'train_ucc_loss': 0.44357, 'train_ucc_acc': 0.875, 'loss': 0.55369}\n",
            "Step 66920: {'train_ae_loss': 0.66391, 'train_ucc_loss': 0.49123, 'train_ucc_acc': 0.8125, 'loss': 0.57757}\n",
            "Step 66940: {'train_ae_loss': 0.66591, 'train_ucc_loss': 0.44734, 'train_ucc_acc': 0.875, 'loss': 0.55663}\n",
            "Step 66960: {'train_ae_loss': 0.6663, 'train_ucc_loss': 0.51514, 'train_ucc_acc': 0.8125, 'loss': 0.59072}\n",
            "Step 66980: {'train_ae_loss': 0.6944, 'train_ucc_loss': 0.44809, 'train_ucc_acc': 0.90625, 'loss': 0.57125}\n",
            "Step 67000: {'train_ae_loss': 0.66391, 'train_ucc_loss': 0.43608, 'train_ucc_acc': 0.875, 'loss': 0.55}\n",
            "step: 67000,eval_ae_loss: 0.65899,eval_ucc_loss: 0.48572,eval_ucc_acc: 0.81934\n",
            "Step 67020: {'train_ae_loss': 0.66429, 'train_ucc_loss': 0.39769, 'train_ucc_acc': 0.90625, 'loss': 0.53099}\n",
            "Step 67040: {'train_ae_loss': 0.66529, 'train_ucc_loss': 0.47372, 'train_ucc_acc': 0.8125, 'loss': 0.56951}\n",
            "Step 67060: {'train_ae_loss': 0.65873, 'train_ucc_loss': 0.50175, 'train_ucc_acc': 0.78125, 'loss': 0.58024}\n",
            "Step 67080: {'train_ae_loss': 0.66599, 'train_ucc_loss': 0.33773, 'train_ucc_acc': 1.0, 'loss': 0.50186}\n",
            "Step 67100: {'train_ae_loss': 0.6674, 'train_ucc_loss': 0.40886, 'train_ucc_acc': 0.90625, 'loss': 0.53813}\n",
            "Step 67120: {'train_ae_loss': 0.65728, 'train_ucc_loss': 0.53526, 'train_ucc_acc': 0.78125, 'loss': 0.59627}\n",
            "Step 67140: {'train_ae_loss': 0.64895, 'train_ucc_loss': 0.43071, 'train_ucc_acc': 0.875, 'loss': 0.53983}\n",
            "Step 67160: {'train_ae_loss': 0.67897, 'train_ucc_loss': 0.52808, 'train_ucc_acc': 0.75, 'loss': 0.60353}\n",
            "Step 67180: {'train_ae_loss': 0.6782, 'train_ucc_loss': 0.44314, 'train_ucc_acc': 0.84375, 'loss': 0.56067}\n",
            "Step 67200: {'train_ae_loss': 0.65565, 'train_ucc_loss': 0.43297, 'train_ucc_acc': 0.84375, 'loss': 0.54431}\n",
            "Step 67220: {'train_ae_loss': 0.67241, 'train_ucc_loss': 0.49541, 'train_ucc_acc': 0.78125, 'loss': 0.58391}\n",
            "Step 67240: {'train_ae_loss': 0.6644, 'train_ucc_loss': 0.38313, 'train_ucc_acc': 0.9375, 'loss': 0.52376}\n",
            "Step 67260: {'train_ae_loss': 0.6737, 'train_ucc_loss': 0.49493, 'train_ucc_acc': 0.8125, 'loss': 0.58431}\n",
            "Step 67280: {'train_ae_loss': 0.67654, 'train_ucc_loss': 0.54716, 'train_ucc_acc': 0.71875, 'loss': 0.61185}\n",
            "Step 67300: {'train_ae_loss': 0.66755, 'train_ucc_loss': 0.48904, 'train_ucc_acc': 0.78125, 'loss': 0.5783}\n",
            "Step 67320: {'train_ae_loss': 0.65861, 'train_ucc_loss': 0.46389, 'train_ucc_acc': 0.84375, 'loss': 0.56125}\n",
            "Step 67340: {'train_ae_loss': 0.67509, 'train_ucc_loss': 0.45245, 'train_ucc_acc': 0.84375, 'loss': 0.56377}\n",
            "Step 67360: {'train_ae_loss': 0.67737, 'train_ucc_loss': 0.46267, 'train_ucc_acc': 0.84375, 'loss': 0.57002}\n",
            "Step 67380: {'train_ae_loss': 0.65734, 'train_ucc_loss': 0.43857, 'train_ucc_acc': 0.875, 'loss': 0.54795}\n",
            "Step 67400: {'train_ae_loss': 0.65606, 'train_ucc_loss': 0.52922, 'train_ucc_acc': 0.75, 'loss': 0.59264}\n",
            "Step 67420: {'train_ae_loss': 0.67233, 'train_ucc_loss': 0.41865, 'train_ucc_acc': 0.90625, 'loss': 0.54549}\n",
            "Step 67440: {'train_ae_loss': 0.67483, 'train_ucc_loss': 0.52766, 'train_ucc_acc': 0.71875, 'loss': 0.60125}\n",
            "Step 67460: {'train_ae_loss': 0.66092, 'train_ucc_loss': 0.51996, 'train_ucc_acc': 0.75, 'loss': 0.59044}\n",
            "Step 67480: {'train_ae_loss': 0.65737, 'train_ucc_loss': 0.5181, 'train_ucc_acc': 0.75, 'loss': 0.58773}\n",
            "Step 67500: {'train_ae_loss': 0.68939, 'train_ucc_loss': 0.48631, 'train_ucc_acc': 0.84375, 'loss': 0.58785}\n",
            "Step 67520: {'train_ae_loss': 0.66106, 'train_ucc_loss': 0.46479, 'train_ucc_acc': 0.875, 'loss': 0.56292}\n",
            "Step 67540: {'train_ae_loss': 0.66202, 'train_ucc_loss': 0.46386, 'train_ucc_acc': 0.84375, 'loss': 0.56294}\n",
            "Step 67560: {'train_ae_loss': 0.66055, 'train_ucc_loss': 0.46032, 'train_ucc_acc': 0.84375, 'loss': 0.56044}\n",
            "Step 67580: {'train_ae_loss': 0.67216, 'train_ucc_loss': 0.58506, 'train_ucc_acc': 0.71875, 'loss': 0.62861}\n",
            "Step 67600: {'train_ae_loss': 0.67368, 'train_ucc_loss': 0.40812, 'train_ucc_acc': 0.875, 'loss': 0.5409}\n",
            "Step 67620: {'train_ae_loss': 0.66014, 'train_ucc_loss': 0.38585, 'train_ucc_acc': 0.9375, 'loss': 0.523}\n",
            "Step 67640: {'train_ae_loss': 0.6493, 'train_ucc_loss': 0.43394, 'train_ucc_acc': 0.875, 'loss': 0.54162}\n",
            "Step 67660: {'train_ae_loss': 0.65794, 'train_ucc_loss': 0.46836, 'train_ucc_acc': 0.84375, 'loss': 0.56315}\n",
            "Step 67680: {'train_ae_loss': 0.67803, 'train_ucc_loss': 0.59761, 'train_ucc_acc': 0.6875, 'loss': 0.63782}\n",
            "Step 67700: {'train_ae_loss': 0.66215, 'train_ucc_loss': 0.44395, 'train_ucc_acc': 0.84375, 'loss': 0.55305}\n",
            "Step 67720: {'train_ae_loss': 0.6565, 'train_ucc_loss': 0.47872, 'train_ucc_acc': 0.84375, 'loss': 0.56761}\n",
            "Step 67740: {'train_ae_loss': 0.69108, 'train_ucc_loss': 0.4502, 'train_ucc_acc': 0.84375, 'loss': 0.57064}\n",
            "Step 67760: {'train_ae_loss': 0.67925, 'train_ucc_loss': 0.35147, 'train_ucc_acc': 0.96875, 'loss': 0.51536}\n",
            "Step 67780: {'train_ae_loss': 0.66087, 'train_ucc_loss': 0.49839, 'train_ucc_acc': 0.8125, 'loss': 0.57963}\n",
            "Step 67800: {'train_ae_loss': 0.67379, 'train_ucc_loss': 0.39909, 'train_ucc_acc': 0.9375, 'loss': 0.53644}\n",
            "Step 67820: {'train_ae_loss': 0.67605, 'train_ucc_loss': 0.38737, 'train_ucc_acc': 0.9375, 'loss': 0.53171}\n",
            "Step 67840: {'train_ae_loss': 0.66126, 'train_ucc_loss': 0.43958, 'train_ucc_acc': 0.875, 'loss': 0.55042}\n",
            "Step 67860: {'train_ae_loss': 0.66836, 'train_ucc_loss': 0.51629, 'train_ucc_acc': 0.78125, 'loss': 0.59232}\n",
            "Step 67880: {'train_ae_loss': 0.68784, 'train_ucc_loss': 0.44815, 'train_ucc_acc': 0.8125, 'loss': 0.568}\n",
            "Step 67900: {'train_ae_loss': 0.66853, 'train_ucc_loss': 0.56734, 'train_ucc_acc': 0.75, 'loss': 0.61793}\n",
            "Step 67920: {'train_ae_loss': 0.67523, 'train_ucc_loss': 0.43533, 'train_ucc_acc': 0.84375, 'loss': 0.55528}\n",
            "Step 67940: {'train_ae_loss': 0.67359, 'train_ucc_loss': 0.40694, 'train_ucc_acc': 0.90625, 'loss': 0.54027}\n",
            "Step 67960: {'train_ae_loss': 0.65288, 'train_ucc_loss': 0.4423, 'train_ucc_acc': 0.875, 'loss': 0.54759}\n",
            "Step 67980: {'train_ae_loss': 0.65588, 'train_ucc_loss': 0.54774, 'train_ucc_acc': 0.75, 'loss': 0.60181}\n",
            "Step 68000: {'train_ae_loss': 0.65498, 'train_ucc_loss': 0.48432, 'train_ucc_acc': 0.8125, 'loss': 0.56965}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 17:16:00 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 68000,eval_ae_loss: 0.65732,eval_ucc_loss: 0.47339,eval_ucc_acc: 0.83789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 17:16:04 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 68020: {'train_ae_loss': 0.6634, 'train_ucc_loss': 0.45149, 'train_ucc_acc': 0.84375, 'loss': 0.55744}\n",
            "Step 68040: {'train_ae_loss': 0.66634, 'train_ucc_loss': 0.39876, 'train_ucc_acc': 0.90625, 'loss': 0.53255}\n",
            "Step 68060: {'train_ae_loss': 0.65485, 'train_ucc_loss': 0.40363, 'train_ucc_acc': 0.90625, 'loss': 0.52924}\n",
            "Step 68080: {'train_ae_loss': 0.66539, 'train_ucc_loss': 0.53398, 'train_ucc_acc': 0.78125, 'loss': 0.59968}\n",
            "Step 68100: {'train_ae_loss': 0.65838, 'train_ucc_loss': 0.38926, 'train_ucc_acc': 0.9375, 'loss': 0.52382}\n",
            "Step 68120: {'train_ae_loss': 0.68082, 'train_ucc_loss': 0.38577, 'train_ucc_acc': 0.9375, 'loss': 0.5333}\n",
            "Step 68140: {'train_ae_loss': 0.67123, 'train_ucc_loss': 0.4772, 'train_ucc_acc': 0.8125, 'loss': 0.57422}\n",
            "Step 68160: {'train_ae_loss': 0.65248, 'train_ucc_loss': 0.38759, 'train_ucc_acc': 0.9375, 'loss': 0.52003}\n",
            "Step 68180: {'train_ae_loss': 0.66873, 'train_ucc_loss': 0.57032, 'train_ucc_acc': 0.71875, 'loss': 0.61953}\n",
            "Step 68200: {'train_ae_loss': 0.66624, 'train_ucc_loss': 0.50426, 'train_ucc_acc': 0.8125, 'loss': 0.58525}\n",
            "Step 68220: {'train_ae_loss': 0.66901, 'train_ucc_loss': 0.3983, 'train_ucc_acc': 0.90625, 'loss': 0.53365}\n",
            "Step 68240: {'train_ae_loss': 0.66137, 'train_ucc_loss': 0.46319, 'train_ucc_acc': 0.84375, 'loss': 0.56228}\n",
            "Step 68260: {'train_ae_loss': 0.68442, 'train_ucc_loss': 0.47712, 'train_ucc_acc': 0.84375, 'loss': 0.58077}\n",
            "Step 68280: {'train_ae_loss': 0.67805, 'train_ucc_loss': 0.40547, 'train_ucc_acc': 0.90625, 'loss': 0.54176}\n",
            "Step 68300: {'train_ae_loss': 0.68697, 'train_ucc_loss': 0.45027, 'train_ucc_acc': 0.875, 'loss': 0.56862}\n",
            "Step 68320: {'train_ae_loss': 0.67029, 'train_ucc_loss': 0.44375, 'train_ucc_acc': 0.875, 'loss': 0.55702}\n",
            "Step 68340: {'train_ae_loss': 0.65835, 'train_ucc_loss': 0.51818, 'train_ucc_acc': 0.75, 'loss': 0.58827}\n",
            "Step 68360: {'train_ae_loss': 0.67933, 'train_ucc_loss': 0.56609, 'train_ucc_acc': 0.75, 'loss': 0.62271}\n",
            "Step 68380: {'train_ae_loss': 0.67113, 'train_ucc_loss': 0.54213, 'train_ucc_acc': 0.75, 'loss': 0.60663}\n",
            "Step 68400: {'train_ae_loss': 0.68897, 'train_ucc_loss': 0.58089, 'train_ucc_acc': 0.71875, 'loss': 0.63493}\n",
            "Step 68420: {'train_ae_loss': 0.66895, 'train_ucc_loss': 0.42053, 'train_ucc_acc': 0.875, 'loss': 0.54474}\n",
            "Step 68440: {'train_ae_loss': 0.66685, 'train_ucc_loss': 0.44379, 'train_ucc_acc': 0.875, 'loss': 0.55532}\n",
            "Step 68460: {'train_ae_loss': 0.67065, 'train_ucc_loss': 0.48498, 'train_ucc_acc': 0.8125, 'loss': 0.57782}\n",
            "Step 68480: {'train_ae_loss': 0.67567, 'train_ucc_loss': 0.45088, 'train_ucc_acc': 0.875, 'loss': 0.56328}\n",
            "Step 68500: {'train_ae_loss': 0.65751, 'train_ucc_loss': 0.43172, 'train_ucc_acc': 0.84375, 'loss': 0.54462}\n",
            "Step 68520: {'train_ae_loss': 0.66754, 'train_ucc_loss': 0.39906, 'train_ucc_acc': 0.9375, 'loss': 0.5333}\n",
            "Step 68540: {'train_ae_loss': 0.65093, 'train_ucc_loss': 0.47367, 'train_ucc_acc': 0.84375, 'loss': 0.5623}\n",
            "Step 68560: {'train_ae_loss': 0.67264, 'train_ucc_loss': 0.42558, 'train_ucc_acc': 0.875, 'loss': 0.54911}\n",
            "Step 68580: {'train_ae_loss': 0.66612, 'train_ucc_loss': 0.39068, 'train_ucc_acc': 0.9375, 'loss': 0.5284}\n",
            "Step 68600: {'train_ae_loss': 0.6639, 'train_ucc_loss': 0.53436, 'train_ucc_acc': 0.75, 'loss': 0.59913}\n",
            "Step 68620: {'train_ae_loss': 0.67234, 'train_ucc_loss': 0.53419, 'train_ucc_acc': 0.78125, 'loss': 0.60326}\n",
            "Step 68640: {'train_ae_loss': 0.65499, 'train_ucc_loss': 0.49638, 'train_ucc_acc': 0.8125, 'loss': 0.57568}\n",
            "Step 68660: {'train_ae_loss': 0.67028, 'train_ucc_loss': 0.34969, 'train_ucc_acc': 0.96875, 'loss': 0.50999}\n",
            "Step 68680: {'train_ae_loss': 0.64775, 'train_ucc_loss': 0.47013, 'train_ucc_acc': 0.84375, 'loss': 0.55894}\n",
            "Step 68700: {'train_ae_loss': 0.65465, 'train_ucc_loss': 0.55673, 'train_ucc_acc': 0.75, 'loss': 0.60569}\n",
            "Step 68720: {'train_ae_loss': 0.67923, 'train_ucc_loss': 0.46954, 'train_ucc_acc': 0.84375, 'loss': 0.57439}\n",
            "Step 68740: {'train_ae_loss': 0.65472, 'train_ucc_loss': 0.51064, 'train_ucc_acc': 0.78125, 'loss': 0.58268}\n",
            "Step 68760: {'train_ae_loss': 0.67994, 'train_ucc_loss': 0.32509, 'train_ucc_acc': 1.0, 'loss': 0.50252}\n",
            "Step 68780: {'train_ae_loss': 0.65004, 'train_ucc_loss': 0.41586, 'train_ucc_acc': 0.90625, 'loss': 0.53295}\n",
            "Step 68800: {'train_ae_loss': 0.67778, 'train_ucc_loss': 0.46999, 'train_ucc_acc': 0.84375, 'loss': 0.57388}\n",
            "Step 68820: {'train_ae_loss': 0.66578, 'train_ucc_loss': 0.50055, 'train_ucc_acc': 0.8125, 'loss': 0.58317}\n",
            "Step 68840: {'train_ae_loss': 0.66441, 'train_ucc_loss': 0.41605, 'train_ucc_acc': 0.90625, 'loss': 0.54023}\n",
            "Step 68860: {'train_ae_loss': 0.67249, 'train_ucc_loss': 0.52775, 'train_ucc_acc': 0.78125, 'loss': 0.60012}\n",
            "Step 68880: {'train_ae_loss': 0.66148, 'train_ucc_loss': 0.45288, 'train_ucc_acc': 0.84375, 'loss': 0.55718}\n",
            "Step 68900: {'train_ae_loss': 0.67252, 'train_ucc_loss': 0.43557, 'train_ucc_acc': 0.875, 'loss': 0.55405}\n",
            "Step 68920: {'train_ae_loss': 0.66265, 'train_ucc_loss': 0.46924, 'train_ucc_acc': 0.84375, 'loss': 0.56595}\n",
            "Step 68940: {'train_ae_loss': 0.65192, 'train_ucc_loss': 0.51298, 'train_ucc_acc': 0.78125, 'loss': 0.58245}\n",
            "Step 68960: {'train_ae_loss': 0.66094, 'train_ucc_loss': 0.43227, 'train_ucc_acc': 0.875, 'loss': 0.5466}\n",
            "Step 68980: {'train_ae_loss': 0.64493, 'train_ucc_loss': 0.48396, 'train_ucc_acc': 0.84375, 'loss': 0.56444}\n",
            "Step 69000: {'train_ae_loss': 0.6554, 'train_ucc_loss': 0.58881, 'train_ucc_acc': 0.6875, 'loss': 0.6221}\n",
            "step: 69000,eval_ae_loss: 0.65689,eval_ucc_loss: 0.49999,eval_ucc_acc: 0.80371\n",
            "Step 69020: {'train_ae_loss': 0.64537, 'train_ucc_loss': 0.4468, 'train_ucc_acc': 0.84375, 'loss': 0.54609}\n",
            "Step 69040: {'train_ae_loss': 0.68849, 'train_ucc_loss': 0.49766, 'train_ucc_acc': 0.8125, 'loss': 0.59308}\n",
            "Step 69060: {'train_ae_loss': 0.65529, 'train_ucc_loss': 0.41378, 'train_ucc_acc': 0.875, 'loss': 0.53454}\n",
            "Step 69080: {'train_ae_loss': 0.66407, 'train_ucc_loss': 0.39714, 'train_ucc_acc': 0.90625, 'loss': 0.53061}\n",
            "Step 69100: {'train_ae_loss': 0.65721, 'train_ucc_loss': 0.54984, 'train_ucc_acc': 0.75, 'loss': 0.60352}\n",
            "Step 69120: {'train_ae_loss': 0.6742, 'train_ucc_loss': 0.5409, 'train_ucc_acc': 0.75, 'loss': 0.60755}\n",
            "Step 69140: {'train_ae_loss': 0.65891, 'train_ucc_loss': 0.48539, 'train_ucc_acc': 0.84375, 'loss': 0.57215}\n",
            "Step 69160: {'train_ae_loss': 0.68944, 'train_ucc_loss': 0.42382, 'train_ucc_acc': 0.875, 'loss': 0.55663}\n",
            "Step 69180: {'train_ae_loss': 0.68742, 'train_ucc_loss': 0.47616, 'train_ucc_acc': 0.8125, 'loss': 0.58179}\n",
            "Step 69200: {'train_ae_loss': 0.6755, 'train_ucc_loss': 0.44156, 'train_ucc_acc': 0.84375, 'loss': 0.55853}\n",
            "Step 69220: {'train_ae_loss': 0.66642, 'train_ucc_loss': 0.39179, 'train_ucc_acc': 0.90625, 'loss': 0.52911}\n",
            "Step 69240: {'train_ae_loss': 0.66297, 'train_ucc_loss': 0.43734, 'train_ucc_acc': 0.875, 'loss': 0.55016}\n",
            "Step 69260: {'train_ae_loss': 0.6692, 'train_ucc_loss': 0.50396, 'train_ucc_acc': 0.78125, 'loss': 0.58658}\n",
            "Step 69280: {'train_ae_loss': 0.65234, 'train_ucc_loss': 0.46594, 'train_ucc_acc': 0.84375, 'loss': 0.55914}\n",
            "Step 69300: {'train_ae_loss': 0.65858, 'train_ucc_loss': 0.52193, 'train_ucc_acc': 0.78125, 'loss': 0.59026}\n",
            "Step 69320: {'train_ae_loss': 0.67778, 'train_ucc_loss': 0.37134, 'train_ucc_acc': 0.96875, 'loss': 0.52456}\n",
            "Step 69340: {'train_ae_loss': 0.67263, 'train_ucc_loss': 0.52996, 'train_ucc_acc': 0.78125, 'loss': 0.6013}\n",
            "Step 69360: {'train_ae_loss': 0.67473, 'train_ucc_loss': 0.38722, 'train_ucc_acc': 0.90625, 'loss': 0.53097}\n",
            "Step 69380: {'train_ae_loss': 0.66677, 'train_ucc_loss': 0.44439, 'train_ucc_acc': 0.84375, 'loss': 0.55558}\n",
            "Step 69400: {'train_ae_loss': 0.64769, 'train_ucc_loss': 0.54934, 'train_ucc_acc': 0.75, 'loss': 0.59851}\n",
            "Step 69420: {'train_ae_loss': 0.67742, 'train_ucc_loss': 0.42953, 'train_ucc_acc': 0.90625, 'loss': 0.55347}\n",
            "Step 69440: {'train_ae_loss': 0.64296, 'train_ucc_loss': 0.44034, 'train_ucc_acc': 0.90625, 'loss': 0.54165}\n",
            "Step 69460: {'train_ae_loss': 0.6477, 'train_ucc_loss': 0.4858, 'train_ucc_acc': 0.8125, 'loss': 0.56675}\n",
            "Step 69480: {'train_ae_loss': 0.67483, 'train_ucc_loss': 0.46559, 'train_ucc_acc': 0.875, 'loss': 0.57021}\n",
            "Step 69500: {'train_ae_loss': 0.65957, 'train_ucc_loss': 0.44346, 'train_ucc_acc': 0.875, 'loss': 0.55152}\n",
            "Step 69520: {'train_ae_loss': 0.65848, 'train_ucc_loss': 0.47055, 'train_ucc_acc': 0.84375, 'loss': 0.56452}\n",
            "Step 69540: {'train_ae_loss': 0.67506, 'train_ucc_loss': 0.42855, 'train_ucc_acc': 0.875, 'loss': 0.5518}\n",
            "Step 69560: {'train_ae_loss': 0.66396, 'train_ucc_loss': 0.40504, 'train_ucc_acc': 0.90625, 'loss': 0.5345}\n",
            "Step 69580: {'train_ae_loss': 0.67116, 'train_ucc_loss': 0.41692, 'train_ucc_acc': 0.90625, 'loss': 0.54404}\n",
            "Step 69600: {'train_ae_loss': 0.68365, 'train_ucc_loss': 0.33186, 'train_ucc_acc': 1.0, 'loss': 0.50775}\n",
            "Step 69620: {'train_ae_loss': 0.66314, 'train_ucc_loss': 0.46822, 'train_ucc_acc': 0.78125, 'loss': 0.56568}\n",
            "Step 69640: {'train_ae_loss': 0.67054, 'train_ucc_loss': 0.54794, 'train_ucc_acc': 0.75, 'loss': 0.60924}\n",
            "Step 69660: {'train_ae_loss': 0.6544, 'train_ucc_loss': 0.52324, 'train_ucc_acc': 0.78125, 'loss': 0.58882}\n",
            "Step 69680: {'train_ae_loss': 0.66306, 'train_ucc_loss': 0.46206, 'train_ucc_acc': 0.84375, 'loss': 0.56256}\n",
            "Step 69700: {'train_ae_loss': 0.68499, 'train_ucc_loss': 0.3697, 'train_ucc_acc': 0.96875, 'loss': 0.52734}\n",
            "Step 69720: {'train_ae_loss': 0.68743, 'train_ucc_loss': 0.41096, 'train_ucc_acc': 0.90625, 'loss': 0.5492}\n",
            "Step 69740: {'train_ae_loss': 0.66902, 'train_ucc_loss': 0.47149, 'train_ucc_acc': 0.84375, 'loss': 0.57026}\n",
            "Step 69760: {'train_ae_loss': 0.67144, 'train_ucc_loss': 0.37846, 'train_ucc_acc': 0.9375, 'loss': 0.52495}\n",
            "Step 69780: {'train_ae_loss': 0.66519, 'train_ucc_loss': 0.43626, 'train_ucc_acc': 0.875, 'loss': 0.55073}\n",
            "Step 69800: {'train_ae_loss': 0.66817, 'train_ucc_loss': 0.46149, 'train_ucc_acc': 0.84375, 'loss': 0.56483}\n",
            "Step 69820: {'train_ae_loss': 0.66291, 'train_ucc_loss': 0.3419, 'train_ucc_acc': 0.96875, 'loss': 0.50241}\n",
            "Step 69840: {'train_ae_loss': 0.66752, 'train_ucc_loss': 0.49637, 'train_ucc_acc': 0.8125, 'loss': 0.58194}\n",
            "Step 69860: {'train_ae_loss': 0.65751, 'train_ucc_loss': 0.50626, 'train_ucc_acc': 0.78125, 'loss': 0.58188}\n",
            "Step 69880: {'train_ae_loss': 0.66485, 'train_ucc_loss': 0.35476, 'train_ucc_acc': 0.9375, 'loss': 0.5098}\n",
            "Step 69900: {'train_ae_loss': 0.65454, 'train_ucc_loss': 0.57731, 'train_ucc_acc': 0.75, 'loss': 0.61593}\n",
            "Step 69920: {'train_ae_loss': 0.68136, 'train_ucc_loss': 0.51361, 'train_ucc_acc': 0.78125, 'loss': 0.59749}\n",
            "Step 69940: {'train_ae_loss': 0.6823, 'train_ucc_loss': 0.43878, 'train_ucc_acc': 0.875, 'loss': 0.56054}\n",
            "Step 69960: {'train_ae_loss': 0.66144, 'train_ucc_loss': 0.50809, 'train_ucc_acc': 0.78125, 'loss': 0.58476}\n",
            "Step 69980: {'train_ae_loss': 0.65565, 'train_ucc_loss': 0.44297, 'train_ucc_acc': 0.875, 'loss': 0.54931}\n",
            "Step 70000: {'train_ae_loss': 0.65821, 'train_ucc_loss': 0.58272, 'train_ucc_acc': 0.71875, 'loss': 0.62047}\n",
            "step: 70000,eval_ae_loss: 0.65623,eval_ucc_loss: 0.50641,eval_ucc_acc: 0.7959\n",
            "Step 70020: {'train_ae_loss': 0.67054, 'train_ucc_loss': 0.35376, 'train_ucc_acc': 0.9375, 'loss': 0.51215}\n",
            "Step 70040: {'train_ae_loss': 0.68575, 'train_ucc_loss': 0.4364, 'train_ucc_acc': 0.875, 'loss': 0.56107}\n",
            "Step 70060: {'train_ae_loss': 0.66778, 'train_ucc_loss': 0.39937, 'train_ucc_acc': 0.90625, 'loss': 0.53358}\n",
            "Step 70080: {'train_ae_loss': 0.6607, 'train_ucc_loss': 0.48237, 'train_ucc_acc': 0.84375, 'loss': 0.57153}\n",
            "Step 70100: {'train_ae_loss': 0.66238, 'train_ucc_loss': 0.52979, 'train_ucc_acc': 0.78125, 'loss': 0.59608}\n",
            "Step 70120: {'train_ae_loss': 0.66778, 'train_ucc_loss': 0.47422, 'train_ucc_acc': 0.8125, 'loss': 0.571}\n",
            "Step 70140: {'train_ae_loss': 0.67485, 'train_ucc_loss': 0.52107, 'train_ucc_acc': 0.78125, 'loss': 0.59796}\n",
            "Step 70160: {'train_ae_loss': 0.66595, 'train_ucc_loss': 0.47256, 'train_ucc_acc': 0.84375, 'loss': 0.56926}\n",
            "Step 70180: {'train_ae_loss': 0.6708, 'train_ucc_loss': 0.44296, 'train_ucc_acc': 0.875, 'loss': 0.55688}\n",
            "Step 70200: {'train_ae_loss': 0.67646, 'train_ucc_loss': 0.58752, 'train_ucc_acc': 0.75, 'loss': 0.63199}\n",
            "Step 70220: {'train_ae_loss': 0.66136, 'train_ucc_loss': 0.52924, 'train_ucc_acc': 0.78125, 'loss': 0.5953}\n",
            "Step 70240: {'train_ae_loss': 0.67517, 'train_ucc_loss': 0.54661, 'train_ucc_acc': 0.78125, 'loss': 0.61089}\n",
            "Step 70260: {'train_ae_loss': 0.68834, 'train_ucc_loss': 0.51957, 'train_ucc_acc': 0.78125, 'loss': 0.60396}\n",
            "Step 70280: {'train_ae_loss': 0.69366, 'train_ucc_loss': 0.42005, 'train_ucc_acc': 0.90625, 'loss': 0.55686}\n",
            "Step 70300: {'train_ae_loss': 0.65772, 'train_ucc_loss': 0.60287, 'train_ucc_acc': 0.71875, 'loss': 0.63029}\n",
            "Step 70320: {'train_ae_loss': 0.68752, 'train_ucc_loss': 0.47693, 'train_ucc_acc': 0.84375, 'loss': 0.58222}\n",
            "Step 70340: {'train_ae_loss': 0.67068, 'train_ucc_loss': 0.4499, 'train_ucc_acc': 0.875, 'loss': 0.56029}\n",
            "Step 70360: {'train_ae_loss': 0.66324, 'train_ucc_loss': 0.36549, 'train_ucc_acc': 0.96875, 'loss': 0.51437}\n",
            "Step 70380: {'train_ae_loss': 0.67993, 'train_ucc_loss': 0.40236, 'train_ucc_acc': 0.90625, 'loss': 0.54114}\n",
            "Step 70400: {'train_ae_loss': 0.66864, 'train_ucc_loss': 0.41956, 'train_ucc_acc': 0.875, 'loss': 0.5441}\n",
            "Step 70420: {'train_ae_loss': 0.67021, 'train_ucc_loss': 0.46473, 'train_ucc_acc': 0.84375, 'loss': 0.56747}\n",
            "Step 70440: {'train_ae_loss': 0.65183, 'train_ucc_loss': 0.45895, 'train_ucc_acc': 0.84375, 'loss': 0.55539}\n",
            "Step 70460: {'train_ae_loss': 0.66225, 'train_ucc_loss': 0.47537, 'train_ucc_acc': 0.8125, 'loss': 0.56881}\n",
            "Step 70480: {'train_ae_loss': 0.66192, 'train_ucc_loss': 0.44969, 'train_ucc_acc': 0.875, 'loss': 0.5558}\n",
            "Step 70500: {'train_ae_loss': 0.66463, 'train_ucc_loss': 0.44773, 'train_ucc_acc': 0.84375, 'loss': 0.55618}\n",
            "Step 70520: {'train_ae_loss': 0.66957, 'train_ucc_loss': 0.43498, 'train_ucc_acc': 0.875, 'loss': 0.55227}\n",
            "Step 70540: {'train_ae_loss': 0.67939, 'train_ucc_loss': 0.42213, 'train_ucc_acc': 0.90625, 'loss': 0.55076}\n",
            "Step 70560: {'train_ae_loss': 0.68591, 'train_ucc_loss': 0.422, 'train_ucc_acc': 0.875, 'loss': 0.55396}\n",
            "Step 70580: {'train_ae_loss': 0.64994, 'train_ucc_loss': 0.60045, 'train_ucc_acc': 0.6875, 'loss': 0.6252}\n",
            "Step 70600: {'train_ae_loss': 0.66181, 'train_ucc_loss': 0.47878, 'train_ucc_acc': 0.84375, 'loss': 0.5703}\n",
            "Step 70620: {'train_ae_loss': 0.65223, 'train_ucc_loss': 0.32736, 'train_ucc_acc': 1.0, 'loss': 0.4898}\n",
            "Step 70640: {'train_ae_loss': 0.66239, 'train_ucc_loss': 0.49142, 'train_ucc_acc': 0.8125, 'loss': 0.57691}\n",
            "Step 70660: {'train_ae_loss': 0.67424, 'train_ucc_loss': 0.35978, 'train_ucc_acc': 0.9375, 'loss': 0.51701}\n",
            "Step 70680: {'train_ae_loss': 0.64973, 'train_ucc_loss': 0.43333, 'train_ucc_acc': 0.875, 'loss': 0.54153}\n",
            "Step 70700: {'train_ae_loss': 0.68466, 'train_ucc_loss': 0.34469, 'train_ucc_acc': 1.0, 'loss': 0.51468}\n",
            "Step 70720: {'train_ae_loss': 0.65397, 'train_ucc_loss': 0.43242, 'train_ucc_acc': 0.875, 'loss': 0.5432}\n",
            "Step 70740: {'train_ae_loss': 0.67793, 'train_ucc_loss': 0.5104, 'train_ucc_acc': 0.78125, 'loss': 0.59417}\n",
            "Step 70760: {'train_ae_loss': 0.67632, 'train_ucc_loss': 0.39981, 'train_ucc_acc': 0.90625, 'loss': 0.53806}\n",
            "Step 70780: {'train_ae_loss': 0.67472, 'train_ucc_loss': 0.35229, 'train_ucc_acc': 0.96875, 'loss': 0.51351}\n",
            "Step 70800: {'train_ae_loss': 0.66709, 'train_ucc_loss': 0.4733, 'train_ucc_acc': 0.84375, 'loss': 0.5702}\n",
            "Step 70820: {'train_ae_loss': 0.68285, 'train_ucc_loss': 0.44935, 'train_ucc_acc': 0.875, 'loss': 0.5661}\n",
            "Step 70840: {'train_ae_loss': 0.66402, 'train_ucc_loss': 0.44635, 'train_ucc_acc': 0.84375, 'loss': 0.55519}\n",
            "Step 70860: {'train_ae_loss': 0.67327, 'train_ucc_loss': 0.47161, 'train_ucc_acc': 0.84375, 'loss': 0.57244}\n",
            "Step 70880: {'train_ae_loss': 0.65784, 'train_ucc_loss': 0.4403, 'train_ucc_acc': 0.875, 'loss': 0.54907}\n",
            "Step 70900: {'train_ae_loss': 0.66741, 'train_ucc_loss': 0.42361, 'train_ucc_acc': 0.875, 'loss': 0.54551}\n",
            "Step 70920: {'train_ae_loss': 0.66441, 'train_ucc_loss': 0.48487, 'train_ucc_acc': 0.8125, 'loss': 0.57464}\n",
            "Step 70940: {'train_ae_loss': 0.66016, 'train_ucc_loss': 0.4826, 'train_ucc_acc': 0.8125, 'loss': 0.57138}\n",
            "Step 70960: {'train_ae_loss': 0.67782, 'train_ucc_loss': 0.44866, 'train_ucc_acc': 0.875, 'loss': 0.56324}\n",
            "Step 70980: {'train_ae_loss': 0.6815, 'train_ucc_loss': 0.42174, 'train_ucc_acc': 0.875, 'loss': 0.55162}\n",
            "Step 71000: {'train_ae_loss': 0.69154, 'train_ucc_loss': 0.4084, 'train_ucc_acc': 0.90625, 'loss': 0.54997}\n",
            "step: 71000,eval_ae_loss: 0.65872,eval_ucc_loss: 0.51629,eval_ucc_acc: 0.78809\n",
            "Step 71020: {'train_ae_loss': 0.6548, 'train_ucc_loss': 0.44301, 'train_ucc_acc': 0.875, 'loss': 0.54891}\n",
            "Step 71040: {'train_ae_loss': 0.66928, 'train_ucc_loss': 0.51352, 'train_ucc_acc': 0.8125, 'loss': 0.5914}\n",
            "Step 71060: {'train_ae_loss': 0.68408, 'train_ucc_loss': 0.49461, 'train_ucc_acc': 0.8125, 'loss': 0.58935}\n",
            "Step 71080: {'train_ae_loss': 0.65523, 'train_ucc_loss': 0.40269, 'train_ucc_acc': 0.90625, 'loss': 0.52896}\n",
            "Step 71100: {'train_ae_loss': 0.65369, 'train_ucc_loss': 0.45385, 'train_ucc_acc': 0.84375, 'loss': 0.55377}\n",
            "Step 71120: {'train_ae_loss': 0.65685, 'train_ucc_loss': 0.45274, 'train_ucc_acc': 0.84375, 'loss': 0.55479}\n",
            "Step 71140: {'train_ae_loss': 0.66806, 'train_ucc_loss': 0.46657, 'train_ucc_acc': 0.8125, 'loss': 0.56731}\n",
            "Step 71160: {'train_ae_loss': 0.6685, 'train_ucc_loss': 0.53765, 'train_ucc_acc': 0.78125, 'loss': 0.60308}\n",
            "Step 71180: {'train_ae_loss': 0.65487, 'train_ucc_loss': 0.45326, 'train_ucc_acc': 0.84375, 'loss': 0.55407}\n",
            "Step 71200: {'train_ae_loss': 0.67074, 'train_ucc_loss': 0.3965, 'train_ucc_acc': 0.90625, 'loss': 0.53362}\n",
            "Step 71220: {'train_ae_loss': 0.66404, 'train_ucc_loss': 0.50854, 'train_ucc_acc': 0.78125, 'loss': 0.58629}\n",
            "Step 71240: {'train_ae_loss': 0.671, 'train_ucc_loss': 0.43328, 'train_ucc_acc': 0.90625, 'loss': 0.55214}\n",
            "Step 71260: {'train_ae_loss': 0.66398, 'train_ucc_loss': 0.3379, 'train_ucc_acc': 0.96875, 'loss': 0.50094}\n",
            "Step 71280: {'train_ae_loss': 0.67326, 'train_ucc_loss': 0.55695, 'train_ucc_acc': 0.75, 'loss': 0.61511}\n",
            "Step 71300: {'train_ae_loss': 0.66383, 'train_ucc_loss': 0.4372, 'train_ucc_acc': 0.875, 'loss': 0.55051}\n",
            "Step 71320: {'train_ae_loss': 0.65386, 'train_ucc_loss': 0.69423, 'train_ucc_acc': 0.59375, 'loss': 0.67405}\n",
            "Step 71340: {'train_ae_loss': 0.6693, 'train_ucc_loss': 0.37594, 'train_ucc_acc': 0.9375, 'loss': 0.52262}\n",
            "Step 71360: {'train_ae_loss': 0.65391, 'train_ucc_loss': 0.45147, 'train_ucc_acc': 0.84375, 'loss': 0.55269}\n",
            "Step 71380: {'train_ae_loss': 0.65793, 'train_ucc_loss': 0.4013, 'train_ucc_acc': 0.90625, 'loss': 0.52961}\n",
            "Step 71400: {'train_ae_loss': 0.65706, 'train_ucc_loss': 0.45729, 'train_ucc_acc': 0.84375, 'loss': 0.55718}\n",
            "Step 71420: {'train_ae_loss': 0.65729, 'train_ucc_loss': 0.35916, 'train_ucc_acc': 0.96875, 'loss': 0.50822}\n",
            "Step 71440: {'train_ae_loss': 0.67046, 'train_ucc_loss': 0.50502, 'train_ucc_acc': 0.8125, 'loss': 0.58774}\n",
            "Step 71460: {'train_ae_loss': 0.67598, 'train_ucc_loss': 0.45112, 'train_ucc_acc': 0.84375, 'loss': 0.56355}\n",
            "Step 71480: {'train_ae_loss': 0.6616, 'train_ucc_loss': 0.43622, 'train_ucc_acc': 0.875, 'loss': 0.54891}\n",
            "Step 71500: {'train_ae_loss': 0.67691, 'train_ucc_loss': 0.46281, 'train_ucc_acc': 0.8125, 'loss': 0.56986}\n",
            "Step 71520: {'train_ae_loss': 0.68553, 'train_ucc_loss': 0.35596, 'train_ucc_acc': 0.96875, 'loss': 0.52074}\n",
            "Step 71540: {'train_ae_loss': 0.67881, 'train_ucc_loss': 0.48883, 'train_ucc_acc': 0.8125, 'loss': 0.58382}\n",
            "Step 71560: {'train_ae_loss': 0.66574, 'train_ucc_loss': 0.4057, 'train_ucc_acc': 0.9375, 'loss': 0.53572}\n",
            "Step 71580: {'train_ae_loss': 0.65956, 'train_ucc_loss': 0.44928, 'train_ucc_acc': 0.84375, 'loss': 0.55442}\n",
            "Step 71600: {'train_ae_loss': 0.65958, 'train_ucc_loss': 0.43639, 'train_ucc_acc': 0.875, 'loss': 0.54799}\n",
            "Step 71620: {'train_ae_loss': 0.66843, 'train_ucc_loss': 0.41036, 'train_ucc_acc': 0.90625, 'loss': 0.5394}\n",
            "Step 71640: {'train_ae_loss': 0.66773, 'train_ucc_loss': 0.49626, 'train_ucc_acc': 0.78125, 'loss': 0.58199}\n",
            "Step 71660: {'train_ae_loss': 0.67585, 'train_ucc_loss': 0.45472, 'train_ucc_acc': 0.84375, 'loss': 0.56529}\n",
            "Step 71680: {'train_ae_loss': 0.67623, 'train_ucc_loss': 0.53743, 'train_ucc_acc': 0.75, 'loss': 0.60683}\n",
            "Step 71700: {'train_ae_loss': 0.66117, 'train_ucc_loss': 0.36107, 'train_ucc_acc': 0.96875, 'loss': 0.51112}\n",
            "Step 71720: {'train_ae_loss': 0.66394, 'train_ucc_loss': 0.47543, 'train_ucc_acc': 0.84375, 'loss': 0.56968}\n",
            "Step 71740: {'train_ae_loss': 0.67732, 'train_ucc_loss': 0.46965, 'train_ucc_acc': 0.84375, 'loss': 0.57349}\n",
            "Step 71760: {'train_ae_loss': 0.6676, 'train_ucc_loss': 0.4031, 'train_ucc_acc': 0.90625, 'loss': 0.53535}\n",
            "Step 71780: {'train_ae_loss': 0.67364, 'train_ucc_loss': 0.40548, 'train_ucc_acc': 0.90625, 'loss': 0.53956}\n",
            "Step 71800: {'train_ae_loss': 0.68018, 'train_ucc_loss': 0.4939, 'train_ucc_acc': 0.8125, 'loss': 0.58704}\n",
            "Step 71820: {'train_ae_loss': 0.66214, 'train_ucc_loss': 0.53692, 'train_ucc_acc': 0.75, 'loss': 0.59953}\n",
            "Step 71840: {'train_ae_loss': 0.65158, 'train_ucc_loss': 0.51304, 'train_ucc_acc': 0.78125, 'loss': 0.58231}\n",
            "Step 71860: {'train_ae_loss': 0.66574, 'train_ucc_loss': 0.44551, 'train_ucc_acc': 0.84375, 'loss': 0.55563}\n",
            "Step 71880: {'train_ae_loss': 0.6549, 'train_ucc_loss': 0.37566, 'train_ucc_acc': 0.9375, 'loss': 0.51528}\n",
            "Step 71900: {'train_ae_loss': 0.65313, 'train_ucc_loss': 0.42958, 'train_ucc_acc': 0.875, 'loss': 0.54135}\n",
            "Step 71920: {'train_ae_loss': 0.67048, 'train_ucc_loss': 0.4878, 'train_ucc_acc': 0.8125, 'loss': 0.57914}\n",
            "Step 71940: {'train_ae_loss': 0.67158, 'train_ucc_loss': 0.40635, 'train_ucc_acc': 0.90625, 'loss': 0.53896}\n",
            "Step 71960: {'train_ae_loss': 0.67015, 'train_ucc_loss': 0.43562, 'train_ucc_acc': 0.875, 'loss': 0.55289}\n",
            "Step 71980: {'train_ae_loss': 0.66325, 'train_ucc_loss': 0.44119, 'train_ucc_acc': 0.875, 'loss': 0.55222}\n",
            "Step 72000: {'train_ae_loss': 0.68134, 'train_ucc_loss': 0.41783, 'train_ucc_acc': 0.875, 'loss': 0.54959}\n",
            "step: 72000,eval_ae_loss: 0.65758,eval_ucc_loss: 0.47893,eval_ucc_acc: 0.83203\n",
            "Step 72020: {'train_ae_loss': 0.64185, 'train_ucc_loss': 0.39922, 'train_ucc_acc': 0.9375, 'loss': 0.52053}\n",
            "Step 72040: {'train_ae_loss': 0.65898, 'train_ucc_loss': 0.52562, 'train_ucc_acc': 0.78125, 'loss': 0.5923}\n",
            "Step 72060: {'train_ae_loss': 0.6631, 'train_ucc_loss': 0.42953, 'train_ucc_acc': 0.90625, 'loss': 0.54632}\n",
            "Step 72080: {'train_ae_loss': 0.67107, 'train_ucc_loss': 0.39662, 'train_ucc_acc': 0.90625, 'loss': 0.53385}\n",
            "Step 72100: {'train_ae_loss': 0.66374, 'train_ucc_loss': 0.42418, 'train_ucc_acc': 0.90625, 'loss': 0.54396}\n",
            "Step 72120: {'train_ae_loss': 0.66793, 'train_ucc_loss': 0.42692, 'train_ucc_acc': 0.90625, 'loss': 0.54743}\n",
            "Step 72140: {'train_ae_loss': 0.67689, 'train_ucc_loss': 0.40636, 'train_ucc_acc': 0.90625, 'loss': 0.54163}\n",
            "Step 72160: {'train_ae_loss': 0.67381, 'train_ucc_loss': 0.43478, 'train_ucc_acc': 0.875, 'loss': 0.55429}\n",
            "Step 72180: {'train_ae_loss': 0.67117, 'train_ucc_loss': 0.33384, 'train_ucc_acc': 1.0, 'loss': 0.50251}\n",
            "Step 72200: {'train_ae_loss': 0.66209, 'train_ucc_loss': 0.54231, 'train_ucc_acc': 0.71875, 'loss': 0.6022}\n",
            "Step 72220: {'train_ae_loss': 0.6592, 'train_ucc_loss': 0.40304, 'train_ucc_acc': 0.90625, 'loss': 0.53112}\n",
            "Step 72240: {'train_ae_loss': 0.66326, 'train_ucc_loss': 0.44493, 'train_ucc_acc': 0.84375, 'loss': 0.5541}\n",
            "Step 72260: {'train_ae_loss': 0.65516, 'train_ucc_loss': 0.43076, 'train_ucc_acc': 0.875, 'loss': 0.54296}\n",
            "Step 72280: {'train_ae_loss': 0.66335, 'train_ucc_loss': 0.5181, 'train_ucc_acc': 0.78125, 'loss': 0.59072}\n",
            "Step 72300: {'train_ae_loss': 0.6631, 'train_ucc_loss': 0.41065, 'train_ucc_acc': 0.90625, 'loss': 0.53687}\n",
            "Step 72320: {'train_ae_loss': 0.66063, 'train_ucc_loss': 0.50934, 'train_ucc_acc': 0.8125, 'loss': 0.58499}\n",
            "Step 72340: {'train_ae_loss': 0.66327, 'train_ucc_loss': 0.53214, 'train_ucc_acc': 0.78125, 'loss': 0.5977}\n",
            "Step 72360: {'train_ae_loss': 0.65133, 'train_ucc_loss': 0.43705, 'train_ucc_acc': 0.90625, 'loss': 0.54419}\n",
            "Step 72380: {'train_ae_loss': 0.67731, 'train_ucc_loss': 0.43188, 'train_ucc_acc': 0.84375, 'loss': 0.55459}\n",
            "Step 72400: {'train_ae_loss': 0.66645, 'train_ucc_loss': 0.41743, 'train_ucc_acc': 0.90625, 'loss': 0.54194}\n",
            "Step 72420: {'train_ae_loss': 0.66279, 'train_ucc_loss': 0.39842, 'train_ucc_acc': 0.90625, 'loss': 0.5306}\n",
            "Step 72440: {'train_ae_loss': 0.68121, 'train_ucc_loss': 0.46039, 'train_ucc_acc': 0.84375, 'loss': 0.5708}\n",
            "Step 72460: {'train_ae_loss': 0.67004, 'train_ucc_loss': 0.46017, 'train_ucc_acc': 0.84375, 'loss': 0.5651}\n",
            "Step 72480: {'train_ae_loss': 0.66687, 'train_ucc_loss': 0.47636, 'train_ucc_acc': 0.8125, 'loss': 0.57162}\n",
            "Step 72500: {'train_ae_loss': 0.66839, 'train_ucc_loss': 0.39181, 'train_ucc_acc': 0.9375, 'loss': 0.5301}\n",
            "Step 72520: {'train_ae_loss': 0.6605, 'train_ucc_loss': 0.44202, 'train_ucc_acc': 0.875, 'loss': 0.55126}\n",
            "Step 72540: {'train_ae_loss': 0.67205, 'train_ucc_loss': 0.48263, 'train_ucc_acc': 0.8125, 'loss': 0.57734}\n",
            "Step 72560: {'train_ae_loss': 0.65151, 'train_ucc_loss': 0.4761, 'train_ucc_acc': 0.8125, 'loss': 0.5638}\n",
            "Step 72580: {'train_ae_loss': 0.68377, 'train_ucc_loss': 0.44138, 'train_ucc_acc': 0.875, 'loss': 0.56257}\n",
            "Step 72600: {'train_ae_loss': 0.67922, 'train_ucc_loss': 0.51793, 'train_ucc_acc': 0.75, 'loss': 0.59858}\n",
            "Step 72620: {'train_ae_loss': 0.66769, 'train_ucc_loss': 0.42503, 'train_ucc_acc': 0.90625, 'loss': 0.54636}\n",
            "Step 72640: {'train_ae_loss': 0.67933, 'train_ucc_loss': 0.54934, 'train_ucc_acc': 0.75, 'loss': 0.61433}\n",
            "Step 72660: {'train_ae_loss': 0.67879, 'train_ucc_loss': 0.39524, 'train_ucc_acc': 0.90625, 'loss': 0.53702}\n",
            "Step 72680: {'train_ae_loss': 0.66402, 'train_ucc_loss': 0.69448, 'train_ucc_acc': 0.625, 'loss': 0.67925}\n",
            "Step 72700: {'train_ae_loss': 0.66112, 'train_ucc_loss': 0.40897, 'train_ucc_acc': 0.90625, 'loss': 0.53505}\n",
            "Step 72720: {'train_ae_loss': 0.68117, 'train_ucc_loss': 0.43132, 'train_ucc_acc': 0.84375, 'loss': 0.55625}\n",
            "Step 72740: {'train_ae_loss': 0.66326, 'train_ucc_loss': 0.57347, 'train_ucc_acc': 0.71875, 'loss': 0.61837}\n",
            "Step 72760: {'train_ae_loss': 0.6586, 'train_ucc_loss': 0.4674, 'train_ucc_acc': 0.8125, 'loss': 0.563}\n",
            "Step 72780: {'train_ae_loss': 0.69262, 'train_ucc_loss': 0.40353, 'train_ucc_acc': 0.90625, 'loss': 0.54808}\n",
            "Step 72800: {'train_ae_loss': 0.66165, 'train_ucc_loss': 0.43828, 'train_ucc_acc': 0.875, 'loss': 0.54997}\n",
            "Step 72820: {'train_ae_loss': 0.65793, 'train_ucc_loss': 0.47422, 'train_ucc_acc': 0.8125, 'loss': 0.56608}\n",
            "Step 72840: {'train_ae_loss': 0.67925, 'train_ucc_loss': 0.50187, 'train_ucc_acc': 0.8125, 'loss': 0.59056}\n",
            "Step 72860: {'train_ae_loss': 0.67236, 'train_ucc_loss': 0.41331, 'train_ucc_acc': 0.875, 'loss': 0.54284}\n",
            "Step 72880: {'train_ae_loss': 0.67928, 'train_ucc_loss': 0.55507, 'train_ucc_acc': 0.75, 'loss': 0.61717}\n",
            "Step 72900: {'train_ae_loss': 0.6676, 'train_ucc_loss': 0.47105, 'train_ucc_acc': 0.84375, 'loss': 0.56933}\n",
            "Step 72920: {'train_ae_loss': 0.67141, 'train_ucc_loss': 0.45777, 'train_ucc_acc': 0.8125, 'loss': 0.56459}\n",
            "Step 72940: {'train_ae_loss': 0.67401, 'train_ucc_loss': 0.56295, 'train_ucc_acc': 0.75, 'loss': 0.61848}\n",
            "Step 72960: {'train_ae_loss': 0.66703, 'train_ucc_loss': 0.4286, 'train_ucc_acc': 0.875, 'loss': 0.54782}\n",
            "Step 72980: {'train_ae_loss': 0.67622, 'train_ucc_loss': 0.4801, 'train_ucc_acc': 0.84375, 'loss': 0.57816}\n",
            "Step 73000: {'train_ae_loss': 0.67777, 'train_ucc_loss': 0.48653, 'train_ucc_acc': 0.8125, 'loss': 0.58215}\n",
            "step: 73000,eval_ae_loss: 0.65474,eval_ucc_loss: 0.49594,eval_ucc_acc: 0.81836\n",
            "Step 73020: {'train_ae_loss': 0.67376, 'train_ucc_loss': 0.41951, 'train_ucc_acc': 0.875, 'loss': 0.54663}\n",
            "Step 73040: {'train_ae_loss': 0.68299, 'train_ucc_loss': 0.45921, 'train_ucc_acc': 0.84375, 'loss': 0.5711}\n",
            "Step 73060: {'train_ae_loss': 0.66959, 'train_ucc_loss': 0.39331, 'train_ucc_acc': 0.9375, 'loss': 0.53145}\n",
            "Step 73080: {'train_ae_loss': 0.67156, 'train_ucc_loss': 0.49804, 'train_ucc_acc': 0.78125, 'loss': 0.5848}\n",
            "Step 73100: {'train_ae_loss': 0.67668, 'train_ucc_loss': 0.52587, 'train_ucc_acc': 0.8125, 'loss': 0.60127}\n",
            "Step 73120: {'train_ae_loss': 0.67743, 'train_ucc_loss': 0.46318, 'train_ucc_acc': 0.84375, 'loss': 0.5703}\n",
            "Step 73140: {'train_ae_loss': 0.6621, 'train_ucc_loss': 0.53584, 'train_ucc_acc': 0.75, 'loss': 0.59897}\n",
            "Step 73160: {'train_ae_loss': 0.68382, 'train_ucc_loss': 0.45622, 'train_ucc_acc': 0.875, 'loss': 0.57002}\n",
            "Step 73180: {'train_ae_loss': 0.66301, 'train_ucc_loss': 0.48851, 'train_ucc_acc': 0.8125, 'loss': 0.57576}\n",
            "Step 73200: {'train_ae_loss': 0.66117, 'train_ucc_loss': 0.39923, 'train_ucc_acc': 0.90625, 'loss': 0.5302}\n",
            "Step 73220: {'train_ae_loss': 0.67039, 'train_ucc_loss': 0.43968, 'train_ucc_acc': 0.875, 'loss': 0.55503}\n",
            "Step 73240: {'train_ae_loss': 0.67771, 'train_ucc_loss': 0.60023, 'train_ucc_acc': 0.6875, 'loss': 0.63897}\n",
            "Step 73260: {'train_ae_loss': 0.66979, 'train_ucc_loss': 0.47699, 'train_ucc_acc': 0.84375, 'loss': 0.57339}\n",
            "Step 73280: {'train_ae_loss': 0.66545, 'train_ucc_loss': 0.35791, 'train_ucc_acc': 0.96875, 'loss': 0.51168}\n",
            "Step 73300: {'train_ae_loss': 0.67461, 'train_ucc_loss': 0.45584, 'train_ucc_acc': 0.875, 'loss': 0.56523}\n",
            "Step 73320: {'train_ae_loss': 0.65277, 'train_ucc_loss': 0.43155, 'train_ucc_acc': 0.875, 'loss': 0.54216}\n",
            "Step 73340: {'train_ae_loss': 0.6677, 'train_ucc_loss': 0.44289, 'train_ucc_acc': 0.875, 'loss': 0.55529}\n",
            "Step 73360: {'train_ae_loss': 0.68587, 'train_ucc_loss': 0.42511, 'train_ucc_acc': 0.875, 'loss': 0.55549}\n",
            "Step 73380: {'train_ae_loss': 0.66154, 'train_ucc_loss': 0.54581, 'train_ucc_acc': 0.78125, 'loss': 0.60367}\n",
            "Step 73400: {'train_ae_loss': 0.66968, 'train_ucc_loss': 0.45082, 'train_ucc_acc': 0.875, 'loss': 0.56025}\n",
            "Step 73420: {'train_ae_loss': 0.67872, 'train_ucc_loss': 0.47893, 'train_ucc_acc': 0.84375, 'loss': 0.57883}\n",
            "Step 73440: {'train_ae_loss': 0.67638, 'train_ucc_loss': 0.53828, 'train_ucc_acc': 0.78125, 'loss': 0.60733}\n",
            "Step 73460: {'train_ae_loss': 0.67617, 'train_ucc_loss': 0.41053, 'train_ucc_acc': 0.875, 'loss': 0.54335}\n",
            "Step 73480: {'train_ae_loss': 0.67708, 'train_ucc_loss': 0.37682, 'train_ucc_acc': 0.9375, 'loss': 0.52695}\n",
            "Step 73500: {'train_ae_loss': 0.67093, 'train_ucc_loss': 0.48257, 'train_ucc_acc': 0.8125, 'loss': 0.57675}\n",
            "Step 73520: {'train_ae_loss': 0.67541, 'train_ucc_loss': 0.44253, 'train_ucc_acc': 0.875, 'loss': 0.55897}\n",
            "Step 73540: {'train_ae_loss': 0.65902, 'train_ucc_loss': 0.38193, 'train_ucc_acc': 0.9375, 'loss': 0.52047}\n",
            "Step 73560: {'train_ae_loss': 0.67406, 'train_ucc_loss': 0.49251, 'train_ucc_acc': 0.8125, 'loss': 0.58329}\n",
            "Step 73580: {'train_ae_loss': 0.68161, 'train_ucc_loss': 0.52569, 'train_ucc_acc': 0.78125, 'loss': 0.60365}\n",
            "Step 73600: {'train_ae_loss': 0.67433, 'train_ucc_loss': 0.49399, 'train_ucc_acc': 0.8125, 'loss': 0.58416}\n",
            "Step 73620: {'train_ae_loss': 0.69024, 'train_ucc_loss': 0.57228, 'train_ucc_acc': 0.71875, 'loss': 0.63126}\n",
            "Step 73640: {'train_ae_loss': 0.66598, 'train_ucc_loss': 0.39108, 'train_ucc_acc': 0.9375, 'loss': 0.52853}\n",
            "Step 73660: {'train_ae_loss': 0.67961, 'train_ucc_loss': 0.50659, 'train_ucc_acc': 0.8125, 'loss': 0.5931}\n",
            "Step 73680: {'train_ae_loss': 0.66237, 'train_ucc_loss': 0.44726, 'train_ucc_acc': 0.8125, 'loss': 0.55482}\n",
            "Step 73700: {'train_ae_loss': 0.64971, 'train_ucc_loss': 0.46751, 'train_ucc_acc': 0.84375, 'loss': 0.55861}\n",
            "Step 73720: {'train_ae_loss': 0.64561, 'train_ucc_loss': 0.46474, 'train_ucc_acc': 0.84375, 'loss': 0.55518}\n",
            "Step 73740: {'train_ae_loss': 0.67071, 'train_ucc_loss': 0.43966, 'train_ucc_acc': 0.875, 'loss': 0.55519}\n",
            "Step 73760: {'train_ae_loss': 0.67092, 'train_ucc_loss': 0.47006, 'train_ucc_acc': 0.8125, 'loss': 0.57049}\n",
            "Step 73780: {'train_ae_loss': 0.66414, 'train_ucc_loss': 0.42463, 'train_ucc_acc': 0.90625, 'loss': 0.54438}\n",
            "Step 73800: {'train_ae_loss': 0.64976, 'train_ucc_loss': 0.49146, 'train_ucc_acc': 0.8125, 'loss': 0.57061}\n",
            "Step 73820: {'train_ae_loss': 0.67832, 'train_ucc_loss': 0.42433, 'train_ucc_acc': 0.90625, 'loss': 0.55133}\n",
            "Step 73840: {'train_ae_loss': 0.66927, 'train_ucc_loss': 0.4977, 'train_ucc_acc': 0.8125, 'loss': 0.58348}\n",
            "Step 73860: {'train_ae_loss': 0.66644, 'train_ucc_loss': 0.48742, 'train_ucc_acc': 0.8125, 'loss': 0.57693}\n",
            "Step 73880: {'train_ae_loss': 0.66496, 'train_ucc_loss': 0.45982, 'train_ucc_acc': 0.84375, 'loss': 0.56239}\n",
            "Step 73900: {'train_ae_loss': 0.64534, 'train_ucc_loss': 0.56591, 'train_ucc_acc': 0.75, 'loss': 0.60562}\n",
            "Step 73920: {'train_ae_loss': 0.67432, 'train_ucc_loss': 0.44464, 'train_ucc_acc': 0.84375, 'loss': 0.55948}\n",
            "Step 73940: {'train_ae_loss': 0.66508, 'train_ucc_loss': 0.50468, 'train_ucc_acc': 0.78125, 'loss': 0.58488}\n",
            "Step 73960: {'train_ae_loss': 0.64812, 'train_ucc_loss': 0.47302, 'train_ucc_acc': 0.875, 'loss': 0.56057}\n",
            "Step 73980: {'train_ae_loss': 0.66082, 'train_ucc_loss': 0.51942, 'train_ucc_acc': 0.78125, 'loss': 0.59012}\n",
            "Step 74000: {'train_ae_loss': 0.68266, 'train_ucc_loss': 0.43537, 'train_ucc_acc': 0.875, 'loss': 0.55901}\n",
            "step: 74000,eval_ae_loss: 0.65366,eval_ucc_loss: 0.47438,eval_ucc_acc: 0.8291\n",
            "Step 74020: {'train_ae_loss': 0.66953, 'train_ucc_loss': 0.50862, 'train_ucc_acc': 0.8125, 'loss': 0.58907}\n",
            "Step 74040: {'train_ae_loss': 0.6553, 'train_ucc_loss': 0.45026, 'train_ucc_acc': 0.875, 'loss': 0.55278}\n",
            "Step 74060: {'train_ae_loss': 0.69153, 'train_ucc_loss': 0.45079, 'train_ucc_acc': 0.84375, 'loss': 0.57116}\n",
            "Step 74080: {'train_ae_loss': 0.65779, 'train_ucc_loss': 0.42354, 'train_ucc_acc': 0.875, 'loss': 0.54066}\n",
            "Step 74100: {'train_ae_loss': 0.66937, 'train_ucc_loss': 0.50947, 'train_ucc_acc': 0.78125, 'loss': 0.58942}\n",
            "Step 74120: {'train_ae_loss': 0.6535, 'train_ucc_loss': 0.46372, 'train_ucc_acc': 0.84375, 'loss': 0.55861}\n",
            "Step 74140: {'train_ae_loss': 0.66823, 'train_ucc_loss': 0.44609, 'train_ucc_acc': 0.875, 'loss': 0.55716}\n",
            "Step 74160: {'train_ae_loss': 0.6816, 'train_ucc_loss': 0.35849, 'train_ucc_acc': 0.96875, 'loss': 0.52005}\n",
            "Step 74180: {'train_ae_loss': 0.6795, 'train_ucc_loss': 0.47153, 'train_ucc_acc': 0.84375, 'loss': 0.57552}\n",
            "Step 74200: {'train_ae_loss': 0.65979, 'train_ucc_loss': 0.39244, 'train_ucc_acc': 0.90625, 'loss': 0.52611}\n",
            "Step 74220: {'train_ae_loss': 0.66299, 'train_ucc_loss': 0.45906, 'train_ucc_acc': 0.875, 'loss': 0.56103}\n",
            "Step 74240: {'train_ae_loss': 0.65548, 'train_ucc_loss': 0.45835, 'train_ucc_acc': 0.875, 'loss': 0.55692}\n",
            "Step 74260: {'train_ae_loss': 0.66248, 'train_ucc_loss': 0.56113, 'train_ucc_acc': 0.75, 'loss': 0.6118}\n",
            "Step 74280: {'train_ae_loss': 0.66446, 'train_ucc_loss': 0.46011, 'train_ucc_acc': 0.84375, 'loss': 0.56229}\n",
            "Step 74300: {'train_ae_loss': 0.67467, 'train_ucc_loss': 0.49285, 'train_ucc_acc': 0.8125, 'loss': 0.58376}\n",
            "Step 74320: {'train_ae_loss': 0.66218, 'train_ucc_loss': 0.49089, 'train_ucc_acc': 0.78125, 'loss': 0.57653}\n",
            "Step 74340: {'train_ae_loss': 0.66191, 'train_ucc_loss': 0.49198, 'train_ucc_acc': 0.8125, 'loss': 0.57695}\n",
            "Step 74360: {'train_ae_loss': 0.68397, 'train_ucc_loss': 0.55316, 'train_ucc_acc': 0.75, 'loss': 0.61856}\n",
            "Step 74380: {'train_ae_loss': 0.66853, 'train_ucc_loss': 0.38449, 'train_ucc_acc': 0.9375, 'loss': 0.52651}\n",
            "Step 74400: {'train_ae_loss': 0.67202, 'train_ucc_loss': 0.37131, 'train_ucc_acc': 0.9375, 'loss': 0.52167}\n",
            "Step 74420: {'train_ae_loss': 0.70234, 'train_ucc_loss': 0.35374, 'train_ucc_acc': 0.96875, 'loss': 0.52804}\n",
            "Step 74440: {'train_ae_loss': 0.66809, 'train_ucc_loss': 0.50157, 'train_ucc_acc': 0.8125, 'loss': 0.58483}\n",
            "Step 74460: {'train_ae_loss': 0.6639, 'train_ucc_loss': 0.39001, 'train_ucc_acc': 0.9375, 'loss': 0.52695}\n",
            "Step 74480: {'train_ae_loss': 0.6577, 'train_ucc_loss': 0.37547, 'train_ucc_acc': 0.9375, 'loss': 0.51659}\n",
            "Step 74500: {'train_ae_loss': 0.65547, 'train_ucc_loss': 0.53056, 'train_ucc_acc': 0.78125, 'loss': 0.59302}\n",
            "Step 74520: {'train_ae_loss': 0.67055, 'train_ucc_loss': 0.46328, 'train_ucc_acc': 0.84375, 'loss': 0.56692}\n",
            "Step 74540: {'train_ae_loss': 0.67747, 'train_ucc_loss': 0.44667, 'train_ucc_acc': 0.875, 'loss': 0.56207}\n",
            "Step 74560: {'train_ae_loss': 0.66841, 'train_ucc_loss': 0.43635, 'train_ucc_acc': 0.875, 'loss': 0.55238}\n",
            "Step 74580: {'train_ae_loss': 0.64596, 'train_ucc_loss': 0.47738, 'train_ucc_acc': 0.8125, 'loss': 0.56167}\n",
            "Step 74600: {'train_ae_loss': 0.68338, 'train_ucc_loss': 0.37797, 'train_ucc_acc': 0.9375, 'loss': 0.53068}\n",
            "Step 74620: {'train_ae_loss': 0.667, 'train_ucc_loss': 0.56653, 'train_ucc_acc': 0.75, 'loss': 0.61677}\n",
            "Step 74640: {'train_ae_loss': 0.67054, 'train_ucc_loss': 0.48806, 'train_ucc_acc': 0.8125, 'loss': 0.5793}\n",
            "Step 74660: {'train_ae_loss': 0.67478, 'train_ucc_loss': 0.51205, 'train_ucc_acc': 0.78125, 'loss': 0.59341}\n",
            "Step 74680: {'train_ae_loss': 0.66887, 'train_ucc_loss': 0.4348, 'train_ucc_acc': 0.875, 'loss': 0.55183}\n",
            "Step 74700: {'train_ae_loss': 0.6855, 'train_ucc_loss': 0.47843, 'train_ucc_acc': 0.8125, 'loss': 0.58196}\n",
            "Step 74720: {'train_ae_loss': 0.65959, 'train_ucc_loss': 0.53167, 'train_ucc_acc': 0.75, 'loss': 0.59563}\n",
            "Step 74740: {'train_ae_loss': 0.68981, 'train_ucc_loss': 0.41751, 'train_ucc_acc': 0.90625, 'loss': 0.55366}\n",
            "Step 74760: {'train_ae_loss': 0.68092, 'train_ucc_loss': 0.45847, 'train_ucc_acc': 0.875, 'loss': 0.5697}\n",
            "Step 74780: {'train_ae_loss': 0.67633, 'train_ucc_loss': 0.58589, 'train_ucc_acc': 0.71875, 'loss': 0.63111}\n",
            "Step 74800: {'train_ae_loss': 0.66965, 'train_ucc_loss': 0.56293, 'train_ucc_acc': 0.71875, 'loss': 0.61629}\n",
            "Step 74820: {'train_ae_loss': 0.66957, 'train_ucc_loss': 0.41866, 'train_ucc_acc': 0.90625, 'loss': 0.54412}\n",
            "Step 74840: {'train_ae_loss': 0.68202, 'train_ucc_loss': 0.43574, 'train_ucc_acc': 0.875, 'loss': 0.55888}\n",
            "Step 74860: {'train_ae_loss': 0.67226, 'train_ucc_loss': 0.47845, 'train_ucc_acc': 0.8125, 'loss': 0.57536}\n",
            "Step 74880: {'train_ae_loss': 0.66802, 'train_ucc_loss': 0.43014, 'train_ucc_acc': 0.875, 'loss': 0.54908}\n",
            "Step 74900: {'train_ae_loss': 0.68199, 'train_ucc_loss': 0.5169, 'train_ucc_acc': 0.78125, 'loss': 0.59945}\n",
            "Step 74920: {'train_ae_loss': 0.67588, 'train_ucc_loss': 0.37744, 'train_ucc_acc': 0.9375, 'loss': 0.52666}\n",
            "Step 74940: {'train_ae_loss': 0.66796, 'train_ucc_loss': 0.46168, 'train_ucc_acc': 0.84375, 'loss': 0.56482}\n",
            "Step 74960: {'train_ae_loss': 0.67, 'train_ucc_loss': 0.51114, 'train_ucc_acc': 0.75, 'loss': 0.59057}\n",
            "Step 74980: {'train_ae_loss': 0.66949, 'train_ucc_loss': 0.39155, 'train_ucc_acc': 0.90625, 'loss': 0.53052}\n",
            "Step 75000: {'train_ae_loss': 0.67199, 'train_ucc_loss': 0.51736, 'train_ucc_acc': 0.78125, 'loss': 0.59468}\n",
            "step: 75000,eval_ae_loss: 0.66054,eval_ucc_loss: 0.48816,eval_ucc_acc: 0.81738\n",
            "Step 75020: {'train_ae_loss': 0.66845, 'train_ucc_loss': 0.51657, 'train_ucc_acc': 0.78125, 'loss': 0.59251}\n",
            "Step 75040: {'train_ae_loss': 0.66869, 'train_ucc_loss': 0.4472, 'train_ucc_acc': 0.875, 'loss': 0.55794}\n",
            "Step 75060: {'train_ae_loss': 0.67316, 'train_ucc_loss': 0.43684, 'train_ucc_acc': 0.875, 'loss': 0.555}\n",
            "Step 75080: {'train_ae_loss': 0.66389, 'train_ucc_loss': 0.52511, 'train_ucc_acc': 0.78125, 'loss': 0.5945}\n",
            "Step 75100: {'train_ae_loss': 0.6633, 'train_ucc_loss': 0.44942, 'train_ucc_acc': 0.84375, 'loss': 0.55636}\n",
            "Step 75120: {'train_ae_loss': 0.64733, 'train_ucc_loss': 0.41747, 'train_ucc_acc': 0.90625, 'loss': 0.5324}\n",
            "Step 75140: {'train_ae_loss': 0.67835, 'train_ucc_loss': 0.39739, 'train_ucc_acc': 0.9375, 'loss': 0.53787}\n",
            "Step 75160: {'train_ae_loss': 0.68273, 'train_ucc_loss': 0.43496, 'train_ucc_acc': 0.875, 'loss': 0.55884}\n",
            "Step 75180: {'train_ae_loss': 0.65958, 'train_ucc_loss': 0.38764, 'train_ucc_acc': 0.9375, 'loss': 0.52361}\n",
            "Step 75200: {'train_ae_loss': 0.68717, 'train_ucc_loss': 0.38939, 'train_ucc_acc': 0.9375, 'loss': 0.53828}\n",
            "Step 75220: {'train_ae_loss': 0.65788, 'train_ucc_loss': 0.37968, 'train_ucc_acc': 0.9375, 'loss': 0.51878}\n",
            "Step 75240: {'train_ae_loss': 0.63993, 'train_ucc_loss': 0.44652, 'train_ucc_acc': 0.875, 'loss': 0.54322}\n",
            "Step 75260: {'train_ae_loss': 0.66596, 'train_ucc_loss': 0.47914, 'train_ucc_acc': 0.84375, 'loss': 0.57255}\n",
            "Step 75280: {'train_ae_loss': 0.64587, 'train_ucc_loss': 0.46234, 'train_ucc_acc': 0.8125, 'loss': 0.5541}\n",
            "Step 75300: {'train_ae_loss': 0.67272, 'train_ucc_loss': 0.45169, 'train_ucc_acc': 0.84375, 'loss': 0.5622}\n",
            "Step 75320: {'train_ae_loss': 0.66622, 'train_ucc_loss': 0.39931, 'train_ucc_acc': 0.90625, 'loss': 0.53277}\n",
            "Step 75340: {'train_ae_loss': 0.66052, 'train_ucc_loss': 0.65238, 'train_ucc_acc': 0.65625, 'loss': 0.65645}\n",
            "Step 75360: {'train_ae_loss': 0.67545, 'train_ucc_loss': 0.42541, 'train_ucc_acc': 0.875, 'loss': 0.55043}\n",
            "Step 75380: {'train_ae_loss': 0.657, 'train_ucc_loss': 0.59211, 'train_ucc_acc': 0.6875, 'loss': 0.62455}\n",
            "Step 75400: {'train_ae_loss': 0.67683, 'train_ucc_loss': 0.46959, 'train_ucc_acc': 0.84375, 'loss': 0.57321}\n",
            "Step 75420: {'train_ae_loss': 0.65466, 'train_ucc_loss': 0.65237, 'train_ucc_acc': 0.59375, 'loss': 0.65352}\n",
            "Step 75440: {'train_ae_loss': 0.65729, 'train_ucc_loss': 0.55239, 'train_ucc_acc': 0.75, 'loss': 0.60484}\n",
            "Step 75460: {'train_ae_loss': 0.65097, 'train_ucc_loss': 0.5524, 'train_ucc_acc': 0.75, 'loss': 0.60169}\n",
            "Step 75480: {'train_ae_loss': 0.65197, 'train_ucc_loss': 0.35909, 'train_ucc_acc': 0.96875, 'loss': 0.50553}\n",
            "Step 75500: {'train_ae_loss': 0.66395, 'train_ucc_loss': 0.50542, 'train_ucc_acc': 0.8125, 'loss': 0.58468}\n",
            "Step 75520: {'train_ae_loss': 0.66305, 'train_ucc_loss': 0.5284, 'train_ucc_acc': 0.78125, 'loss': 0.59573}\n",
            "Step 75540: {'train_ae_loss': 0.66634, 'train_ucc_loss': 0.4969, 'train_ucc_acc': 0.8125, 'loss': 0.58162}\n",
            "Step 75560: {'train_ae_loss': 0.67314, 'train_ucc_loss': 0.35201, 'train_ucc_acc': 0.96875, 'loss': 0.51257}\n",
            "Step 75580: {'train_ae_loss': 0.68525, 'train_ucc_loss': 0.45389, 'train_ucc_acc': 0.875, 'loss': 0.56957}\n",
            "Step 75600: {'train_ae_loss': 0.66169, 'train_ucc_loss': 0.40514, 'train_ucc_acc': 0.9375, 'loss': 0.53341}\n",
            "Step 75620: {'train_ae_loss': 0.66743, 'train_ucc_loss': 0.4723, 'train_ucc_acc': 0.78125, 'loss': 0.56986}\n",
            "Step 75640: {'train_ae_loss': 0.6612, 'train_ucc_loss': 0.47114, 'train_ucc_acc': 0.84375, 'loss': 0.56617}\n",
            "Step 75660: {'train_ae_loss': 0.66589, 'train_ucc_loss': 0.60088, 'train_ucc_acc': 0.71875, 'loss': 0.63339}\n",
            "Step 75680: {'train_ae_loss': 0.66224, 'train_ucc_loss': 0.48268, 'train_ucc_acc': 0.8125, 'loss': 0.57246}\n",
            "Step 75700: {'train_ae_loss': 0.6601, 'train_ucc_loss': 0.40974, 'train_ucc_acc': 0.90625, 'loss': 0.53492}\n",
            "Step 75720: {'train_ae_loss': 0.67587, 'train_ucc_loss': 0.47075, 'train_ucc_acc': 0.84375, 'loss': 0.57331}\n",
            "Step 75740: {'train_ae_loss': 0.67436, 'train_ucc_loss': 0.472, 'train_ucc_acc': 0.84375, 'loss': 0.57318}\n",
            "Step 75760: {'train_ae_loss': 0.66613, 'train_ucc_loss': 0.44339, 'train_ucc_acc': 0.875, 'loss': 0.55476}\n",
            "Step 75780: {'train_ae_loss': 0.67314, 'train_ucc_loss': 0.478, 'train_ucc_acc': 0.84375, 'loss': 0.57557}\n",
            "Step 75800: {'train_ae_loss': 0.67128, 'train_ucc_loss': 0.49865, 'train_ucc_acc': 0.8125, 'loss': 0.58496}\n",
            "Step 75820: {'train_ae_loss': 0.66757, 'train_ucc_loss': 0.43019, 'train_ucc_acc': 0.875, 'loss': 0.54888}\n",
            "Step 75840: {'train_ae_loss': 0.67149, 'train_ucc_loss': 0.48981, 'train_ucc_acc': 0.8125, 'loss': 0.58065}\n",
            "Step 75860: {'train_ae_loss': 0.67336, 'train_ucc_loss': 0.46252, 'train_ucc_acc': 0.84375, 'loss': 0.56794}\n",
            "Step 75880: {'train_ae_loss': 0.67238, 'train_ucc_loss': 0.46134, 'train_ucc_acc': 0.875, 'loss': 0.56686}\n",
            "Step 75900: {'train_ae_loss': 0.65862, 'train_ucc_loss': 0.44088, 'train_ucc_acc': 0.875, 'loss': 0.54975}\n",
            "Step 75920: {'train_ae_loss': 0.66715, 'train_ucc_loss': 0.48523, 'train_ucc_acc': 0.8125, 'loss': 0.57619}\n",
            "Step 75940: {'train_ae_loss': 0.66529, 'train_ucc_loss': 0.46877, 'train_ucc_acc': 0.8125, 'loss': 0.56703}\n",
            "Step 75960: {'train_ae_loss': 0.67945, 'train_ucc_loss': 0.47134, 'train_ucc_acc': 0.84375, 'loss': 0.57539}\n",
            "Step 75980: {'train_ae_loss': 0.64253, 'train_ucc_loss': 0.47597, 'train_ucc_acc': 0.875, 'loss': 0.55925}\n",
            "Step 76000: {'train_ae_loss': 0.6599, 'train_ucc_loss': 0.48097, 'train_ucc_acc': 0.8125, 'loss': 0.57043}\n",
            "step: 76000,eval_ae_loss: 0.6512,eval_ucc_loss: 0.50708,eval_ucc_acc: 0.79688\n",
            "Step 76020: {'train_ae_loss': 0.66629, 'train_ucc_loss': 0.43923, 'train_ucc_acc': 0.84375, 'loss': 0.55276}\n",
            "Step 76040: {'train_ae_loss': 0.66293, 'train_ucc_loss': 0.54756, 'train_ucc_acc': 0.78125, 'loss': 0.60524}\n",
            "Step 76060: {'train_ae_loss': 0.67538, 'train_ucc_loss': 0.3887, 'train_ucc_acc': 0.90625, 'loss': 0.53204}\n",
            "Step 76080: {'train_ae_loss': 0.66971, 'train_ucc_loss': 0.43285, 'train_ucc_acc': 0.875, 'loss': 0.55128}\n",
            "Step 76100: {'train_ae_loss': 0.66728, 'train_ucc_loss': 0.49309, 'train_ucc_acc': 0.78125, 'loss': 0.58019}\n",
            "Step 76120: {'train_ae_loss': 0.6713, 'train_ucc_loss': 0.48037, 'train_ucc_acc': 0.8125, 'loss': 0.57584}\n",
            "Step 76140: {'train_ae_loss': 0.67898, 'train_ucc_loss': 0.4241, 'train_ucc_acc': 0.90625, 'loss': 0.55154}\n",
            "Step 76160: {'train_ae_loss': 0.65961, 'train_ucc_loss': 0.51797, 'train_ucc_acc': 0.8125, 'loss': 0.58879}\n",
            "Step 76180: {'train_ae_loss': 0.67306, 'train_ucc_loss': 0.50829, 'train_ucc_acc': 0.8125, 'loss': 0.59068}\n",
            "Step 76200: {'train_ae_loss': 0.65733, 'train_ucc_loss': 0.41594, 'train_ucc_acc': 0.875, 'loss': 0.53664}\n",
            "Step 76220: {'train_ae_loss': 0.65937, 'train_ucc_loss': 0.52095, 'train_ucc_acc': 0.8125, 'loss': 0.59016}\n",
            "Step 76240: {'train_ae_loss': 0.66837, 'train_ucc_loss': 0.41361, 'train_ucc_acc': 0.875, 'loss': 0.54099}\n",
            "Step 76260: {'train_ae_loss': 0.65639, 'train_ucc_loss': 0.48352, 'train_ucc_acc': 0.8125, 'loss': 0.56995}\n",
            "Step 76280: {'train_ae_loss': 0.64661, 'train_ucc_loss': 0.49778, 'train_ucc_acc': 0.8125, 'loss': 0.57219}\n",
            "Step 76300: {'train_ae_loss': 0.6693, 'train_ucc_loss': 0.39581, 'train_ucc_acc': 0.90625, 'loss': 0.53255}\n",
            "Step 76320: {'train_ae_loss': 0.68018, 'train_ucc_loss': 0.42654, 'train_ucc_acc': 0.875, 'loss': 0.55336}\n",
            "Step 76340: {'train_ae_loss': 0.66104, 'train_ucc_loss': 0.4958, 'train_ucc_acc': 0.8125, 'loss': 0.57842}\n",
            "Step 76360: {'train_ae_loss': 0.64356, 'train_ucc_loss': 0.5342, 'train_ucc_acc': 0.78125, 'loss': 0.58888}\n",
            "Step 76380: {'train_ae_loss': 0.6552, 'train_ucc_loss': 0.54233, 'train_ucc_acc': 0.78125, 'loss': 0.59876}\n",
            "Step 76400: {'train_ae_loss': 0.66467, 'train_ucc_loss': 0.41412, 'train_ucc_acc': 0.90625, 'loss': 0.5394}\n",
            "Step 76420: {'train_ae_loss': 0.66916, 'train_ucc_loss': 0.48414, 'train_ucc_acc': 0.8125, 'loss': 0.57665}\n",
            "Step 76440: {'train_ae_loss': 0.68119, 'train_ucc_loss': 0.42887, 'train_ucc_acc': 0.84375, 'loss': 0.55503}\n",
            "Step 76460: {'train_ae_loss': 0.66173, 'train_ucc_loss': 0.48588, 'train_ucc_acc': 0.8125, 'loss': 0.57381}\n",
            "Step 76480: {'train_ae_loss': 0.66294, 'train_ucc_loss': 0.50034, 'train_ucc_acc': 0.8125, 'loss': 0.58164}\n",
            "Step 76500: {'train_ae_loss': 0.67889, 'train_ucc_loss': 0.4181, 'train_ucc_acc': 0.875, 'loss': 0.5485}\n",
            "Step 76520: {'train_ae_loss': 0.65848, 'train_ucc_loss': 0.47866, 'train_ucc_acc': 0.8125, 'loss': 0.56857}\n",
            "Step 76540: {'train_ae_loss': 0.68808, 'train_ucc_loss': 0.40839, 'train_ucc_acc': 0.90625, 'loss': 0.54823}\n",
            "Step 76560: {'train_ae_loss': 0.68426, 'train_ucc_loss': 0.36884, 'train_ucc_acc': 0.9375, 'loss': 0.52655}\n",
            "Step 76580: {'train_ae_loss': 0.67179, 'train_ucc_loss': 0.42783, 'train_ucc_acc': 0.90625, 'loss': 0.54981}\n",
            "Step 76600: {'train_ae_loss': 0.65739, 'train_ucc_loss': 0.46188, 'train_ucc_acc': 0.875, 'loss': 0.55964}\n",
            "Step 76620: {'train_ae_loss': 0.6808, 'train_ucc_loss': 0.4342, 'train_ucc_acc': 0.875, 'loss': 0.5575}\n",
            "Step 76640: {'train_ae_loss': 0.66845, 'train_ucc_loss': 0.32646, 'train_ucc_acc': 1.0, 'loss': 0.49745}\n",
            "Step 76660: {'train_ae_loss': 0.66454, 'train_ucc_loss': 0.4245, 'train_ucc_acc': 0.875, 'loss': 0.54452}\n",
            "Step 76680: {'train_ae_loss': 0.66764, 'train_ucc_loss': 0.50114, 'train_ucc_acc': 0.8125, 'loss': 0.58439}\n",
            "Step 76700: {'train_ae_loss': 0.65905, 'train_ucc_loss': 0.55278, 'train_ucc_acc': 0.78125, 'loss': 0.60591}\n",
            "Step 76720: {'train_ae_loss': 0.64479, 'train_ucc_loss': 0.42599, 'train_ucc_acc': 0.90625, 'loss': 0.53539}\n",
            "Step 76740: {'train_ae_loss': 0.67219, 'train_ucc_loss': 0.48714, 'train_ucc_acc': 0.84375, 'loss': 0.57967}\n",
            "Step 76760: {'train_ae_loss': 0.6571, 'train_ucc_loss': 0.44666, 'train_ucc_acc': 0.875, 'loss': 0.55188}\n",
            "Step 76780: {'train_ae_loss': 0.67958, 'train_ucc_loss': 0.49054, 'train_ucc_acc': 0.78125, 'loss': 0.58506}\n",
            "Step 76800: {'train_ae_loss': 0.65561, 'train_ucc_loss': 0.54864, 'train_ucc_acc': 0.75, 'loss': 0.60213}\n",
            "Step 76820: {'train_ae_loss': 0.66716, 'train_ucc_loss': 0.4771, 'train_ucc_acc': 0.8125, 'loss': 0.57213}\n",
            "Step 76840: {'train_ae_loss': 0.66594, 'train_ucc_loss': 0.38244, 'train_ucc_acc': 0.9375, 'loss': 0.52419}\n",
            "Step 76860: {'train_ae_loss': 0.69527, 'train_ucc_loss': 0.43973, 'train_ucc_acc': 0.875, 'loss': 0.5675}\n",
            "Step 76880: {'train_ae_loss': 0.67557, 'train_ucc_loss': 0.39637, 'train_ucc_acc': 0.90625, 'loss': 0.53597}\n",
            "Step 76900: {'train_ae_loss': 0.67113, 'train_ucc_loss': 0.42711, 'train_ucc_acc': 0.875, 'loss': 0.54912}\n",
            "Step 76920: {'train_ae_loss': 0.65947, 'train_ucc_loss': 0.41218, 'train_ucc_acc': 0.90625, 'loss': 0.53583}\n",
            "Step 76940: {'train_ae_loss': 0.6808, 'train_ucc_loss': 0.6, 'train_ucc_acc': 0.6875, 'loss': 0.6404}\n",
            "Step 76960: {'train_ae_loss': 0.66571, 'train_ucc_loss': 0.5234, 'train_ucc_acc': 0.78125, 'loss': 0.59455}\n",
            "Step 76980: {'train_ae_loss': 0.65346, 'train_ucc_loss': 0.52492, 'train_ucc_acc': 0.75, 'loss': 0.58919}\n",
            "Step 77000: {'train_ae_loss': 0.68031, 'train_ucc_loss': 0.44143, 'train_ucc_acc': 0.875, 'loss': 0.56087}\n",
            "step: 77000,eval_ae_loss: 0.66106,eval_ucc_loss: 0.53634,eval_ucc_acc: 0.76465\n",
            "Step 77020: {'train_ae_loss': 0.66577, 'train_ucc_loss': 0.41668, 'train_ucc_acc': 0.90625, 'loss': 0.54123}\n",
            "Step 77040: {'train_ae_loss': 0.66534, 'train_ucc_loss': 0.42272, 'train_ucc_acc': 0.84375, 'loss': 0.54403}\n",
            "Step 77060: {'train_ae_loss': 0.66153, 'train_ucc_loss': 0.43762, 'train_ucc_acc': 0.875, 'loss': 0.54957}\n",
            "Step 77080: {'train_ae_loss': 0.6689, 'train_ucc_loss': 0.4433, 'train_ucc_acc': 0.875, 'loss': 0.5561}\n",
            "Step 77100: {'train_ae_loss': 0.67769, 'train_ucc_loss': 0.47343, 'train_ucc_acc': 0.84375, 'loss': 0.57556}\n",
            "Step 77120: {'train_ae_loss': 0.65975, 'train_ucc_loss': 0.48767, 'train_ucc_acc': 0.8125, 'loss': 0.57371}\n",
            "Step 77140: {'train_ae_loss': 0.67316, 'train_ucc_loss': 0.44756, 'train_ucc_acc': 0.84375, 'loss': 0.56036}\n",
            "Step 77160: {'train_ae_loss': 0.65446, 'train_ucc_loss': 0.51433, 'train_ucc_acc': 0.78125, 'loss': 0.5844}\n",
            "Step 77180: {'train_ae_loss': 0.68036, 'train_ucc_loss': 0.40033, 'train_ucc_acc': 0.90625, 'loss': 0.54034}\n",
            "Step 77200: {'train_ae_loss': 0.6729, 'train_ucc_loss': 0.50842, 'train_ucc_acc': 0.8125, 'loss': 0.59066}\n",
            "Step 77220: {'train_ae_loss': 0.67502, 'train_ucc_loss': 0.4478, 'train_ucc_acc': 0.875, 'loss': 0.56141}\n",
            "Step 77240: {'train_ae_loss': 0.66564, 'train_ucc_loss': 0.43746, 'train_ucc_acc': 0.875, 'loss': 0.55155}\n",
            "Step 77260: {'train_ae_loss': 0.65721, 'train_ucc_loss': 0.53113, 'train_ucc_acc': 0.78125, 'loss': 0.59417}\n",
            "Step 77280: {'train_ae_loss': 0.67323, 'train_ucc_loss': 0.4688, 'train_ucc_acc': 0.8125, 'loss': 0.57101}\n",
            "Step 77300: {'train_ae_loss': 0.67069, 'train_ucc_loss': 0.56554, 'train_ucc_acc': 0.6875, 'loss': 0.61812}\n",
            "Step 77320: {'train_ae_loss': 0.69328, 'train_ucc_loss': 0.62944, 'train_ucc_acc': 0.65625, 'loss': 0.66136}\n",
            "Step 77340: {'train_ae_loss': 0.6714, 'train_ucc_loss': 0.47451, 'train_ucc_acc': 0.8125, 'loss': 0.57295}\n",
            "Step 77360: {'train_ae_loss': 0.66279, 'train_ucc_loss': 0.45635, 'train_ucc_acc': 0.8125, 'loss': 0.55957}\n",
            "Step 77380: {'train_ae_loss': 0.68473, 'train_ucc_loss': 0.48992, 'train_ucc_acc': 0.84375, 'loss': 0.58733}\n",
            "Step 77400: {'train_ae_loss': 0.67306, 'train_ucc_loss': 0.41879, 'train_ucc_acc': 0.90625, 'loss': 0.54593}\n",
            "Step 77420: {'train_ae_loss': 0.6769, 'train_ucc_loss': 0.52708, 'train_ucc_acc': 0.71875, 'loss': 0.60199}\n",
            "Step 77440: {'train_ae_loss': 0.66879, 'train_ucc_loss': 0.46079, 'train_ucc_acc': 0.8125, 'loss': 0.56479}\n",
            "Step 77460: {'train_ae_loss': 0.6663, 'train_ucc_loss': 0.39197, 'train_ucc_acc': 0.9375, 'loss': 0.52913}\n",
            "Step 77480: {'train_ae_loss': 0.67681, 'train_ucc_loss': 0.3802, 'train_ucc_acc': 0.9375, 'loss': 0.5285}\n",
            "Step 77500: {'train_ae_loss': 0.67253, 'train_ucc_loss': 0.47913, 'train_ucc_acc': 0.875, 'loss': 0.57583}\n",
            "Step 77520: {'train_ae_loss': 0.67324, 'train_ucc_loss': 0.6553, 'train_ucc_acc': 0.625, 'loss': 0.66427}\n",
            "Step 77540: {'train_ae_loss': 0.66915, 'train_ucc_loss': 0.41458, 'train_ucc_acc': 0.90625, 'loss': 0.54187}\n",
            "Step 77560: {'train_ae_loss': 0.65673, 'train_ucc_loss': 0.52399, 'train_ucc_acc': 0.78125, 'loss': 0.59036}\n",
            "Step 77580: {'train_ae_loss': 0.66712, 'train_ucc_loss': 0.44261, 'train_ucc_acc': 0.875, 'loss': 0.55487}\n",
            "Step 77600: {'train_ae_loss': 0.69261, 'train_ucc_loss': 0.39144, 'train_ucc_acc': 0.9375, 'loss': 0.54202}\n",
            "Step 77620: {'train_ae_loss': 0.69607, 'train_ucc_loss': 0.43794, 'train_ucc_acc': 0.84375, 'loss': 0.56701}\n",
            "Step 77640: {'train_ae_loss': 0.67902, 'train_ucc_loss': 0.46445, 'train_ucc_acc': 0.84375, 'loss': 0.57173}\n",
            "Step 77660: {'train_ae_loss': 0.68986, 'train_ucc_loss': 0.45368, 'train_ucc_acc': 0.84375, 'loss': 0.57177}\n",
            "Step 77680: {'train_ae_loss': 0.68329, 'train_ucc_loss': 0.44809, 'train_ucc_acc': 0.8125, 'loss': 0.56569}\n",
            "Step 77700: {'train_ae_loss': 0.68481, 'train_ucc_loss': 0.44289, 'train_ucc_acc': 0.84375, 'loss': 0.56385}\n",
            "Step 77720: {'train_ae_loss': 0.68176, 'train_ucc_loss': 0.483, 'train_ucc_acc': 0.8125, 'loss': 0.58238}\n",
            "Step 77740: {'train_ae_loss': 0.68066, 'train_ucc_loss': 0.42606, 'train_ucc_acc': 0.875, 'loss': 0.55336}\n",
            "Step 77760: {'train_ae_loss': 0.68537, 'train_ucc_loss': 0.39458, 'train_ucc_acc': 0.90625, 'loss': 0.53997}\n",
            "Step 77780: {'train_ae_loss': 0.68012, 'train_ucc_loss': 0.38485, 'train_ucc_acc': 0.90625, 'loss': 0.53248}\n",
            "Step 77800: {'train_ae_loss': 0.68771, 'train_ucc_loss': 0.43769, 'train_ucc_acc': 0.875, 'loss': 0.5627}\n",
            "Step 77820: {'train_ae_loss': 0.66467, 'train_ucc_loss': 0.43653, 'train_ucc_acc': 0.875, 'loss': 0.5506}\n",
            "Step 77840: {'train_ae_loss': 0.67363, 'train_ucc_loss': 0.57207, 'train_ucc_acc': 0.71875, 'loss': 0.62285}\n",
            "Step 77860: {'train_ae_loss': 0.68764, 'train_ucc_loss': 0.39943, 'train_ucc_acc': 0.9375, 'loss': 0.54354}\n",
            "Step 77880: {'train_ae_loss': 0.67135, 'train_ucc_loss': 0.48737, 'train_ucc_acc': 0.8125, 'loss': 0.57936}\n",
            "Step 77900: {'train_ae_loss': 0.67201, 'train_ucc_loss': 0.39185, 'train_ucc_acc': 0.9375, 'loss': 0.53193}\n",
            "Step 77920: {'train_ae_loss': 0.68398, 'train_ucc_loss': 0.32602, 'train_ucc_acc': 1.0, 'loss': 0.505}\n",
            "Step 77940: {'train_ae_loss': 0.6802, 'train_ucc_loss': 0.51134, 'train_ucc_acc': 0.78125, 'loss': 0.59577}\n",
            "Step 77960: {'train_ae_loss': 0.67112, 'train_ucc_loss': 0.49043, 'train_ucc_acc': 0.8125, 'loss': 0.58078}\n",
            "Step 77980: {'train_ae_loss': 0.65348, 'train_ucc_loss': 0.45804, 'train_ucc_acc': 0.84375, 'loss': 0.55576}\n",
            "Step 78000: {'train_ae_loss': 0.67408, 'train_ucc_loss': 0.52661, 'train_ucc_acc': 0.78125, 'loss': 0.60035}\n",
            "step: 78000,eval_ae_loss: 0.65863,eval_ucc_loss: 0.48084,eval_ucc_acc: 0.82129\n",
            "Step 78020: {'train_ae_loss': 0.67117, 'train_ucc_loss': 0.556, 'train_ucc_acc': 0.75, 'loss': 0.61359}\n",
            "Step 78040: {'train_ae_loss': 0.66422, 'train_ucc_loss': 0.38399, 'train_ucc_acc': 0.9375, 'loss': 0.5241}\n",
            "Step 78060: {'train_ae_loss': 0.67688, 'train_ucc_loss': 0.39886, 'train_ucc_acc': 0.9375, 'loss': 0.53787}\n",
            "Step 78080: {'train_ae_loss': 0.68152, 'train_ucc_loss': 0.47698, 'train_ucc_acc': 0.8125, 'loss': 0.57925}\n",
            "Step 78100: {'train_ae_loss': 0.66791, 'train_ucc_loss': 0.50397, 'train_ucc_acc': 0.8125, 'loss': 0.58594}\n",
            "Step 78120: {'train_ae_loss': 0.66285, 'train_ucc_loss': 0.73555, 'train_ucc_acc': 0.53125, 'loss': 0.6992}\n",
            "Step 78140: {'train_ae_loss': 0.66393, 'train_ucc_loss': 0.33707, 'train_ucc_acc': 1.0, 'loss': 0.5005}\n",
            "Step 78160: {'train_ae_loss': 0.65878, 'train_ucc_loss': 0.3875, 'train_ucc_acc': 0.9375, 'loss': 0.52314}\n",
            "Step 78180: {'train_ae_loss': 0.66279, 'train_ucc_loss': 0.45678, 'train_ucc_acc': 0.84375, 'loss': 0.55978}\n",
            "Step 78200: {'train_ae_loss': 0.67665, 'train_ucc_loss': 0.482, 'train_ucc_acc': 0.78125, 'loss': 0.57932}\n",
            "Step 78220: {'train_ae_loss': 0.65604, 'train_ucc_loss': 0.57952, 'train_ucc_acc': 0.65625, 'loss': 0.61778}\n",
            "Step 78240: {'train_ae_loss': 0.66235, 'train_ucc_loss': 0.50034, 'train_ucc_acc': 0.8125, 'loss': 0.58134}\n",
            "Step 78260: {'train_ae_loss': 0.67821, 'train_ucc_loss': 0.4098, 'train_ucc_acc': 0.90625, 'loss': 0.54401}\n",
            "Step 78280: {'train_ae_loss': 0.6658, 'train_ucc_loss': 0.43231, 'train_ucc_acc': 0.875, 'loss': 0.54905}\n",
            "Step 78300: {'train_ae_loss': 0.67442, 'train_ucc_loss': 0.48603, 'train_ucc_acc': 0.8125, 'loss': 0.58022}\n",
            "Step 78320: {'train_ae_loss': 0.65952, 'train_ucc_loss': 0.42992, 'train_ucc_acc': 0.90625, 'loss': 0.54472}\n",
            "Step 78340: {'train_ae_loss': 0.68641, 'train_ucc_loss': 0.6055, 'train_ucc_acc': 0.71875, 'loss': 0.64595}\n",
            "Step 78360: {'train_ae_loss': 0.65988, 'train_ucc_loss': 0.41703, 'train_ucc_acc': 0.90625, 'loss': 0.53845}\n",
            "Step 78380: {'train_ae_loss': 0.65578, 'train_ucc_loss': 0.51888, 'train_ucc_acc': 0.78125, 'loss': 0.58733}\n",
            "Step 78400: {'train_ae_loss': 0.6751, 'train_ucc_loss': 0.48919, 'train_ucc_acc': 0.8125, 'loss': 0.58215}\n",
            "Step 78420: {'train_ae_loss': 0.6543, 'train_ucc_loss': 0.44318, 'train_ucc_acc': 0.875, 'loss': 0.54874}\n",
            "Step 78440: {'train_ae_loss': 0.67322, 'train_ucc_loss': 0.39631, 'train_ucc_acc': 0.90625, 'loss': 0.53477}\n",
            "Step 78460: {'train_ae_loss': 0.66035, 'train_ucc_loss': 0.52692, 'train_ucc_acc': 0.8125, 'loss': 0.59363}\n",
            "Step 78480: {'train_ae_loss': 0.66345, 'train_ucc_loss': 0.53867, 'train_ucc_acc': 0.78125, 'loss': 0.60106}\n",
            "Step 78500: {'train_ae_loss': 0.6677, 'train_ucc_loss': 0.45123, 'train_ucc_acc': 0.8125, 'loss': 0.55946}\n",
            "Step 78520: {'train_ae_loss': 0.66697, 'train_ucc_loss': 0.39505, 'train_ucc_acc': 0.90625, 'loss': 0.53101}\n",
            "Step 78540: {'train_ae_loss': 0.68611, 'train_ucc_loss': 0.51593, 'train_ucc_acc': 0.78125, 'loss': 0.60102}\n",
            "Step 78560: {'train_ae_loss': 0.6785, 'train_ucc_loss': 0.50342, 'train_ucc_acc': 0.78125, 'loss': 0.59096}\n",
            "Step 78580: {'train_ae_loss': 0.6777, 'train_ucc_loss': 0.47843, 'train_ucc_acc': 0.8125, 'loss': 0.57806}\n",
            "Step 78600: {'train_ae_loss': 0.66866, 'train_ucc_loss': 0.45914, 'train_ucc_acc': 0.875, 'loss': 0.5639}\n",
            "Step 78620: {'train_ae_loss': 0.67316, 'train_ucc_loss': 0.51627, 'train_ucc_acc': 0.78125, 'loss': 0.59471}\n",
            "Step 78640: {'train_ae_loss': 0.68192, 'train_ucc_loss': 0.34718, 'train_ucc_acc': 0.96875, 'loss': 0.51455}\n",
            "Step 78660: {'train_ae_loss': 0.66533, 'train_ucc_loss': 0.59101, 'train_ucc_acc': 0.65625, 'loss': 0.62817}\n",
            "Step 78680: {'train_ae_loss': 0.67946, 'train_ucc_loss': 0.34555, 'train_ucc_acc': 0.96875, 'loss': 0.51251}\n",
            "Step 78700: {'train_ae_loss': 0.65791, 'train_ucc_loss': 0.47777, 'train_ucc_acc': 0.8125, 'loss': 0.56784}\n",
            "Step 78720: {'train_ae_loss': 0.65541, 'train_ucc_loss': 0.49019, 'train_ucc_acc': 0.8125, 'loss': 0.5728}\n",
            "Step 78740: {'train_ae_loss': 0.66724, 'train_ucc_loss': 0.37224, 'train_ucc_acc': 0.9375, 'loss': 0.51974}\n",
            "Step 78760: {'train_ae_loss': 0.67936, 'train_ucc_loss': 0.45416, 'train_ucc_acc': 0.875, 'loss': 0.56676}\n",
            "Step 78780: {'train_ae_loss': 0.6755, 'train_ucc_loss': 0.46051, 'train_ucc_acc': 0.875, 'loss': 0.568}\n",
            "Step 78800: {'train_ae_loss': 0.66597, 'train_ucc_loss': 0.4779, 'train_ucc_acc': 0.8125, 'loss': 0.57193}\n",
            "Step 78820: {'train_ae_loss': 0.67877, 'train_ucc_loss': 0.40093, 'train_ucc_acc': 0.9375, 'loss': 0.53985}\n",
            "Step 78840: {'train_ae_loss': 0.65667, 'train_ucc_loss': 0.49511, 'train_ucc_acc': 0.78125, 'loss': 0.57589}\n",
            "Step 78860: {'train_ae_loss': 0.66815, 'train_ucc_loss': 0.57472, 'train_ucc_acc': 0.71875, 'loss': 0.62143}\n",
            "Step 78880: {'train_ae_loss': 0.6694, 'train_ucc_loss': 0.4788, 'train_ucc_acc': 0.8125, 'loss': 0.5741}\n",
            "Step 78900: {'train_ae_loss': 0.66879, 'train_ucc_loss': 0.44536, 'train_ucc_acc': 0.875, 'loss': 0.55707}\n",
            "Step 78920: {'train_ae_loss': 0.66706, 'train_ucc_loss': 0.43481, 'train_ucc_acc': 0.90625, 'loss': 0.55094}\n",
            "Step 78940: {'train_ae_loss': 0.66927, 'train_ucc_loss': 0.41136, 'train_ucc_acc': 0.90625, 'loss': 0.54031}\n",
            "Step 78960: {'train_ae_loss': 0.66354, 'train_ucc_loss': 0.44126, 'train_ucc_acc': 0.875, 'loss': 0.5524}\n",
            "Step 78980: {'train_ae_loss': 0.69355, 'train_ucc_loss': 0.47345, 'train_ucc_acc': 0.8125, 'loss': 0.5835}\n",
            "Step 79000: {'train_ae_loss': 0.67704, 'train_ucc_loss': 0.5843, 'train_ucc_acc': 0.71875, 'loss': 0.63067}\n",
            "step: 79000,eval_ae_loss: 0.65814,eval_ucc_loss: 0.57559,eval_ucc_acc: 0.72949\n",
            "Step 79020: {'train_ae_loss': 0.65421, 'train_ucc_loss': 0.46199, 'train_ucc_acc': 0.84375, 'loss': 0.5581}\n",
            "Step 79040: {'train_ae_loss': 0.6686, 'train_ucc_loss': 0.44584, 'train_ucc_acc': 0.875, 'loss': 0.55722}\n",
            "Step 79060: {'train_ae_loss': 0.68429, 'train_ucc_loss': 0.44476, 'train_ucc_acc': 0.875, 'loss': 0.56453}\n",
            "Step 79080: {'train_ae_loss': 0.68234, 'train_ucc_loss': 0.42899, 'train_ucc_acc': 0.875, 'loss': 0.55567}\n",
            "Step 79100: {'train_ae_loss': 0.66388, 'train_ucc_loss': 0.51475, 'train_ucc_acc': 0.78125, 'loss': 0.58932}\n",
            "Step 79120: {'train_ae_loss': 0.65984, 'train_ucc_loss': 0.42322, 'train_ucc_acc': 0.90625, 'loss': 0.54153}\n",
            "Step 79140: {'train_ae_loss': 0.66825, 'train_ucc_loss': 0.38306, 'train_ucc_acc': 0.9375, 'loss': 0.52566}\n",
            "Step 79160: {'train_ae_loss': 0.67122, 'train_ucc_loss': 0.4746, 'train_ucc_acc': 0.84375, 'loss': 0.57291}\n",
            "Step 79180: {'train_ae_loss': 0.66364, 'train_ucc_loss': 0.4917, 'train_ucc_acc': 0.8125, 'loss': 0.57767}\n",
            "Step 79200: {'train_ae_loss': 0.66356, 'train_ucc_loss': 0.64977, 'train_ucc_acc': 0.625, 'loss': 0.65666}\n",
            "Step 79220: {'train_ae_loss': 0.67278, 'train_ucc_loss': 0.48334, 'train_ucc_acc': 0.8125, 'loss': 0.57806}\n",
            "Step 79240: {'train_ae_loss': 0.65635, 'train_ucc_loss': 0.53574, 'train_ucc_acc': 0.75, 'loss': 0.59604}\n",
            "Step 79260: {'train_ae_loss': 0.66518, 'train_ucc_loss': 0.39961, 'train_ucc_acc': 0.90625, 'loss': 0.5324}\n",
            "Step 79280: {'train_ae_loss': 0.67859, 'train_ucc_loss': 0.36922, 'train_ucc_acc': 0.9375, 'loss': 0.5239}\n",
            "Step 79300: {'train_ae_loss': 0.65917, 'train_ucc_loss': 0.41895, 'train_ucc_acc': 0.875, 'loss': 0.53906}\n",
            "Step 79320: {'train_ae_loss': 0.66486, 'train_ucc_loss': 0.45898, 'train_ucc_acc': 0.84375, 'loss': 0.56192}\n",
            "Step 79340: {'train_ae_loss': 0.67421, 'train_ucc_loss': 0.4375, 'train_ucc_acc': 0.875, 'loss': 0.55586}\n",
            "Step 79360: {'train_ae_loss': 0.64725, 'train_ucc_loss': 0.43437, 'train_ucc_acc': 0.875, 'loss': 0.54081}\n",
            "Step 79380: {'train_ae_loss': 0.67223, 'train_ucc_loss': 0.48104, 'train_ucc_acc': 0.8125, 'loss': 0.57663}\n",
            "Step 79400: {'train_ae_loss': 0.66692, 'train_ucc_loss': 0.49032, 'train_ucc_acc': 0.78125, 'loss': 0.57862}\n",
            "Step 79420: {'train_ae_loss': 0.6712, 'train_ucc_loss': 0.39934, 'train_ucc_acc': 0.90625, 'loss': 0.53527}\n",
            "Step 79440: {'train_ae_loss': 0.6823, 'train_ucc_loss': 0.48632, 'train_ucc_acc': 0.8125, 'loss': 0.58431}\n",
            "Step 79460: {'train_ae_loss': 0.66608, 'train_ucc_loss': 0.48433, 'train_ucc_acc': 0.8125, 'loss': 0.5752}\n",
            "Step 79480: {'train_ae_loss': 0.65403, 'train_ucc_loss': 0.53685, 'train_ucc_acc': 0.78125, 'loss': 0.59544}\n",
            "Step 79500: {'train_ae_loss': 0.66903, 'train_ucc_loss': 0.41637, 'train_ucc_acc': 0.90625, 'loss': 0.5427}\n",
            "Step 79520: {'train_ae_loss': 0.6631, 'train_ucc_loss': 0.50775, 'train_ucc_acc': 0.78125, 'loss': 0.58543}\n",
            "Step 79540: {'train_ae_loss': 0.67059, 'train_ucc_loss': 0.531, 'train_ucc_acc': 0.75, 'loss': 0.60079}\n",
            "Step 79560: {'train_ae_loss': 0.65447, 'train_ucc_loss': 0.43966, 'train_ucc_acc': 0.875, 'loss': 0.54707}\n",
            "Step 79580: {'train_ae_loss': 0.65696, 'train_ucc_loss': 0.3413, 'train_ucc_acc': 0.96875, 'loss': 0.49913}\n",
            "Step 79600: {'train_ae_loss': 0.68141, 'train_ucc_loss': 0.42359, 'train_ucc_acc': 0.90625, 'loss': 0.5525}\n",
            "Step 79620: {'train_ae_loss': 0.66841, 'train_ucc_loss': 0.54204, 'train_ucc_acc': 0.75, 'loss': 0.60522}\n",
            "Step 79640: {'train_ae_loss': 0.67344, 'train_ucc_loss': 0.53249, 'train_ucc_acc': 0.75, 'loss': 0.60296}\n",
            "Step 79660: {'train_ae_loss': 0.66806, 'train_ucc_loss': 0.47332, 'train_ucc_acc': 0.8125, 'loss': 0.57069}\n",
            "Step 79680: {'train_ae_loss': 0.68009, 'train_ucc_loss': 0.5076, 'train_ucc_acc': 0.78125, 'loss': 0.59385}\n",
            "Step 79700: {'train_ae_loss': 0.66886, 'train_ucc_loss': 0.43982, 'train_ucc_acc': 0.875, 'loss': 0.55434}\n",
            "Step 79720: {'train_ae_loss': 0.65983, 'train_ucc_loss': 0.38716, 'train_ucc_acc': 0.9375, 'loss': 0.52349}\n",
            "Step 79740: {'train_ae_loss': 0.6432, 'train_ucc_loss': 0.60557, 'train_ucc_acc': 0.6875, 'loss': 0.62438}\n",
            "Step 79760: {'train_ae_loss': 0.66534, 'train_ucc_loss': 0.53786, 'train_ucc_acc': 0.75, 'loss': 0.6016}\n",
            "Step 79780: {'train_ae_loss': 0.66438, 'train_ucc_loss': 0.44997, 'train_ucc_acc': 0.875, 'loss': 0.55717}\n",
            "Step 79800: {'train_ae_loss': 0.66036, 'train_ucc_loss': 0.45602, 'train_ucc_acc': 0.875, 'loss': 0.55819}\n",
            "Step 79820: {'train_ae_loss': 0.67342, 'train_ucc_loss': 0.36229, 'train_ucc_acc': 0.96875, 'loss': 0.51786}\n",
            "Step 79840: {'train_ae_loss': 0.656, 'train_ucc_loss': 0.54343, 'train_ucc_acc': 0.75, 'loss': 0.59971}\n",
            "Step 79860: {'train_ae_loss': 0.6825, 'train_ucc_loss': 0.42195, 'train_ucc_acc': 0.84375, 'loss': 0.55223}\n",
            "Step 79880: {'train_ae_loss': 0.69252, 'train_ucc_loss': 0.49444, 'train_ucc_acc': 0.8125, 'loss': 0.59348}\n",
            "Step 79900: {'train_ae_loss': 0.68871, 'train_ucc_loss': 0.39778, 'train_ucc_acc': 0.90625, 'loss': 0.54325}\n",
            "Step 79920: {'train_ae_loss': 0.64327, 'train_ucc_loss': 0.49946, 'train_ucc_acc': 0.8125, 'loss': 0.57136}\n",
            "Step 79940: {'train_ae_loss': 0.66515, 'train_ucc_loss': 0.6279, 'train_ucc_acc': 0.65625, 'loss': 0.64652}\n",
            "Step 79960: {'train_ae_loss': 0.66405, 'train_ucc_loss': 0.49845, 'train_ucc_acc': 0.8125, 'loss': 0.58125}\n",
            "Step 79980: {'train_ae_loss': 0.64027, 'train_ucc_loss': 0.57242, 'train_ucc_acc': 0.75, 'loss': 0.60635}\n",
            "Step 80000: {'train_ae_loss': 0.6693, 'train_ucc_loss': 0.43464, 'train_ucc_acc': 0.84375, 'loss': 0.55197}\n",
            "step: 80000,eval_ae_loss: 0.65848,eval_ucc_loss: 0.49652,eval_ucc_acc: 0.80762\n",
            "Step 80020: {'train_ae_loss': 0.67355, 'train_ucc_loss': 0.54264, 'train_ucc_acc': 0.75, 'loss': 0.6081}\n",
            "Step 80040: {'train_ae_loss': 0.6648, 'train_ucc_loss': 0.48958, 'train_ucc_acc': 0.84375, 'loss': 0.57719}\n",
            "Step 80060: {'train_ae_loss': 0.65333, 'train_ucc_loss': 0.49775, 'train_ucc_acc': 0.8125, 'loss': 0.57554}\n",
            "Step 80080: {'train_ae_loss': 0.66512, 'train_ucc_loss': 0.44632, 'train_ucc_acc': 0.875, 'loss': 0.55572}\n",
            "Step 80100: {'train_ae_loss': 0.66959, 'train_ucc_loss': 0.45859, 'train_ucc_acc': 0.8125, 'loss': 0.56409}\n",
            "Step 80120: {'train_ae_loss': 0.65593, 'train_ucc_loss': 0.54026, 'train_ucc_acc': 0.78125, 'loss': 0.59809}\n",
            "Step 80140: {'train_ae_loss': 0.65796, 'train_ucc_loss': 0.50408, 'train_ucc_acc': 0.8125, 'loss': 0.58102}\n",
            "Step 80160: {'train_ae_loss': 0.65874, 'train_ucc_loss': 0.42418, 'train_ucc_acc': 0.90625, 'loss': 0.54146}\n",
            "Step 80180: {'train_ae_loss': 0.67893, 'train_ucc_loss': 0.55651, 'train_ucc_acc': 0.75, 'loss': 0.61772}\n",
            "Step 80200: {'train_ae_loss': 0.65671, 'train_ucc_loss': 0.45153, 'train_ucc_acc': 0.8125, 'loss': 0.55412}\n",
            "Step 80220: {'train_ae_loss': 0.67291, 'train_ucc_loss': 0.48654, 'train_ucc_acc': 0.8125, 'loss': 0.57972}\n",
            "Step 80240: {'train_ae_loss': 0.67452, 'train_ucc_loss': 0.38726, 'train_ucc_acc': 0.9375, 'loss': 0.53089}\n",
            "Step 80260: {'train_ae_loss': 0.65518, 'train_ucc_loss': 0.47963, 'train_ucc_acc': 0.8125, 'loss': 0.56741}\n",
            "Step 80280: {'train_ae_loss': 0.67686, 'train_ucc_loss': 0.39848, 'train_ucc_acc': 0.9375, 'loss': 0.53767}\n",
            "Step 80300: {'train_ae_loss': 0.6637, 'train_ucc_loss': 0.51978, 'train_ucc_acc': 0.8125, 'loss': 0.59174}\n",
            "Step 80320: {'train_ae_loss': 0.66625, 'train_ucc_loss': 0.44333, 'train_ucc_acc': 0.875, 'loss': 0.55479}\n",
            "Step 80340: {'train_ae_loss': 0.65216, 'train_ucc_loss': 0.53701, 'train_ucc_acc': 0.75, 'loss': 0.59458}\n",
            "Step 80360: {'train_ae_loss': 0.67147, 'train_ucc_loss': 0.43197, 'train_ucc_acc': 0.90625, 'loss': 0.55172}\n",
            "Step 80380: {'train_ae_loss': 0.64786, 'train_ucc_loss': 0.40484, 'train_ucc_acc': 0.9375, 'loss': 0.52635}\n",
            "Step 80400: {'train_ae_loss': 0.67098, 'train_ucc_loss': 0.43736, 'train_ucc_acc': 0.875, 'loss': 0.55417}\n",
            "Step 80420: {'train_ae_loss': 0.68348, 'train_ucc_loss': 0.43319, 'train_ucc_acc': 0.875, 'loss': 0.55834}\n",
            "Step 80440: {'train_ae_loss': 0.66569, 'train_ucc_loss': 0.35768, 'train_ucc_acc': 0.96875, 'loss': 0.51169}\n",
            "Step 80460: {'train_ae_loss': 0.67894, 'train_ucc_loss': 0.46849, 'train_ucc_acc': 0.84375, 'loss': 0.57371}\n",
            "Step 80480: {'train_ae_loss': 0.67506, 'train_ucc_loss': 0.52012, 'train_ucc_acc': 0.75, 'loss': 0.59759}\n",
            "Step 80500: {'train_ae_loss': 0.66677, 'train_ucc_loss': 0.44126, 'train_ucc_acc': 0.875, 'loss': 0.55402}\n",
            "Step 80520: {'train_ae_loss': 0.66703, 'train_ucc_loss': 0.46781, 'train_ucc_acc': 0.8125, 'loss': 0.56742}\n",
            "Step 80540: {'train_ae_loss': 0.67317, 'train_ucc_loss': 0.3934, 'train_ucc_acc': 0.9375, 'loss': 0.53328}\n",
            "Step 80560: {'train_ae_loss': 0.67955, 'train_ucc_loss': 0.48668, 'train_ucc_acc': 0.84375, 'loss': 0.58312}\n",
            "Step 80580: {'train_ae_loss': 0.68953, 'train_ucc_loss': 0.42141, 'train_ucc_acc': 0.90625, 'loss': 0.55547}\n",
            "Step 80600: {'train_ae_loss': 0.67975, 'train_ucc_loss': 0.4922, 'train_ucc_acc': 0.8125, 'loss': 0.58598}\n",
            "Step 80620: {'train_ae_loss': 0.66392, 'train_ucc_loss': 0.45938, 'train_ucc_acc': 0.84375, 'loss': 0.56165}\n",
            "Step 80640: {'train_ae_loss': 0.66732, 'train_ucc_loss': 0.43267, 'train_ucc_acc': 0.875, 'loss': 0.55}\n",
            "Step 80660: {'train_ae_loss': 0.64857, 'train_ucc_loss': 0.46176, 'train_ucc_acc': 0.84375, 'loss': 0.55517}\n",
            "Step 80680: {'train_ae_loss': 0.66544, 'train_ucc_loss': 0.48634, 'train_ucc_acc': 0.8125, 'loss': 0.57589}\n",
            "Step 80700: {'train_ae_loss': 0.66887, 'train_ucc_loss': 0.3606, 'train_ucc_acc': 0.96875, 'loss': 0.51474}\n",
            "Step 80720: {'train_ae_loss': 0.66866, 'train_ucc_loss': 0.47429, 'train_ucc_acc': 0.84375, 'loss': 0.57148}\n",
            "Step 80740: {'train_ae_loss': 0.65973, 'train_ucc_loss': 0.39024, 'train_ucc_acc': 0.9375, 'loss': 0.52499}\n",
            "Step 80760: {'train_ae_loss': 0.67543, 'train_ucc_loss': 0.50575, 'train_ucc_acc': 0.78125, 'loss': 0.59059}\n",
            "Step 80780: {'train_ae_loss': 0.66386, 'train_ucc_loss': 0.45322, 'train_ucc_acc': 0.84375, 'loss': 0.55854}\n",
            "Step 80800: {'train_ae_loss': 0.66998, 'train_ucc_loss': 0.47052, 'train_ucc_acc': 0.84375, 'loss': 0.57025}\n",
            "Step 80820: {'train_ae_loss': 0.68622, 'train_ucc_loss': 0.43419, 'train_ucc_acc': 0.875, 'loss': 0.5602}\n",
            "Step 80840: {'train_ae_loss': 0.66076, 'train_ucc_loss': 0.47477, 'train_ucc_acc': 0.84375, 'loss': 0.56777}\n",
            "Step 80860: {'train_ae_loss': 0.69389, 'train_ucc_loss': 0.43871, 'train_ucc_acc': 0.875, 'loss': 0.5663}\n",
            "Step 80880: {'train_ae_loss': 0.67353, 'train_ucc_loss': 0.45911, 'train_ucc_acc': 0.8125, 'loss': 0.56632}\n",
            "Step 80900: {'train_ae_loss': 0.67201, 'train_ucc_loss': 0.51965, 'train_ucc_acc': 0.78125, 'loss': 0.59583}\n",
            "Step 80920: {'train_ae_loss': 0.67943, 'train_ucc_loss': 0.43643, 'train_ucc_acc': 0.875, 'loss': 0.55793}\n",
            "Step 80940: {'train_ae_loss': 0.66053, 'train_ucc_loss': 0.45429, 'train_ucc_acc': 0.84375, 'loss': 0.55741}\n",
            "Step 80960: {'train_ae_loss': 0.6836, 'train_ucc_loss': 0.55045, 'train_ucc_acc': 0.75, 'loss': 0.61702}\n",
            "Step 80980: {'train_ae_loss': 0.66998, 'train_ucc_loss': 0.50095, 'train_ucc_acc': 0.8125, 'loss': 0.58546}\n",
            "Step 81000: {'train_ae_loss': 0.68278, 'train_ucc_loss': 0.38206, 'train_ucc_acc': 0.96875, 'loss': 0.53242}\n",
            "step: 81000,eval_ae_loss: 0.66787,eval_ucc_loss: 0.49883,eval_ucc_acc: 0.80469\n",
            "Step 81020: {'train_ae_loss': 0.65368, 'train_ucc_loss': 0.52328, 'train_ucc_acc': 0.78125, 'loss': 0.58848}\n",
            "Step 81040: {'train_ae_loss': 0.68817, 'train_ucc_loss': 0.50214, 'train_ucc_acc': 0.8125, 'loss': 0.59516}\n",
            "Step 81060: {'train_ae_loss': 0.65817, 'train_ucc_loss': 0.47193, 'train_ucc_acc': 0.84375, 'loss': 0.56505}\n",
            "Step 81080: {'train_ae_loss': 0.68855, 'train_ucc_loss': 0.49986, 'train_ucc_acc': 0.8125, 'loss': 0.59421}\n",
            "Step 81100: {'train_ae_loss': 0.67885, 'train_ucc_loss': 0.3858, 'train_ucc_acc': 0.9375, 'loss': 0.53232}\n",
            "Step 81120: {'train_ae_loss': 0.68719, 'train_ucc_loss': 0.46651, 'train_ucc_acc': 0.84375, 'loss': 0.57685}\n",
            "Step 81140: {'train_ae_loss': 0.65693, 'train_ucc_loss': 0.5554, 'train_ucc_acc': 0.71875, 'loss': 0.60617}\n",
            "Step 81160: {'train_ae_loss': 0.672, 'train_ucc_loss': 0.41016, 'train_ucc_acc': 0.90625, 'loss': 0.54108}\n",
            "Step 81180: {'train_ae_loss': 0.65608, 'train_ucc_loss': 0.47861, 'train_ucc_acc': 0.84375, 'loss': 0.56735}\n",
            "Step 81200: {'train_ae_loss': 0.66475, 'train_ucc_loss': 0.41627, 'train_ucc_acc': 0.90625, 'loss': 0.54051}\n",
            "Step 81220: {'train_ae_loss': 0.67978, 'train_ucc_loss': 0.45624, 'train_ucc_acc': 0.84375, 'loss': 0.56801}\n",
            "Step 81240: {'train_ae_loss': 0.66387, 'train_ucc_loss': 0.39149, 'train_ucc_acc': 0.90625, 'loss': 0.52768}\n",
            "Step 81260: {'train_ae_loss': 0.6802, 'train_ucc_loss': 0.54683, 'train_ucc_acc': 0.75, 'loss': 0.61351}\n",
            "Step 81280: {'train_ae_loss': 0.6659, 'train_ucc_loss': 0.40747, 'train_ucc_acc': 0.90625, 'loss': 0.53668}\n",
            "Step 81300: {'train_ae_loss': 0.66225, 'train_ucc_loss': 0.54636, 'train_ucc_acc': 0.78125, 'loss': 0.60431}\n",
            "Step 81320: {'train_ae_loss': 0.65446, 'train_ucc_loss': 0.53888, 'train_ucc_acc': 0.71875, 'loss': 0.59667}\n",
            "Step 81340: {'train_ae_loss': 0.6728, 'train_ucc_loss': 0.44183, 'train_ucc_acc': 0.84375, 'loss': 0.55732}\n",
            "Step 81360: {'train_ae_loss': 0.67785, 'train_ucc_loss': 0.41959, 'train_ucc_acc': 0.875, 'loss': 0.54872}\n",
            "Step 81380: {'train_ae_loss': 0.64635, 'train_ucc_loss': 0.41172, 'train_ucc_acc': 0.90625, 'loss': 0.52903}\n",
            "Step 81400: {'train_ae_loss': 0.67009, 'train_ucc_loss': 0.43285, 'train_ucc_acc': 0.84375, 'loss': 0.55147}\n",
            "Step 81420: {'train_ae_loss': 0.66603, 'train_ucc_loss': 0.4013, 'train_ucc_acc': 0.9375, 'loss': 0.53366}\n",
            "Step 81440: {'train_ae_loss': 0.6643, 'train_ucc_loss': 0.45685, 'train_ucc_acc': 0.84375, 'loss': 0.56057}\n",
            "Step 81460: {'train_ae_loss': 0.64802, 'train_ucc_loss': 0.48986, 'train_ucc_acc': 0.8125, 'loss': 0.56894}\n",
            "Step 81480: {'train_ae_loss': 0.69856, 'train_ucc_loss': 0.41293, 'train_ucc_acc': 0.90625, 'loss': 0.55574}\n",
            "Step 81500: {'train_ae_loss': 0.67283, 'train_ucc_loss': 0.56287, 'train_ucc_acc': 0.75, 'loss': 0.61785}\n",
            "Step 81520: {'train_ae_loss': 0.69178, 'train_ucc_loss': 0.38949, 'train_ucc_acc': 0.90625, 'loss': 0.54064}\n",
            "Step 81540: {'train_ae_loss': 0.67414, 'train_ucc_loss': 0.48172, 'train_ucc_acc': 0.8125, 'loss': 0.57793}\n",
            "Step 81560: {'train_ae_loss': 0.70537, 'train_ucc_loss': 0.49082, 'train_ucc_acc': 0.8125, 'loss': 0.5981}\n",
            "Step 81580: {'train_ae_loss': 0.66685, 'train_ucc_loss': 0.43395, 'train_ucc_acc': 0.875, 'loss': 0.5504}\n",
            "Step 81600: {'train_ae_loss': 0.65598, 'train_ucc_loss': 0.49729, 'train_ucc_acc': 0.8125, 'loss': 0.57664}\n",
            "Step 81620: {'train_ae_loss': 0.66914, 'train_ucc_loss': 0.38413, 'train_ucc_acc': 0.9375, 'loss': 0.52664}\n",
            "Step 81640: {'train_ae_loss': 0.66501, 'train_ucc_loss': 0.48659, 'train_ucc_acc': 0.8125, 'loss': 0.5758}\n",
            "Step 81660: {'train_ae_loss': 0.67383, 'train_ucc_loss': 0.3794, 'train_ucc_acc': 0.96875, 'loss': 0.52661}\n",
            "Step 81680: {'train_ae_loss': 0.65615, 'train_ucc_loss': 0.38752, 'train_ucc_acc': 0.9375, 'loss': 0.52183}\n",
            "Step 81700: {'train_ae_loss': 0.65483, 'train_ucc_loss': 0.46876, 'train_ucc_acc': 0.84375, 'loss': 0.5618}\n",
            "Step 81720: {'train_ae_loss': 0.66873, 'train_ucc_loss': 0.42693, 'train_ucc_acc': 0.875, 'loss': 0.54783}\n",
            "Step 81740: {'train_ae_loss': 0.63984, 'train_ucc_loss': 0.46692, 'train_ucc_acc': 0.84375, 'loss': 0.55338}\n",
            "Step 81760: {'train_ae_loss': 0.67249, 'train_ucc_loss': 0.54231, 'train_ucc_acc': 0.75, 'loss': 0.6074}\n",
            "Step 81780: {'train_ae_loss': 0.65342, 'train_ucc_loss': 0.45926, 'train_ucc_acc': 0.84375, 'loss': 0.55634}\n",
            "Step 81800: {'train_ae_loss': 0.68117, 'train_ucc_loss': 0.52195, 'train_ucc_acc': 0.78125, 'loss': 0.60156}\n",
            "Step 81820: {'train_ae_loss': 0.66896, 'train_ucc_loss': 0.6685, 'train_ucc_acc': 0.625, 'loss': 0.66873}\n",
            "Step 81840: {'train_ae_loss': 0.66821, 'train_ucc_loss': 0.40978, 'train_ucc_acc': 0.90625, 'loss': 0.539}\n",
            "Step 81860: {'train_ae_loss': 0.67155, 'train_ucc_loss': 0.34998, 'train_ucc_acc': 0.96875, 'loss': 0.51077}\n",
            "Step 81880: {'train_ae_loss': 0.65765, 'train_ucc_loss': 0.44601, 'train_ucc_acc': 0.84375, 'loss': 0.55183}\n",
            "Step 81900: {'train_ae_loss': 0.6583, 'train_ucc_loss': 0.39065, 'train_ucc_acc': 0.9375, 'loss': 0.52447}\n",
            "Step 81920: {'train_ae_loss': 0.65547, 'train_ucc_loss': 0.51859, 'train_ucc_acc': 0.78125, 'loss': 0.58703}\n",
            "Step 81940: {'train_ae_loss': 0.66036, 'train_ucc_loss': 0.40004, 'train_ucc_acc': 0.90625, 'loss': 0.5302}\n",
            "Step 81960: {'train_ae_loss': 0.67727, 'train_ucc_loss': 0.45857, 'train_ucc_acc': 0.84375, 'loss': 0.56792}\n",
            "Step 81980: {'train_ae_loss': 0.65263, 'train_ucc_loss': 0.51413, 'train_ucc_acc': 0.78125, 'loss': 0.58338}\n",
            "Step 82000: {'train_ae_loss': 0.66491, 'train_ucc_loss': 0.52602, 'train_ucc_acc': 0.78125, 'loss': 0.59547}\n",
            "step: 82000,eval_ae_loss: 0.65849,eval_ucc_loss: 0.49514,eval_ucc_acc: 0.80957\n",
            "Step 82020: {'train_ae_loss': 0.66022, 'train_ucc_loss': 0.37817, 'train_ucc_acc': 0.9375, 'loss': 0.5192}\n",
            "Step 82040: {'train_ae_loss': 0.67133, 'train_ucc_loss': 0.42068, 'train_ucc_acc': 0.875, 'loss': 0.546}\n",
            "Step 82060: {'train_ae_loss': 0.64176, 'train_ucc_loss': 0.63371, 'train_ucc_acc': 0.65625, 'loss': 0.63773}\n",
            "Step 82080: {'train_ae_loss': 0.67918, 'train_ucc_loss': 0.43011, 'train_ucc_acc': 0.90625, 'loss': 0.55465}\n",
            "Step 82100: {'train_ae_loss': 0.68351, 'train_ucc_loss': 0.40749, 'train_ucc_acc': 0.875, 'loss': 0.5455}\n",
            "Step 82120: {'train_ae_loss': 0.66522, 'train_ucc_loss': 0.49134, 'train_ucc_acc': 0.8125, 'loss': 0.57828}\n",
            "Step 82140: {'train_ae_loss': 0.67493, 'train_ucc_loss': 0.46639, 'train_ucc_acc': 0.8125, 'loss': 0.57066}\n",
            "Step 82160: {'train_ae_loss': 0.65956, 'train_ucc_loss': 0.52702, 'train_ucc_acc': 0.78125, 'loss': 0.59329}\n",
            "Step 82180: {'train_ae_loss': 0.68435, 'train_ucc_loss': 0.45584, 'train_ucc_acc': 0.875, 'loss': 0.5701}\n",
            "Step 82200: {'train_ae_loss': 0.65752, 'train_ucc_loss': 0.47057, 'train_ucc_acc': 0.84375, 'loss': 0.56404}\n",
            "Step 82220: {'train_ae_loss': 0.66534, 'train_ucc_loss': 0.52632, 'train_ucc_acc': 0.75, 'loss': 0.59583}\n",
            "Step 82240: {'train_ae_loss': 0.66768, 'train_ucc_loss': 0.44199, 'train_ucc_acc': 0.875, 'loss': 0.55483}\n",
            "Step 82260: {'train_ae_loss': 0.68221, 'train_ucc_loss': 0.42928, 'train_ucc_acc': 0.84375, 'loss': 0.55574}\n",
            "Step 82280: {'train_ae_loss': 0.65517, 'train_ucc_loss': 0.51419, 'train_ucc_acc': 0.8125, 'loss': 0.58468}\n",
            "Step 82300: {'train_ae_loss': 0.67983, 'train_ucc_loss': 0.39092, 'train_ucc_acc': 0.90625, 'loss': 0.53537}\n",
            "Step 82320: {'train_ae_loss': 0.66751, 'train_ucc_loss': 0.48739, 'train_ucc_acc': 0.8125, 'loss': 0.57745}\n",
            "Step 82340: {'train_ae_loss': 0.66516, 'train_ucc_loss': 0.39479, 'train_ucc_acc': 0.90625, 'loss': 0.52998}\n",
            "Step 82360: {'train_ae_loss': 0.67519, 'train_ucc_loss': 0.45475, 'train_ucc_acc': 0.875, 'loss': 0.56497}\n",
            "Step 82380: {'train_ae_loss': 0.67056, 'train_ucc_loss': 0.4971, 'train_ucc_acc': 0.8125, 'loss': 0.58383}\n",
            "Step 82400: {'train_ae_loss': 0.6649, 'train_ucc_loss': 0.44292, 'train_ucc_acc': 0.875, 'loss': 0.55391}\n",
            "Step 82420: {'train_ae_loss': 0.6639, 'train_ucc_loss': 0.57224, 'train_ucc_acc': 0.71875, 'loss': 0.61807}\n",
            "Step 82440: {'train_ae_loss': 0.66303, 'train_ucc_loss': 0.49093, 'train_ucc_acc': 0.84375, 'loss': 0.57698}\n",
            "Step 82460: {'train_ae_loss': 0.66458, 'train_ucc_loss': 0.4516, 'train_ucc_acc': 0.84375, 'loss': 0.55809}\n",
            "Step 82480: {'train_ae_loss': 0.67837, 'train_ucc_loss': 0.46031, 'train_ucc_acc': 0.84375, 'loss': 0.56934}\n",
            "Step 82500: {'train_ae_loss': 0.64481, 'train_ucc_loss': 0.53125, 'train_ucc_acc': 0.78125, 'loss': 0.58803}\n",
            "Step 82520: {'train_ae_loss': 0.66437, 'train_ucc_loss': 0.47402, 'train_ucc_acc': 0.8125, 'loss': 0.56919}\n",
            "Step 82540: {'train_ae_loss': 0.64543, 'train_ucc_loss': 0.52214, 'train_ucc_acc': 0.78125, 'loss': 0.58379}\n",
            "Step 82560: {'train_ae_loss': 0.66233, 'train_ucc_loss': 0.39302, 'train_ucc_acc': 0.90625, 'loss': 0.52768}\n",
            "Step 82580: {'train_ae_loss': 0.66398, 'train_ucc_loss': 0.47239, 'train_ucc_acc': 0.8125, 'loss': 0.56819}\n",
            "Step 82600: {'train_ae_loss': 0.67184, 'train_ucc_loss': 0.47758, 'train_ucc_acc': 0.8125, 'loss': 0.57471}\n",
            "Step 82620: {'train_ae_loss': 0.67972, 'train_ucc_loss': 0.47141, 'train_ucc_acc': 0.8125, 'loss': 0.57556}\n",
            "Step 82640: {'train_ae_loss': 0.66271, 'train_ucc_loss': 0.38599, 'train_ucc_acc': 0.9375, 'loss': 0.52435}\n",
            "Step 82660: {'train_ae_loss': 0.63854, 'train_ucc_loss': 0.36333, 'train_ucc_acc': 0.96875, 'loss': 0.50093}\n",
            "Step 82680: {'train_ae_loss': 0.67841, 'train_ucc_loss': 0.47281, 'train_ucc_acc': 0.8125, 'loss': 0.57561}\n",
            "Step 82700: {'train_ae_loss': 0.65969, 'train_ucc_loss': 0.53969, 'train_ucc_acc': 0.78125, 'loss': 0.59969}\n",
            "Step 82720: {'train_ae_loss': 0.6695, 'train_ucc_loss': 0.3979, 'train_ucc_acc': 0.90625, 'loss': 0.5337}\n",
            "Step 82740: {'train_ae_loss': 0.66533, 'train_ucc_loss': 0.35196, 'train_ucc_acc': 0.96875, 'loss': 0.50864}\n",
            "Step 82760: {'train_ae_loss': 0.6719, 'train_ucc_loss': 0.51511, 'train_ucc_acc': 0.78125, 'loss': 0.59351}\n",
            "Step 82780: {'train_ae_loss': 0.67348, 'train_ucc_loss': 0.41536, 'train_ucc_acc': 0.90625, 'loss': 0.54442}\n",
            "Step 82800: {'train_ae_loss': 0.68171, 'train_ucc_loss': 0.40669, 'train_ucc_acc': 0.90625, 'loss': 0.5442}\n",
            "Step 82820: {'train_ae_loss': 0.66001, 'train_ucc_loss': 0.34661, 'train_ucc_acc': 0.96875, 'loss': 0.50331}\n",
            "Step 82840: {'train_ae_loss': 0.66983, 'train_ucc_loss': 0.54892, 'train_ucc_acc': 0.75, 'loss': 0.60937}\n",
            "Step 82860: {'train_ae_loss': 0.67047, 'train_ucc_loss': 0.42481, 'train_ucc_acc': 0.875, 'loss': 0.54764}\n",
            "Step 82880: {'train_ae_loss': 0.6835, 'train_ucc_loss': 0.52163, 'train_ucc_acc': 0.78125, 'loss': 0.60256}\n",
            "Step 82900: {'train_ae_loss': 0.64542, 'train_ucc_loss': 0.48127, 'train_ucc_acc': 0.8125, 'loss': 0.56335}\n",
            "Step 82920: {'train_ae_loss': 0.6668, 'train_ucc_loss': 0.34442, 'train_ucc_acc': 0.96875, 'loss': 0.50561}\n",
            "Step 82940: {'train_ae_loss': 0.65474, 'train_ucc_loss': 0.51059, 'train_ucc_acc': 0.78125, 'loss': 0.58266}\n",
            "Step 82960: {'train_ae_loss': 0.65101, 'train_ucc_loss': 0.50068, 'train_ucc_acc': 0.8125, 'loss': 0.57584}\n",
            "Step 82980: {'train_ae_loss': 0.64887, 'train_ucc_loss': 0.52318, 'train_ucc_acc': 0.78125, 'loss': 0.58603}\n",
            "Step 83000: {'train_ae_loss': 0.67686, 'train_ucc_loss': 0.45459, 'train_ucc_acc': 0.84375, 'loss': 0.56573}\n",
            "step: 83000,eval_ae_loss: 0.66201,eval_ucc_loss: 0.47854,eval_ucc_acc: 0.82617\n",
            "Step 83020: {'train_ae_loss': 0.6626, 'train_ucc_loss': 0.36977, 'train_ucc_acc': 0.9375, 'loss': 0.51619}\n",
            "Step 83040: {'train_ae_loss': 0.67289, 'train_ucc_loss': 0.46827, 'train_ucc_acc': 0.84375, 'loss': 0.57058}\n",
            "Step 83060: {'train_ae_loss': 0.67317, 'train_ucc_loss': 0.42919, 'train_ucc_acc': 0.875, 'loss': 0.55118}\n",
            "Step 83080: {'train_ae_loss': 0.65358, 'train_ucc_loss': 0.46239, 'train_ucc_acc': 0.875, 'loss': 0.55799}\n",
            "Step 83100: {'train_ae_loss': 0.65274, 'train_ucc_loss': 0.44879, 'train_ucc_acc': 0.875, 'loss': 0.55076}\n",
            "Step 83120: {'train_ae_loss': 0.66283, 'train_ucc_loss': 0.48055, 'train_ucc_acc': 0.84375, 'loss': 0.57169}\n",
            "Step 83140: {'train_ae_loss': 0.65955, 'train_ucc_loss': 0.54405, 'train_ucc_acc': 0.75, 'loss': 0.6018}\n",
            "Step 83160: {'train_ae_loss': 0.65867, 'train_ucc_loss': 0.48805, 'train_ucc_acc': 0.78125, 'loss': 0.57336}\n",
            "Step 83180: {'train_ae_loss': 0.67356, 'train_ucc_loss': 0.46971, 'train_ucc_acc': 0.84375, 'loss': 0.57164}\n",
            "Step 83200: {'train_ae_loss': 0.65614, 'train_ucc_loss': 0.51205, 'train_ucc_acc': 0.78125, 'loss': 0.5841}\n",
            "Step 83220: {'train_ae_loss': 0.66329, 'train_ucc_loss': 0.43141, 'train_ucc_acc': 0.875, 'loss': 0.54735}\n",
            "Step 83240: {'train_ae_loss': 0.65307, 'train_ucc_loss': 0.50661, 'train_ucc_acc': 0.78125, 'loss': 0.57984}\n",
            "Step 83260: {'train_ae_loss': 0.64932, 'train_ucc_loss': 0.53551, 'train_ucc_acc': 0.75, 'loss': 0.59242}\n",
            "Step 83280: {'train_ae_loss': 0.67658, 'train_ucc_loss': 0.5098, 'train_ucc_acc': 0.8125, 'loss': 0.59319}\n",
            "Step 83300: {'train_ae_loss': 0.65868, 'train_ucc_loss': 0.53076, 'train_ucc_acc': 0.78125, 'loss': 0.59472}\n",
            "Step 83320: {'train_ae_loss': 0.68271, 'train_ucc_loss': 0.40428, 'train_ucc_acc': 0.90625, 'loss': 0.54349}\n",
            "Step 83340: {'train_ae_loss': 0.66082, 'train_ucc_loss': 0.45788, 'train_ucc_acc': 0.875, 'loss': 0.55935}\n",
            "Step 83360: {'train_ae_loss': 0.66054, 'train_ucc_loss': 0.40656, 'train_ucc_acc': 0.9375, 'loss': 0.53355}\n",
            "Step 83380: {'train_ae_loss': 0.66588, 'train_ucc_loss': 0.44748, 'train_ucc_acc': 0.875, 'loss': 0.55668}\n",
            "Step 83400: {'train_ae_loss': 0.63143, 'train_ucc_loss': 0.53241, 'train_ucc_acc': 0.8125, 'loss': 0.58192}\n",
            "Step 83420: {'train_ae_loss': 0.666, 'train_ucc_loss': 0.47358, 'train_ucc_acc': 0.8125, 'loss': 0.56979}\n",
            "Step 83440: {'train_ae_loss': 0.64883, 'train_ucc_loss': 0.49593, 'train_ucc_acc': 0.8125, 'loss': 0.57238}\n",
            "Step 83460: {'train_ae_loss': 0.65891, 'train_ucc_loss': 0.47036, 'train_ucc_acc': 0.84375, 'loss': 0.56464}\n",
            "Step 83480: {'train_ae_loss': 0.66207, 'train_ucc_loss': 0.43624, 'train_ucc_acc': 0.84375, 'loss': 0.54916}\n",
            "Step 83500: {'train_ae_loss': 0.66503, 'train_ucc_loss': 0.49025, 'train_ucc_acc': 0.8125, 'loss': 0.57764}\n",
            "Step 83520: {'train_ae_loss': 0.66944, 'train_ucc_loss': 0.51556, 'train_ucc_acc': 0.78125, 'loss': 0.5925}\n",
            "Step 83540: {'train_ae_loss': 0.66135, 'train_ucc_loss': 0.39174, 'train_ucc_acc': 0.9375, 'loss': 0.52654}\n",
            "Step 83560: {'train_ae_loss': 0.65608, 'train_ucc_loss': 0.41134, 'train_ucc_acc': 0.90625, 'loss': 0.53371}\n",
            "Step 83580: {'train_ae_loss': 0.65291, 'train_ucc_loss': 0.47579, 'train_ucc_acc': 0.84375, 'loss': 0.56435}\n",
            "Step 83600: {'train_ae_loss': 0.68411, 'train_ucc_loss': 0.5519, 'train_ucc_acc': 0.75, 'loss': 0.618}\n",
            "Step 83620: {'train_ae_loss': 0.67936, 'train_ucc_loss': 0.49385, 'train_ucc_acc': 0.8125, 'loss': 0.58661}\n",
            "Step 83640: {'train_ae_loss': 0.67173, 'train_ucc_loss': 0.45711, 'train_ucc_acc': 0.84375, 'loss': 0.56442}\n",
            "Step 83660: {'train_ae_loss': 0.6676, 'train_ucc_loss': 0.3976, 'train_ucc_acc': 0.90625, 'loss': 0.5326}\n",
            "Step 83680: {'train_ae_loss': 0.65876, 'train_ucc_loss': 0.44529, 'train_ucc_acc': 0.90625, 'loss': 0.55202}\n",
            "Step 83700: {'train_ae_loss': 0.66749, 'train_ucc_loss': 0.52175, 'train_ucc_acc': 0.78125, 'loss': 0.59462}\n",
            "Step 83720: {'train_ae_loss': 0.68844, 'train_ucc_loss': 0.38302, 'train_ucc_acc': 0.90625, 'loss': 0.53573}\n",
            "Step 83740: {'train_ae_loss': 0.66697, 'train_ucc_loss': 0.40037, 'train_ucc_acc': 0.90625, 'loss': 0.53367}\n",
            "Step 83760: {'train_ae_loss': 0.67257, 'train_ucc_loss': 0.41242, 'train_ucc_acc': 0.90625, 'loss': 0.54249}\n",
            "Step 83780: {'train_ae_loss': 0.66747, 'train_ucc_loss': 0.3758, 'train_ucc_acc': 0.9375, 'loss': 0.52163}\n",
            "Step 83800: {'train_ae_loss': 0.67289, 'train_ucc_loss': 0.35828, 'train_ucc_acc': 0.96875, 'loss': 0.51559}\n",
            "Step 83820: {'train_ae_loss': 0.66823, 'train_ucc_loss': 0.48203, 'train_ucc_acc': 0.84375, 'loss': 0.57513}\n",
            "Step 83840: {'train_ae_loss': 0.65294, 'train_ucc_loss': 0.44289, 'train_ucc_acc': 0.875, 'loss': 0.54792}\n",
            "Step 83860: {'train_ae_loss': 0.67906, 'train_ucc_loss': 0.43485, 'train_ucc_acc': 0.875, 'loss': 0.55696}\n",
            "Step 83880: {'train_ae_loss': 0.6628, 'train_ucc_loss': 0.44077, 'train_ucc_acc': 0.84375, 'loss': 0.55179}\n",
            "Step 83900: {'train_ae_loss': 0.68401, 'train_ucc_loss': 0.38646, 'train_ucc_acc': 0.9375, 'loss': 0.53524}\n",
            "Step 83920: {'train_ae_loss': 0.67587, 'train_ucc_loss': 0.4099, 'train_ucc_acc': 0.90625, 'loss': 0.54288}\n",
            "Step 83940: {'train_ae_loss': 0.66086, 'train_ucc_loss': 0.34396, 'train_ucc_acc': 0.96875, 'loss': 0.50241}\n",
            "Step 83960: {'train_ae_loss': 0.64413, 'train_ucc_loss': 0.44337, 'train_ucc_acc': 0.875, 'loss': 0.54375}\n",
            "Step 83980: {'train_ae_loss': 0.66834, 'train_ucc_loss': 0.41439, 'train_ucc_acc': 0.875, 'loss': 0.54136}\n",
            "Step 84000: {'train_ae_loss': 0.67655, 'train_ucc_loss': 0.46591, 'train_ucc_acc': 0.84375, 'loss': 0.57123}\n",
            "step: 84000,eval_ae_loss: 0.66024,eval_ucc_loss: 0.5046,eval_ucc_acc: 0.7998\n",
            "Step 84020: {'train_ae_loss': 0.66159, 'train_ucc_loss': 0.50663, 'train_ucc_acc': 0.78125, 'loss': 0.58411}\n",
            "Step 84040: {'train_ae_loss': 0.67897, 'train_ucc_loss': 0.40976, 'train_ucc_acc': 0.9375, 'loss': 0.54437}\n",
            "Step 84060: {'train_ae_loss': 0.66556, 'train_ucc_loss': 0.45088, 'train_ucc_acc': 0.875, 'loss': 0.55822}\n",
            "Step 84080: {'train_ae_loss': 0.65462, 'train_ucc_loss': 0.50363, 'train_ucc_acc': 0.78125, 'loss': 0.57912}\n",
            "Step 84100: {'train_ae_loss': 0.64944, 'train_ucc_loss': 0.78592, 'train_ucc_acc': 0.5, 'loss': 0.71768}\n",
            "Step 84120: {'train_ae_loss': 0.67397, 'train_ucc_loss': 0.44391, 'train_ucc_acc': 0.875, 'loss': 0.55894}\n",
            "Step 84140: {'train_ae_loss': 0.66703, 'train_ucc_loss': 0.43288, 'train_ucc_acc': 0.84375, 'loss': 0.54995}\n",
            "Step 84160: {'train_ae_loss': 0.67268, 'train_ucc_loss': 0.37998, 'train_ucc_acc': 0.9375, 'loss': 0.52633}\n",
            "Step 84180: {'train_ae_loss': 0.66448, 'train_ucc_loss': 0.41288, 'train_ucc_acc': 0.90625, 'loss': 0.53868}\n",
            "Step 84200: {'train_ae_loss': 0.66353, 'train_ucc_loss': 0.42116, 'train_ucc_acc': 0.875, 'loss': 0.54234}\n",
            "Step 84220: {'train_ae_loss': 0.65803, 'train_ucc_loss': 0.52656, 'train_ucc_acc': 0.75, 'loss': 0.5923}\n",
            "Step 84240: {'train_ae_loss': 0.66217, 'train_ucc_loss': 0.39532, 'train_ucc_acc': 0.9375, 'loss': 0.52875}\n",
            "Step 84260: {'train_ae_loss': 0.66709, 'train_ucc_loss': 0.48665, 'train_ucc_acc': 0.8125, 'loss': 0.57687}\n",
            "Step 84280: {'train_ae_loss': 0.67261, 'train_ucc_loss': 0.41953, 'train_ucc_acc': 0.90625, 'loss': 0.54607}\n",
            "Step 84300: {'train_ae_loss': 0.6755, 'train_ucc_loss': 0.48489, 'train_ucc_acc': 0.84375, 'loss': 0.58019}\n",
            "Step 84320: {'train_ae_loss': 0.67811, 'train_ucc_loss': 0.36005, 'train_ucc_acc': 0.96875, 'loss': 0.51908}\n",
            "Step 84340: {'train_ae_loss': 0.67225, 'train_ucc_loss': 0.45731, 'train_ucc_acc': 0.84375, 'loss': 0.56478}\n",
            "Step 84360: {'train_ae_loss': 0.65611, 'train_ucc_loss': 0.52914, 'train_ucc_acc': 0.78125, 'loss': 0.59262}\n",
            "Step 84380: {'train_ae_loss': 0.68157, 'train_ucc_loss': 0.43901, 'train_ucc_acc': 0.875, 'loss': 0.56029}\n",
            "Step 84400: {'train_ae_loss': 0.66688, 'train_ucc_loss': 0.42735, 'train_ucc_acc': 0.875, 'loss': 0.54711}\n",
            "Step 84420: {'train_ae_loss': 0.65592, 'train_ucc_loss': 0.48316, 'train_ucc_acc': 0.84375, 'loss': 0.56954}\n",
            "Step 84440: {'train_ae_loss': 0.67068, 'train_ucc_loss': 0.37822, 'train_ucc_acc': 0.9375, 'loss': 0.52445}\n",
            "Step 84460: {'train_ae_loss': 0.65776, 'train_ucc_loss': 0.43521, 'train_ucc_acc': 0.875, 'loss': 0.54649}\n",
            "Step 84480: {'train_ae_loss': 0.6686, 'train_ucc_loss': 0.45797, 'train_ucc_acc': 0.84375, 'loss': 0.56328}\n",
            "Step 84500: {'train_ae_loss': 0.66768, 'train_ucc_loss': 0.56853, 'train_ucc_acc': 0.75, 'loss': 0.6181}\n",
            "Step 84520: {'train_ae_loss': 0.65587, 'train_ucc_loss': 0.47989, 'train_ucc_acc': 0.84375, 'loss': 0.56788}\n",
            "Step 84540: {'train_ae_loss': 0.65594, 'train_ucc_loss': 0.43338, 'train_ucc_acc': 0.875, 'loss': 0.54466}\n",
            "Step 84560: {'train_ae_loss': 0.67526, 'train_ucc_loss': 0.4145, 'train_ucc_acc': 0.875, 'loss': 0.54488}\n",
            "Step 84580: {'train_ae_loss': 0.67753, 'train_ucc_loss': 0.44272, 'train_ucc_acc': 0.875, 'loss': 0.56013}\n",
            "Step 84600: {'train_ae_loss': 0.68257, 'train_ucc_loss': 0.44359, 'train_ucc_acc': 0.875, 'loss': 0.56308}\n",
            "Step 84620: {'train_ae_loss': 0.6744, 'train_ucc_loss': 0.51329, 'train_ucc_acc': 0.8125, 'loss': 0.59385}\n",
            "Step 84640: {'train_ae_loss': 0.67744, 'train_ucc_loss': 0.40565, 'train_ucc_acc': 0.90625, 'loss': 0.54154}\n",
            "Step 84660: {'train_ae_loss': 0.66677, 'train_ucc_loss': 0.45051, 'train_ucc_acc': 0.84375, 'loss': 0.55864}\n",
            "Step 84680: {'train_ae_loss': 0.67748, 'train_ucc_loss': 0.41746, 'train_ucc_acc': 0.90625, 'loss': 0.54747}\n",
            "Step 84700: {'train_ae_loss': 0.65891, 'train_ucc_loss': 0.45844, 'train_ucc_acc': 0.84375, 'loss': 0.55868}\n",
            "Step 84720: {'train_ae_loss': 0.65091, 'train_ucc_loss': 0.47836, 'train_ucc_acc': 0.8125, 'loss': 0.56464}\n",
            "Step 84740: {'train_ae_loss': 0.68051, 'train_ucc_loss': 0.41154, 'train_ucc_acc': 0.90625, 'loss': 0.54603}\n",
            "Step 84760: {'train_ae_loss': 0.66687, 'train_ucc_loss': 0.43712, 'train_ucc_acc': 0.875, 'loss': 0.55199}\n",
            "Step 84780: {'train_ae_loss': 0.66537, 'train_ucc_loss': 0.40854, 'train_ucc_acc': 0.90625, 'loss': 0.53695}\n",
            "Step 84800: {'train_ae_loss': 0.65631, 'train_ucc_loss': 0.47607, 'train_ucc_acc': 0.8125, 'loss': 0.56619}\n",
            "Step 84820: {'train_ae_loss': 0.65755, 'train_ucc_loss': 0.42852, 'train_ucc_acc': 0.875, 'loss': 0.54303}\n",
            "Step 84840: {'train_ae_loss': 0.66436, 'train_ucc_loss': 0.44711, 'train_ucc_acc': 0.84375, 'loss': 0.55574}\n",
            "Step 84860: {'train_ae_loss': 0.66453, 'train_ucc_loss': 0.43296, 'train_ucc_acc': 0.875, 'loss': 0.54874}\n",
            "Step 84880: {'train_ae_loss': 0.65485, 'train_ucc_loss': 0.45129, 'train_ucc_acc': 0.875, 'loss': 0.55307}\n",
            "Step 84900: {'train_ae_loss': 0.66657, 'train_ucc_loss': 0.36208, 'train_ucc_acc': 0.96875, 'loss': 0.51432}\n",
            "Step 84920: {'train_ae_loss': 0.66094, 'train_ucc_loss': 0.43727, 'train_ucc_acc': 0.84375, 'loss': 0.54911}\n",
            "Step 84940: {'train_ae_loss': 0.6603, 'train_ucc_loss': 0.40813, 'train_ucc_acc': 0.875, 'loss': 0.53421}\n",
            "Step 84960: {'train_ae_loss': 0.67664, 'train_ucc_loss': 0.40617, 'train_ucc_acc': 0.875, 'loss': 0.5414}\n",
            "Step 84980: {'train_ae_loss': 0.67509, 'train_ucc_loss': 0.46355, 'train_ucc_acc': 0.84375, 'loss': 0.56932}\n",
            "Step 85000: {'train_ae_loss': 0.64743, 'train_ucc_loss': 0.46804, 'train_ucc_acc': 0.84375, 'loss': 0.55773}\n",
            "step: 85000,eval_ae_loss: 0.66384,eval_ucc_loss: 0.48573,eval_ucc_acc: 0.82324\n",
            "Step 85020: {'train_ae_loss': 0.66841, 'train_ucc_loss': 0.47515, 'train_ucc_acc': 0.84375, 'loss': 0.57178}\n",
            "Step 85040: {'train_ae_loss': 0.6707, 'train_ucc_loss': 0.4319, 'train_ucc_acc': 0.90625, 'loss': 0.5513}\n",
            "Step 85060: {'train_ae_loss': 0.66686, 'train_ucc_loss': 0.49458, 'train_ucc_acc': 0.84375, 'loss': 0.58072}\n",
            "Step 85080: {'train_ae_loss': 0.66265, 'train_ucc_loss': 0.38874, 'train_ucc_acc': 0.9375, 'loss': 0.5257}\n",
            "Step 85100: {'train_ae_loss': 0.66684, 'train_ucc_loss': 0.44841, 'train_ucc_acc': 0.875, 'loss': 0.55762}\n",
            "Step 85120: {'train_ae_loss': 0.68527, 'train_ucc_loss': 0.45641, 'train_ucc_acc': 0.84375, 'loss': 0.57084}\n",
            "Step 85140: {'train_ae_loss': 0.69258, 'train_ucc_loss': 0.47715, 'train_ucc_acc': 0.8125, 'loss': 0.58486}\n",
            "Step 85160: {'train_ae_loss': 0.66584, 'train_ucc_loss': 0.52647, 'train_ucc_acc': 0.78125, 'loss': 0.59616}\n",
            "Step 85180: {'train_ae_loss': 0.66659, 'train_ucc_loss': 0.46249, 'train_ucc_acc': 0.84375, 'loss': 0.56454}\n",
            "Step 85200: {'train_ae_loss': 0.66716, 'train_ucc_loss': 0.44261, 'train_ucc_acc': 0.84375, 'loss': 0.55489}\n",
            "Step 85220: {'train_ae_loss': 0.6654, 'train_ucc_loss': 0.48476, 'train_ucc_acc': 0.84375, 'loss': 0.57508}\n",
            "Step 85240: {'train_ae_loss': 0.67203, 'train_ucc_loss': 0.43744, 'train_ucc_acc': 0.90625, 'loss': 0.55474}\n",
            "Step 85260: {'train_ae_loss': 0.68445, 'train_ucc_loss': 0.43754, 'train_ucc_acc': 0.875, 'loss': 0.56099}\n",
            "Step 85280: {'train_ae_loss': 0.67004, 'train_ucc_loss': 0.54032, 'train_ucc_acc': 0.78125, 'loss': 0.60518}\n",
            "Step 85300: {'train_ae_loss': 0.68551, 'train_ucc_loss': 0.4153, 'train_ucc_acc': 0.90625, 'loss': 0.5504}\n",
            "Step 85320: {'train_ae_loss': 0.67125, 'train_ucc_loss': 0.47241, 'train_ucc_acc': 0.84375, 'loss': 0.57183}\n",
            "Step 85340: {'train_ae_loss': 0.67729, 'train_ucc_loss': 0.40009, 'train_ucc_acc': 0.90625, 'loss': 0.53869}\n",
            "Step 85360: {'train_ae_loss': 0.67055, 'train_ucc_loss': 0.43767, 'train_ucc_acc': 0.875, 'loss': 0.55411}\n",
            "Step 85380: {'train_ae_loss': 0.65623, 'train_ucc_loss': 0.41504, 'train_ucc_acc': 0.90625, 'loss': 0.53564}\n",
            "Step 85400: {'train_ae_loss': 0.66407, 'train_ucc_loss': 0.35134, 'train_ucc_acc': 0.96875, 'loss': 0.5077}\n",
            "Step 85420: {'train_ae_loss': 0.65411, 'train_ucc_loss': 0.46284, 'train_ucc_acc': 0.875, 'loss': 0.55848}\n",
            "Step 85440: {'train_ae_loss': 0.6646, 'train_ucc_loss': 0.37105, 'train_ucc_acc': 0.9375, 'loss': 0.51782}\n",
            "Step 85460: {'train_ae_loss': 0.65037, 'train_ucc_loss': 0.45287, 'train_ucc_acc': 0.875, 'loss': 0.55162}\n",
            "Step 85480: {'train_ae_loss': 0.65742, 'train_ucc_loss': 0.40841, 'train_ucc_acc': 0.90625, 'loss': 0.53292}\n",
            "Step 85500: {'train_ae_loss': 0.67264, 'train_ucc_loss': 0.428, 'train_ucc_acc': 0.90625, 'loss': 0.55032}\n",
            "Step 85520: {'train_ae_loss': 0.65194, 'train_ucc_loss': 0.46835, 'train_ucc_acc': 0.84375, 'loss': 0.56014}\n",
            "Step 85540: {'train_ae_loss': 0.66135, 'train_ucc_loss': 0.45016, 'train_ucc_acc': 0.875, 'loss': 0.55576}\n",
            "Step 85560: {'train_ae_loss': 0.66147, 'train_ucc_loss': 0.3829, 'train_ucc_acc': 0.90625, 'loss': 0.52218}\n",
            "Step 85580: {'train_ae_loss': 0.66567, 'train_ucc_loss': 0.44257, 'train_ucc_acc': 0.875, 'loss': 0.55412}\n",
            "Step 85600: {'train_ae_loss': 0.67235, 'train_ucc_loss': 0.42831, 'train_ucc_acc': 0.875, 'loss': 0.55033}\n",
            "Step 85620: {'train_ae_loss': 0.64763, 'train_ucc_loss': 0.44358, 'train_ucc_acc': 0.875, 'loss': 0.54561}\n",
            "Step 85640: {'train_ae_loss': 0.67175, 'train_ucc_loss': 0.41093, 'train_ucc_acc': 0.90625, 'loss': 0.54134}\n",
            "Step 85660: {'train_ae_loss': 0.65688, 'train_ucc_loss': 0.46469, 'train_ucc_acc': 0.84375, 'loss': 0.56078}\n",
            "Step 85680: {'train_ae_loss': 0.65657, 'train_ucc_loss': 0.47329, 'train_ucc_acc': 0.84375, 'loss': 0.56493}\n",
            "Step 85700: {'train_ae_loss': 0.66748, 'train_ucc_loss': 0.41891, 'train_ucc_acc': 0.875, 'loss': 0.54319}\n",
            "Step 85720: {'train_ae_loss': 0.66573, 'train_ucc_loss': 0.5555, 'train_ucc_acc': 0.75, 'loss': 0.61061}\n",
            "Step 85740: {'train_ae_loss': 0.67858, 'train_ucc_loss': 0.52843, 'train_ucc_acc': 0.78125, 'loss': 0.60351}\n",
            "Step 85760: {'train_ae_loss': 0.67632, 'train_ucc_loss': 0.45026, 'train_ucc_acc': 0.84375, 'loss': 0.56329}\n",
            "Step 85780: {'train_ae_loss': 0.66034, 'train_ucc_loss': 0.4301, 'train_ucc_acc': 0.90625, 'loss': 0.54522}\n",
            "Step 85800: {'train_ae_loss': 0.66147, 'train_ucc_loss': 0.5167, 'train_ucc_acc': 0.78125, 'loss': 0.58909}\n",
            "Step 85820: {'train_ae_loss': 0.64627, 'train_ucc_loss': 0.4166, 'train_ucc_acc': 0.90625, 'loss': 0.53143}\n",
            "Step 85840: {'train_ae_loss': 0.64841, 'train_ucc_loss': 0.53498, 'train_ucc_acc': 0.75, 'loss': 0.5917}\n",
            "Step 85860: {'train_ae_loss': 0.67441, 'train_ucc_loss': 0.36088, 'train_ucc_acc': 0.96875, 'loss': 0.51764}\n",
            "Step 85880: {'train_ae_loss': 0.67064, 'train_ucc_loss': 0.45255, 'train_ucc_acc': 0.875, 'loss': 0.56159}\n",
            "Step 85900: {'train_ae_loss': 0.68513, 'train_ucc_loss': 0.36449, 'train_ucc_acc': 0.96875, 'loss': 0.52481}\n",
            "Step 85920: {'train_ae_loss': 0.65706, 'train_ucc_loss': 0.50003, 'train_ucc_acc': 0.8125, 'loss': 0.57855}\n",
            "Step 85940: {'train_ae_loss': 0.66418, 'train_ucc_loss': 0.49677, 'train_ucc_acc': 0.8125, 'loss': 0.58047}\n",
            "Step 85960: {'train_ae_loss': 0.65446, 'train_ucc_loss': 0.40777, 'train_ucc_acc': 0.90625, 'loss': 0.53111}\n",
            "Step 85980: {'train_ae_loss': 0.65551, 'train_ucc_loss': 0.38917, 'train_ucc_acc': 0.9375, 'loss': 0.52234}\n",
            "Step 86000: {'train_ae_loss': 0.66928, 'train_ucc_loss': 0.53087, 'train_ucc_acc': 0.78125, 'loss': 0.60007}\n",
            "step: 86000,eval_ae_loss: 0.65281,eval_ucc_loss: 0.48095,eval_ucc_acc: 0.82129\n",
            "Step 86020: {'train_ae_loss': 0.67207, 'train_ucc_loss': 0.39711, 'train_ucc_acc': 0.875, 'loss': 0.53459}\n",
            "Step 86040: {'train_ae_loss': 0.66693, 'train_ucc_loss': 0.40176, 'train_ucc_acc': 0.9375, 'loss': 0.53435}\n",
            "Step 86060: {'train_ae_loss': 0.6414, 'train_ucc_loss': 0.44394, 'train_ucc_acc': 0.84375, 'loss': 0.54267}\n",
            "Step 86080: {'train_ae_loss': 0.64044, 'train_ucc_loss': 0.43169, 'train_ucc_acc': 0.90625, 'loss': 0.53606}\n",
            "Step 86100: {'train_ae_loss': 0.65836, 'train_ucc_loss': 0.5629, 'train_ucc_acc': 0.71875, 'loss': 0.61063}\n",
            "Step 86120: {'train_ae_loss': 0.6679, 'train_ucc_loss': 0.41558, 'train_ucc_acc': 0.90625, 'loss': 0.54174}\n",
            "Step 86140: {'train_ae_loss': 0.65942, 'train_ucc_loss': 0.40062, 'train_ucc_acc': 0.90625, 'loss': 0.53002}\n",
            "Step 86160: {'train_ae_loss': 0.65405, 'train_ucc_loss': 0.51113, 'train_ucc_acc': 0.8125, 'loss': 0.58259}\n",
            "Step 86180: {'train_ae_loss': 0.67274, 'train_ucc_loss': 0.34425, 'train_ucc_acc': 0.96875, 'loss': 0.50849}\n",
            "Step 86200: {'train_ae_loss': 0.67298, 'train_ucc_loss': 0.3183, 'train_ucc_acc': 1.0, 'loss': 0.49564}\n",
            "Step 86220: {'train_ae_loss': 0.65099, 'train_ucc_loss': 0.54503, 'train_ucc_acc': 0.78125, 'loss': 0.59801}\n",
            "Step 86240: {'train_ae_loss': 0.65788, 'train_ucc_loss': 0.56238, 'train_ucc_acc': 0.75, 'loss': 0.61013}\n",
            "Step 86260: {'train_ae_loss': 0.6701, 'train_ucc_loss': 0.48398, 'train_ucc_acc': 0.8125, 'loss': 0.57704}\n",
            "Step 86280: {'train_ae_loss': 0.66309, 'train_ucc_loss': 0.48958, 'train_ucc_acc': 0.8125, 'loss': 0.57633}\n",
            "Step 86300: {'train_ae_loss': 0.66336, 'train_ucc_loss': 0.67477, 'train_ucc_acc': 0.59375, 'loss': 0.66907}\n",
            "Step 86320: {'train_ae_loss': 0.66998, 'train_ucc_loss': 0.3854, 'train_ucc_acc': 0.875, 'loss': 0.52769}\n",
            "Step 86340: {'train_ae_loss': 0.68401, 'train_ucc_loss': 0.54628, 'train_ucc_acc': 0.75, 'loss': 0.61515}\n",
            "Step 86360: {'train_ae_loss': 0.65733, 'train_ucc_loss': 0.46585, 'train_ucc_acc': 0.84375, 'loss': 0.56159}\n",
            "Step 86380: {'train_ae_loss': 0.66179, 'train_ucc_loss': 0.39412, 'train_ucc_acc': 0.9375, 'loss': 0.52795}\n",
            "Step 86400: {'train_ae_loss': 0.67351, 'train_ucc_loss': 0.49709, 'train_ucc_acc': 0.78125, 'loss': 0.5853}\n",
            "Step 86420: {'train_ae_loss': 0.66799, 'train_ucc_loss': 0.47307, 'train_ucc_acc': 0.875, 'loss': 0.57053}\n",
            "Step 86440: {'train_ae_loss': 0.66686, 'train_ucc_loss': 0.52848, 'train_ucc_acc': 0.78125, 'loss': 0.59767}\n",
            "Step 86460: {'train_ae_loss': 0.68365, 'train_ucc_loss': 0.56834, 'train_ucc_acc': 0.71875, 'loss': 0.62599}\n",
            "Step 86480: {'train_ae_loss': 0.66635, 'train_ucc_loss': 0.66601, 'train_ucc_acc': 0.625, 'loss': 0.66618}\n",
            "Step 86500: {'train_ae_loss': 0.65875, 'train_ucc_loss': 0.44621, 'train_ucc_acc': 0.875, 'loss': 0.55248}\n",
            "Step 86520: {'train_ae_loss': 0.66294, 'train_ucc_loss': 0.50728, 'train_ucc_acc': 0.8125, 'loss': 0.58511}\n",
            "Step 86540: {'train_ae_loss': 0.66936, 'train_ucc_loss': 0.44068, 'train_ucc_acc': 0.875, 'loss': 0.55502}\n",
            "Step 86560: {'train_ae_loss': 0.65065, 'train_ucc_loss': 0.41153, 'train_ucc_acc': 0.90625, 'loss': 0.53109}\n",
            "Step 86580: {'train_ae_loss': 0.65943, 'train_ucc_loss': 0.5571, 'train_ucc_acc': 0.75, 'loss': 0.60826}\n",
            "Step 86600: {'train_ae_loss': 0.66528, 'train_ucc_loss': 0.38015, 'train_ucc_acc': 0.9375, 'loss': 0.52272}\n",
            "Step 86620: {'train_ae_loss': 0.65696, 'train_ucc_loss': 0.42239, 'train_ucc_acc': 0.90625, 'loss': 0.53968}\n",
            "Step 86640: {'train_ae_loss': 0.64798, 'train_ucc_loss': 0.40717, 'train_ucc_acc': 0.90625, 'loss': 0.52758}\n",
            "Step 86660: {'train_ae_loss': 0.67127, 'train_ucc_loss': 0.43829, 'train_ucc_acc': 0.875, 'loss': 0.55478}\n",
            "Step 86680: {'train_ae_loss': 0.66151, 'train_ucc_loss': 0.41418, 'train_ucc_acc': 0.875, 'loss': 0.53784}\n",
            "Step 86700: {'train_ae_loss': 0.66454, 'train_ucc_loss': 0.35104, 'train_ucc_acc': 0.96875, 'loss': 0.50779}\n",
            "Step 86720: {'train_ae_loss': 0.6526, 'train_ucc_loss': 0.4159, 'train_ucc_acc': 0.90625, 'loss': 0.53425}\n",
            "Step 86740: {'train_ae_loss': 0.64051, 'train_ucc_loss': 0.50967, 'train_ucc_acc': 0.78125, 'loss': 0.57509}\n",
            "Step 86760: {'train_ae_loss': 0.6598, 'train_ucc_loss': 0.42623, 'train_ucc_acc': 0.90625, 'loss': 0.54302}\n",
            "Step 86780: {'train_ae_loss': 0.64268, 'train_ucc_loss': 0.52984, 'train_ucc_acc': 0.75, 'loss': 0.58626}\n",
            "Step 86800: {'train_ae_loss': 0.65096, 'train_ucc_loss': 0.48859, 'train_ucc_acc': 0.84375, 'loss': 0.56977}\n",
            "Step 86820: {'train_ae_loss': 0.67163, 'train_ucc_loss': 0.41171, 'train_ucc_acc': 0.90625, 'loss': 0.54167}\n",
            "Step 86840: {'train_ae_loss': 0.65938, 'train_ucc_loss': 0.39181, 'train_ucc_acc': 0.9375, 'loss': 0.5256}\n",
            "Step 86860: {'train_ae_loss': 0.65006, 'train_ucc_loss': 0.50201, 'train_ucc_acc': 0.8125, 'loss': 0.57603}\n",
            "Step 86880: {'train_ae_loss': 0.65714, 'train_ucc_loss': 0.46091, 'train_ucc_acc': 0.8125, 'loss': 0.55902}\n",
            "Step 86900: {'train_ae_loss': 0.67518, 'train_ucc_loss': 0.38161, 'train_ucc_acc': 0.9375, 'loss': 0.5284}\n",
            "Step 86920: {'train_ae_loss': 0.64382, 'train_ucc_loss': 0.46564, 'train_ucc_acc': 0.84375, 'loss': 0.55473}\n",
            "Step 86940: {'train_ae_loss': 0.6416, 'train_ucc_loss': 0.40965, 'train_ucc_acc': 0.875, 'loss': 0.52563}\n",
            "Step 86960: {'train_ae_loss': 0.67121, 'train_ucc_loss': 0.46319, 'train_ucc_acc': 0.875, 'loss': 0.5672}\n",
            "Step 86980: {'train_ae_loss': 0.66761, 'train_ucc_loss': 0.43342, 'train_ucc_acc': 0.875, 'loss': 0.55052}\n",
            "Step 87000: {'train_ae_loss': 0.69316, 'train_ucc_loss': 0.39409, 'train_ucc_acc': 0.90625, 'loss': 0.54363}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 20:01:32 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 87000,eval_ae_loss: 0.65655,eval_ucc_loss: 0.46288,eval_ucc_acc: 0.84277\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 20:01:37 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 87020: {'train_ae_loss': 0.676, 'train_ucc_loss': 0.47611, 'train_ucc_acc': 0.84375, 'loss': 0.57605}\n",
            "Step 87040: {'train_ae_loss': 0.68384, 'train_ucc_loss': 0.48313, 'train_ucc_acc': 0.84375, 'loss': 0.58349}\n",
            "Step 87060: {'train_ae_loss': 0.66874, 'train_ucc_loss': 0.50561, 'train_ucc_acc': 0.8125, 'loss': 0.58718}\n",
            "Step 87080: {'train_ae_loss': 0.66309, 'train_ucc_loss': 0.48191, 'train_ucc_acc': 0.84375, 'loss': 0.5725}\n",
            "Step 87100: {'train_ae_loss': 0.65558, 'train_ucc_loss': 0.45097, 'train_ucc_acc': 0.84375, 'loss': 0.55328}\n",
            "Step 87120: {'train_ae_loss': 0.66603, 'train_ucc_loss': 0.39145, 'train_ucc_acc': 0.875, 'loss': 0.52874}\n",
            "Step 87140: {'train_ae_loss': 0.66944, 'train_ucc_loss': 0.53067, 'train_ucc_acc': 0.78125, 'loss': 0.60005}\n",
            "Step 87160: {'train_ae_loss': 0.64425, 'train_ucc_loss': 0.42032, 'train_ucc_acc': 0.90625, 'loss': 0.53228}\n",
            "Step 87180: {'train_ae_loss': 0.67006, 'train_ucc_loss': 0.36067, 'train_ucc_acc': 0.9375, 'loss': 0.51536}\n",
            "Step 87200: {'train_ae_loss': 0.67929, 'train_ucc_loss': 0.45382, 'train_ucc_acc': 0.875, 'loss': 0.56656}\n",
            "Step 87220: {'train_ae_loss': 0.65208, 'train_ucc_loss': 0.44292, 'train_ucc_acc': 0.875, 'loss': 0.5475}\n",
            "Step 87240: {'train_ae_loss': 0.65427, 'train_ucc_loss': 0.4947, 'train_ucc_acc': 0.8125, 'loss': 0.57448}\n",
            "Step 87260: {'train_ae_loss': 0.66333, 'train_ucc_loss': 0.39949, 'train_ucc_acc': 0.90625, 'loss': 0.53141}\n",
            "Step 87280: {'train_ae_loss': 0.65874, 'train_ucc_loss': 0.43027, 'train_ucc_acc': 0.84375, 'loss': 0.5445}\n",
            "Step 87300: {'train_ae_loss': 0.66545, 'train_ucc_loss': 0.41273, 'train_ucc_acc': 0.875, 'loss': 0.53909}\n",
            "Step 87320: {'train_ae_loss': 0.63945, 'train_ucc_loss': 0.48374, 'train_ucc_acc': 0.8125, 'loss': 0.5616}\n",
            "Step 87340: {'train_ae_loss': 0.6633, 'train_ucc_loss': 0.37104, 'train_ucc_acc': 0.96875, 'loss': 0.51717}\n",
            "Step 87360: {'train_ae_loss': 0.66729, 'train_ucc_loss': 0.445, 'train_ucc_acc': 0.875, 'loss': 0.55615}\n",
            "Step 87380: {'train_ae_loss': 0.67582, 'train_ucc_loss': 0.38466, 'train_ucc_acc': 0.9375, 'loss': 0.53024}\n",
            "Step 87400: {'train_ae_loss': 0.67074, 'train_ucc_loss': 0.39293, 'train_ucc_acc': 0.9375, 'loss': 0.53184}\n",
            "Step 87420: {'train_ae_loss': 0.65831, 'train_ucc_loss': 0.46947, 'train_ucc_acc': 0.84375, 'loss': 0.56389}\n",
            "Step 87440: {'train_ae_loss': 0.65608, 'train_ucc_loss': 0.4612, 'train_ucc_acc': 0.84375, 'loss': 0.55864}\n",
            "Step 87460: {'train_ae_loss': 0.66123, 'train_ucc_loss': 0.47879, 'train_ucc_acc': 0.84375, 'loss': 0.57001}\n",
            "Step 87480: {'train_ae_loss': 0.65772, 'train_ucc_loss': 0.43497, 'train_ucc_acc': 0.875, 'loss': 0.54634}\n",
            "Step 87500: {'train_ae_loss': 0.67501, 'train_ucc_loss': 0.41024, 'train_ucc_acc': 0.90625, 'loss': 0.54263}\n",
            "Step 87520: {'train_ae_loss': 0.66557, 'train_ucc_loss': 0.56518, 'train_ucc_acc': 0.71875, 'loss': 0.61538}\n",
            "Step 87540: {'train_ae_loss': 0.6615, 'train_ucc_loss': 0.48459, 'train_ucc_acc': 0.8125, 'loss': 0.57305}\n",
            "Step 87560: {'train_ae_loss': 0.65888, 'train_ucc_loss': 0.4848, 'train_ucc_acc': 0.84375, 'loss': 0.57184}\n",
            "Step 87580: {'train_ae_loss': 0.6561, 'train_ucc_loss': 0.49715, 'train_ucc_acc': 0.8125, 'loss': 0.57663}\n",
            "Step 87600: {'train_ae_loss': 0.64532, 'train_ucc_loss': 0.44354, 'train_ucc_acc': 0.875, 'loss': 0.54443}\n",
            "Step 87620: {'train_ae_loss': 0.65974, 'train_ucc_loss': 0.38438, 'train_ucc_acc': 0.9375, 'loss': 0.52206}\n",
            "Step 87640: {'train_ae_loss': 0.67334, 'train_ucc_loss': 0.3835, 'train_ucc_acc': 0.90625, 'loss': 0.52842}\n",
            "Step 87660: {'train_ae_loss': 0.65979, 'train_ucc_loss': 0.48173, 'train_ucc_acc': 0.84375, 'loss': 0.57076}\n",
            "Step 87680: {'train_ae_loss': 0.66016, 'train_ucc_loss': 0.47793, 'train_ucc_acc': 0.84375, 'loss': 0.56905}\n",
            "Step 87700: {'train_ae_loss': 0.66314, 'train_ucc_loss': 0.4111, 'train_ucc_acc': 0.90625, 'loss': 0.53712}\n",
            "Step 87720: {'train_ae_loss': 0.64652, 'train_ucc_loss': 0.48155, 'train_ucc_acc': 0.84375, 'loss': 0.56403}\n",
            "Step 87740: {'train_ae_loss': 0.65908, 'train_ucc_loss': 0.50394, 'train_ucc_acc': 0.78125, 'loss': 0.58151}\n",
            "Step 87760: {'train_ae_loss': 0.64815, 'train_ucc_loss': 0.45967, 'train_ucc_acc': 0.84375, 'loss': 0.55391}\n",
            "Step 87780: {'train_ae_loss': 0.67284, 'train_ucc_loss': 0.32496, 'train_ucc_acc': 1.0, 'loss': 0.4989}\n",
            "Step 87800: {'train_ae_loss': 0.68743, 'train_ucc_loss': 0.45307, 'train_ucc_acc': 0.84375, 'loss': 0.57025}\n",
            "Step 87820: {'train_ae_loss': 0.67246, 'train_ucc_loss': 0.50428, 'train_ucc_acc': 0.8125, 'loss': 0.58837}\n",
            "Step 87840: {'train_ae_loss': 0.66129, 'train_ucc_loss': 0.40092, 'train_ucc_acc': 0.90625, 'loss': 0.5311}\n",
            "Step 87860: {'train_ae_loss': 0.67788, 'train_ucc_loss': 0.42171, 'train_ucc_acc': 0.90625, 'loss': 0.54979}\n",
            "Step 87880: {'train_ae_loss': 0.67394, 'train_ucc_loss': 0.44655, 'train_ucc_acc': 0.84375, 'loss': 0.56024}\n",
            "Step 87900: {'train_ae_loss': 0.6623, 'train_ucc_loss': 0.46469, 'train_ucc_acc': 0.84375, 'loss': 0.5635}\n",
            "Step 87920: {'train_ae_loss': 0.66315, 'train_ucc_loss': 0.45181, 'train_ucc_acc': 0.875, 'loss': 0.55748}\n",
            "Step 87940: {'train_ae_loss': 0.6717, 'train_ucc_loss': 0.47857, 'train_ucc_acc': 0.8125, 'loss': 0.57513}\n",
            "Step 87960: {'train_ae_loss': 0.66922, 'train_ucc_loss': 0.4654, 'train_ucc_acc': 0.875, 'loss': 0.56731}\n",
            "Step 87980: {'train_ae_loss': 0.6772, 'train_ucc_loss': 0.49784, 'train_ucc_acc': 0.8125, 'loss': 0.58752}\n",
            "Step 88000: {'train_ae_loss': 0.66064, 'train_ucc_loss': 0.57562, 'train_ucc_acc': 0.6875, 'loss': 0.61813}\n",
            "step: 88000,eval_ae_loss: 0.65493,eval_ucc_loss: 0.47852,eval_ucc_acc: 0.83105\n",
            "Step 88020: {'train_ae_loss': 0.65299, 'train_ucc_loss': 0.49253, 'train_ucc_acc': 0.8125, 'loss': 0.57276}\n",
            "Step 88040: {'train_ae_loss': 0.67032, 'train_ucc_loss': 0.41787, 'train_ucc_acc': 0.90625, 'loss': 0.5441}\n",
            "Step 88060: {'train_ae_loss': 0.66379, 'train_ucc_loss': 0.43084, 'train_ucc_acc': 0.875, 'loss': 0.54732}\n",
            "Step 88080: {'train_ae_loss': 0.67806, 'train_ucc_loss': 0.39154, 'train_ucc_acc': 0.90625, 'loss': 0.5348}\n",
            "Step 88100: {'train_ae_loss': 0.66108, 'train_ucc_loss': 0.49932, 'train_ucc_acc': 0.78125, 'loss': 0.5802}\n",
            "Step 88120: {'train_ae_loss': 0.65173, 'train_ucc_loss': 0.40645, 'train_ucc_acc': 0.90625, 'loss': 0.52909}\n",
            "Step 88140: {'train_ae_loss': 0.65068, 'train_ucc_loss': 0.43394, 'train_ucc_acc': 0.84375, 'loss': 0.54231}\n",
            "Step 88160: {'train_ae_loss': 0.65181, 'train_ucc_loss': 0.4387, 'train_ucc_acc': 0.875, 'loss': 0.54525}\n",
            "Step 88180: {'train_ae_loss': 0.67308, 'train_ucc_loss': 0.46108, 'train_ucc_acc': 0.84375, 'loss': 0.56708}\n",
            "Step 88200: {'train_ae_loss': 0.64344, 'train_ucc_loss': 0.5001, 'train_ucc_acc': 0.8125, 'loss': 0.57177}\n",
            "Step 88220: {'train_ae_loss': 0.67728, 'train_ucc_loss': 0.4164, 'train_ucc_acc': 0.875, 'loss': 0.54684}\n",
            "Step 88240: {'train_ae_loss': 0.67043, 'train_ucc_loss': 0.6313, 'train_ucc_acc': 0.65625, 'loss': 0.65086}\n",
            "Step 88260: {'train_ae_loss': 0.64411, 'train_ucc_loss': 0.45333, 'train_ucc_acc': 0.84375, 'loss': 0.54872}\n",
            "Step 88280: {'train_ae_loss': 0.6857, 'train_ucc_loss': 0.40587, 'train_ucc_acc': 0.90625, 'loss': 0.54578}\n",
            "Step 88300: {'train_ae_loss': 0.66196, 'train_ucc_loss': 0.44752, 'train_ucc_acc': 0.875, 'loss': 0.55474}\n",
            "Step 88320: {'train_ae_loss': 0.66481, 'train_ucc_loss': 0.4728, 'train_ucc_acc': 0.8125, 'loss': 0.56881}\n",
            "Step 88340: {'train_ae_loss': 0.65953, 'train_ucc_loss': 0.46371, 'train_ucc_acc': 0.84375, 'loss': 0.56162}\n",
            "Step 88360: {'train_ae_loss': 0.65552, 'train_ucc_loss': 0.37694, 'train_ucc_acc': 0.96875, 'loss': 0.51623}\n",
            "Step 88380: {'train_ae_loss': 0.6581, 'train_ucc_loss': 0.45254, 'train_ucc_acc': 0.84375, 'loss': 0.55532}\n",
            "Step 88400: {'train_ae_loss': 0.66495, 'train_ucc_loss': 0.52969, 'train_ucc_acc': 0.78125, 'loss': 0.59732}\n",
            "Step 88420: {'train_ae_loss': 0.66323, 'train_ucc_loss': 0.3658, 'train_ucc_acc': 0.96875, 'loss': 0.51452}\n",
            "Step 88440: {'train_ae_loss': 0.65769, 'train_ucc_loss': 0.41305, 'train_ucc_acc': 0.90625, 'loss': 0.53537}\n",
            "Step 88460: {'train_ae_loss': 0.6696, 'train_ucc_loss': 0.35381, 'train_ucc_acc': 0.96875, 'loss': 0.51171}\n",
            "Step 88480: {'train_ae_loss': 0.65719, 'train_ucc_loss': 0.50619, 'train_ucc_acc': 0.78125, 'loss': 0.58169}\n",
            "Step 88500: {'train_ae_loss': 0.67228, 'train_ucc_loss': 0.50363, 'train_ucc_acc': 0.78125, 'loss': 0.58796}\n",
            "Step 88520: {'train_ae_loss': 0.68205, 'train_ucc_loss': 0.40373, 'train_ucc_acc': 0.9375, 'loss': 0.54289}\n",
            "Step 88540: {'train_ae_loss': 0.67469, 'train_ucc_loss': 0.4686, 'train_ucc_acc': 0.84375, 'loss': 0.57165}\n",
            "Step 88560: {'train_ae_loss': 0.65484, 'train_ucc_loss': 0.35079, 'train_ucc_acc': 0.96875, 'loss': 0.50281}\n",
            "Step 88580: {'train_ae_loss': 0.67026, 'train_ucc_loss': 0.47554, 'train_ucc_acc': 0.84375, 'loss': 0.5729}\n",
            "Step 88600: {'train_ae_loss': 0.65861, 'train_ucc_loss': 0.65454, 'train_ucc_acc': 0.65625, 'loss': 0.65658}\n",
            "Step 88620: {'train_ae_loss': 0.66942, 'train_ucc_loss': 0.35131, 'train_ucc_acc': 0.96875, 'loss': 0.51037}\n",
            "Step 88640: {'train_ae_loss': 0.66128, 'train_ucc_loss': 0.3962, 'train_ucc_acc': 0.9375, 'loss': 0.52874}\n",
            "Step 88660: {'train_ae_loss': 0.65154, 'train_ucc_loss': 0.48849, 'train_ucc_acc': 0.8125, 'loss': 0.57002}\n",
            "Step 88680: {'train_ae_loss': 0.64974, 'train_ucc_loss': 0.44951, 'train_ucc_acc': 0.84375, 'loss': 0.54963}\n",
            "Step 88700: {'train_ae_loss': 0.66696, 'train_ucc_loss': 0.32172, 'train_ucc_acc': 1.0, 'loss': 0.49434}\n",
            "Step 88720: {'train_ae_loss': 0.66677, 'train_ucc_loss': 0.50167, 'train_ucc_acc': 0.8125, 'loss': 0.58422}\n",
            "Step 88740: {'train_ae_loss': 0.67484, 'train_ucc_loss': 0.44792, 'train_ucc_acc': 0.875, 'loss': 0.56138}\n",
            "Step 88760: {'train_ae_loss': 0.67447, 'train_ucc_loss': 0.43878, 'train_ucc_acc': 0.875, 'loss': 0.55662}\n",
            "Step 88780: {'train_ae_loss': 0.66855, 'train_ucc_loss': 0.34917, 'train_ucc_acc': 0.96875, 'loss': 0.50886}\n",
            "Step 88800: {'train_ae_loss': 0.67343, 'train_ucc_loss': 0.37318, 'train_ucc_acc': 0.9375, 'loss': 0.52331}\n",
            "Step 88820: {'train_ae_loss': 0.65301, 'train_ucc_loss': 0.45011, 'train_ucc_acc': 0.875, 'loss': 0.55156}\n",
            "Step 88840: {'train_ae_loss': 0.6694, 'train_ucc_loss': 0.51652, 'train_ucc_acc': 0.78125, 'loss': 0.59296}\n",
            "Step 88860: {'train_ae_loss': 0.65019, 'train_ucc_loss': 0.46366, 'train_ucc_acc': 0.875, 'loss': 0.55693}\n",
            "Step 88880: {'train_ae_loss': 0.66472, 'train_ucc_loss': 0.43663, 'train_ucc_acc': 0.875, 'loss': 0.55067}\n",
            "Step 88900: {'train_ae_loss': 0.64925, 'train_ucc_loss': 0.36285, 'train_ucc_acc': 0.9375, 'loss': 0.50605}\n",
            "Step 88920: {'train_ae_loss': 0.65448, 'train_ucc_loss': 0.52099, 'train_ucc_acc': 0.75, 'loss': 0.58774}\n",
            "Step 88940: {'train_ae_loss': 0.64799, 'train_ucc_loss': 0.51224, 'train_ucc_acc': 0.78125, 'loss': 0.58012}\n",
            "Step 88960: {'train_ae_loss': 0.66809, 'train_ucc_loss': 0.4605, 'train_ucc_acc': 0.875, 'loss': 0.56429}\n",
            "Step 88980: {'train_ae_loss': 0.65747, 'train_ucc_loss': 0.48326, 'train_ucc_acc': 0.8125, 'loss': 0.57037}\n",
            "Step 89000: {'train_ae_loss': 0.65428, 'train_ucc_loss': 0.43014, 'train_ucc_acc': 0.875, 'loss': 0.54221}\n",
            "step: 89000,eval_ae_loss: 0.64885,eval_ucc_loss: 0.48787,eval_ucc_acc: 0.81836\n",
            "Step 89020: {'train_ae_loss': 0.67308, 'train_ucc_loss': 0.48273, 'train_ucc_acc': 0.8125, 'loss': 0.57791}\n",
            "Step 89040: {'train_ae_loss': 0.66536, 'train_ucc_loss': 0.39817, 'train_ucc_acc': 0.90625, 'loss': 0.53176}\n",
            "Step 89060: {'train_ae_loss': 0.65451, 'train_ucc_loss': 0.43415, 'train_ucc_acc': 0.875, 'loss': 0.54433}\n",
            "Step 89080: {'train_ae_loss': 0.67261, 'train_ucc_loss': 0.47267, 'train_ucc_acc': 0.8125, 'loss': 0.57264}\n",
            "Step 89100: {'train_ae_loss': 0.6821, 'train_ucc_loss': 0.45886, 'train_ucc_acc': 0.8125, 'loss': 0.57048}\n",
            "Step 89120: {'train_ae_loss': 0.67706, 'train_ucc_loss': 0.49683, 'train_ucc_acc': 0.8125, 'loss': 0.58695}\n",
            "Step 89140: {'train_ae_loss': 0.6658, 'train_ucc_loss': 0.4545, 'train_ucc_acc': 0.84375, 'loss': 0.56015}\n",
            "Step 89160: {'train_ae_loss': 0.67367, 'train_ucc_loss': 0.36025, 'train_ucc_acc': 0.96875, 'loss': 0.51696}\n",
            "Step 89180: {'train_ae_loss': 0.66593, 'train_ucc_loss': 0.35609, 'train_ucc_acc': 0.96875, 'loss': 0.51101}\n",
            "Step 89200: {'train_ae_loss': 0.67173, 'train_ucc_loss': 0.44246, 'train_ucc_acc': 0.875, 'loss': 0.55709}\n",
            "Step 89220: {'train_ae_loss': 0.66702, 'train_ucc_loss': 0.39262, 'train_ucc_acc': 0.90625, 'loss': 0.52982}\n",
            "Step 89240: {'train_ae_loss': 0.68115, 'train_ucc_loss': 0.41944, 'train_ucc_acc': 0.90625, 'loss': 0.5503}\n",
            "Step 89260: {'train_ae_loss': 0.66815, 'train_ucc_loss': 0.46425, 'train_ucc_acc': 0.84375, 'loss': 0.5662}\n",
            "Step 89280: {'train_ae_loss': 0.66402, 'train_ucc_loss': 0.46226, 'train_ucc_acc': 0.84375, 'loss': 0.56314}\n",
            "Step 89300: {'train_ae_loss': 0.64882, 'train_ucc_loss': 0.4983, 'train_ucc_acc': 0.8125, 'loss': 0.57356}\n",
            "Step 89320: {'train_ae_loss': 0.66528, 'train_ucc_loss': 0.47585, 'train_ucc_acc': 0.8125, 'loss': 0.57057}\n",
            "Step 89340: {'train_ae_loss': 0.6666, 'train_ucc_loss': 0.50119, 'train_ucc_acc': 0.78125, 'loss': 0.5839}\n",
            "Step 89360: {'train_ae_loss': 0.67104, 'train_ucc_loss': 0.32341, 'train_ucc_acc': 1.0, 'loss': 0.49723}\n",
            "Step 89380: {'train_ae_loss': 0.65884, 'train_ucc_loss': 0.36689, 'train_ucc_acc': 0.9375, 'loss': 0.51287}\n",
            "Step 89400: {'train_ae_loss': 0.66974, 'train_ucc_loss': 0.47816, 'train_ucc_acc': 0.84375, 'loss': 0.57395}\n",
            "Step 89420: {'train_ae_loss': 0.67374, 'train_ucc_loss': 0.42234, 'train_ucc_acc': 0.90625, 'loss': 0.54804}\n",
            "Step 89440: {'train_ae_loss': 0.6656, 'train_ucc_loss': 0.3877, 'train_ucc_acc': 0.9375, 'loss': 0.52665}\n",
            "Step 89460: {'train_ae_loss': 0.67967, 'train_ucc_loss': 0.54402, 'train_ucc_acc': 0.71875, 'loss': 0.61185}\n",
            "Step 89480: {'train_ae_loss': 0.67029, 'train_ucc_loss': 0.47058, 'train_ucc_acc': 0.84375, 'loss': 0.57044}\n",
            "Step 89500: {'train_ae_loss': 0.67168, 'train_ucc_loss': 0.43165, 'train_ucc_acc': 0.875, 'loss': 0.55167}\n",
            "Step 89520: {'train_ae_loss': 0.67395, 'train_ucc_loss': 0.38885, 'train_ucc_acc': 0.9375, 'loss': 0.5314}\n",
            "Step 89540: {'train_ae_loss': 0.64815, 'train_ucc_loss': 0.46443, 'train_ucc_acc': 0.8125, 'loss': 0.55629}\n",
            "Step 89560: {'train_ae_loss': 0.66985, 'train_ucc_loss': 0.47013, 'train_ucc_acc': 0.84375, 'loss': 0.56999}\n",
            "Step 89580: {'train_ae_loss': 0.66547, 'train_ucc_loss': 0.4528, 'train_ucc_acc': 0.875, 'loss': 0.55914}\n",
            "Step 89600: {'train_ae_loss': 0.65531, 'train_ucc_loss': 0.44729, 'train_ucc_acc': 0.875, 'loss': 0.5513}\n",
            "Step 89620: {'train_ae_loss': 0.67347, 'train_ucc_loss': 0.5118, 'train_ucc_acc': 0.78125, 'loss': 0.59264}\n",
            "Step 89640: {'train_ae_loss': 0.66365, 'train_ucc_loss': 0.41605, 'train_ucc_acc': 0.90625, 'loss': 0.53985}\n",
            "Step 89660: {'train_ae_loss': 0.67216, 'train_ucc_loss': 0.40575, 'train_ucc_acc': 0.90625, 'loss': 0.53895}\n",
            "Step 89680: {'train_ae_loss': 0.66605, 'train_ucc_loss': 0.44731, 'train_ucc_acc': 0.84375, 'loss': 0.55668}\n",
            "Step 89700: {'train_ae_loss': 0.66749, 'train_ucc_loss': 0.42202, 'train_ucc_acc': 0.90625, 'loss': 0.54475}\n",
            "Step 89720: {'train_ae_loss': 0.66684, 'train_ucc_loss': 0.38133, 'train_ucc_acc': 0.9375, 'loss': 0.52409}\n",
            "Step 89740: {'train_ae_loss': 0.69239, 'train_ucc_loss': 0.56678, 'train_ucc_acc': 0.75, 'loss': 0.62958}\n",
            "Step 89760: {'train_ae_loss': 0.66351, 'train_ucc_loss': 0.56057, 'train_ucc_acc': 0.75, 'loss': 0.61204}\n",
            "Step 89780: {'train_ae_loss': 0.66494, 'train_ucc_loss': 0.45694, 'train_ucc_acc': 0.875, 'loss': 0.56094}\n",
            "Step 89800: {'train_ae_loss': 0.68181, 'train_ucc_loss': 0.43041, 'train_ucc_acc': 0.90625, 'loss': 0.55611}\n",
            "Step 89820: {'train_ae_loss': 0.67946, 'train_ucc_loss': 0.37617, 'train_ucc_acc': 0.9375, 'loss': 0.52781}\n",
            "Step 89840: {'train_ae_loss': 0.67049, 'train_ucc_loss': 0.43797, 'train_ucc_acc': 0.84375, 'loss': 0.55423}\n",
            "Step 89860: {'train_ae_loss': 0.67665, 'train_ucc_loss': 0.41195, 'train_ucc_acc': 0.90625, 'loss': 0.5443}\n",
            "Step 89880: {'train_ae_loss': 0.65969, 'train_ucc_loss': 0.41837, 'train_ucc_acc': 0.90625, 'loss': 0.53903}\n",
            "Step 89900: {'train_ae_loss': 0.67138, 'train_ucc_loss': 0.46435, 'train_ucc_acc': 0.84375, 'loss': 0.56787}\n",
            "Step 89920: {'train_ae_loss': 0.6591, 'train_ucc_loss': 0.46446, 'train_ucc_acc': 0.84375, 'loss': 0.56178}\n",
            "Step 89940: {'train_ae_loss': 0.67807, 'train_ucc_loss': 0.49956, 'train_ucc_acc': 0.8125, 'loss': 0.58882}\n",
            "Step 89960: {'train_ae_loss': 0.66459, 'train_ucc_loss': 0.42014, 'train_ucc_acc': 0.90625, 'loss': 0.54236}\n",
            "Step 89980: {'train_ae_loss': 0.65874, 'train_ucc_loss': 0.4358, 'train_ucc_acc': 0.875, 'loss': 0.54727}\n",
            "Step 90000: {'train_ae_loss': 0.67479, 'train_ucc_loss': 0.47934, 'train_ucc_acc': 0.84375, 'loss': 0.57707}\n",
            "step: 90000,eval_ae_loss: 0.6594,eval_ucc_loss: 0.48829,eval_ucc_acc: 0.81641\n",
            "Step 90020: {'train_ae_loss': 0.66897, 'train_ucc_loss': 0.44903, 'train_ucc_acc': 0.84375, 'loss': 0.559}\n",
            "Step 90040: {'train_ae_loss': 0.67879, 'train_ucc_loss': 0.54542, 'train_ucc_acc': 0.71875, 'loss': 0.6121}\n",
            "Step 90060: {'train_ae_loss': 0.67966, 'train_ucc_loss': 0.38372, 'train_ucc_acc': 0.90625, 'loss': 0.53169}\n",
            "Step 90080: {'train_ae_loss': 0.68119, 'train_ucc_loss': 0.35055, 'train_ucc_acc': 0.96875, 'loss': 0.51587}\n",
            "Step 90100: {'train_ae_loss': 0.66418, 'train_ucc_loss': 0.40841, 'train_ucc_acc': 0.90625, 'loss': 0.53629}\n",
            "Step 90120: {'train_ae_loss': 0.67602, 'train_ucc_loss': 0.4845, 'train_ucc_acc': 0.8125, 'loss': 0.58026}\n",
            "Step 90140: {'train_ae_loss': 0.64039, 'train_ucc_loss': 0.50412, 'train_ucc_acc': 0.8125, 'loss': 0.57225}\n",
            "Step 90160: {'train_ae_loss': 0.63699, 'train_ucc_loss': 0.40008, 'train_ucc_acc': 0.90625, 'loss': 0.51853}\n",
            "Step 90180: {'train_ae_loss': 0.66211, 'train_ucc_loss': 0.4277, 'train_ucc_acc': 0.90625, 'loss': 0.5449}\n",
            "Step 90200: {'train_ae_loss': 0.66493, 'train_ucc_loss': 0.45836, 'train_ucc_acc': 0.84375, 'loss': 0.56164}\n",
            "Step 90220: {'train_ae_loss': 0.67759, 'train_ucc_loss': 0.40466, 'train_ucc_acc': 0.90625, 'loss': 0.54113}\n",
            "Step 90240: {'train_ae_loss': 0.67017, 'train_ucc_loss': 0.4506, 'train_ucc_acc': 0.84375, 'loss': 0.56038}\n",
            "Step 90260: {'train_ae_loss': 0.66782, 'train_ucc_loss': 0.35785, 'train_ucc_acc': 0.96875, 'loss': 0.51284}\n",
            "Step 90280: {'train_ae_loss': 0.66092, 'train_ucc_loss': 0.44947, 'train_ucc_acc': 0.875, 'loss': 0.55519}\n",
            "Step 90300: {'train_ae_loss': 0.65759, 'train_ucc_loss': 0.39201, 'train_ucc_acc': 0.90625, 'loss': 0.5248}\n",
            "Step 90320: {'train_ae_loss': 0.66362, 'train_ucc_loss': 0.43897, 'train_ucc_acc': 0.875, 'loss': 0.55129}\n",
            "Step 90340: {'train_ae_loss': 0.66964, 'train_ucc_loss': 0.54553, 'train_ucc_acc': 0.78125, 'loss': 0.60758}\n",
            "Step 90360: {'train_ae_loss': 0.66155, 'train_ucc_loss': 0.43505, 'train_ucc_acc': 0.875, 'loss': 0.5483}\n",
            "Step 90380: {'train_ae_loss': 0.6629, 'train_ucc_loss': 0.40482, 'train_ucc_acc': 0.90625, 'loss': 0.53386}\n",
            "Step 90400: {'train_ae_loss': 0.6751, 'train_ucc_loss': 0.37413, 'train_ucc_acc': 0.9375, 'loss': 0.52462}\n",
            "Step 90420: {'train_ae_loss': 0.67364, 'train_ucc_loss': 0.45721, 'train_ucc_acc': 0.84375, 'loss': 0.56542}\n",
            "Step 90440: {'train_ae_loss': 0.67087, 'train_ucc_loss': 0.38256, 'train_ucc_acc': 0.90625, 'loss': 0.52671}\n",
            "Step 90460: {'train_ae_loss': 0.66842, 'train_ucc_loss': 0.4553, 'train_ucc_acc': 0.84375, 'loss': 0.56186}\n",
            "Step 90480: {'train_ae_loss': 0.6836, 'train_ucc_loss': 0.48491, 'train_ucc_acc': 0.84375, 'loss': 0.58426}\n",
            "Step 90500: {'train_ae_loss': 0.66246, 'train_ucc_loss': 0.50624, 'train_ucc_acc': 0.8125, 'loss': 0.58435}\n",
            "Step 90520: {'train_ae_loss': 0.66728, 'train_ucc_loss': 0.49804, 'train_ucc_acc': 0.8125, 'loss': 0.58266}\n",
            "Step 90540: {'train_ae_loss': 0.65906, 'train_ucc_loss': 0.51624, 'train_ucc_acc': 0.78125, 'loss': 0.58765}\n",
            "Step 90560: {'train_ae_loss': 0.67113, 'train_ucc_loss': 0.35341, 'train_ucc_acc': 0.96875, 'loss': 0.51227}\n",
            "Step 90580: {'train_ae_loss': 0.65722, 'train_ucc_loss': 0.50114, 'train_ucc_acc': 0.8125, 'loss': 0.57918}\n",
            "Step 90600: {'train_ae_loss': 0.68076, 'train_ucc_loss': 0.50119, 'train_ucc_acc': 0.75, 'loss': 0.59098}\n",
            "Step 90620: {'train_ae_loss': 0.67902, 'train_ucc_loss': 0.37865, 'train_ucc_acc': 0.9375, 'loss': 0.52883}\n",
            "Step 90640: {'train_ae_loss': 0.68352, 'train_ucc_loss': 0.37717, 'train_ucc_acc': 0.9375, 'loss': 0.53035}\n",
            "Step 90660: {'train_ae_loss': 0.64128, 'train_ucc_loss': 0.41113, 'train_ucc_acc': 0.90625, 'loss': 0.52621}\n",
            "Step 90680: {'train_ae_loss': 0.63787, 'train_ucc_loss': 0.54558, 'train_ucc_acc': 0.75, 'loss': 0.59172}\n",
            "Step 90700: {'train_ae_loss': 0.65674, 'train_ucc_loss': 0.4419, 'train_ucc_acc': 0.875, 'loss': 0.54932}\n",
            "Step 90720: {'train_ae_loss': 0.66994, 'train_ucc_loss': 0.47892, 'train_ucc_acc': 0.84375, 'loss': 0.57443}\n",
            "Step 90740: {'train_ae_loss': 0.67128, 'train_ucc_loss': 0.4591, 'train_ucc_acc': 0.875, 'loss': 0.56519}\n",
            "Step 90760: {'train_ae_loss': 0.68261, 'train_ucc_loss': 0.46602, 'train_ucc_acc': 0.8125, 'loss': 0.57431}\n",
            "Step 90780: {'train_ae_loss': 0.66362, 'train_ucc_loss': 0.40697, 'train_ucc_acc': 0.9375, 'loss': 0.53529}\n",
            "Step 90800: {'train_ae_loss': 0.66169, 'train_ucc_loss': 0.39571, 'train_ucc_acc': 0.90625, 'loss': 0.5287}\n",
            "Step 90820: {'train_ae_loss': 0.66691, 'train_ucc_loss': 0.49504, 'train_ucc_acc': 0.78125, 'loss': 0.58098}\n",
            "Step 90840: {'train_ae_loss': 0.66956, 'train_ucc_loss': 0.51936, 'train_ucc_acc': 0.75, 'loss': 0.59446}\n",
            "Step 90860: {'train_ae_loss': 0.67739, 'train_ucc_loss': 0.4597, 'train_ucc_acc': 0.84375, 'loss': 0.56855}\n",
            "Step 90880: {'train_ae_loss': 0.66986, 'train_ucc_loss': 0.50414, 'train_ucc_acc': 0.8125, 'loss': 0.587}\n",
            "Step 90900: {'train_ae_loss': 0.67898, 'train_ucc_loss': 0.58575, 'train_ucc_acc': 0.6875, 'loss': 0.63237}\n",
            "Step 90920: {'train_ae_loss': 0.69517, 'train_ucc_loss': 0.43448, 'train_ucc_acc': 0.875, 'loss': 0.56483}\n",
            "Step 90940: {'train_ae_loss': 0.69888, 'train_ucc_loss': 0.49623, 'train_ucc_acc': 0.8125, 'loss': 0.59756}\n",
            "Step 90960: {'train_ae_loss': 0.6807, 'train_ucc_loss': 0.41586, 'train_ucc_acc': 0.875, 'loss': 0.54828}\n",
            "Step 90980: {'train_ae_loss': 0.6712, 'train_ucc_loss': 0.57923, 'train_ucc_acc': 0.6875, 'loss': 0.62522}\n",
            "Step 91000: {'train_ae_loss': 0.66658, 'train_ucc_loss': 0.41317, 'train_ucc_acc': 0.90625, 'loss': 0.53987}\n",
            "step: 91000,eval_ae_loss: 0.65696,eval_ucc_loss: 0.49888,eval_ucc_acc: 0.80664\n",
            "Step 91020: {'train_ae_loss': 0.64109, 'train_ucc_loss': 0.52243, 'train_ucc_acc': 0.8125, 'loss': 0.58176}\n",
            "Step 91040: {'train_ae_loss': 0.65473, 'train_ucc_loss': 0.48473, 'train_ucc_acc': 0.8125, 'loss': 0.56973}\n",
            "Step 91060: {'train_ae_loss': 0.66899, 'train_ucc_loss': 0.44422, 'train_ucc_acc': 0.875, 'loss': 0.5566}\n",
            "Step 91080: {'train_ae_loss': 0.66745, 'train_ucc_loss': 0.46831, 'train_ucc_acc': 0.84375, 'loss': 0.56788}\n",
            "Step 91100: {'train_ae_loss': 0.66608, 'train_ucc_loss': 0.37753, 'train_ucc_acc': 0.90625, 'loss': 0.5218}\n",
            "Step 91120: {'train_ae_loss': 0.66086, 'train_ucc_loss': 0.44338, 'train_ucc_acc': 0.84375, 'loss': 0.55212}\n",
            "Step 91140: {'train_ae_loss': 0.65473, 'train_ucc_loss': 0.48333, 'train_ucc_acc': 0.8125, 'loss': 0.56903}\n",
            "Step 91160: {'train_ae_loss': 0.65925, 'train_ucc_loss': 0.36942, 'train_ucc_acc': 0.9375, 'loss': 0.51434}\n",
            "Step 91180: {'train_ae_loss': 0.65114, 'train_ucc_loss': 0.54793, 'train_ucc_acc': 0.78125, 'loss': 0.59953}\n",
            "Step 91200: {'train_ae_loss': 0.67619, 'train_ucc_loss': 0.5271, 'train_ucc_acc': 0.75, 'loss': 0.60165}\n",
            "Step 91220: {'train_ae_loss': 0.66298, 'train_ucc_loss': 0.39828, 'train_ucc_acc': 0.90625, 'loss': 0.53063}\n",
            "Step 91240: {'train_ae_loss': 0.65806, 'train_ucc_loss': 0.47785, 'train_ucc_acc': 0.8125, 'loss': 0.56796}\n",
            "Step 91260: {'train_ae_loss': 0.68924, 'train_ucc_loss': 0.47527, 'train_ucc_acc': 0.8125, 'loss': 0.58225}\n",
            "Step 91280: {'train_ae_loss': 0.65932, 'train_ucc_loss': 0.40773, 'train_ucc_acc': 0.90625, 'loss': 0.53352}\n",
            "Step 91300: {'train_ae_loss': 0.64127, 'train_ucc_loss': 0.59313, 'train_ucc_acc': 0.71875, 'loss': 0.6172}\n",
            "Step 91320: {'train_ae_loss': 0.66023, 'train_ucc_loss': 0.42823, 'train_ucc_acc': 0.90625, 'loss': 0.54423}\n",
            "Step 91340: {'train_ae_loss': 0.66964, 'train_ucc_loss': 0.45346, 'train_ucc_acc': 0.84375, 'loss': 0.56155}\n",
            "Step 91360: {'train_ae_loss': 0.65113, 'train_ucc_loss': 0.45731, 'train_ucc_acc': 0.84375, 'loss': 0.55422}\n",
            "Step 91380: {'train_ae_loss': 0.65344, 'train_ucc_loss': 0.39189, 'train_ucc_acc': 0.875, 'loss': 0.52267}\n",
            "Step 91400: {'train_ae_loss': 0.67771, 'train_ucc_loss': 0.40015, 'train_ucc_acc': 0.90625, 'loss': 0.53893}\n",
            "Step 91420: {'train_ae_loss': 0.66678, 'train_ucc_loss': 0.50921, 'train_ucc_acc': 0.78125, 'loss': 0.58799}\n",
            "Step 91440: {'train_ae_loss': 0.64518, 'train_ucc_loss': 0.47481, 'train_ucc_acc': 0.8125, 'loss': 0.56}\n",
            "Step 91460: {'train_ae_loss': 0.6725, 'train_ucc_loss': 0.44109, 'train_ucc_acc': 0.84375, 'loss': 0.55679}\n",
            "Step 91480: {'train_ae_loss': 0.64539, 'train_ucc_loss': 0.5167, 'train_ucc_acc': 0.78125, 'loss': 0.58104}\n",
            "Step 91500: {'train_ae_loss': 0.6844, 'train_ucc_loss': 0.45663, 'train_ucc_acc': 0.84375, 'loss': 0.57052}\n",
            "Step 91520: {'train_ae_loss': 0.66382, 'train_ucc_loss': 0.4617, 'train_ucc_acc': 0.84375, 'loss': 0.56276}\n",
            "Step 91540: {'train_ae_loss': 0.65981, 'train_ucc_loss': 0.43164, 'train_ucc_acc': 0.875, 'loss': 0.54572}\n",
            "Step 91560: {'train_ae_loss': 0.66615, 'train_ucc_loss': 0.42475, 'train_ucc_acc': 0.875, 'loss': 0.54545}\n",
            "Step 91580: {'train_ae_loss': 0.65278, 'train_ucc_loss': 0.42264, 'train_ucc_acc': 0.875, 'loss': 0.53771}\n",
            "Step 91600: {'train_ae_loss': 0.65031, 'train_ucc_loss': 0.36562, 'train_ucc_acc': 0.9375, 'loss': 0.50797}\n",
            "Step 91620: {'train_ae_loss': 0.67702, 'train_ucc_loss': 0.46422, 'train_ucc_acc': 0.84375, 'loss': 0.57062}\n",
            "Step 91640: {'train_ae_loss': 0.64778, 'train_ucc_loss': 0.49428, 'train_ucc_acc': 0.8125, 'loss': 0.57103}\n",
            "Step 91660: {'train_ae_loss': 0.64973, 'train_ucc_loss': 0.52201, 'train_ucc_acc': 0.78125, 'loss': 0.58587}\n",
            "Step 91680: {'train_ae_loss': 0.65445, 'train_ucc_loss': 0.4876, 'train_ucc_acc': 0.78125, 'loss': 0.57103}\n",
            "Step 91700: {'train_ae_loss': 0.66845, 'train_ucc_loss': 0.44705, 'train_ucc_acc': 0.84375, 'loss': 0.55775}\n",
            "Step 91720: {'train_ae_loss': 0.65357, 'train_ucc_loss': 0.46762, 'train_ucc_acc': 0.8125, 'loss': 0.56059}\n",
            "Step 91740: {'train_ae_loss': 0.65678, 'train_ucc_loss': 0.50976, 'train_ucc_acc': 0.78125, 'loss': 0.58327}\n",
            "Step 91760: {'train_ae_loss': 0.66226, 'train_ucc_loss': 0.49021, 'train_ucc_acc': 0.8125, 'loss': 0.57624}\n",
            "Step 91780: {'train_ae_loss': 0.66345, 'train_ucc_loss': 0.5073, 'train_ucc_acc': 0.8125, 'loss': 0.58538}\n",
            "Step 91800: {'train_ae_loss': 0.66266, 'train_ucc_loss': 0.44122, 'train_ucc_acc': 0.875, 'loss': 0.55194}\n",
            "Step 91820: {'train_ae_loss': 0.67311, 'train_ucc_loss': 0.43837, 'train_ucc_acc': 0.90625, 'loss': 0.55574}\n",
            "Step 91840: {'train_ae_loss': 0.66734, 'train_ucc_loss': 0.44017, 'train_ucc_acc': 0.875, 'loss': 0.55376}\n",
            "Step 91860: {'train_ae_loss': 0.66266, 'train_ucc_loss': 0.4266, 'train_ucc_acc': 0.875, 'loss': 0.54463}\n",
            "Step 91880: {'train_ae_loss': 0.66302, 'train_ucc_loss': 0.557, 'train_ucc_acc': 0.75, 'loss': 0.61001}\n",
            "Step 91900: {'train_ae_loss': 0.66918, 'train_ucc_loss': 0.4112, 'train_ucc_acc': 0.84375, 'loss': 0.54019}\n",
            "Step 91920: {'train_ae_loss': 0.65547, 'train_ucc_loss': 0.441, 'train_ucc_acc': 0.84375, 'loss': 0.54824}\n",
            "Step 91940: {'train_ae_loss': 0.68629, 'train_ucc_loss': 0.4012, 'train_ucc_acc': 0.9375, 'loss': 0.54375}\n",
            "Step 91960: {'train_ae_loss': 0.67659, 'train_ucc_loss': 0.4281, 'train_ucc_acc': 0.875, 'loss': 0.55234}\n",
            "Step 91980: {'train_ae_loss': 0.66016, 'train_ucc_loss': 0.42888, 'train_ucc_acc': 0.875, 'loss': 0.54452}\n",
            "Step 92000: {'train_ae_loss': 0.65028, 'train_ucc_loss': 0.47527, 'train_ucc_acc': 0.8125, 'loss': 0.56278}\n",
            "step: 92000,eval_ae_loss: 0.65737,eval_ucc_loss: 0.46442,eval_ucc_acc: 0.8418\n",
            "Step 92020: {'train_ae_loss': 0.65957, 'train_ucc_loss': 0.43187, 'train_ucc_acc': 0.875, 'loss': 0.54572}\n",
            "Step 92040: {'train_ae_loss': 0.65607, 'train_ucc_loss': 0.48975, 'train_ucc_acc': 0.8125, 'loss': 0.57291}\n",
            "Step 92060: {'train_ae_loss': 0.68032, 'train_ucc_loss': 0.45526, 'train_ucc_acc': 0.8125, 'loss': 0.56779}\n",
            "Step 92080: {'train_ae_loss': 0.68407, 'train_ucc_loss': 0.3796, 'train_ucc_acc': 0.9375, 'loss': 0.53184}\n",
            "Step 92100: {'train_ae_loss': 0.67634, 'train_ucc_loss': 0.41145, 'train_ucc_acc': 0.875, 'loss': 0.54389}\n",
            "Step 92120: {'train_ae_loss': 0.65301, 'train_ucc_loss': 0.33983, 'train_ucc_acc': 0.96875, 'loss': 0.49642}\n",
            "Step 92140: {'train_ae_loss': 0.64418, 'train_ucc_loss': 0.58029, 'train_ucc_acc': 0.6875, 'loss': 0.61223}\n",
            "Step 92160: {'train_ae_loss': 0.64168, 'train_ucc_loss': 0.48066, 'train_ucc_acc': 0.8125, 'loss': 0.56117}\n",
            "Step 92180: {'train_ae_loss': 0.66582, 'train_ucc_loss': 0.46845, 'train_ucc_acc': 0.8125, 'loss': 0.56714}\n",
            "Step 92200: {'train_ae_loss': 0.67182, 'train_ucc_loss': 0.49304, 'train_ucc_acc': 0.8125, 'loss': 0.58243}\n",
            "Step 92220: {'train_ae_loss': 0.66241, 'train_ucc_loss': 0.50564, 'train_ucc_acc': 0.78125, 'loss': 0.58402}\n",
            "Step 92240: {'train_ae_loss': 0.64603, 'train_ucc_loss': 0.49268, 'train_ucc_acc': 0.8125, 'loss': 0.56935}\n",
            "Step 92260: {'train_ae_loss': 0.66074, 'train_ucc_loss': 0.38611, 'train_ucc_acc': 0.9375, 'loss': 0.52342}\n",
            "Step 92280: {'train_ae_loss': 0.65768, 'train_ucc_loss': 0.44107, 'train_ucc_acc': 0.875, 'loss': 0.54937}\n",
            "Step 92300: {'train_ae_loss': 0.66668, 'train_ucc_loss': 0.49851, 'train_ucc_acc': 0.8125, 'loss': 0.5826}\n",
            "Step 92320: {'train_ae_loss': 0.64824, 'train_ucc_loss': 0.6399, 'train_ucc_acc': 0.65625, 'loss': 0.64407}\n",
            "Step 92340: {'train_ae_loss': 0.6653, 'train_ucc_loss': 0.3871, 'train_ucc_acc': 0.9375, 'loss': 0.5262}\n",
            "Step 92360: {'train_ae_loss': 0.66937, 'train_ucc_loss': 0.47633, 'train_ucc_acc': 0.84375, 'loss': 0.57285}\n",
            "Step 92380: {'train_ae_loss': 0.66612, 'train_ucc_loss': 0.52652, 'train_ucc_acc': 0.78125, 'loss': 0.59632}\n",
            "Step 92400: {'train_ae_loss': 0.65695, 'train_ucc_loss': 0.42934, 'train_ucc_acc': 0.875, 'loss': 0.54315}\n",
            "Step 92420: {'train_ae_loss': 0.66302, 'train_ucc_loss': 0.48263, 'train_ucc_acc': 0.8125, 'loss': 0.57282}\n",
            "Step 92440: {'train_ae_loss': 0.67116, 'train_ucc_loss': 0.45962, 'train_ucc_acc': 0.875, 'loss': 0.56539}\n",
            "Step 92460: {'train_ae_loss': 0.66657, 'train_ucc_loss': 0.45827, 'train_ucc_acc': 0.84375, 'loss': 0.56242}\n",
            "Step 92480: {'train_ae_loss': 0.66818, 'train_ucc_loss': 0.34501, 'train_ucc_acc': 0.96875, 'loss': 0.5066}\n",
            "Step 92500: {'train_ae_loss': 0.65961, 'train_ucc_loss': 0.4913, 'train_ucc_acc': 0.8125, 'loss': 0.57545}\n",
            "Step 92520: {'train_ae_loss': 0.68728, 'train_ucc_loss': 0.39736, 'train_ucc_acc': 0.90625, 'loss': 0.54232}\n",
            "Step 92540: {'train_ae_loss': 0.65139, 'train_ucc_loss': 0.45457, 'train_ucc_acc': 0.84375, 'loss': 0.55298}\n",
            "Step 92560: {'train_ae_loss': 0.66669, 'train_ucc_loss': 0.51753, 'train_ucc_acc': 0.78125, 'loss': 0.59211}\n",
            "Step 92580: {'train_ae_loss': 0.67936, 'train_ucc_loss': 0.42301, 'train_ucc_acc': 0.875, 'loss': 0.55118}\n",
            "Step 92600: {'train_ae_loss': 0.65431, 'train_ucc_loss': 0.49071, 'train_ucc_acc': 0.8125, 'loss': 0.57251}\n",
            "Step 92620: {'train_ae_loss': 0.66628, 'train_ucc_loss': 0.53599, 'train_ucc_acc': 0.75, 'loss': 0.60114}\n",
            "Step 92640: {'train_ae_loss': 0.65418, 'train_ucc_loss': 0.51472, 'train_ucc_acc': 0.8125, 'loss': 0.58445}\n",
            "Step 92660: {'train_ae_loss': 0.66752, 'train_ucc_loss': 0.41966, 'train_ucc_acc': 0.90625, 'loss': 0.54359}\n",
            "Step 92680: {'train_ae_loss': 0.65614, 'train_ucc_loss': 0.36935, 'train_ucc_acc': 0.96875, 'loss': 0.51275}\n",
            "Step 92700: {'train_ae_loss': 0.6692, 'train_ucc_loss': 0.45126, 'train_ucc_acc': 0.84375, 'loss': 0.56023}\n",
            "Step 92720: {'train_ae_loss': 0.65276, 'train_ucc_loss': 0.67489, 'train_ucc_acc': 0.59375, 'loss': 0.66382}\n",
            "Step 92740: {'train_ae_loss': 0.65887, 'train_ucc_loss': 0.50393, 'train_ucc_acc': 0.78125, 'loss': 0.5814}\n",
            "Step 92760: {'train_ae_loss': 0.66069, 'train_ucc_loss': 0.48969, 'train_ucc_acc': 0.8125, 'loss': 0.57519}\n",
            "Step 92780: {'train_ae_loss': 0.67079, 'train_ucc_loss': 0.44449, 'train_ucc_acc': 0.875, 'loss': 0.55764}\n",
            "Step 92800: {'train_ae_loss': 0.65686, 'train_ucc_loss': 0.42301, 'train_ucc_acc': 0.875, 'loss': 0.53993}\n",
            "Step 92820: {'train_ae_loss': 0.66497, 'train_ucc_loss': 0.65882, 'train_ucc_acc': 0.65625, 'loss': 0.6619}\n",
            "Step 92840: {'train_ae_loss': 0.66312, 'train_ucc_loss': 0.40239, 'train_ucc_acc': 0.9375, 'loss': 0.53276}\n",
            "Step 92860: {'train_ae_loss': 0.64646, 'train_ucc_loss': 0.44532, 'train_ucc_acc': 0.84375, 'loss': 0.54589}\n",
            "Step 92880: {'train_ae_loss': 0.65352, 'train_ucc_loss': 0.53613, 'train_ucc_acc': 0.78125, 'loss': 0.59483}\n",
            "Step 92900: {'train_ae_loss': 0.67825, 'train_ucc_loss': 0.41735, 'train_ucc_acc': 0.90625, 'loss': 0.5478}\n",
            "Step 92920: {'train_ae_loss': 0.66258, 'train_ucc_loss': 0.41188, 'train_ucc_acc': 0.90625, 'loss': 0.53723}\n",
            "Step 92940: {'train_ae_loss': 0.66721, 'train_ucc_loss': 0.46777, 'train_ucc_acc': 0.84375, 'loss': 0.56749}\n",
            "Step 92960: {'train_ae_loss': 0.68203, 'train_ucc_loss': 0.45323, 'train_ucc_acc': 0.875, 'loss': 0.56763}\n",
            "Step 92980: {'train_ae_loss': 0.65662, 'train_ucc_loss': 0.40285, 'train_ucc_acc': 0.9375, 'loss': 0.52973}\n",
            "Step 93000: {'train_ae_loss': 0.65772, 'train_ucc_loss': 0.57122, 'train_ucc_acc': 0.75, 'loss': 0.61447}\n",
            "step: 93000,eval_ae_loss: 0.65569,eval_ucc_loss: 0.5205,eval_ucc_acc: 0.7832\n",
            "Step 93020: {'train_ae_loss': 0.66092, 'train_ucc_loss': 0.50898, 'train_ucc_acc': 0.78125, 'loss': 0.58495}\n",
            "Step 93040: {'train_ae_loss': 0.68002, 'train_ucc_loss': 0.46048, 'train_ucc_acc': 0.84375, 'loss': 0.57025}\n",
            "Step 93060: {'train_ae_loss': 0.6712, 'train_ucc_loss': 0.51164, 'train_ucc_acc': 0.8125, 'loss': 0.59142}\n",
            "Step 93080: {'train_ae_loss': 0.67198, 'train_ucc_loss': 0.42852, 'train_ucc_acc': 0.875, 'loss': 0.55025}\n",
            "Step 93100: {'train_ae_loss': 0.6791, 'train_ucc_loss': 0.43033, 'train_ucc_acc': 0.875, 'loss': 0.55472}\n",
            "Step 93120: {'train_ae_loss': 0.63356, 'train_ucc_loss': 0.55776, 'train_ucc_acc': 0.78125, 'loss': 0.59566}\n",
            "Step 93140: {'train_ae_loss': 0.66366, 'train_ucc_loss': 0.39032, 'train_ucc_acc': 0.90625, 'loss': 0.52699}\n",
            "Step 93160: {'train_ae_loss': 0.64951, 'train_ucc_loss': 0.46178, 'train_ucc_acc': 0.875, 'loss': 0.55565}\n",
            "Step 93180: {'train_ae_loss': 0.66337, 'train_ucc_loss': 0.44276, 'train_ucc_acc': 0.875, 'loss': 0.55307}\n",
            "Step 93200: {'train_ae_loss': 0.65946, 'train_ucc_loss': 0.45464, 'train_ucc_acc': 0.84375, 'loss': 0.55705}\n",
            "Step 93220: {'train_ae_loss': 0.63776, 'train_ucc_loss': 0.50419, 'train_ucc_acc': 0.8125, 'loss': 0.57098}\n",
            "Step 93240: {'train_ae_loss': 0.66324, 'train_ucc_loss': 0.39091, 'train_ucc_acc': 0.9375, 'loss': 0.52707}\n",
            "Step 93260: {'train_ae_loss': 0.63666, 'train_ucc_loss': 0.37526, 'train_ucc_acc': 0.9375, 'loss': 0.50596}\n",
            "Step 93280: {'train_ae_loss': 0.66329, 'train_ucc_loss': 0.48084, 'train_ucc_acc': 0.8125, 'loss': 0.57207}\n",
            "Step 93300: {'train_ae_loss': 0.65705, 'train_ucc_loss': 0.42885, 'train_ucc_acc': 0.875, 'loss': 0.54295}\n",
            "Step 93320: {'train_ae_loss': 0.66043, 'train_ucc_loss': 0.35875, 'train_ucc_acc': 0.96875, 'loss': 0.50959}\n",
            "Step 93340: {'train_ae_loss': 0.66062, 'train_ucc_loss': 0.41614, 'train_ucc_acc': 0.90625, 'loss': 0.53838}\n",
            "Step 93360: {'train_ae_loss': 0.67118, 'train_ucc_loss': 0.40695, 'train_ucc_acc': 0.90625, 'loss': 0.53907}\n",
            "Step 93380: {'train_ae_loss': 0.65605, 'train_ucc_loss': 0.41958, 'train_ucc_acc': 0.90625, 'loss': 0.53781}\n",
            "Step 93400: {'train_ae_loss': 0.67132, 'train_ucc_loss': 0.45373, 'train_ucc_acc': 0.84375, 'loss': 0.56252}\n",
            "Step 93420: {'train_ae_loss': 0.66656, 'train_ucc_loss': 0.40051, 'train_ucc_acc': 0.90625, 'loss': 0.53354}\n",
            "Step 93440: {'train_ae_loss': 0.67016, 'train_ucc_loss': 0.42776, 'train_ucc_acc': 0.90625, 'loss': 0.54896}\n",
            "Step 93460: {'train_ae_loss': 0.66382, 'train_ucc_loss': 0.39807, 'train_ucc_acc': 0.9375, 'loss': 0.53094}\n",
            "Step 93480: {'train_ae_loss': 0.67026, 'train_ucc_loss': 0.51803, 'train_ucc_acc': 0.8125, 'loss': 0.59415}\n",
            "Step 93500: {'train_ae_loss': 0.64977, 'train_ucc_loss': 0.35494, 'train_ucc_acc': 0.96875, 'loss': 0.50235}\n",
            "Step 93520: {'train_ae_loss': 0.65344, 'train_ucc_loss': 0.42586, 'train_ucc_acc': 0.875, 'loss': 0.53965}\n",
            "Step 93540: {'train_ae_loss': 0.65956, 'train_ucc_loss': 0.35147, 'train_ucc_acc': 0.96875, 'loss': 0.50551}\n",
            "Step 93560: {'train_ae_loss': 0.6636, 'train_ucc_loss': 0.48612, 'train_ucc_acc': 0.8125, 'loss': 0.57486}\n",
            "Step 93580: {'train_ae_loss': 0.65841, 'train_ucc_loss': 0.33518, 'train_ucc_acc': 1.0, 'loss': 0.49679}\n",
            "Step 93600: {'train_ae_loss': 0.65815, 'train_ucc_loss': 0.44481, 'train_ucc_acc': 0.84375, 'loss': 0.55148}\n",
            "Step 93620: {'train_ae_loss': 0.66554, 'train_ucc_loss': 0.41968, 'train_ucc_acc': 0.90625, 'loss': 0.54261}\n",
            "Step 93640: {'train_ae_loss': 0.67027, 'train_ucc_loss': 0.40579, 'train_ucc_acc': 0.90625, 'loss': 0.53803}\n",
            "Step 93660: {'train_ae_loss': 0.67569, 'train_ucc_loss': 0.46715, 'train_ucc_acc': 0.84375, 'loss': 0.57142}\n",
            "Step 93680: {'train_ae_loss': 0.6664, 'train_ucc_loss': 0.55971, 'train_ucc_acc': 0.75, 'loss': 0.61305}\n",
            "Step 93700: {'train_ae_loss': 0.65621, 'train_ucc_loss': 0.50964, 'train_ucc_acc': 0.8125, 'loss': 0.58292}\n",
            "Step 93720: {'train_ae_loss': 0.66632, 'train_ucc_loss': 0.41043, 'train_ucc_acc': 0.90625, 'loss': 0.53837}\n",
            "Step 93740: {'train_ae_loss': 0.65897, 'train_ucc_loss': 0.52221, 'train_ucc_acc': 0.8125, 'loss': 0.59059}\n",
            "Step 93760: {'train_ae_loss': 0.65711, 'train_ucc_loss': 0.40205, 'train_ucc_acc': 0.9375, 'loss': 0.52958}\n",
            "Step 93780: {'train_ae_loss': 0.67228, 'train_ucc_loss': 0.44683, 'train_ucc_acc': 0.84375, 'loss': 0.55955}\n",
            "Step 93800: {'train_ae_loss': 0.64575, 'train_ucc_loss': 0.39318, 'train_ucc_acc': 0.9375, 'loss': 0.51946}\n",
            "Step 93820: {'train_ae_loss': 0.67062, 'train_ucc_loss': 0.50984, 'train_ucc_acc': 0.8125, 'loss': 0.59023}\n",
            "Step 93840: {'train_ae_loss': 0.67568, 'train_ucc_loss': 0.39688, 'train_ucc_acc': 0.9375, 'loss': 0.53628}\n",
            "Step 93860: {'train_ae_loss': 0.67984, 'train_ucc_loss': 0.36671, 'train_ucc_acc': 0.9375, 'loss': 0.52327}\n",
            "Step 93880: {'train_ae_loss': 0.66231, 'train_ucc_loss': 0.42215, 'train_ucc_acc': 0.875, 'loss': 0.54223}\n",
            "Step 93900: {'train_ae_loss': 0.6657, 'train_ucc_loss': 0.51612, 'train_ucc_acc': 0.78125, 'loss': 0.59091}\n",
            "Step 93920: {'train_ae_loss': 0.68464, 'train_ucc_loss': 0.53137, 'train_ucc_acc': 0.75, 'loss': 0.60801}\n",
            "Step 93940: {'train_ae_loss': 0.66012, 'train_ucc_loss': 0.48989, 'train_ucc_acc': 0.8125, 'loss': 0.57501}\n",
            "Step 93960: {'train_ae_loss': 0.68798, 'train_ucc_loss': 0.49603, 'train_ucc_acc': 0.8125, 'loss': 0.592}\n",
            "Step 93980: {'train_ae_loss': 0.66825, 'train_ucc_loss': 0.37718, 'train_ucc_acc': 0.9375, 'loss': 0.52272}\n",
            "Step 94000: {'train_ae_loss': 0.66726, 'train_ucc_loss': 0.44266, 'train_ucc_acc': 0.875, 'loss': 0.55496}\n",
            "step: 94000,eval_ae_loss: 0.65562,eval_ucc_loss: 0.4992,eval_ucc_acc: 0.80664\n",
            "Step 94020: {'train_ae_loss': 0.65075, 'train_ucc_loss': 0.39994, 'train_ucc_acc': 0.90625, 'loss': 0.52535}\n",
            "Step 94040: {'train_ae_loss': 0.66958, 'train_ucc_loss': 0.42626, 'train_ucc_acc': 0.875, 'loss': 0.54792}\n",
            "Step 94060: {'train_ae_loss': 0.67109, 'train_ucc_loss': 0.45822, 'train_ucc_acc': 0.84375, 'loss': 0.56466}\n",
            "Step 94080: {'train_ae_loss': 0.67386, 'train_ucc_loss': 0.44888, 'train_ucc_acc': 0.84375, 'loss': 0.56137}\n",
            "Step 94100: {'train_ae_loss': 0.65106, 'train_ucc_loss': 0.44712, 'train_ucc_acc': 0.875, 'loss': 0.54909}\n",
            "Step 94120: {'train_ae_loss': 0.67562, 'train_ucc_loss': 0.43882, 'train_ucc_acc': 0.875, 'loss': 0.55722}\n",
            "Step 94140: {'train_ae_loss': 0.65109, 'train_ucc_loss': 0.40617, 'train_ucc_acc': 0.90625, 'loss': 0.52863}\n",
            "Step 94160: {'train_ae_loss': 0.65408, 'train_ucc_loss': 0.5554, 'train_ucc_acc': 0.78125, 'loss': 0.60474}\n",
            "Step 94180: {'train_ae_loss': 0.67364, 'train_ucc_loss': 0.49917, 'train_ucc_acc': 0.8125, 'loss': 0.58641}\n",
            "Step 94200: {'train_ae_loss': 0.67458, 'train_ucc_loss': 0.5007, 'train_ucc_acc': 0.78125, 'loss': 0.58764}\n",
            "Step 94220: {'train_ae_loss': 0.67675, 'train_ucc_loss': 0.54693, 'train_ucc_acc': 0.71875, 'loss': 0.61184}\n",
            "Step 94240: {'train_ae_loss': 0.66214, 'train_ucc_loss': 0.40605, 'train_ucc_acc': 0.875, 'loss': 0.53409}\n",
            "Step 94260: {'train_ae_loss': 0.66543, 'train_ucc_loss': 0.44858, 'train_ucc_acc': 0.84375, 'loss': 0.55701}\n",
            "Step 94280: {'train_ae_loss': 0.66052, 'train_ucc_loss': 0.35509, 'train_ucc_acc': 0.96875, 'loss': 0.5078}\n",
            "Step 94300: {'train_ae_loss': 0.67978, 'train_ucc_loss': 0.35699, 'train_ucc_acc': 0.9375, 'loss': 0.51839}\n",
            "Step 94320: {'train_ae_loss': 0.66135, 'train_ucc_loss': 0.45414, 'train_ucc_acc': 0.875, 'loss': 0.55775}\n",
            "Step 94340: {'train_ae_loss': 0.65403, 'train_ucc_loss': 0.45204, 'train_ucc_acc': 0.875, 'loss': 0.55303}\n",
            "Step 94360: {'train_ae_loss': 0.67721, 'train_ucc_loss': 0.38332, 'train_ucc_acc': 0.90625, 'loss': 0.53027}\n",
            "Step 94380: {'train_ae_loss': 0.65823, 'train_ucc_loss': 0.52882, 'train_ucc_acc': 0.78125, 'loss': 0.59353}\n",
            "Step 94400: {'train_ae_loss': 0.6523, 'train_ucc_loss': 0.45674, 'train_ucc_acc': 0.875, 'loss': 0.55452}\n",
            "Step 94420: {'train_ae_loss': 0.66477, 'train_ucc_loss': 0.41485, 'train_ucc_acc': 0.90625, 'loss': 0.53981}\n",
            "Step 94440: {'train_ae_loss': 0.65311, 'train_ucc_loss': 0.46454, 'train_ucc_acc': 0.8125, 'loss': 0.55882}\n",
            "Step 94460: {'train_ae_loss': 0.66004, 'train_ucc_loss': 0.61905, 'train_ucc_acc': 0.625, 'loss': 0.63954}\n",
            "Step 94480: {'train_ae_loss': 0.68248, 'train_ucc_loss': 0.45246, 'train_ucc_acc': 0.84375, 'loss': 0.56747}\n",
            "Step 94500: {'train_ae_loss': 0.65496, 'train_ucc_loss': 0.51464, 'train_ucc_acc': 0.78125, 'loss': 0.5848}\n",
            "Step 94520: {'train_ae_loss': 0.68697, 'train_ucc_loss': 0.39481, 'train_ucc_acc': 0.90625, 'loss': 0.54089}\n",
            "Step 94540: {'train_ae_loss': 0.66111, 'train_ucc_loss': 0.41429, 'train_ucc_acc': 0.875, 'loss': 0.5377}\n",
            "Step 94560: {'train_ae_loss': 0.66101, 'train_ucc_loss': 0.50982, 'train_ucc_acc': 0.78125, 'loss': 0.58542}\n",
            "Step 94580: {'train_ae_loss': 0.66711, 'train_ucc_loss': 0.43884, 'train_ucc_acc': 0.875, 'loss': 0.55298}\n",
            "Step 94600: {'train_ae_loss': 0.66737, 'train_ucc_loss': 0.49324, 'train_ucc_acc': 0.75, 'loss': 0.5803}\n",
            "Step 94620: {'train_ae_loss': 0.65802, 'train_ucc_loss': 0.46759, 'train_ucc_acc': 0.84375, 'loss': 0.56281}\n",
            "Step 94640: {'train_ae_loss': 0.66505, 'train_ucc_loss': 0.44637, 'train_ucc_acc': 0.84375, 'loss': 0.55571}\n",
            "Step 94660: {'train_ae_loss': 0.64452, 'train_ucc_loss': 0.56947, 'train_ucc_acc': 0.75, 'loss': 0.60699}\n",
            "Step 94680: {'train_ae_loss': 0.66112, 'train_ucc_loss': 0.51852, 'train_ucc_acc': 0.78125, 'loss': 0.58982}\n",
            "Step 94700: {'train_ae_loss': 0.6647, 'train_ucc_loss': 0.40756, 'train_ucc_acc': 0.90625, 'loss': 0.53613}\n",
            "Step 94720: {'train_ae_loss': 0.66954, 'train_ucc_loss': 0.51607, 'train_ucc_acc': 0.8125, 'loss': 0.5928}\n",
            "Step 94740: {'train_ae_loss': 0.66488, 'train_ucc_loss': 0.39954, 'train_ucc_acc': 0.875, 'loss': 0.53221}\n",
            "Step 94760: {'train_ae_loss': 0.67124, 'train_ucc_loss': 0.4975, 'train_ucc_acc': 0.8125, 'loss': 0.58437}\n",
            "Step 94780: {'train_ae_loss': 0.67088, 'train_ucc_loss': 0.44933, 'train_ucc_acc': 0.84375, 'loss': 0.5601}\n",
            "Step 94800: {'train_ae_loss': 0.66311, 'train_ucc_loss': 0.37984, 'train_ucc_acc': 0.9375, 'loss': 0.52148}\n",
            "Step 94820: {'train_ae_loss': 0.67072, 'train_ucc_loss': 0.45279, 'train_ucc_acc': 0.84375, 'loss': 0.56175}\n",
            "Step 94840: {'train_ae_loss': 0.67626, 'train_ucc_loss': 0.44383, 'train_ucc_acc': 0.875, 'loss': 0.56005}\n",
            "Step 94860: {'train_ae_loss': 0.67081, 'train_ucc_loss': 0.40648, 'train_ucc_acc': 0.90625, 'loss': 0.53865}\n",
            "Step 94880: {'train_ae_loss': 0.66842, 'train_ucc_loss': 0.44065, 'train_ucc_acc': 0.84375, 'loss': 0.55453}\n",
            "Step 94900: {'train_ae_loss': 0.65019, 'train_ucc_loss': 0.54608, 'train_ucc_acc': 0.78125, 'loss': 0.59814}\n",
            "Step 94920: {'train_ae_loss': 0.67318, 'train_ucc_loss': 0.3611, 'train_ucc_acc': 0.96875, 'loss': 0.51714}\n",
            "Step 94940: {'train_ae_loss': 0.64622, 'train_ucc_loss': 0.46786, 'train_ucc_acc': 0.84375, 'loss': 0.55704}\n",
            "Step 94960: {'train_ae_loss': 0.66682, 'train_ucc_loss': 0.41884, 'train_ucc_acc': 0.90625, 'loss': 0.54283}\n",
            "Step 94980: {'train_ae_loss': 0.65521, 'train_ucc_loss': 0.44727, 'train_ucc_acc': 0.875, 'loss': 0.55124}\n",
            "Step 95000: {'train_ae_loss': 0.67471, 'train_ucc_loss': 0.41913, 'train_ucc_acc': 0.90625, 'loss': 0.54692}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 21:11:27 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 95000,eval_ae_loss: 0.65277,eval_ucc_loss: 0.46138,eval_ucc_acc: 0.85352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/07 21:11:31 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 95020: {'train_ae_loss': 0.63796, 'train_ucc_loss': 0.50584, 'train_ucc_acc': 0.8125, 'loss': 0.5719}\n",
            "Step 95040: {'train_ae_loss': 0.66951, 'train_ucc_loss': 0.38477, 'train_ucc_acc': 0.9375, 'loss': 0.52714}\n",
            "Step 95060: {'train_ae_loss': 0.65812, 'train_ucc_loss': 0.55013, 'train_ucc_acc': 0.75, 'loss': 0.60413}\n",
            "Step 95080: {'train_ae_loss': 0.66565, 'train_ucc_loss': 0.52144, 'train_ucc_acc': 0.78125, 'loss': 0.59354}\n",
            "Step 95100: {'train_ae_loss': 0.6808, 'train_ucc_loss': 0.4281, 'train_ucc_acc': 0.875, 'loss': 0.55445}\n",
            "Step 95120: {'train_ae_loss': 0.67075, 'train_ucc_loss': 0.37404, 'train_ucc_acc': 0.9375, 'loss': 0.5224}\n",
            "Step 95140: {'train_ae_loss': 0.67029, 'train_ucc_loss': 0.49072, 'train_ucc_acc': 0.84375, 'loss': 0.58051}\n",
            "Step 95160: {'train_ae_loss': 0.66482, 'train_ucc_loss': 0.453, 'train_ucc_acc': 0.84375, 'loss': 0.55891}\n",
            "Step 95180: {'train_ae_loss': 0.67193, 'train_ucc_loss': 0.56056, 'train_ucc_acc': 0.6875, 'loss': 0.61624}\n",
            "Step 95200: {'train_ae_loss': 0.65108, 'train_ucc_loss': 0.37903, 'train_ucc_acc': 0.90625, 'loss': 0.51506}\n",
            "Step 95220: {'train_ae_loss': 0.65764, 'train_ucc_loss': 0.48151, 'train_ucc_acc': 0.8125, 'loss': 0.56957}\n",
            "Step 95240: {'train_ae_loss': 0.65458, 'train_ucc_loss': 0.56942, 'train_ucc_acc': 0.6875, 'loss': 0.612}\n",
            "Step 95260: {'train_ae_loss': 0.67484, 'train_ucc_loss': 0.49031, 'train_ucc_acc': 0.84375, 'loss': 0.58257}\n",
            "Step 95280: {'train_ae_loss': 0.64775, 'train_ucc_loss': 0.40362, 'train_ucc_acc': 0.90625, 'loss': 0.52568}\n",
            "Step 95300: {'train_ae_loss': 0.65169, 'train_ucc_loss': 0.51146, 'train_ucc_acc': 0.8125, 'loss': 0.58158}\n",
            "Step 95320: {'train_ae_loss': 0.65744, 'train_ucc_loss': 0.53251, 'train_ucc_acc': 0.78125, 'loss': 0.59497}\n",
            "Step 95340: {'train_ae_loss': 0.6546, 'train_ucc_loss': 0.44893, 'train_ucc_acc': 0.875, 'loss': 0.55176}\n",
            "Step 95360: {'train_ae_loss': 0.66715, 'train_ucc_loss': 0.45789, 'train_ucc_acc': 0.875, 'loss': 0.56252}\n",
            "Step 95380: {'train_ae_loss': 0.65955, 'train_ucc_loss': 0.54274, 'train_ucc_acc': 0.78125, 'loss': 0.60114}\n",
            "Step 95400: {'train_ae_loss': 0.65836, 'train_ucc_loss': 0.45501, 'train_ucc_acc': 0.875, 'loss': 0.55669}\n",
            "Step 95420: {'train_ae_loss': 0.68115, 'train_ucc_loss': 0.51511, 'train_ucc_acc': 0.78125, 'loss': 0.59813}\n",
            "Step 95440: {'train_ae_loss': 0.66336, 'train_ucc_loss': 0.41073, 'train_ucc_acc': 0.90625, 'loss': 0.53704}\n",
            "Step 95460: {'train_ae_loss': 0.66327, 'train_ucc_loss': 0.31832, 'train_ucc_acc': 1.0, 'loss': 0.49079}\n",
            "Step 95480: {'train_ae_loss': 0.65857, 'train_ucc_loss': 0.58107, 'train_ucc_acc': 0.71875, 'loss': 0.61982}\n",
            "Step 95500: {'train_ae_loss': 0.65349, 'train_ucc_loss': 0.39514, 'train_ucc_acc': 0.9375, 'loss': 0.52432}\n",
            "Step 95520: {'train_ae_loss': 0.67014, 'train_ucc_loss': 0.45452, 'train_ucc_acc': 0.84375, 'loss': 0.56233}\n",
            "Step 95540: {'train_ae_loss': 0.66309, 'train_ucc_loss': 0.39396, 'train_ucc_acc': 0.90625, 'loss': 0.52852}\n",
            "Step 95560: {'train_ae_loss': 0.68011, 'train_ucc_loss': 0.54907, 'train_ucc_acc': 0.75, 'loss': 0.61459}\n",
            "Step 95580: {'train_ae_loss': 0.6712, 'train_ucc_loss': 0.47329, 'train_ucc_acc': 0.8125, 'loss': 0.57225}\n",
            "Step 95600: {'train_ae_loss': 0.66669, 'train_ucc_loss': 0.37138, 'train_ucc_acc': 0.96875, 'loss': 0.51903}\n",
            "Step 95620: {'train_ae_loss': 0.67071, 'train_ucc_loss': 0.44964, 'train_ucc_acc': 0.875, 'loss': 0.56018}\n",
            "Step 95640: {'train_ae_loss': 0.66459, 'train_ucc_loss': 0.41528, 'train_ucc_acc': 0.90625, 'loss': 0.53993}\n",
            "Step 95660: {'train_ae_loss': 0.66774, 'train_ucc_loss': 0.40998, 'train_ucc_acc': 0.9375, 'loss': 0.53886}\n",
            "Step 95680: {'train_ae_loss': 0.65427, 'train_ucc_loss': 0.44927, 'train_ucc_acc': 0.84375, 'loss': 0.55177}\n",
            "Step 95700: {'train_ae_loss': 0.67522, 'train_ucc_loss': 0.47852, 'train_ucc_acc': 0.84375, 'loss': 0.57687}\n",
            "Step 95720: {'train_ae_loss': 0.66479, 'train_ucc_loss': 0.39772, 'train_ucc_acc': 0.90625, 'loss': 0.53126}\n",
            "Step 95740: {'train_ae_loss': 0.65028, 'train_ucc_loss': 0.52322, 'train_ucc_acc': 0.78125, 'loss': 0.58675}\n",
            "Step 95760: {'train_ae_loss': 0.67502, 'train_ucc_loss': 0.50903, 'train_ucc_acc': 0.8125, 'loss': 0.59202}\n",
            "Step 95780: {'train_ae_loss': 0.67768, 'train_ucc_loss': 0.48793, 'train_ucc_acc': 0.8125, 'loss': 0.5828}\n",
            "Step 95800: {'train_ae_loss': 0.65938, 'train_ucc_loss': 0.43587, 'train_ucc_acc': 0.875, 'loss': 0.54762}\n",
            "Step 95820: {'train_ae_loss': 0.66574, 'train_ucc_loss': 0.37384, 'train_ucc_acc': 0.9375, 'loss': 0.51979}\n",
            "Step 95840: {'train_ae_loss': 0.65387, 'train_ucc_loss': 0.48418, 'train_ucc_acc': 0.84375, 'loss': 0.56903}\n",
            "Step 95860: {'train_ae_loss': 0.66411, 'train_ucc_loss': 0.44558, 'train_ucc_acc': 0.84375, 'loss': 0.55484}\n",
            "Step 95880: {'train_ae_loss': 0.66342, 'train_ucc_loss': 0.3841, 'train_ucc_acc': 0.9375, 'loss': 0.52376}\n",
            "Step 95900: {'train_ae_loss': 0.65136, 'train_ucc_loss': 0.43157, 'train_ucc_acc': 0.875, 'loss': 0.54147}\n",
            "Step 95920: {'train_ae_loss': 0.64422, 'train_ucc_loss': 0.37567, 'train_ucc_acc': 0.9375, 'loss': 0.50994}\n",
            "Step 95940: {'train_ae_loss': 0.66889, 'train_ucc_loss': 0.39237, 'train_ucc_acc': 0.90625, 'loss': 0.53063}\n",
            "Step 95960: {'train_ae_loss': 0.66568, 'train_ucc_loss': 0.44028, 'train_ucc_acc': 0.875, 'loss': 0.55298}\n",
            "Step 95980: {'train_ae_loss': 0.66541, 'train_ucc_loss': 0.49745, 'train_ucc_acc': 0.8125, 'loss': 0.58143}\n",
            "Step 96000: {'train_ae_loss': 0.66513, 'train_ucc_loss': 0.45946, 'train_ucc_acc': 0.84375, 'loss': 0.5623}\n",
            "step: 96000,eval_ae_loss: 0.65562,eval_ucc_loss: 0.46943,eval_ucc_acc: 0.83691\n",
            "Step 96020: {'train_ae_loss': 0.66869, 'train_ucc_loss': 0.51581, 'train_ucc_acc': 0.78125, 'loss': 0.59225}\n",
            "Step 96040: {'train_ae_loss': 0.65304, 'train_ucc_loss': 0.43363, 'train_ucc_acc': 0.875, 'loss': 0.54334}\n",
            "Step 96060: {'train_ae_loss': 0.66167, 'train_ucc_loss': 0.45575, 'train_ucc_acc': 0.875, 'loss': 0.55871}\n",
            "Step 96080: {'train_ae_loss': 0.66654, 'train_ucc_loss': 0.42406, 'train_ucc_acc': 0.84375, 'loss': 0.5453}\n",
            "Step 96100: {'train_ae_loss': 0.66661, 'train_ucc_loss': 0.59001, 'train_ucc_acc': 0.71875, 'loss': 0.62831}\n",
            "Step 96120: {'train_ae_loss': 0.67736, 'train_ucc_loss': 0.4515, 'train_ucc_acc': 0.84375, 'loss': 0.56443}\n",
            "Step 96140: {'train_ae_loss': 0.66782, 'train_ucc_loss': 0.40605, 'train_ucc_acc': 0.9375, 'loss': 0.53693}\n",
            "Step 96160: {'train_ae_loss': 0.66806, 'train_ucc_loss': 0.41163, 'train_ucc_acc': 0.90625, 'loss': 0.53985}\n",
            "Step 96180: {'train_ae_loss': 0.6684, 'train_ucc_loss': 0.56148, 'train_ucc_acc': 0.75, 'loss': 0.61494}\n",
            "Step 96200: {'train_ae_loss': 0.65968, 'train_ucc_loss': 0.46635, 'train_ucc_acc': 0.84375, 'loss': 0.56301}\n",
            "Step 96220: {'train_ae_loss': 0.67764, 'train_ucc_loss': 0.44481, 'train_ucc_acc': 0.875, 'loss': 0.56123}\n",
            "Step 96240: {'train_ae_loss': 0.66097, 'train_ucc_loss': 0.45994, 'train_ucc_acc': 0.84375, 'loss': 0.56046}\n",
            "Step 96260: {'train_ae_loss': 0.65943, 'train_ucc_loss': 0.51525, 'train_ucc_acc': 0.8125, 'loss': 0.58734}\n",
            "Step 96280: {'train_ae_loss': 0.65843, 'train_ucc_loss': 0.46874, 'train_ucc_acc': 0.84375, 'loss': 0.56359}\n",
            "Step 96300: {'train_ae_loss': 0.64925, 'train_ucc_loss': 0.40127, 'train_ucc_acc': 0.90625, 'loss': 0.52526}\n",
            "Step 96320: {'train_ae_loss': 0.66156, 'train_ucc_loss': 0.3863, 'train_ucc_acc': 0.9375, 'loss': 0.52393}\n",
            "Step 96340: {'train_ae_loss': 0.65549, 'train_ucc_loss': 0.35513, 'train_ucc_acc': 0.96875, 'loss': 0.50531}\n",
            "Step 96360: {'train_ae_loss': 0.66829, 'train_ucc_loss': 0.46272, 'train_ucc_acc': 0.875, 'loss': 0.56551}\n",
            "Step 96380: {'train_ae_loss': 0.66285, 'train_ucc_loss': 0.39307, 'train_ucc_acc': 0.90625, 'loss': 0.52796}\n",
            "Step 96400: {'train_ae_loss': 0.65872, 'train_ucc_loss': 0.50844, 'train_ucc_acc': 0.78125, 'loss': 0.58358}\n",
            "Step 96420: {'train_ae_loss': 0.66217, 'train_ucc_loss': 0.44079, 'train_ucc_acc': 0.90625, 'loss': 0.55148}\n",
            "Step 96440: {'train_ae_loss': 0.66556, 'train_ucc_loss': 0.53282, 'train_ucc_acc': 0.78125, 'loss': 0.59919}\n",
            "Step 96460: {'train_ae_loss': 0.66746, 'train_ucc_loss': 0.36749, 'train_ucc_acc': 0.9375, 'loss': 0.51747}\n",
            "Step 96480: {'train_ae_loss': 0.64914, 'train_ucc_loss': 0.43848, 'train_ucc_acc': 0.875, 'loss': 0.54381}\n",
            "Step 96500: {'train_ae_loss': 0.6657, 'train_ucc_loss': 0.43704, 'train_ucc_acc': 0.875, 'loss': 0.55137}\n",
            "Step 96520: {'train_ae_loss': 0.64538, 'train_ucc_loss': 0.40439, 'train_ucc_acc': 0.875, 'loss': 0.52489}\n",
            "Step 96540: {'train_ae_loss': 0.66281, 'train_ucc_loss': 0.34714, 'train_ucc_acc': 0.96875, 'loss': 0.50497}\n",
            "Step 96560: {'train_ae_loss': 0.67132, 'train_ucc_loss': 0.4858, 'train_ucc_acc': 0.8125, 'loss': 0.57856}\n",
            "Step 96580: {'train_ae_loss': 0.67346, 'train_ucc_loss': 0.44414, 'train_ucc_acc': 0.875, 'loss': 0.5588}\n",
            "Step 96600: {'train_ae_loss': 0.66467, 'train_ucc_loss': 0.48896, 'train_ucc_acc': 0.8125, 'loss': 0.57681}\n",
            "Step 96620: {'train_ae_loss': 0.66873, 'train_ucc_loss': 0.37112, 'train_ucc_acc': 0.96875, 'loss': 0.51992}\n",
            "Step 96640: {'train_ae_loss': 0.67881, 'train_ucc_loss': 0.36083, 'train_ucc_acc': 0.9375, 'loss': 0.51982}\n",
            "Step 96660: {'train_ae_loss': 0.6646, 'train_ucc_loss': 0.44644, 'train_ucc_acc': 0.90625, 'loss': 0.55552}\n",
            "Step 96680: {'train_ae_loss': 0.65801, 'train_ucc_loss': 0.38078, 'train_ucc_acc': 0.9375, 'loss': 0.5194}\n",
            "Step 96700: {'train_ae_loss': 0.66732, 'train_ucc_loss': 0.48859, 'train_ucc_acc': 0.84375, 'loss': 0.57796}\n",
            "Step 96720: {'train_ae_loss': 0.67286, 'train_ucc_loss': 0.52338, 'train_ucc_acc': 0.78125, 'loss': 0.59812}\n",
            "Step 96740: {'train_ae_loss': 0.65712, 'train_ucc_loss': 0.43328, 'train_ucc_acc': 0.875, 'loss': 0.5452}\n",
            "Step 96760: {'train_ae_loss': 0.66689, 'train_ucc_loss': 0.46336, 'train_ucc_acc': 0.875, 'loss': 0.56512}\n",
            "Step 96780: {'train_ae_loss': 0.68611, 'train_ucc_loss': 0.45041, 'train_ucc_acc': 0.84375, 'loss': 0.56826}\n",
            "Step 96800: {'train_ae_loss': 0.66925, 'train_ucc_loss': 0.42545, 'train_ucc_acc': 0.875, 'loss': 0.54735}\n",
            "Step 96820: {'train_ae_loss': 0.66916, 'train_ucc_loss': 0.46703, 'train_ucc_acc': 0.84375, 'loss': 0.5681}\n",
            "Step 96840: {'train_ae_loss': 0.66788, 'train_ucc_loss': 0.46128, 'train_ucc_acc': 0.84375, 'loss': 0.56458}\n",
            "Step 96860: {'train_ae_loss': 0.64932, 'train_ucc_loss': 0.46044, 'train_ucc_acc': 0.84375, 'loss': 0.55488}\n",
            "Step 96880: {'train_ae_loss': 0.64208, 'train_ucc_loss': 0.47734, 'train_ucc_acc': 0.8125, 'loss': 0.55971}\n",
            "Step 96900: {'train_ae_loss': 0.65573, 'train_ucc_loss': 0.53191, 'train_ucc_acc': 0.75, 'loss': 0.59382}\n",
            "Step 96920: {'train_ae_loss': 0.66473, 'train_ucc_loss': 0.48481, 'train_ucc_acc': 0.78125, 'loss': 0.57477}\n",
            "Step 96940: {'train_ae_loss': 0.66616, 'train_ucc_loss': 0.39817, 'train_ucc_acc': 0.90625, 'loss': 0.53217}\n",
            "Step 96960: {'train_ae_loss': 0.65192, 'train_ucc_loss': 0.36599, 'train_ucc_acc': 0.9375, 'loss': 0.50895}\n",
            "Step 96980: {'train_ae_loss': 0.67474, 'train_ucc_loss': 0.47262, 'train_ucc_acc': 0.84375, 'loss': 0.57368}\n",
            "Step 97000: {'train_ae_loss': 0.65824, 'train_ucc_loss': 0.4044, 'train_ucc_acc': 0.875, 'loss': 0.53132}\n",
            "step: 97000,eval_ae_loss: 0.65289,eval_ucc_loss: 0.47994,eval_ucc_acc: 0.82812\n",
            "Step 97020: {'train_ae_loss': 0.64327, 'train_ucc_loss': 0.4129, 'train_ucc_acc': 0.90625, 'loss': 0.52809}\n",
            "Step 97040: {'train_ae_loss': 0.66448, 'train_ucc_loss': 0.39172, 'train_ucc_acc': 0.90625, 'loss': 0.5281}\n",
            "Step 97060: {'train_ae_loss': 0.65244, 'train_ucc_loss': 0.48429, 'train_ucc_acc': 0.84375, 'loss': 0.56837}\n",
            "Step 97080: {'train_ae_loss': 0.65139, 'train_ucc_loss': 0.41601, 'train_ucc_acc': 0.875, 'loss': 0.5337}\n",
            "Step 97100: {'train_ae_loss': 0.66568, 'train_ucc_loss': 0.43258, 'train_ucc_acc': 0.90625, 'loss': 0.54913}\n",
            "Step 97120: {'train_ae_loss': 0.6714, 'train_ucc_loss': 0.56514, 'train_ucc_acc': 0.75, 'loss': 0.61827}\n",
            "Step 97140: {'train_ae_loss': 0.67666, 'train_ucc_loss': 0.55235, 'train_ucc_acc': 0.75, 'loss': 0.61451}\n",
            "Step 97160: {'train_ae_loss': 0.6701, 'train_ucc_loss': 0.40872, 'train_ucc_acc': 0.90625, 'loss': 0.53941}\n",
            "Step 97180: {'train_ae_loss': 0.66589, 'train_ucc_loss': 0.46734, 'train_ucc_acc': 0.8125, 'loss': 0.56662}\n",
            "Step 97200: {'train_ae_loss': 0.65325, 'train_ucc_loss': 0.51978, 'train_ucc_acc': 0.8125, 'loss': 0.58652}\n",
            "Step 97220: {'train_ae_loss': 0.67303, 'train_ucc_loss': 0.39074, 'train_ucc_acc': 0.9375, 'loss': 0.53189}\n",
            "Step 97240: {'train_ae_loss': 0.66039, 'train_ucc_loss': 0.47827, 'train_ucc_acc': 0.84375, 'loss': 0.56933}\n",
            "Step 97260: {'train_ae_loss': 0.66309, 'train_ucc_loss': 0.41641, 'train_ucc_acc': 0.90625, 'loss': 0.53975}\n",
            "Step 97280: {'train_ae_loss': 0.67438, 'train_ucc_loss': 0.43664, 'train_ucc_acc': 0.875, 'loss': 0.55551}\n",
            "Step 97300: {'train_ae_loss': 0.66365, 'train_ucc_loss': 0.44122, 'train_ucc_acc': 0.90625, 'loss': 0.55244}\n",
            "Step 97320: {'train_ae_loss': 0.66273, 'train_ucc_loss': 0.54827, 'train_ucc_acc': 0.75, 'loss': 0.6055}\n",
            "Step 97340: {'train_ae_loss': 0.66702, 'train_ucc_loss': 0.56816, 'train_ucc_acc': 0.6875, 'loss': 0.61759}\n",
            "Step 97360: {'train_ae_loss': 0.64571, 'train_ucc_loss': 0.58687, 'train_ucc_acc': 0.71875, 'loss': 0.61629}\n",
            "Step 97380: {'train_ae_loss': 0.65561, 'train_ucc_loss': 0.54244, 'train_ucc_acc': 0.78125, 'loss': 0.59902}\n",
            "Step 97400: {'train_ae_loss': 0.65734, 'train_ucc_loss': 0.59792, 'train_ucc_acc': 0.65625, 'loss': 0.62763}\n",
            "Step 97420: {'train_ae_loss': 0.67556, 'train_ucc_loss': 0.4558, 'train_ucc_acc': 0.875, 'loss': 0.56568}\n",
            "Step 97440: {'train_ae_loss': 0.67094, 'train_ucc_loss': 0.40095, 'train_ucc_acc': 0.875, 'loss': 0.53594}\n",
            "Step 97460: {'train_ae_loss': 0.64821, 'train_ucc_loss': 0.44653, 'train_ucc_acc': 0.875, 'loss': 0.54737}\n",
            "Step 97480: {'train_ae_loss': 0.65401, 'train_ucc_loss': 0.48612, 'train_ucc_acc': 0.84375, 'loss': 0.57006}\n",
            "Step 97500: {'train_ae_loss': 0.65576, 'train_ucc_loss': 0.49246, 'train_ucc_acc': 0.84375, 'loss': 0.57411}\n",
            "Step 97520: {'train_ae_loss': 0.6649, 'train_ucc_loss': 0.40633, 'train_ucc_acc': 0.90625, 'loss': 0.53562}\n",
            "Step 97540: {'train_ae_loss': 0.65746, 'train_ucc_loss': 0.40966, 'train_ucc_acc': 0.90625, 'loss': 0.53356}\n",
            "Step 97560: {'train_ae_loss': 0.66636, 'train_ucc_loss': 0.40884, 'train_ucc_acc': 0.90625, 'loss': 0.5376}\n",
            "Step 97580: {'train_ae_loss': 0.66242, 'train_ucc_loss': 0.37919, 'train_ucc_acc': 0.9375, 'loss': 0.52081}\n",
            "Step 97600: {'train_ae_loss': 0.65622, 'train_ucc_loss': 0.45125, 'train_ucc_acc': 0.84375, 'loss': 0.55374}\n",
            "Step 97620: {'train_ae_loss': 0.66285, 'train_ucc_loss': 0.44137, 'train_ucc_acc': 0.875, 'loss': 0.55211}\n",
            "Step 97640: {'train_ae_loss': 0.65932, 'train_ucc_loss': 0.43992, 'train_ucc_acc': 0.90625, 'loss': 0.54962}\n",
            "Step 97660: {'train_ae_loss': 0.66814, 'train_ucc_loss': 0.5163, 'train_ucc_acc': 0.78125, 'loss': 0.59222}\n",
            "Step 97680: {'train_ae_loss': 0.67959, 'train_ucc_loss': 0.43638, 'train_ucc_acc': 0.875, 'loss': 0.55798}\n",
            "Step 97700: {'train_ae_loss': 0.66815, 'train_ucc_loss': 0.42859, 'train_ucc_acc': 0.875, 'loss': 0.54837}\n",
            "Step 97720: {'train_ae_loss': 0.64642, 'train_ucc_loss': 0.48129, 'train_ucc_acc': 0.78125, 'loss': 0.56386}\n",
            "Step 97740: {'train_ae_loss': 0.65904, 'train_ucc_loss': 0.4009, 'train_ucc_acc': 0.9375, 'loss': 0.52997}\n",
            "Step 97760: {'train_ae_loss': 0.65234, 'train_ucc_loss': 0.47187, 'train_ucc_acc': 0.8125, 'loss': 0.56211}\n",
            "Step 97780: {'train_ae_loss': 0.66754, 'train_ucc_loss': 0.46195, 'train_ucc_acc': 0.84375, 'loss': 0.56474}\n",
            "Step 97800: {'train_ae_loss': 0.67563, 'train_ucc_loss': 0.40425, 'train_ucc_acc': 0.90625, 'loss': 0.53994}\n",
            "Step 97820: {'train_ae_loss': 0.65665, 'train_ucc_loss': 0.41799, 'train_ucc_acc': 0.90625, 'loss': 0.53732}\n",
            "Step 97840: {'train_ae_loss': 0.67629, 'train_ucc_loss': 0.51733, 'train_ucc_acc': 0.8125, 'loss': 0.59681}\n",
            "Step 97860: {'train_ae_loss': 0.68087, 'train_ucc_loss': 0.52508, 'train_ucc_acc': 0.78125, 'loss': 0.60298}\n",
            "Step 97880: {'train_ae_loss': 0.66826, 'train_ucc_loss': 0.42598, 'train_ucc_acc': 0.90625, 'loss': 0.54712}\n",
            "Step 97900: {'train_ae_loss': 0.66622, 'train_ucc_loss': 0.42004, 'train_ucc_acc': 0.90625, 'loss': 0.54313}\n",
            "Step 97920: {'train_ae_loss': 0.67571, 'train_ucc_loss': 0.36702, 'train_ucc_acc': 0.9375, 'loss': 0.52136}\n",
            "Step 97940: {'train_ae_loss': 0.65588, 'train_ucc_loss': 0.35126, 'train_ucc_acc': 0.9375, 'loss': 0.50357}\n",
            "Step 97960: {'train_ae_loss': 0.65651, 'train_ucc_loss': 0.45629, 'train_ucc_acc': 0.8125, 'loss': 0.5564}\n",
            "Step 97980: {'train_ae_loss': 0.64708, 'train_ucc_loss': 0.35689, 'train_ucc_acc': 0.96875, 'loss': 0.50199}\n",
            "Step 98000: {'train_ae_loss': 0.66062, 'train_ucc_loss': 0.52566, 'train_ucc_acc': 0.78125, 'loss': 0.59314}\n",
            "step: 98000,eval_ae_loss: 0.64803,eval_ucc_loss: 0.50476,eval_ucc_acc: 0.80664\n",
            "Step 98020: {'train_ae_loss': 0.65831, 'train_ucc_loss': 0.44577, 'train_ucc_acc': 0.875, 'loss': 0.55204}\n",
            "Step 98040: {'train_ae_loss': 0.65924, 'train_ucc_loss': 0.44976, 'train_ucc_acc': 0.84375, 'loss': 0.5545}\n",
            "Step 98060: {'train_ae_loss': 0.66121, 'train_ucc_loss': 0.54102, 'train_ucc_acc': 0.78125, 'loss': 0.60111}\n",
            "Step 98080: {'train_ae_loss': 0.66734, 'train_ucc_loss': 0.47081, 'train_ucc_acc': 0.8125, 'loss': 0.56908}\n",
            "Step 98100: {'train_ae_loss': 0.66024, 'train_ucc_loss': 0.50383, 'train_ucc_acc': 0.8125, 'loss': 0.58204}\n",
            "Step 98120: {'train_ae_loss': 0.66983, 'train_ucc_loss': 0.38147, 'train_ucc_acc': 0.9375, 'loss': 0.52565}\n",
            "Step 98140: {'train_ae_loss': 0.67029, 'train_ucc_loss': 0.3675, 'train_ucc_acc': 0.9375, 'loss': 0.51889}\n",
            "Step 98160: {'train_ae_loss': 0.66151, 'train_ucc_loss': 0.36847, 'train_ucc_acc': 0.96875, 'loss': 0.51499}\n",
            "Step 98180: {'train_ae_loss': 0.65782, 'train_ucc_loss': 0.44985, 'train_ucc_acc': 0.875, 'loss': 0.55383}\n",
            "Step 98200: {'train_ae_loss': 0.65416, 'train_ucc_loss': 0.40162, 'train_ucc_acc': 0.90625, 'loss': 0.52789}\n",
            "Step 98220: {'train_ae_loss': 0.64989, 'train_ucc_loss': 0.53025, 'train_ucc_acc': 0.78125, 'loss': 0.59007}\n",
            "Step 98240: {'train_ae_loss': 0.65489, 'train_ucc_loss': 0.4711, 'train_ucc_acc': 0.84375, 'loss': 0.563}\n",
            "Step 98260: {'train_ae_loss': 0.67476, 'train_ucc_loss': 0.4204, 'train_ucc_acc': 0.90625, 'loss': 0.54758}\n",
            "Step 98280: {'train_ae_loss': 0.6714, 'train_ucc_loss': 0.39777, 'train_ucc_acc': 0.90625, 'loss': 0.53459}\n",
            "Step 98300: {'train_ae_loss': 0.66413, 'train_ucc_loss': 0.44667, 'train_ucc_acc': 0.875, 'loss': 0.5554}\n",
            "Step 98320: {'train_ae_loss': 0.659, 'train_ucc_loss': 0.37495, 'train_ucc_acc': 0.9375, 'loss': 0.51698}\n",
            "Step 98340: {'train_ae_loss': 0.65567, 'train_ucc_loss': 0.44615, 'train_ucc_acc': 0.84375, 'loss': 0.55091}\n",
            "Step 98360: {'train_ae_loss': 0.65167, 'train_ucc_loss': 0.58124, 'train_ucc_acc': 0.75, 'loss': 0.61646}\n",
            "Step 98380: {'train_ae_loss': 0.66368, 'train_ucc_loss': 0.5091, 'train_ucc_acc': 0.8125, 'loss': 0.58639}\n",
            "Step 98400: {'train_ae_loss': 0.65593, 'train_ucc_loss': 0.50471, 'train_ucc_acc': 0.78125, 'loss': 0.58032}\n",
            "Step 98420: {'train_ae_loss': 0.6576, 'train_ucc_loss': 0.50303, 'train_ucc_acc': 0.78125, 'loss': 0.58031}\n",
            "Step 98440: {'train_ae_loss': 0.65909, 'train_ucc_loss': 0.48663, 'train_ucc_acc': 0.8125, 'loss': 0.57286}\n",
            "Step 98460: {'train_ae_loss': 0.65925, 'train_ucc_loss': 0.54388, 'train_ucc_acc': 0.75, 'loss': 0.60157}\n",
            "Step 98480: {'train_ae_loss': 0.65137, 'train_ucc_loss': 0.40686, 'train_ucc_acc': 0.90625, 'loss': 0.52911}\n",
            "Step 98500: {'train_ae_loss': 0.66385, 'train_ucc_loss': 0.42132, 'train_ucc_acc': 0.875, 'loss': 0.54259}\n",
            "Step 98520: {'train_ae_loss': 0.64844, 'train_ucc_loss': 0.48966, 'train_ucc_acc': 0.78125, 'loss': 0.56905}\n",
            "Step 98540: {'train_ae_loss': 0.65435, 'train_ucc_loss': 0.46243, 'train_ucc_acc': 0.84375, 'loss': 0.55839}\n",
            "Step 98560: {'train_ae_loss': 0.66661, 'train_ucc_loss': 0.4023, 'train_ucc_acc': 0.9375, 'loss': 0.53446}\n",
            "Step 98580: {'train_ae_loss': 0.65555, 'train_ucc_loss': 0.48724, 'train_ucc_acc': 0.84375, 'loss': 0.5714}\n",
            "Step 98600: {'train_ae_loss': 0.67278, 'train_ucc_loss': 0.49452, 'train_ucc_acc': 0.84375, 'loss': 0.58365}\n",
            "Step 98620: {'train_ae_loss': 0.65237, 'train_ucc_loss': 0.43083, 'train_ucc_acc': 0.90625, 'loss': 0.5416}\n",
            "Step 98640: {'train_ae_loss': 0.63155, 'train_ucc_loss': 0.53473, 'train_ucc_acc': 0.75, 'loss': 0.58314}\n",
            "Step 98660: {'train_ae_loss': 0.66736, 'train_ucc_loss': 0.43788, 'train_ucc_acc': 0.84375, 'loss': 0.55262}\n",
            "Step 98680: {'train_ae_loss': 0.6722, 'train_ucc_loss': 0.47307, 'train_ucc_acc': 0.84375, 'loss': 0.57264}\n",
            "Step 98700: {'train_ae_loss': 0.68646, 'train_ucc_loss': 0.43097, 'train_ucc_acc': 0.84375, 'loss': 0.55871}\n",
            "Step 98720: {'train_ae_loss': 0.67526, 'train_ucc_loss': 0.44652, 'train_ucc_acc': 0.875, 'loss': 0.56089}\n",
            "Step 98740: {'train_ae_loss': 0.66833, 'train_ucc_loss': 0.44096, 'train_ucc_acc': 0.875, 'loss': 0.55464}\n",
            "Step 98760: {'train_ae_loss': 0.66191, 'train_ucc_loss': 0.44562, 'train_ucc_acc': 0.875, 'loss': 0.55377}\n",
            "Step 98780: {'train_ae_loss': 0.67096, 'train_ucc_loss': 0.45504, 'train_ucc_acc': 0.84375, 'loss': 0.563}\n",
            "Step 98800: {'train_ae_loss': 0.67495, 'train_ucc_loss': 0.40214, 'train_ucc_acc': 0.90625, 'loss': 0.53855}\n",
            "Step 98820: {'train_ae_loss': 0.66929, 'train_ucc_loss': 0.38677, 'train_ucc_acc': 0.90625, 'loss': 0.52803}\n",
            "Step 98840: {'train_ae_loss': 0.66068, 'train_ucc_loss': 0.50861, 'train_ucc_acc': 0.78125, 'loss': 0.58465}\n",
            "Step 98860: {'train_ae_loss': 0.65609, 'train_ucc_loss': 0.45553, 'train_ucc_acc': 0.84375, 'loss': 0.55581}\n",
            "Step 98880: {'train_ae_loss': 0.68, 'train_ucc_loss': 0.37596, 'train_ucc_acc': 0.9375, 'loss': 0.52798}\n",
            "Step 98900: {'train_ae_loss': 0.66448, 'train_ucc_loss': 0.45714, 'train_ucc_acc': 0.84375, 'loss': 0.56081}\n",
            "Step 98920: {'train_ae_loss': 0.66284, 'train_ucc_loss': 0.45368, 'train_ucc_acc': 0.875, 'loss': 0.55826}\n",
            "Step 98940: {'train_ae_loss': 0.66985, 'train_ucc_loss': 0.45606, 'train_ucc_acc': 0.84375, 'loss': 0.56296}\n",
            "Step 98960: {'train_ae_loss': 0.66251, 'train_ucc_loss': 0.46465, 'train_ucc_acc': 0.84375, 'loss': 0.56358}\n",
            "Step 98980: {'train_ae_loss': 0.68667, 'train_ucc_loss': 0.39583, 'train_ucc_acc': 0.9375, 'loss': 0.54125}\n",
            "Step 99000: {'train_ae_loss': 0.6638, 'train_ucc_loss': 0.42145, 'train_ucc_acc': 0.90625, 'loss': 0.54262}\n",
            "step: 99000,eval_ae_loss: 0.65505,eval_ucc_loss: 0.4876,eval_ucc_acc: 0.82129\n",
            "Step 99020: {'train_ae_loss': 0.68177, 'train_ucc_loss': 0.50321, 'train_ucc_acc': 0.78125, 'loss': 0.59249}\n",
            "Step 99040: {'train_ae_loss': 0.65894, 'train_ucc_loss': 0.4147, 'train_ucc_acc': 0.875, 'loss': 0.53682}\n",
            "Step 99060: {'train_ae_loss': 0.67526, 'train_ucc_loss': 0.41033, 'train_ucc_acc': 0.90625, 'loss': 0.5428}\n",
            "Step 99080: {'train_ae_loss': 0.64359, 'train_ucc_loss': 0.52822, 'train_ucc_acc': 0.78125, 'loss': 0.5859}\n",
            "Step 99100: {'train_ae_loss': 0.65458, 'train_ucc_loss': 0.5215, 'train_ucc_acc': 0.78125, 'loss': 0.58804}\n",
            "Step 99120: {'train_ae_loss': 0.66638, 'train_ucc_loss': 0.4622, 'train_ucc_acc': 0.84375, 'loss': 0.56429}\n",
            "Step 99140: {'train_ae_loss': 0.66378, 'train_ucc_loss': 0.55174, 'train_ucc_acc': 0.78125, 'loss': 0.60776}\n",
            "Step 99160: {'train_ae_loss': 0.66659, 'train_ucc_loss': 0.35588, 'train_ucc_acc': 0.96875, 'loss': 0.51123}\n",
            "Step 99180: {'train_ae_loss': 0.66137, 'train_ucc_loss': 0.46936, 'train_ucc_acc': 0.8125, 'loss': 0.56537}\n",
            "Step 99200: {'train_ae_loss': 0.67494, 'train_ucc_loss': 0.41602, 'train_ucc_acc': 0.875, 'loss': 0.54548}\n",
            "Step 99220: {'train_ae_loss': 0.65314, 'train_ucc_loss': 0.44799, 'train_ucc_acc': 0.84375, 'loss': 0.55057}\n",
            "Step 99240: {'train_ae_loss': 0.6645, 'train_ucc_loss': 0.38066, 'train_ucc_acc': 0.9375, 'loss': 0.52258}\n",
            "Step 99260: {'train_ae_loss': 0.65335, 'train_ucc_loss': 0.42251, 'train_ucc_acc': 0.84375, 'loss': 0.53793}\n",
            "Step 99280: {'train_ae_loss': 0.64841, 'train_ucc_loss': 0.3813, 'train_ucc_acc': 0.9375, 'loss': 0.51486}\n",
            "Step 99300: {'train_ae_loss': 0.66933, 'train_ucc_loss': 0.48334, 'train_ucc_acc': 0.78125, 'loss': 0.57634}\n",
            "Step 99320: {'train_ae_loss': 0.64965, 'train_ucc_loss': 0.46167, 'train_ucc_acc': 0.84375, 'loss': 0.55566}\n",
            "Step 99340: {'train_ae_loss': 0.65885, 'train_ucc_loss': 0.44541, 'train_ucc_acc': 0.875, 'loss': 0.55213}\n",
            "Step 99360: {'train_ae_loss': 0.67873, 'train_ucc_loss': 0.42269, 'train_ucc_acc': 0.875, 'loss': 0.55071}\n",
            "Step 99380: {'train_ae_loss': 0.65377, 'train_ucc_loss': 0.38014, 'train_ucc_acc': 0.9375, 'loss': 0.51695}\n",
            "Step 99400: {'train_ae_loss': 0.65885, 'train_ucc_loss': 0.43708, 'train_ucc_acc': 0.875, 'loss': 0.54797}\n",
            "Step 99420: {'train_ae_loss': 0.65224, 'train_ucc_loss': 0.45303, 'train_ucc_acc': 0.875, 'loss': 0.55264}\n",
            "Step 99440: {'train_ae_loss': 0.6592, 'train_ucc_loss': 0.40666, 'train_ucc_acc': 0.90625, 'loss': 0.53293}\n",
            "Step 99460: {'train_ae_loss': 0.64632, 'train_ucc_loss': 0.5842, 'train_ucc_acc': 0.71875, 'loss': 0.61526}\n",
            "Step 99480: {'train_ae_loss': 0.66988, 'train_ucc_loss': 0.38062, 'train_ucc_acc': 0.9375, 'loss': 0.52525}\n",
            "Step 99500: {'train_ae_loss': 0.67202, 'train_ucc_loss': 0.49031, 'train_ucc_acc': 0.78125, 'loss': 0.58116}\n",
            "Step 99520: {'train_ae_loss': 0.66721, 'train_ucc_loss': 0.51499, 'train_ucc_acc': 0.78125, 'loss': 0.5911}\n",
            "Step 99540: {'train_ae_loss': 0.67232, 'train_ucc_loss': 0.41531, 'train_ucc_acc': 0.90625, 'loss': 0.54382}\n",
            "Step 99560: {'train_ae_loss': 0.67137, 'train_ucc_loss': 0.39389, 'train_ucc_acc': 0.90625, 'loss': 0.53263}\n",
            "Step 99580: {'train_ae_loss': 0.66998, 'train_ucc_loss': 0.48881, 'train_ucc_acc': 0.8125, 'loss': 0.5794}\n",
            "Step 99600: {'train_ae_loss': 0.6616, 'train_ucc_loss': 0.53164, 'train_ucc_acc': 0.75, 'loss': 0.59662}\n",
            "Step 99620: {'train_ae_loss': 0.65272, 'train_ucc_loss': 0.43596, 'train_ucc_acc': 0.84375, 'loss': 0.54434}\n",
            "Step 99640: {'train_ae_loss': 0.66861, 'train_ucc_loss': 0.44125, 'train_ucc_acc': 0.84375, 'loss': 0.55493}\n",
            "Step 99660: {'train_ae_loss': 0.65884, 'train_ucc_loss': 0.45297, 'train_ucc_acc': 0.84375, 'loss': 0.5559}\n",
            "Step 99680: {'train_ae_loss': 0.69124, 'train_ucc_loss': 0.38485, 'train_ucc_acc': 0.9375, 'loss': 0.53804}\n",
            "Step 99700: {'train_ae_loss': 0.66826, 'train_ucc_loss': 0.45189, 'train_ucc_acc': 0.875, 'loss': 0.56007}\n",
            "Step 99720: {'train_ae_loss': 0.6654, 'train_ucc_loss': 0.4553, 'train_ucc_acc': 0.84375, 'loss': 0.56035}\n",
            "Step 99740: {'train_ae_loss': 0.65984, 'train_ucc_loss': 0.45467, 'train_ucc_acc': 0.84375, 'loss': 0.55726}\n",
            "Step 99760: {'train_ae_loss': 0.67315, 'train_ucc_loss': 0.39628, 'train_ucc_acc': 0.9375, 'loss': 0.53471}\n",
            "Step 99780: {'train_ae_loss': 0.66207, 'train_ucc_loss': 0.49793, 'train_ucc_acc': 0.78125, 'loss': 0.58}\n",
            "Step 99800: {'train_ae_loss': 0.66463, 'train_ucc_loss': 0.48442, 'train_ucc_acc': 0.8125, 'loss': 0.57453}\n",
            "Step 99820: {'train_ae_loss': 0.6615, 'train_ucc_loss': 0.56491, 'train_ucc_acc': 0.75, 'loss': 0.6132}\n",
            "Step 99840: {'train_ae_loss': 0.67186, 'train_ucc_loss': 0.52475, 'train_ucc_acc': 0.78125, 'loss': 0.59831}\n",
            "Step 99860: {'train_ae_loss': 0.66634, 'train_ucc_loss': 0.46853, 'train_ucc_acc': 0.84375, 'loss': 0.56744}\n",
            "Step 99880: {'train_ae_loss': 0.64008, 'train_ucc_loss': 0.45547, 'train_ucc_acc': 0.84375, 'loss': 0.54777}\n",
            "Step 99900: {'train_ae_loss': 0.66019, 'train_ucc_loss': 0.49198, 'train_ucc_acc': 0.78125, 'loss': 0.57609}\n",
            "Step 99920: {'train_ae_loss': 0.68114, 'train_ucc_loss': 0.46313, 'train_ucc_acc': 0.84375, 'loss': 0.57214}\n",
            "Step 99940: {'train_ae_loss': 0.65971, 'train_ucc_loss': 0.3954, 'train_ucc_acc': 0.9375, 'loss': 0.52755}\n",
            "Step 99960: {'train_ae_loss': 0.66455, 'train_ucc_loss': 0.34619, 'train_ucc_acc': 0.96875, 'loss': 0.50537}\n",
            "Step 99980: {'train_ae_loss': 0.67856, 'train_ucc_loss': 0.42756, 'train_ucc_acc': 0.875, 'loss': 0.55306}\n",
            "Step 100000: {'train_ae_loss': 0.68464, 'train_ucc_loss': 0.52022, 'train_ucc_acc': 0.78125, 'loss': 0.60243}\n",
            "step: 100000,eval_ae_loss: 0.65558,eval_ucc_loss: 0.48553,eval_ucc_acc: 0.8252\n",
            "Step 100020: {'train_ae_loss': 0.67756, 'train_ucc_loss': 0.50446, 'train_ucc_acc': 0.8125, 'loss': 0.59101}\n",
            "Step 100040: {'train_ae_loss': 0.68322, 'train_ucc_loss': 0.42627, 'train_ucc_acc': 0.875, 'loss': 0.55474}\n",
            "Step 100060: {'train_ae_loss': 0.6647, 'train_ucc_loss': 0.4167, 'train_ucc_acc': 0.90625, 'loss': 0.5407}\n",
            "Step 100080: {'train_ae_loss': 0.64642, 'train_ucc_loss': 0.52409, 'train_ucc_acc': 0.78125, 'loss': 0.58525}\n",
            "Step 100100: {'train_ae_loss': 0.67665, 'train_ucc_loss': 0.53757, 'train_ucc_acc': 0.75, 'loss': 0.60711}\n",
            "Step 100120: {'train_ae_loss': 0.65554, 'train_ucc_loss': 0.42285, 'train_ucc_acc': 0.875, 'loss': 0.5392}\n",
            "Step 100140: {'train_ae_loss': 0.65597, 'train_ucc_loss': 0.5103, 'train_ucc_acc': 0.78125, 'loss': 0.58313}\n",
            "Step 100160: {'train_ae_loss': 0.66836, 'train_ucc_loss': 0.42659, 'train_ucc_acc': 0.875, 'loss': 0.54747}\n",
            "Step 100180: {'train_ae_loss': 0.67189, 'train_ucc_loss': 0.54935, 'train_ucc_acc': 0.75, 'loss': 0.61062}\n",
            "Step 100200: {'train_ae_loss': 0.64427, 'train_ucc_loss': 0.42419, 'train_ucc_acc': 0.90625, 'loss': 0.53423}\n",
            "Step 100220: {'train_ae_loss': 0.66109, 'train_ucc_loss': 0.44234, 'train_ucc_acc': 0.84375, 'loss': 0.55171}\n",
            "Step 100240: {'train_ae_loss': 0.66416, 'train_ucc_loss': 0.4332, 'train_ucc_acc': 0.875, 'loss': 0.54868}\n",
            "Step 100260: {'train_ae_loss': 0.6707, 'train_ucc_loss': 0.50597, 'train_ucc_acc': 0.8125, 'loss': 0.58833}\n",
            "Step 100280: {'train_ae_loss': 0.66183, 'train_ucc_loss': 0.4754, 'train_ucc_acc': 0.84375, 'loss': 0.56861}\n",
            "Step 100300: {'train_ae_loss': 0.6637, 'train_ucc_loss': 0.39669, 'train_ucc_acc': 0.90625, 'loss': 0.5302}\n",
            "Step 100320: {'train_ae_loss': 0.65886, 'train_ucc_loss': 0.38823, 'train_ucc_acc': 0.90625, 'loss': 0.52355}\n",
            "Step 100340: {'train_ae_loss': 0.67438, 'train_ucc_loss': 0.45038, 'train_ucc_acc': 0.875, 'loss': 0.56238}\n",
            "Step 100360: {'train_ae_loss': 0.66157, 'train_ucc_loss': 0.45977, 'train_ucc_acc': 0.84375, 'loss': 0.56067}\n",
            "Step 100380: {'train_ae_loss': 0.67097, 'train_ucc_loss': 0.38847, 'train_ucc_acc': 0.9375, 'loss': 0.52972}\n",
            "Step 100400: {'train_ae_loss': 0.6588, 'train_ucc_loss': 0.51018, 'train_ucc_acc': 0.78125, 'loss': 0.58449}\n",
            "Step 100420: {'train_ae_loss': 0.65363, 'train_ucc_loss': 0.55954, 'train_ucc_acc': 0.75, 'loss': 0.60658}\n",
            "Step 100440: {'train_ae_loss': 0.66421, 'train_ucc_loss': 0.62088, 'train_ucc_acc': 0.65625, 'loss': 0.64254}\n",
            "Step 100460: {'train_ae_loss': 0.66564, 'train_ucc_loss': 0.49048, 'train_ucc_acc': 0.8125, 'loss': 0.57806}\n",
            "Step 100480: {'train_ae_loss': 0.66428, 'train_ucc_loss': 0.38534, 'train_ucc_acc': 0.9375, 'loss': 0.52481}\n",
            "Step 100500: {'train_ae_loss': 0.67343, 'train_ucc_loss': 0.38956, 'train_ucc_acc': 0.90625, 'loss': 0.53149}\n",
            "Step 100520: {'train_ae_loss': 0.66095, 'train_ucc_loss': 0.46654, 'train_ucc_acc': 0.8125, 'loss': 0.56374}\n",
            "Step 100540: {'train_ae_loss': 0.67268, 'train_ucc_loss': 0.50904, 'train_ucc_acc': 0.8125, 'loss': 0.59086}\n",
            "Step 100560: {'train_ae_loss': 0.66359, 'train_ucc_loss': 0.42036, 'train_ucc_acc': 0.90625, 'loss': 0.54197}\n",
            "Step 100580: {'train_ae_loss': 0.66085, 'train_ucc_loss': 0.37057, 'train_ucc_acc': 0.9375, 'loss': 0.51571}\n",
            "Step 100600: {'train_ae_loss': 0.67398, 'train_ucc_loss': 0.38024, 'train_ucc_acc': 0.90625, 'loss': 0.52711}\n",
            "Step 100620: {'train_ae_loss': 0.67278, 'train_ucc_loss': 0.45624, 'train_ucc_acc': 0.84375, 'loss': 0.56451}\n",
            "Step 100640: {'train_ae_loss': 0.66975, 'train_ucc_loss': 0.46046, 'train_ucc_acc': 0.875, 'loss': 0.56511}\n",
            "Step 100660: {'train_ae_loss': 0.65791, 'train_ucc_loss': 0.45782, 'train_ucc_acc': 0.84375, 'loss': 0.55786}\n",
            "Step 100680: {'train_ae_loss': 0.6681, 'train_ucc_loss': 0.37424, 'train_ucc_acc': 0.9375, 'loss': 0.52117}\n",
            "Step 100700: {'train_ae_loss': 0.66521, 'train_ucc_loss': 0.42541, 'train_ucc_acc': 0.90625, 'loss': 0.54531}\n",
            "Step 100720: {'train_ae_loss': 0.66764, 'train_ucc_loss': 0.42244, 'train_ucc_acc': 0.90625, 'loss': 0.54504}\n",
            "Step 100740: {'train_ae_loss': 0.65209, 'train_ucc_loss': 0.40337, 'train_ucc_acc': 0.90625, 'loss': 0.52773}\n",
            "Step 100760: {'train_ae_loss': 0.65021, 'train_ucc_loss': 0.51527, 'train_ucc_acc': 0.78125, 'loss': 0.58274}\n",
            "Step 100780: {'train_ae_loss': 0.65558, 'train_ucc_loss': 0.5402, 'train_ucc_acc': 0.75, 'loss': 0.59789}\n",
            "Step 100800: {'train_ae_loss': 0.68942, 'train_ucc_loss': 0.46505, 'train_ucc_acc': 0.84375, 'loss': 0.57723}\n",
            "Step 100820: {'train_ae_loss': 0.67319, 'train_ucc_loss': 0.44987, 'train_ucc_acc': 0.875, 'loss': 0.56153}\n",
            "Step 100840: {'train_ae_loss': 0.6664, 'train_ucc_loss': 0.44669, 'train_ucc_acc': 0.875, 'loss': 0.55655}\n",
            "Step 100860: {'train_ae_loss': 0.69782, 'train_ucc_loss': 0.37012, 'train_ucc_acc': 0.96875, 'loss': 0.53397}\n",
            "Step 100880: {'train_ae_loss': 0.69225, 'train_ucc_loss': 0.49605, 'train_ucc_acc': 0.8125, 'loss': 0.59414}\n",
            "Step 100900: {'train_ae_loss': 0.6759, 'train_ucc_loss': 0.33945, 'train_ucc_acc': 0.96875, 'loss': 0.50767}\n",
            "Step 100920: {'train_ae_loss': 0.66907, 'train_ucc_loss': 0.51789, 'train_ucc_acc': 0.8125, 'loss': 0.59348}\n",
            "Step 100940: {'train_ae_loss': 0.68363, 'train_ucc_loss': 0.46352, 'train_ucc_acc': 0.84375, 'loss': 0.57358}\n",
            "Step 100960: {'train_ae_loss': 0.67331, 'train_ucc_loss': 0.46241, 'train_ucc_acc': 0.8125, 'loss': 0.56786}\n",
            "Step 100980: {'train_ae_loss': 0.66382, 'train_ucc_loss': 0.49976, 'train_ucc_acc': 0.8125, 'loss': 0.58179}\n",
            "Step 101000: {'train_ae_loss': 0.67808, 'train_ucc_loss': 0.42823, 'train_ucc_acc': 0.875, 'loss': 0.55315}\n",
            "step: 101000,eval_ae_loss: 0.66247,eval_ucc_loss: 0.49559,eval_ucc_acc: 0.80566\n",
            "Step 101020: {'train_ae_loss': 0.66198, 'train_ucc_loss': 0.4572, 'train_ucc_acc': 0.8125, 'loss': 0.55959}\n",
            "Step 101040: {'train_ae_loss': 0.66525, 'train_ucc_loss': 0.42209, 'train_ucc_acc': 0.90625, 'loss': 0.54367}\n",
            "Step 101060: {'train_ae_loss': 0.66095, 'train_ucc_loss': 0.63386, 'train_ucc_acc': 0.65625, 'loss': 0.64741}\n",
            "Step 101080: {'train_ae_loss': 0.67087, 'train_ucc_loss': 0.48094, 'train_ucc_acc': 0.8125, 'loss': 0.5759}\n",
            "Step 101100: {'train_ae_loss': 0.65449, 'train_ucc_loss': 0.46608, 'train_ucc_acc': 0.84375, 'loss': 0.56029}\n",
            "Step 101120: {'train_ae_loss': 0.66834, 'train_ucc_loss': 0.46326, 'train_ucc_acc': 0.875, 'loss': 0.5658}\n",
            "Step 101140: {'train_ae_loss': 0.65897, 'train_ucc_loss': 0.43347, 'train_ucc_acc': 0.84375, 'loss': 0.54622}\n",
            "Step 101160: {'train_ae_loss': 0.68132, 'train_ucc_loss': 0.48488, 'train_ucc_acc': 0.8125, 'loss': 0.5831}\n",
            "Step 101180: {'train_ae_loss': 0.64775, 'train_ucc_loss': 0.50955, 'train_ucc_acc': 0.78125, 'loss': 0.57865}\n",
            "Step 101200: {'train_ae_loss': 0.68176, 'train_ucc_loss': 0.37805, 'train_ucc_acc': 0.9375, 'loss': 0.52991}\n",
            "Step 101220: {'train_ae_loss': 0.67836, 'train_ucc_loss': 0.41043, 'train_ucc_acc': 0.90625, 'loss': 0.5444}\n",
            "Step 101240: {'train_ae_loss': 0.65109, 'train_ucc_loss': 0.46098, 'train_ucc_acc': 0.84375, 'loss': 0.55603}\n",
            "Step 101260: {'train_ae_loss': 0.66577, 'train_ucc_loss': 0.37702, 'train_ucc_acc': 0.9375, 'loss': 0.52139}\n",
            "Step 101280: {'train_ae_loss': 0.68948, 'train_ucc_loss': 0.44322, 'train_ucc_acc': 0.875, 'loss': 0.56635}\n",
            "Step 101300: {'train_ae_loss': 0.67734, 'train_ucc_loss': 0.39009, 'train_ucc_acc': 0.9375, 'loss': 0.53371}\n",
            "Step 101320: {'train_ae_loss': 0.68049, 'train_ucc_loss': 0.35251, 'train_ucc_acc': 0.96875, 'loss': 0.5165}\n",
            "Step 101340: {'train_ae_loss': 0.66154, 'train_ucc_loss': 0.56157, 'train_ucc_acc': 0.71875, 'loss': 0.61156}\n",
            "Step 101360: {'train_ae_loss': 0.65699, 'train_ucc_loss': 0.42842, 'train_ucc_acc': 0.875, 'loss': 0.5427}\n",
            "Step 101380: {'train_ae_loss': 0.65489, 'train_ucc_loss': 0.43732, 'train_ucc_acc': 0.875, 'loss': 0.5461}\n",
            "Step 101400: {'train_ae_loss': 0.65391, 'train_ucc_loss': 0.42312, 'train_ucc_acc': 0.875, 'loss': 0.53851}\n",
            "Step 101420: {'train_ae_loss': 0.66848, 'train_ucc_loss': 0.32371, 'train_ucc_acc': 1.0, 'loss': 0.4961}\n",
            "Step 101440: {'train_ae_loss': 0.68066, 'train_ucc_loss': 0.4985, 'train_ucc_acc': 0.8125, 'loss': 0.58958}\n",
            "Step 101460: {'train_ae_loss': 0.68153, 'train_ucc_loss': 0.41547, 'train_ucc_acc': 0.90625, 'loss': 0.5485}\n",
            "Step 101480: {'train_ae_loss': 0.67491, 'train_ucc_loss': 0.45869, 'train_ucc_acc': 0.8125, 'loss': 0.5668}\n",
            "Step 101500: {'train_ae_loss': 0.65903, 'train_ucc_loss': 0.45642, 'train_ucc_acc': 0.84375, 'loss': 0.55773}\n",
            "Step 101520: {'train_ae_loss': 0.65654, 'train_ucc_loss': 0.49898, 'train_ucc_acc': 0.84375, 'loss': 0.57776}\n",
            "Step 101540: {'train_ae_loss': 0.64953, 'train_ucc_loss': 0.4728, 'train_ucc_acc': 0.84375, 'loss': 0.56116}\n",
            "Step 101560: {'train_ae_loss': 0.65458, 'train_ucc_loss': 0.3698, 'train_ucc_acc': 0.96875, 'loss': 0.51219}\n",
            "Step 101580: {'train_ae_loss': 0.65584, 'train_ucc_loss': 0.46614, 'train_ucc_acc': 0.84375, 'loss': 0.56099}\n",
            "Step 101600: {'train_ae_loss': 0.66245, 'train_ucc_loss': 0.49045, 'train_ucc_acc': 0.8125, 'loss': 0.57645}\n",
            "Step 101620: {'train_ae_loss': 0.66757, 'train_ucc_loss': 0.48807, 'train_ucc_acc': 0.8125, 'loss': 0.57782}\n",
            "Step 101640: {'train_ae_loss': 0.6685, 'train_ucc_loss': 0.41836, 'train_ucc_acc': 0.90625, 'loss': 0.54343}\n",
            "Step 101660: {'train_ae_loss': 0.66571, 'train_ucc_loss': 0.41215, 'train_ucc_acc': 0.875, 'loss': 0.53893}\n",
            "Step 101680: {'train_ae_loss': 0.67654, 'train_ucc_loss': 0.48011, 'train_ucc_acc': 0.84375, 'loss': 0.57833}\n",
            "Step 101700: {'train_ae_loss': 0.66197, 'train_ucc_loss': 0.42773, 'train_ucc_acc': 0.875, 'loss': 0.54485}\n",
            "Step 101720: {'train_ae_loss': 0.63112, 'train_ucc_loss': 0.45744, 'train_ucc_acc': 0.84375, 'loss': 0.54428}\n",
            "Step 101740: {'train_ae_loss': 0.64884, 'train_ucc_loss': 0.46212, 'train_ucc_acc': 0.84375, 'loss': 0.55548}\n",
            "Step 101760: {'train_ae_loss': 0.66647, 'train_ucc_loss': 0.37681, 'train_ucc_acc': 0.9375, 'loss': 0.52164}\n",
            "Step 101780: {'train_ae_loss': 0.6763, 'train_ucc_loss': 0.43873, 'train_ucc_acc': 0.875, 'loss': 0.55752}\n",
            "Step 101800: {'train_ae_loss': 0.66511, 'train_ucc_loss': 0.41421, 'train_ucc_acc': 0.90625, 'loss': 0.53966}\n",
            "Step 101820: {'train_ae_loss': 0.65635, 'train_ucc_loss': 0.45251, 'train_ucc_acc': 0.875, 'loss': 0.55443}\n",
            "Step 101840: {'train_ae_loss': 0.67621, 'train_ucc_loss': 0.44088, 'train_ucc_acc': 0.875, 'loss': 0.55854}\n",
            "Step 101860: {'train_ae_loss': 0.66721, 'train_ucc_loss': 0.40759, 'train_ucc_acc': 0.875, 'loss': 0.5374}\n",
            "Step 101880: {'train_ae_loss': 0.66918, 'train_ucc_loss': 0.49403, 'train_ucc_acc': 0.8125, 'loss': 0.58161}\n",
            "Step 101900: {'train_ae_loss': 0.64478, 'train_ucc_loss': 0.52796, 'train_ucc_acc': 0.78125, 'loss': 0.58637}\n",
            "Step 101920: {'train_ae_loss': 0.65081, 'train_ucc_loss': 0.48694, 'train_ucc_acc': 0.8125, 'loss': 0.56887}\n",
            "Step 101940: {'train_ae_loss': 0.66656, 'train_ucc_loss': 0.47969, 'train_ucc_acc': 0.84375, 'loss': 0.57312}\n",
            "Step 101960: {'train_ae_loss': 0.66978, 'train_ucc_loss': 0.41894, 'train_ucc_acc': 0.875, 'loss': 0.54436}\n",
            "Step 101980: {'train_ae_loss': 0.66295, 'train_ucc_loss': 0.4061, 'train_ucc_acc': 0.90625, 'loss': 0.53452}\n",
            "Step 102000: {'train_ae_loss': 0.67508, 'train_ucc_loss': 0.3844, 'train_ucc_acc': 0.9375, 'loss': 0.52974}\n",
            "step: 102000,eval_ae_loss: 0.64931,eval_ucc_loss: 0.46452,eval_ucc_acc: 0.83887\n",
            "Step 102020: {'train_ae_loss': 0.68253, 'train_ucc_loss': 0.48955, 'train_ucc_acc': 0.8125, 'loss': 0.58604}\n",
            "Step 102040: {'train_ae_loss': 0.66124, 'train_ucc_loss': 0.39471, 'train_ucc_acc': 0.9375, 'loss': 0.52797}\n",
            "Step 102060: {'train_ae_loss': 0.67207, 'train_ucc_loss': 0.44452, 'train_ucc_acc': 0.8125, 'loss': 0.55829}\n",
            "Step 102080: {'train_ae_loss': 0.67356, 'train_ucc_loss': 0.40022, 'train_ucc_acc': 0.9375, 'loss': 0.53689}\n",
            "Step 102100: {'train_ae_loss': 0.64324, 'train_ucc_loss': 0.52519, 'train_ucc_acc': 0.8125, 'loss': 0.58422}\n",
            "Step 102120: {'train_ae_loss': 0.64573, 'train_ucc_loss': 0.59551, 'train_ucc_acc': 0.71875, 'loss': 0.62062}\n",
            "Step 102140: {'train_ae_loss': 0.66663, 'train_ucc_loss': 0.36994, 'train_ucc_acc': 0.9375, 'loss': 0.51829}\n",
            "Step 102160: {'train_ae_loss': 0.6804, 'train_ucc_loss': 0.39349, 'train_ucc_acc': 0.9375, 'loss': 0.53695}\n",
            "Step 102180: {'train_ae_loss': 0.66222, 'train_ucc_loss': 0.54183, 'train_ucc_acc': 0.78125, 'loss': 0.60203}\n",
            "Step 102200: {'train_ae_loss': 0.66034, 'train_ucc_loss': 0.44959, 'train_ucc_acc': 0.84375, 'loss': 0.55496}\n",
            "Step 102220: {'train_ae_loss': 0.66762, 'train_ucc_loss': 0.40581, 'train_ucc_acc': 0.9375, 'loss': 0.53672}\n",
            "Step 102240: {'train_ae_loss': 0.67178, 'train_ucc_loss': 0.51717, 'train_ucc_acc': 0.78125, 'loss': 0.59447}\n",
            "Step 102260: {'train_ae_loss': 0.65096, 'train_ucc_loss': 0.39723, 'train_ucc_acc': 0.90625, 'loss': 0.5241}\n",
            "Step 102280: {'train_ae_loss': 0.65624, 'train_ucc_loss': 0.46043, 'train_ucc_acc': 0.84375, 'loss': 0.55833}\n",
            "Step 102300: {'train_ae_loss': 0.67408, 'train_ucc_loss': 0.39099, 'train_ucc_acc': 0.90625, 'loss': 0.53253}\n",
            "Step 102320: {'train_ae_loss': 0.65496, 'train_ucc_loss': 0.51954, 'train_ucc_acc': 0.78125, 'loss': 0.58725}\n",
            "Step 102340: {'train_ae_loss': 0.65908, 'train_ucc_loss': 0.39736, 'train_ucc_acc': 0.90625, 'loss': 0.52822}\n",
            "Step 102360: {'train_ae_loss': 0.65708, 'train_ucc_loss': 0.48031, 'train_ucc_acc': 0.84375, 'loss': 0.56869}\n",
            "Step 102380: {'train_ae_loss': 0.66419, 'train_ucc_loss': 0.49291, 'train_ucc_acc': 0.78125, 'loss': 0.57855}\n",
            "Step 102400: {'train_ae_loss': 0.65548, 'train_ucc_loss': 0.45485, 'train_ucc_acc': 0.84375, 'loss': 0.55517}\n",
            "Step 102420: {'train_ae_loss': 0.66419, 'train_ucc_loss': 0.49427, 'train_ucc_acc': 0.8125, 'loss': 0.57923}\n",
            "Step 102440: {'train_ae_loss': 0.67158, 'train_ucc_loss': 0.47168, 'train_ucc_acc': 0.8125, 'loss': 0.57163}\n",
            "Step 102460: {'train_ae_loss': 0.65234, 'train_ucc_loss': 0.47498, 'train_ucc_acc': 0.8125, 'loss': 0.56366}\n",
            "Step 102480: {'train_ae_loss': 0.66693, 'train_ucc_loss': 0.43464, 'train_ucc_acc': 0.875, 'loss': 0.55078}\n",
            "Step 102500: {'train_ae_loss': 0.65428, 'train_ucc_loss': 0.5427, 'train_ucc_acc': 0.78125, 'loss': 0.59849}\n",
            "Step 102520: {'train_ae_loss': 0.64542, 'train_ucc_loss': 0.3887, 'train_ucc_acc': 0.9375, 'loss': 0.51706}\n",
            "Step 102540: {'train_ae_loss': 0.65394, 'train_ucc_loss': 0.44375, 'train_ucc_acc': 0.90625, 'loss': 0.54884}\n",
            "Step 102560: {'train_ae_loss': 0.65519, 'train_ucc_loss': 0.43704, 'train_ucc_acc': 0.84375, 'loss': 0.54611}\n",
            "Step 102580: {'train_ae_loss': 0.6595, 'train_ucc_loss': 0.44716, 'train_ucc_acc': 0.875, 'loss': 0.55333}\n",
            "Step 102600: {'train_ae_loss': 0.67199, 'train_ucc_loss': 0.50283, 'train_ucc_acc': 0.8125, 'loss': 0.58741}\n",
            "Step 102620: {'train_ae_loss': 0.6542, 'train_ucc_loss': 0.44175, 'train_ucc_acc': 0.875, 'loss': 0.54798}\n",
            "Step 102640: {'train_ae_loss': 0.66612, 'train_ucc_loss': 0.37665, 'train_ucc_acc': 0.9375, 'loss': 0.52138}\n",
            "Step 102660: {'train_ae_loss': 0.68122, 'train_ucc_loss': 0.407, 'train_ucc_acc': 0.875, 'loss': 0.54411}\n",
            "Step 102680: {'train_ae_loss': 0.67159, 'train_ucc_loss': 0.38623, 'train_ucc_acc': 0.9375, 'loss': 0.52891}\n",
            "Step 102700: {'train_ae_loss': 0.66496, 'train_ucc_loss': 0.39412, 'train_ucc_acc': 0.9375, 'loss': 0.52954}\n",
            "Step 102720: {'train_ae_loss': 0.6541, 'train_ucc_loss': 0.51244, 'train_ucc_acc': 0.8125, 'loss': 0.58327}\n",
            "Step 102740: {'train_ae_loss': 0.65343, 'train_ucc_loss': 0.44264, 'train_ucc_acc': 0.84375, 'loss': 0.54804}\n",
            "Step 102760: {'train_ae_loss': 0.67232, 'train_ucc_loss': 0.40648, 'train_ucc_acc': 0.90625, 'loss': 0.5394}\n",
            "Step 102780: {'train_ae_loss': 0.65332, 'train_ucc_loss': 0.35365, 'train_ucc_acc': 0.96875, 'loss': 0.50349}\n",
            "Step 102800: {'train_ae_loss': 0.64867, 'train_ucc_loss': 0.45542, 'train_ucc_acc': 0.875, 'loss': 0.55205}\n",
            "Step 102820: {'train_ae_loss': 0.65645, 'train_ucc_loss': 0.43281, 'train_ucc_acc': 0.875, 'loss': 0.54463}\n",
            "Step 102840: {'train_ae_loss': 0.65694, 'train_ucc_loss': 0.43498, 'train_ucc_acc': 0.875, 'loss': 0.54596}\n",
            "Step 102860: {'train_ae_loss': 0.65335, 'train_ucc_loss': 0.48941, 'train_ucc_acc': 0.8125, 'loss': 0.57138}\n",
            "Step 102880: {'train_ae_loss': 0.65893, 'train_ucc_loss': 0.48907, 'train_ucc_acc': 0.8125, 'loss': 0.574}\n",
            "Step 102900: {'train_ae_loss': 0.65348, 'train_ucc_loss': 0.55417, 'train_ucc_acc': 0.75, 'loss': 0.60382}\n",
            "Step 102920: {'train_ae_loss': 0.66614, 'train_ucc_loss': 0.44317, 'train_ucc_acc': 0.84375, 'loss': 0.55465}\n",
            "Step 102940: {'train_ae_loss': 0.65645, 'train_ucc_loss': 0.55488, 'train_ucc_acc': 0.75, 'loss': 0.60566}\n",
            "Step 102960: {'train_ae_loss': 0.67035, 'train_ucc_loss': 0.43341, 'train_ucc_acc': 0.90625, 'loss': 0.55188}\n",
            "Step 102980: {'train_ae_loss': 0.67394, 'train_ucc_loss': 0.46256, 'train_ucc_acc': 0.84375, 'loss': 0.56825}\n",
            "Step 103000: {'train_ae_loss': 0.68462, 'train_ucc_loss': 0.36063, 'train_ucc_acc': 0.9375, 'loss': 0.52262}\n",
            "step: 103000,eval_ae_loss: 0.65774,eval_ucc_loss: 0.48209,eval_ucc_acc: 0.82422\n",
            "Step 103020: {'train_ae_loss': 0.66688, 'train_ucc_loss': 0.43431, 'train_ucc_acc': 0.875, 'loss': 0.5506}\n",
            "Step 103040: {'train_ae_loss': 0.67136, 'train_ucc_loss': 0.48829, 'train_ucc_acc': 0.8125, 'loss': 0.57982}\n",
            "Step 103060: {'train_ae_loss': 0.67238, 'train_ucc_loss': 0.48022, 'train_ucc_acc': 0.8125, 'loss': 0.5763}\n",
            "Step 103080: {'train_ae_loss': 0.65541, 'train_ucc_loss': 0.51772, 'train_ucc_acc': 0.78125, 'loss': 0.58657}\n",
            "Step 103100: {'train_ae_loss': 0.65327, 'train_ucc_loss': 0.42439, 'train_ucc_acc': 0.90625, 'loss': 0.53883}\n",
            "Step 103120: {'train_ae_loss': 0.6745, 'train_ucc_loss': 0.50756, 'train_ucc_acc': 0.8125, 'loss': 0.59103}\n",
            "Step 103140: {'train_ae_loss': 0.66135, 'train_ucc_loss': 0.4011, 'train_ucc_acc': 0.90625, 'loss': 0.53122}\n",
            "Step 103160: {'train_ae_loss': 0.65906, 'train_ucc_loss': 0.38852, 'train_ucc_acc': 0.90625, 'loss': 0.52379}\n",
            "Step 103180: {'train_ae_loss': 0.65289, 'train_ucc_loss': 0.40188, 'train_ucc_acc': 0.90625, 'loss': 0.52739}\n",
            "Step 103200: {'train_ae_loss': 0.65569, 'train_ucc_loss': 0.44448, 'train_ucc_acc': 0.875, 'loss': 0.55008}\n",
            "Step 103220: {'train_ae_loss': 0.66362, 'train_ucc_loss': 0.4234, 'train_ucc_acc': 0.90625, 'loss': 0.54351}\n",
            "Step 103240: {'train_ae_loss': 0.65757, 'train_ucc_loss': 0.4318, 'train_ucc_acc': 0.84375, 'loss': 0.54469}\n",
            "Step 103260: {'train_ae_loss': 0.67115, 'train_ucc_loss': 0.3439, 'train_ucc_acc': 0.96875, 'loss': 0.50753}\n",
            "Step 103280: {'train_ae_loss': 0.65774, 'train_ucc_loss': 0.57888, 'train_ucc_acc': 0.71875, 'loss': 0.61831}\n",
            "Step 103300: {'train_ae_loss': 0.6457, 'train_ucc_loss': 0.3929, 'train_ucc_acc': 0.9375, 'loss': 0.5193}\n",
            "Step 103320: {'train_ae_loss': 0.66549, 'train_ucc_loss': 0.37232, 'train_ucc_acc': 0.9375, 'loss': 0.5189}\n",
            "Step 103340: {'train_ae_loss': 0.68441, 'train_ucc_loss': 0.39916, 'train_ucc_acc': 0.90625, 'loss': 0.54179}\n",
            "Step 103360: {'train_ae_loss': 0.66492, 'train_ucc_loss': 0.40235, 'train_ucc_acc': 0.90625, 'loss': 0.53364}\n",
            "Step 103380: {'train_ae_loss': 0.65218, 'train_ucc_loss': 0.48701, 'train_ucc_acc': 0.8125, 'loss': 0.5696}\n",
            "Step 103400: {'train_ae_loss': 0.6516, 'train_ucc_loss': 0.47713, 'train_ucc_acc': 0.84375, 'loss': 0.56437}\n",
            "Step 103420: {'train_ae_loss': 0.65445, 'train_ucc_loss': 0.37452, 'train_ucc_acc': 0.9375, 'loss': 0.51448}\n",
            "Step 103440: {'train_ae_loss': 0.65532, 'train_ucc_loss': 0.41303, 'train_ucc_acc': 0.90625, 'loss': 0.53417}\n",
            "Step 103460: {'train_ae_loss': 0.67133, 'train_ucc_loss': 0.49477, 'train_ucc_acc': 0.8125, 'loss': 0.58305}\n",
            "Step 103480: {'train_ae_loss': 0.67381, 'train_ucc_loss': 0.47695, 'train_ucc_acc': 0.84375, 'loss': 0.57538}\n",
            "Step 103500: {'train_ae_loss': 0.66761, 'train_ucc_loss': 0.56115, 'train_ucc_acc': 0.6875, 'loss': 0.61438}\n",
            "Step 103520: {'train_ae_loss': 0.65316, 'train_ucc_loss': 0.41568, 'train_ucc_acc': 0.90625, 'loss': 0.53442}\n",
            "Step 103540: {'train_ae_loss': 0.67935, 'train_ucc_loss': 0.48238, 'train_ucc_acc': 0.8125, 'loss': 0.58087}\n",
            "Step 103560: {'train_ae_loss': 0.66199, 'train_ucc_loss': 0.44958, 'train_ucc_acc': 0.875, 'loss': 0.55578}\n",
            "Step 103580: {'train_ae_loss': 0.66854, 'train_ucc_loss': 0.45171, 'train_ucc_acc': 0.84375, 'loss': 0.56013}\n",
            "Step 103600: {'train_ae_loss': 0.65372, 'train_ucc_loss': 0.55754, 'train_ucc_acc': 0.75, 'loss': 0.60563}\n",
            "Step 103620: {'train_ae_loss': 0.65481, 'train_ucc_loss': 0.53298, 'train_ucc_acc': 0.8125, 'loss': 0.59389}\n",
            "Step 103640: {'train_ae_loss': 0.65779, 'train_ucc_loss': 0.44871, 'train_ucc_acc': 0.8125, 'loss': 0.55325}\n",
            "Step 103660: {'train_ae_loss': 0.64596, 'train_ucc_loss': 0.37454, 'train_ucc_acc': 0.9375, 'loss': 0.51025}\n",
            "Step 103680: {'train_ae_loss': 0.66566, 'train_ucc_loss': 0.4312, 'train_ucc_acc': 0.875, 'loss': 0.54843}\n",
            "Step 103700: {'train_ae_loss': 0.65442, 'train_ucc_loss': 0.42599, 'train_ucc_acc': 0.875, 'loss': 0.54021}\n",
            "Step 103720: {'train_ae_loss': 0.66107, 'train_ucc_loss': 0.38947, 'train_ucc_acc': 0.9375, 'loss': 0.52527}\n",
            "Step 103740: {'train_ae_loss': 0.64844, 'train_ucc_loss': 0.60807, 'train_ucc_acc': 0.71875, 'loss': 0.62825}\n",
            "Step 103760: {'train_ae_loss': 0.63536, 'train_ucc_loss': 0.55492, 'train_ucc_acc': 0.75, 'loss': 0.59514}\n",
            "Step 103780: {'train_ae_loss': 0.64996, 'train_ucc_loss': 0.37204, 'train_ucc_acc': 0.9375, 'loss': 0.511}\n",
            "Step 103800: {'train_ae_loss': 0.65792, 'train_ucc_loss': 0.4097, 'train_ucc_acc': 0.90625, 'loss': 0.53381}\n",
            "Step 103820: {'train_ae_loss': 0.66556, 'train_ucc_loss': 0.42309, 'train_ucc_acc': 0.875, 'loss': 0.54432}\n",
            "Step 103840: {'train_ae_loss': 0.65135, 'train_ucc_loss': 0.38304, 'train_ucc_acc': 0.90625, 'loss': 0.51719}\n",
            "Step 103860: {'train_ae_loss': 0.65951, 'train_ucc_loss': 0.44084, 'train_ucc_acc': 0.875, 'loss': 0.55017}\n",
            "Step 103880: {'train_ae_loss': 0.64108, 'train_ucc_loss': 0.48507, 'train_ucc_acc': 0.84375, 'loss': 0.56307}\n",
            "Step 103900: {'train_ae_loss': 0.65155, 'train_ucc_loss': 0.43101, 'train_ucc_acc': 0.875, 'loss': 0.54128}\n",
            "Step 103920: {'train_ae_loss': 0.65435, 'train_ucc_loss': 0.56507, 'train_ucc_acc': 0.75, 'loss': 0.60971}\n",
            "Step 103940: {'train_ae_loss': 0.66746, 'train_ucc_loss': 0.48743, 'train_ucc_acc': 0.84375, 'loss': 0.57745}\n",
            "Step 103960: {'train_ae_loss': 0.65142, 'train_ucc_loss': 0.60708, 'train_ucc_acc': 0.65625, 'loss': 0.62925}\n",
            "Step 103980: {'train_ae_loss': 0.67613, 'train_ucc_loss': 0.49096, 'train_ucc_acc': 0.78125, 'loss': 0.58355}\n",
            "Step 104000: {'train_ae_loss': 0.65988, 'train_ucc_loss': 0.34909, 'train_ucc_acc': 0.96875, 'loss': 0.50449}\n",
            "step: 104000,eval_ae_loss: 0.64825,eval_ucc_loss: 0.4849,eval_ucc_acc: 0.82422\n",
            "Step 104020: {'train_ae_loss': 0.65104, 'train_ucc_loss': 0.53027, 'train_ucc_acc': 0.75, 'loss': 0.59065}\n",
            "Step 104040: {'train_ae_loss': 0.66233, 'train_ucc_loss': 0.41156, 'train_ucc_acc': 0.875, 'loss': 0.53695}\n",
            "Step 104060: {'train_ae_loss': 0.64787, 'train_ucc_loss': 0.44611, 'train_ucc_acc': 0.875, 'loss': 0.54699}\n",
            "Step 104080: {'train_ae_loss': 0.66291, 'train_ucc_loss': 0.39898, 'train_ucc_acc': 0.9375, 'loss': 0.53095}\n",
            "Step 104100: {'train_ae_loss': 0.67183, 'train_ucc_loss': 0.47466, 'train_ucc_acc': 0.84375, 'loss': 0.57325}\n",
            "Step 104120: {'train_ae_loss': 0.66127, 'train_ucc_loss': 0.50253, 'train_ucc_acc': 0.78125, 'loss': 0.5819}\n",
            "Step 104140: {'train_ae_loss': 0.6695, 'train_ucc_loss': 0.57695, 'train_ucc_acc': 0.71875, 'loss': 0.62323}\n",
            "Step 104160: {'train_ae_loss': 0.65906, 'train_ucc_loss': 0.44009, 'train_ucc_acc': 0.875, 'loss': 0.54958}\n",
            "Step 104180: {'train_ae_loss': 0.65521, 'train_ucc_loss': 0.5355, 'train_ucc_acc': 0.75, 'loss': 0.59536}\n",
            "Step 104200: {'train_ae_loss': 0.6674, 'train_ucc_loss': 0.50826, 'train_ucc_acc': 0.78125, 'loss': 0.58783}\n",
            "Step 104220: {'train_ae_loss': 0.66869, 'train_ucc_loss': 0.43927, 'train_ucc_acc': 0.875, 'loss': 0.55398}\n",
            "Step 104240: {'train_ae_loss': 0.65244, 'train_ucc_loss': 0.49049, 'train_ucc_acc': 0.8125, 'loss': 0.57147}\n",
            "Step 104260: {'train_ae_loss': 0.65315, 'train_ucc_loss': 0.45996, 'train_ucc_acc': 0.875, 'loss': 0.55656}\n",
            "Step 104280: {'train_ae_loss': 0.65648, 'train_ucc_loss': 0.37754, 'train_ucc_acc': 0.9375, 'loss': 0.51701}\n",
            "Step 104300: {'train_ae_loss': 0.66706, 'train_ucc_loss': 0.52187, 'train_ucc_acc': 0.78125, 'loss': 0.59446}\n",
            "Step 104320: {'train_ae_loss': 0.65755, 'train_ucc_loss': 0.38795, 'train_ucc_acc': 0.9375, 'loss': 0.52275}\n",
            "Step 104340: {'train_ae_loss': 0.65682, 'train_ucc_loss': 0.53687, 'train_ucc_acc': 0.78125, 'loss': 0.59684}\n",
            "Step 104360: {'train_ae_loss': 0.66198, 'train_ucc_loss': 0.3745, 'train_ucc_acc': 0.9375, 'loss': 0.51824}\n",
            "Step 104380: {'train_ae_loss': 0.66544, 'train_ucc_loss': 0.42338, 'train_ucc_acc': 0.875, 'loss': 0.54441}\n",
            "Step 104400: {'train_ae_loss': 0.64863, 'train_ucc_loss': 0.4065, 'train_ucc_acc': 0.875, 'loss': 0.52756}\n",
            "Step 104420: {'train_ae_loss': 0.65892, 'train_ucc_loss': 0.44132, 'train_ucc_acc': 0.875, 'loss': 0.55012}\n",
            "Step 104440: {'train_ae_loss': 0.65721, 'train_ucc_loss': 0.47008, 'train_ucc_acc': 0.84375, 'loss': 0.56365}\n",
            "Step 104460: {'train_ae_loss': 0.67057, 'train_ucc_loss': 0.47254, 'train_ucc_acc': 0.84375, 'loss': 0.57156}\n",
            "Step 104480: {'train_ae_loss': 0.6549, 'train_ucc_loss': 0.50757, 'train_ucc_acc': 0.78125, 'loss': 0.58123}\n",
            "Step 104500: {'train_ae_loss': 0.66317, 'train_ucc_loss': 0.44678, 'train_ucc_acc': 0.84375, 'loss': 0.55497}\n",
            "Step 104520: {'train_ae_loss': 0.64784, 'train_ucc_loss': 0.43284, 'train_ucc_acc': 0.875, 'loss': 0.54034}\n",
            "Step 104540: {'train_ae_loss': 0.65166, 'train_ucc_loss': 0.34378, 'train_ucc_acc': 1.0, 'loss': 0.49772}\n",
            "Step 104560: {'train_ae_loss': 0.65993, 'train_ucc_loss': 0.63586, 'train_ucc_acc': 0.625, 'loss': 0.6479}\n",
            "Step 104580: {'train_ae_loss': 0.65034, 'train_ucc_loss': 0.40145, 'train_ucc_acc': 0.9375, 'loss': 0.52589}\n",
            "Step 104600: {'train_ae_loss': 0.66939, 'train_ucc_loss': 0.42096, 'train_ucc_acc': 0.90625, 'loss': 0.54517}\n",
            "Step 104620: {'train_ae_loss': 0.6847, 'train_ucc_loss': 0.35766, 'train_ucc_acc': 0.96875, 'loss': 0.52118}\n",
            "Step 104640: {'train_ae_loss': 0.66422, 'train_ucc_loss': 0.32891, 'train_ucc_acc': 1.0, 'loss': 0.49656}\n",
            "Step 104660: {'train_ae_loss': 0.67549, 'train_ucc_loss': 0.39948, 'train_ucc_acc': 0.90625, 'loss': 0.53749}\n",
            "Step 104680: {'train_ae_loss': 0.6624, 'train_ucc_loss': 0.44415, 'train_ucc_acc': 0.875, 'loss': 0.55327}\n",
            "Step 104700: {'train_ae_loss': 0.65846, 'train_ucc_loss': 0.39653, 'train_ucc_acc': 0.9375, 'loss': 0.52749}\n",
            "Step 104720: {'train_ae_loss': 0.66419, 'train_ucc_loss': 0.47503, 'train_ucc_acc': 0.84375, 'loss': 0.56961}\n",
            "Step 104740: {'train_ae_loss': 0.66205, 'train_ucc_loss': 0.43612, 'train_ucc_acc': 0.875, 'loss': 0.54908}\n",
            "Step 104760: {'train_ae_loss': 0.68227, 'train_ucc_loss': 0.44823, 'train_ucc_acc': 0.875, 'loss': 0.56525}\n",
            "Step 104780: {'train_ae_loss': 0.65878, 'train_ucc_loss': 0.43188, 'train_ucc_acc': 0.875, 'loss': 0.54533}\n",
            "Step 104800: {'train_ae_loss': 0.6598, 'train_ucc_loss': 0.4151, 'train_ucc_acc': 0.875, 'loss': 0.53745}\n",
            "Step 104820: {'train_ae_loss': 0.6569, 'train_ucc_loss': 0.44521, 'train_ucc_acc': 0.875, 'loss': 0.55106}\n",
            "Step 104840: {'train_ae_loss': 0.6468, 'train_ucc_loss': 0.41582, 'train_ucc_acc': 0.90625, 'loss': 0.53131}\n",
            "Step 104860: {'train_ae_loss': 0.64469, 'train_ucc_loss': 0.4852, 'train_ucc_acc': 0.84375, 'loss': 0.56494}\n",
            "Step 104880: {'train_ae_loss': 0.65803, 'train_ucc_loss': 0.49139, 'train_ucc_acc': 0.8125, 'loss': 0.57471}\n",
            "Step 104900: {'train_ae_loss': 0.66647, 'train_ucc_loss': 0.47859, 'train_ucc_acc': 0.84375, 'loss': 0.57253}\n",
            "Step 104920: {'train_ae_loss': 0.63734, 'train_ucc_loss': 0.44913, 'train_ucc_acc': 0.84375, 'loss': 0.54323}\n",
            "Step 104940: {'train_ae_loss': 0.6797, 'train_ucc_loss': 0.46643, 'train_ucc_acc': 0.8125, 'loss': 0.57306}\n",
            "Step 104960: {'train_ae_loss': 0.64843, 'train_ucc_loss': 0.35352, 'train_ucc_acc': 0.96875, 'loss': 0.50098}\n",
            "Step 104980: {'train_ae_loss': 0.65508, 'train_ucc_loss': 0.3831, 'train_ucc_acc': 0.90625, 'loss': 0.51909}\n",
            "Step 105000: {'train_ae_loss': 0.65157, 'train_ucc_loss': 0.52844, 'train_ucc_acc': 0.78125, 'loss': 0.59001}\n",
            "step: 105000,eval_ae_loss: 0.64813,eval_ucc_loss: 0.5136,eval_ucc_acc: 0.78906\n",
            "Step 105020: {'train_ae_loss': 0.65997, 'train_ucc_loss': 0.5085, 'train_ucc_acc': 0.8125, 'loss': 0.58423}\n",
            "Step 105040: {'train_ae_loss': 0.67967, 'train_ucc_loss': 0.42784, 'train_ucc_acc': 0.875, 'loss': 0.55376}\n",
            "Step 105060: {'train_ae_loss': 0.6729, 'train_ucc_loss': 0.40422, 'train_ucc_acc': 0.90625, 'loss': 0.53856}\n",
            "Step 105080: {'train_ae_loss': 0.66144, 'train_ucc_loss': 0.45539, 'train_ucc_acc': 0.875, 'loss': 0.55842}\n",
            "Step 105100: {'train_ae_loss': 0.66086, 'train_ucc_loss': 0.47106, 'train_ucc_acc': 0.84375, 'loss': 0.56596}\n",
            "Step 105120: {'train_ae_loss': 0.65892, 'train_ucc_loss': 0.43106, 'train_ucc_acc': 0.84375, 'loss': 0.54499}\n",
            "Step 105140: {'train_ae_loss': 0.64985, 'train_ucc_loss': 0.45198, 'train_ucc_acc': 0.8125, 'loss': 0.55092}\n",
            "Step 105160: {'train_ae_loss': 0.66168, 'train_ucc_loss': 0.46907, 'train_ucc_acc': 0.8125, 'loss': 0.56538}\n",
            "Step 105180: {'train_ae_loss': 0.6706, 'train_ucc_loss': 0.44855, 'train_ucc_acc': 0.8125, 'loss': 0.55958}\n",
            "Step 105200: {'train_ae_loss': 0.66943, 'train_ucc_loss': 0.43847, 'train_ucc_acc': 0.875, 'loss': 0.55395}\n",
            "Step 105220: {'train_ae_loss': 0.67623, 'train_ucc_loss': 0.43466, 'train_ucc_acc': 0.875, 'loss': 0.55545}\n",
            "Step 105240: {'train_ae_loss': 0.67005, 'train_ucc_loss': 0.38675, 'train_ucc_acc': 0.90625, 'loss': 0.5284}\n",
            "Step 105260: {'train_ae_loss': 0.65811, 'train_ucc_loss': 0.46485, 'train_ucc_acc': 0.8125, 'loss': 0.56148}\n",
            "Step 105280: {'train_ae_loss': 0.66569, 'train_ucc_loss': 0.45689, 'train_ucc_acc': 0.84375, 'loss': 0.56129}\n",
            "Step 105300: {'train_ae_loss': 0.66139, 'train_ucc_loss': 0.3924, 'train_ucc_acc': 0.90625, 'loss': 0.52689}\n",
            "Step 105320: {'train_ae_loss': 0.64982, 'train_ucc_loss': 0.4916, 'train_ucc_acc': 0.78125, 'loss': 0.57071}\n",
            "Step 105340: {'train_ae_loss': 0.65836, 'train_ucc_loss': 0.40295, 'train_ucc_acc': 0.9375, 'loss': 0.53066}\n",
            "Step 105360: {'train_ae_loss': 0.66943, 'train_ucc_loss': 0.59916, 'train_ucc_acc': 0.6875, 'loss': 0.63429}\n",
            "Step 105380: {'train_ae_loss': 0.66122, 'train_ucc_loss': 0.55507, 'train_ucc_acc': 0.75, 'loss': 0.60814}\n",
            "Step 105400: {'train_ae_loss': 0.66519, 'train_ucc_loss': 0.42016, 'train_ucc_acc': 0.875, 'loss': 0.54268}\n",
            "Step 105420: {'train_ae_loss': 0.68704, 'train_ucc_loss': 0.6002, 'train_ucc_acc': 0.6875, 'loss': 0.64362}\n",
            "Step 105440: {'train_ae_loss': 0.65605, 'train_ucc_loss': 0.42271, 'train_ucc_acc': 0.875, 'loss': 0.53938}\n",
            "Step 105460: {'train_ae_loss': 0.666, 'train_ucc_loss': 0.42089, 'train_ucc_acc': 0.90625, 'loss': 0.54345}\n",
            "Step 105480: {'train_ae_loss': 0.64955, 'train_ucc_loss': 0.45519, 'train_ucc_acc': 0.875, 'loss': 0.55237}\n",
            "Step 105500: {'train_ae_loss': 0.65263, 'train_ucc_loss': 0.43471, 'train_ucc_acc': 0.84375, 'loss': 0.54367}\n",
            "Step 105520: {'train_ae_loss': 0.67128, 'train_ucc_loss': 0.43442, 'train_ucc_acc': 0.875, 'loss': 0.55285}\n",
            "Step 105540: {'train_ae_loss': 0.66714, 'train_ucc_loss': 0.43764, 'train_ucc_acc': 0.875, 'loss': 0.55239}\n",
            "Step 105560: {'train_ae_loss': 0.64195, 'train_ucc_loss': 0.4357, 'train_ucc_acc': 0.875, 'loss': 0.53882}\n",
            "Step 105580: {'train_ae_loss': 0.66049, 'train_ucc_loss': 0.45354, 'train_ucc_acc': 0.875, 'loss': 0.55702}\n",
            "Step 105600: {'train_ae_loss': 0.65499, 'train_ucc_loss': 0.46173, 'train_ucc_acc': 0.8125, 'loss': 0.55836}\n",
            "Step 105620: {'train_ae_loss': 0.66732, 'train_ucc_loss': 0.60286, 'train_ucc_acc': 0.6875, 'loss': 0.63509}\n",
            "Step 105640: {'train_ae_loss': 0.66434, 'train_ucc_loss': 0.3671, 'train_ucc_acc': 0.9375, 'loss': 0.51572}\n",
            "Step 105660: {'train_ae_loss': 0.66567, 'train_ucc_loss': 0.45495, 'train_ucc_acc': 0.875, 'loss': 0.56031}\n",
            "Step 105680: {'train_ae_loss': 0.66121, 'train_ucc_loss': 0.37412, 'train_ucc_acc': 0.9375, 'loss': 0.51766}\n",
            "Step 105700: {'train_ae_loss': 0.63237, 'train_ucc_loss': 0.5662, 'train_ucc_acc': 0.78125, 'loss': 0.59929}\n",
            "Step 105720: {'train_ae_loss': 0.65494, 'train_ucc_loss': 0.48343, 'train_ucc_acc': 0.8125, 'loss': 0.56918}\n",
            "Step 105740: {'train_ae_loss': 0.65803, 'train_ucc_loss': 0.45408, 'train_ucc_acc': 0.875, 'loss': 0.55606}\n",
            "Step 105760: {'train_ae_loss': 0.64703, 'train_ucc_loss': 0.47768, 'train_ucc_acc': 0.84375, 'loss': 0.56236}\n",
            "Step 105780: {'train_ae_loss': 0.66566, 'train_ucc_loss': 0.44616, 'train_ucc_acc': 0.875, 'loss': 0.55591}\n",
            "Step 105800: {'train_ae_loss': 0.67075, 'train_ucc_loss': 0.38323, 'train_ucc_acc': 0.9375, 'loss': 0.52699}\n",
            "Step 105820: {'train_ae_loss': 0.66405, 'train_ucc_loss': 0.39104, 'train_ucc_acc': 0.90625, 'loss': 0.52755}\n",
            "Step 105840: {'train_ae_loss': 0.65575, 'train_ucc_loss': 0.39083, 'train_ucc_acc': 0.90625, 'loss': 0.52329}\n",
            "Step 105860: {'train_ae_loss': 0.65802, 'train_ucc_loss': 0.41299, 'train_ucc_acc': 0.90625, 'loss': 0.53551}\n",
            "Step 105880: {'train_ae_loss': 0.64514, 'train_ucc_loss': 0.51949, 'train_ucc_acc': 0.78125, 'loss': 0.58231}\n",
            "Step 105900: {'train_ae_loss': 0.6567, 'train_ucc_loss': 0.51051, 'train_ucc_acc': 0.78125, 'loss': 0.58361}\n",
            "Step 105920: {'train_ae_loss': 0.6601, 'train_ucc_loss': 0.47632, 'train_ucc_acc': 0.84375, 'loss': 0.56821}\n",
            "Step 105940: {'train_ae_loss': 0.6701, 'train_ucc_loss': 0.37234, 'train_ucc_acc': 0.9375, 'loss': 0.52122}\n",
            "Step 105960: {'train_ae_loss': 0.64373, 'train_ucc_loss': 0.42146, 'train_ucc_acc': 0.90625, 'loss': 0.5326}\n",
            "Step 105980: {'train_ae_loss': 0.67311, 'train_ucc_loss': 0.34258, 'train_ucc_acc': 1.0, 'loss': 0.50784}\n",
            "Step 106000: {'train_ae_loss': 0.65951, 'train_ucc_loss': 0.47464, 'train_ucc_acc': 0.84375, 'loss': 0.56708}\n",
            "step: 106000,eval_ae_loss: 0.65027,eval_ucc_loss: 0.50655,eval_ucc_acc: 0.79492\n",
            "Step 106020: {'train_ae_loss': 0.64731, 'train_ucc_loss': 0.5262, 'train_ucc_acc': 0.75, 'loss': 0.58676}\n",
            "Step 106040: {'train_ae_loss': 0.66996, 'train_ucc_loss': 0.4373, 'train_ucc_acc': 0.84375, 'loss': 0.55363}\n",
            "Step 106060: {'train_ae_loss': 0.65255, 'train_ucc_loss': 0.41188, 'train_ucc_acc': 0.90625, 'loss': 0.53221}\n",
            "Step 106080: {'train_ae_loss': 0.66922, 'train_ucc_loss': 0.45611, 'train_ucc_acc': 0.84375, 'loss': 0.56267}\n",
            "Step 106100: {'train_ae_loss': 0.64643, 'train_ucc_loss': 0.41495, 'train_ucc_acc': 0.90625, 'loss': 0.53069}\n",
            "Step 106120: {'train_ae_loss': 0.66924, 'train_ucc_loss': 0.42126, 'train_ucc_acc': 0.84375, 'loss': 0.54525}\n",
            "Step 106140: {'train_ae_loss': 0.65526, 'train_ucc_loss': 0.34645, 'train_ucc_acc': 0.96875, 'loss': 0.50086}\n",
            "Step 106160: {'train_ae_loss': 0.66534, 'train_ucc_loss': 0.38528, 'train_ucc_acc': 0.90625, 'loss': 0.52531}\n",
            "Step 106180: {'train_ae_loss': 0.64879, 'train_ucc_loss': 0.45445, 'train_ucc_acc': 0.84375, 'loss': 0.55162}\n",
            "Step 106200: {'train_ae_loss': 0.65595, 'train_ucc_loss': 0.56262, 'train_ucc_acc': 0.71875, 'loss': 0.60929}\n",
            "Step 106220: {'train_ae_loss': 0.66494, 'train_ucc_loss': 0.37466, 'train_ucc_acc': 0.9375, 'loss': 0.5198}\n",
            "Step 106240: {'train_ae_loss': 0.67821, 'train_ucc_loss': 0.40714, 'train_ucc_acc': 0.90625, 'loss': 0.54267}\n",
            "Step 106260: {'train_ae_loss': 0.66811, 'train_ucc_loss': 0.4961, 'train_ucc_acc': 0.78125, 'loss': 0.5821}\n",
            "Step 106280: {'train_ae_loss': 0.64636, 'train_ucc_loss': 0.32664, 'train_ucc_acc': 1.0, 'loss': 0.4865}\n",
            "Step 106300: {'train_ae_loss': 0.64702, 'train_ucc_loss': 0.53405, 'train_ucc_acc': 0.78125, 'loss': 0.59053}\n",
            "Step 106320: {'train_ae_loss': 0.65097, 'train_ucc_loss': 0.51065, 'train_ucc_acc': 0.75, 'loss': 0.58081}\n",
            "Step 106340: {'train_ae_loss': 0.66333, 'train_ucc_loss': 0.42329, 'train_ucc_acc': 0.90625, 'loss': 0.54331}\n",
            "Step 106360: {'train_ae_loss': 0.66445, 'train_ucc_loss': 0.39741, 'train_ucc_acc': 0.9375, 'loss': 0.53093}\n",
            "Step 106380: {'train_ae_loss': 0.67161, 'train_ucc_loss': 0.5261, 'train_ucc_acc': 0.75, 'loss': 0.59886}\n",
            "Step 106400: {'train_ae_loss': 0.64991, 'train_ucc_loss': 0.39198, 'train_ucc_acc': 0.90625, 'loss': 0.52094}\n",
            "Step 106420: {'train_ae_loss': 0.67813, 'train_ucc_loss': 0.44368, 'train_ucc_acc': 0.875, 'loss': 0.56091}\n",
            "Step 106440: {'train_ae_loss': 0.66538, 'train_ucc_loss': 0.4355, 'train_ucc_acc': 0.8125, 'loss': 0.55044}\n",
            "Step 106460: {'train_ae_loss': 0.65314, 'train_ucc_loss': 0.38647, 'train_ucc_acc': 0.9375, 'loss': 0.5198}\n",
            "Step 106480: {'train_ae_loss': 0.6533, 'train_ucc_loss': 0.49293, 'train_ucc_acc': 0.78125, 'loss': 0.57311}\n",
            "Step 106500: {'train_ae_loss': 0.65947, 'train_ucc_loss': 0.48016, 'train_ucc_acc': 0.78125, 'loss': 0.56982}\n",
            "Step 106520: {'train_ae_loss': 0.63793, 'train_ucc_loss': 0.54398, 'train_ucc_acc': 0.75, 'loss': 0.59095}\n",
            "Step 106540: {'train_ae_loss': 0.65046, 'train_ucc_loss': 0.42517, 'train_ucc_acc': 0.875, 'loss': 0.53782}\n",
            "Step 106560: {'train_ae_loss': 0.65663, 'train_ucc_loss': 0.4103, 'train_ucc_acc': 0.9375, 'loss': 0.53347}\n",
            "Step 106580: {'train_ae_loss': 0.6571, 'train_ucc_loss': 0.44463, 'train_ucc_acc': 0.84375, 'loss': 0.55087}\n",
            "Step 106600: {'train_ae_loss': 0.6737, 'train_ucc_loss': 0.40352, 'train_ucc_acc': 0.90625, 'loss': 0.53861}\n",
            "Step 106620: {'train_ae_loss': 0.6506, 'train_ucc_loss': 0.38545, 'train_ucc_acc': 0.9375, 'loss': 0.51802}\n",
            "Step 106640: {'train_ae_loss': 0.66876, 'train_ucc_loss': 0.43265, 'train_ucc_acc': 0.875, 'loss': 0.5507}\n",
            "Step 106660: {'train_ae_loss': 0.66781, 'train_ucc_loss': 0.43435, 'train_ucc_acc': 0.875, 'loss': 0.55108}\n",
            "Step 106680: {'train_ae_loss': 0.64689, 'train_ucc_loss': 0.4284, 'train_ucc_acc': 0.875, 'loss': 0.53764}\n",
            "Step 106700: {'train_ae_loss': 0.66315, 'train_ucc_loss': 0.47777, 'train_ucc_acc': 0.84375, 'loss': 0.57046}\n",
            "Step 106720: {'train_ae_loss': 0.65217, 'train_ucc_loss': 0.42771, 'train_ucc_acc': 0.875, 'loss': 0.53994}\n",
            "Step 106740: {'train_ae_loss': 0.6705, 'train_ucc_loss': 0.49772, 'train_ucc_acc': 0.8125, 'loss': 0.58411}\n",
            "Step 106760: {'train_ae_loss': 0.67354, 'train_ucc_loss': 0.43221, 'train_ucc_acc': 0.84375, 'loss': 0.55288}\n",
            "Step 106780: {'train_ae_loss': 0.6493, 'train_ucc_loss': 0.41032, 'train_ucc_acc': 0.875, 'loss': 0.52981}\n",
            "Step 106800: {'train_ae_loss': 0.65999, 'train_ucc_loss': 0.44263, 'train_ucc_acc': 0.875, 'loss': 0.55131}\n",
            "Step 106820: {'train_ae_loss': 0.67845, 'train_ucc_loss': 0.44617, 'train_ucc_acc': 0.8125, 'loss': 0.56231}\n",
            "Step 106840: {'train_ae_loss': 0.68712, 'train_ucc_loss': 0.46069, 'train_ucc_acc': 0.84375, 'loss': 0.5739}\n",
            "Step 106860: {'train_ae_loss': 0.65776, 'train_ucc_loss': 0.54324, 'train_ucc_acc': 0.75, 'loss': 0.6005}\n",
            "Step 106880: {'train_ae_loss': 0.64829, 'train_ucc_loss': 0.44439, 'train_ucc_acc': 0.875, 'loss': 0.54634}\n",
            "Step 106900: {'train_ae_loss': 0.65244, 'train_ucc_loss': 0.37131, 'train_ucc_acc': 0.9375, 'loss': 0.51188}\n",
            "Step 106920: {'train_ae_loss': 0.67205, 'train_ucc_loss': 0.46713, 'train_ucc_acc': 0.84375, 'loss': 0.56959}\n",
            "Step 106940: {'train_ae_loss': 0.65352, 'train_ucc_loss': 0.4292, 'train_ucc_acc': 0.875, 'loss': 0.54136}\n",
            "Step 106960: {'train_ae_loss': 0.64384, 'train_ucc_loss': 0.42839, 'train_ucc_acc': 0.875, 'loss': 0.53611}\n",
            "Step 106980: {'train_ae_loss': 0.64573, 'train_ucc_loss': 0.56257, 'train_ucc_acc': 0.78125, 'loss': 0.60415}\n",
            "Step 107000: {'train_ae_loss': 0.66418, 'train_ucc_loss': 0.49619, 'train_ucc_acc': 0.8125, 'loss': 0.58018}\n",
            "step: 107000,eval_ae_loss: 0.65079,eval_ucc_loss: 0.46132,eval_ucc_acc: 0.84766\n",
            "Step 107020: {'train_ae_loss': 0.66977, 'train_ucc_loss': 0.50077, 'train_ucc_acc': 0.78125, 'loss': 0.58527}\n",
            "Step 107040: {'train_ae_loss': 0.65854, 'train_ucc_loss': 0.4786, 'train_ucc_acc': 0.8125, 'loss': 0.56857}\n",
            "Step 107060: {'train_ae_loss': 0.66123, 'train_ucc_loss': 0.52067, 'train_ucc_acc': 0.75, 'loss': 0.59095}\n",
            "Step 107080: {'train_ae_loss': 0.662, 'train_ucc_loss': 0.48341, 'train_ucc_acc': 0.8125, 'loss': 0.5727}\n",
            "Step 107100: {'train_ae_loss': 0.66434, 'train_ucc_loss': 0.50325, 'train_ucc_acc': 0.8125, 'loss': 0.5838}\n",
            "Step 107120: {'train_ae_loss': 0.65962, 'train_ucc_loss': 0.63646, 'train_ucc_acc': 0.65625, 'loss': 0.64804}\n",
            "Step 107140: {'train_ae_loss': 0.64579, 'train_ucc_loss': 0.4299, 'train_ucc_acc': 0.875, 'loss': 0.53784}\n",
            "Step 107160: {'train_ae_loss': 0.6701, 'train_ucc_loss': 0.47522, 'train_ucc_acc': 0.84375, 'loss': 0.57266}\n",
            "Step 107180: {'train_ae_loss': 0.65636, 'train_ucc_loss': 0.48394, 'train_ucc_acc': 0.84375, 'loss': 0.57015}\n",
            "Step 107200: {'train_ae_loss': 0.67477, 'train_ucc_loss': 0.35562, 'train_ucc_acc': 0.96875, 'loss': 0.51519}\n",
            "Step 107220: {'train_ae_loss': 0.65519, 'train_ucc_loss': 0.40502, 'train_ucc_acc': 0.90625, 'loss': 0.53011}\n",
            "Step 107240: {'train_ae_loss': 0.6586, 'train_ucc_loss': 0.40945, 'train_ucc_acc': 0.90625, 'loss': 0.53403}\n",
            "Step 107260: {'train_ae_loss': 0.66764, 'train_ucc_loss': 0.39006, 'train_ucc_acc': 0.9375, 'loss': 0.52885}\n",
            "Step 107280: {'train_ae_loss': 0.6819, 'train_ucc_loss': 0.43772, 'train_ucc_acc': 0.84375, 'loss': 0.55981}\n",
            "Step 107300: {'train_ae_loss': 0.65375, 'train_ucc_loss': 0.48417, 'train_ucc_acc': 0.84375, 'loss': 0.56896}\n",
            "Step 107320: {'train_ae_loss': 0.6516, 'train_ucc_loss': 0.42375, 'train_ucc_acc': 0.875, 'loss': 0.53768}\n",
            "Step 107340: {'train_ae_loss': 0.66159, 'train_ucc_loss': 0.39857, 'train_ucc_acc': 0.875, 'loss': 0.53008}\n",
            "Step 107360: {'train_ae_loss': 0.68407, 'train_ucc_loss': 0.37209, 'train_ucc_acc': 0.9375, 'loss': 0.52808}\n",
            "Step 107380: {'train_ae_loss': 0.65199, 'train_ucc_loss': 0.42496, 'train_ucc_acc': 0.84375, 'loss': 0.53848}\n",
            "Step 107400: {'train_ae_loss': 0.66223, 'train_ucc_loss': 0.41323, 'train_ucc_acc': 0.90625, 'loss': 0.53773}\n",
            "Step 107420: {'train_ae_loss': 0.67008, 'train_ucc_loss': 0.37912, 'train_ucc_acc': 0.90625, 'loss': 0.5246}\n",
            "Step 107440: {'train_ae_loss': 0.67362, 'train_ucc_loss': 0.41897, 'train_ucc_acc': 0.90625, 'loss': 0.54629}\n",
            "Step 107460: {'train_ae_loss': 0.67811, 'train_ucc_loss': 0.41444, 'train_ucc_acc': 0.90625, 'loss': 0.54628}\n",
            "Step 107480: {'train_ae_loss': 0.6589, 'train_ucc_loss': 0.49505, 'train_ucc_acc': 0.78125, 'loss': 0.57697}\n",
            "Step 107500: {'train_ae_loss': 0.66949, 'train_ucc_loss': 0.4366, 'train_ucc_acc': 0.84375, 'loss': 0.55305}\n",
            "Step 107520: {'train_ae_loss': 0.66361, 'train_ucc_loss': 0.55348, 'train_ucc_acc': 0.71875, 'loss': 0.60855}\n",
            "Step 107540: {'train_ae_loss': 0.65741, 'train_ucc_loss': 0.44292, 'train_ucc_acc': 0.875, 'loss': 0.55017}\n",
            "Step 107560: {'train_ae_loss': 0.66641, 'train_ucc_loss': 0.39505, 'train_ucc_acc': 0.90625, 'loss': 0.53073}\n",
            "Step 107580: {'train_ae_loss': 0.65613, 'train_ucc_loss': 0.45761, 'train_ucc_acc': 0.84375, 'loss': 0.55687}\n",
            "Step 107600: {'train_ae_loss': 0.65239, 'train_ucc_loss': 0.50024, 'train_ucc_acc': 0.8125, 'loss': 0.57631}\n",
            "Step 107620: {'train_ae_loss': 0.65446, 'train_ucc_loss': 0.44967, 'train_ucc_acc': 0.84375, 'loss': 0.55206}\n",
            "Step 107640: {'train_ae_loss': 0.6518, 'train_ucc_loss': 0.57703, 'train_ucc_acc': 0.71875, 'loss': 0.61442}\n",
            "Step 107660: {'train_ae_loss': 0.65136, 'train_ucc_loss': 0.51493, 'train_ucc_acc': 0.8125, 'loss': 0.58314}\n",
            "Step 107680: {'train_ae_loss': 0.64913, 'train_ucc_loss': 0.51995, 'train_ucc_acc': 0.75, 'loss': 0.58454}\n",
            "Step 107700: {'train_ae_loss': 0.65617, 'train_ucc_loss': 0.39688, 'train_ucc_acc': 0.875, 'loss': 0.52653}\n",
            "Step 107720: {'train_ae_loss': 0.66083, 'train_ucc_loss': 0.50871, 'train_ucc_acc': 0.78125, 'loss': 0.58477}\n",
            "Step 107740: {'train_ae_loss': 0.68134, 'train_ucc_loss': 0.41716, 'train_ucc_acc': 0.90625, 'loss': 0.54925}\n",
            "Step 107760: {'train_ae_loss': 0.6617, 'train_ucc_loss': 0.48242, 'train_ucc_acc': 0.8125, 'loss': 0.57206}\n",
            "Step 107780: {'train_ae_loss': 0.65262, 'train_ucc_loss': 0.56902, 'train_ucc_acc': 0.71875, 'loss': 0.61082}\n",
            "Step 107800: {'train_ae_loss': 0.65433, 'train_ucc_loss': 0.39622, 'train_ucc_acc': 0.90625, 'loss': 0.52528}\n",
            "Step 107820: {'train_ae_loss': 0.66726, 'train_ucc_loss': 0.38506, 'train_ucc_acc': 0.96875, 'loss': 0.52616}\n",
            "Step 107840: {'train_ae_loss': 0.64324, 'train_ucc_loss': 0.54594, 'train_ucc_acc': 0.75, 'loss': 0.59459}\n",
            "Step 107860: {'train_ae_loss': 0.65905, 'train_ucc_loss': 0.39338, 'train_ucc_acc': 0.9375, 'loss': 0.52621}\n",
            "Step 107880: {'train_ae_loss': 0.65529, 'train_ucc_loss': 0.37876, 'train_ucc_acc': 0.9375, 'loss': 0.51702}\n",
            "Step 107900: {'train_ae_loss': 0.65879, 'train_ucc_loss': 0.37969, 'train_ucc_acc': 0.9375, 'loss': 0.51924}\n",
            "Step 107920: {'train_ae_loss': 0.65561, 'train_ucc_loss': 0.39038, 'train_ucc_acc': 0.90625, 'loss': 0.523}\n",
            "Step 107940: {'train_ae_loss': 0.65907, 'train_ucc_loss': 0.3506, 'train_ucc_acc': 0.96875, 'loss': 0.50483}\n",
            "Step 107960: {'train_ae_loss': 0.64971, 'train_ucc_loss': 0.45498, 'train_ucc_acc': 0.84375, 'loss': 0.55235}\n",
            "Step 107980: {'train_ae_loss': 0.65313, 'train_ucc_loss': 0.49541, 'train_ucc_acc': 0.8125, 'loss': 0.57427}\n",
            "Step 108000: {'train_ae_loss': 0.64388, 'train_ucc_loss': 0.53595, 'train_ucc_acc': 0.78125, 'loss': 0.58991}\n",
            "step: 108000,eval_ae_loss: 0.64958,eval_ucc_loss: 0.48053,eval_ucc_acc: 0.82422\n",
            "Step 108020: {'train_ae_loss': 0.66308, 'train_ucc_loss': 0.45819, 'train_ucc_acc': 0.875, 'loss': 0.56064}\n",
            "Step 108040: {'train_ae_loss': 0.65322, 'train_ucc_loss': 0.44122, 'train_ucc_acc': 0.875, 'loss': 0.54722}\n",
            "Step 108060: {'train_ae_loss': 0.65086, 'train_ucc_loss': 0.42942, 'train_ucc_acc': 0.875, 'loss': 0.54014}\n",
            "Step 108080: {'train_ae_loss': 0.63825, 'train_ucc_loss': 0.43303, 'train_ucc_acc': 0.90625, 'loss': 0.53564}\n",
            "Step 108100: {'train_ae_loss': 0.65504, 'train_ucc_loss': 0.39472, 'train_ucc_acc': 0.90625, 'loss': 0.52488}\n",
            "Step 108120: {'train_ae_loss': 0.66216, 'train_ucc_loss': 0.52915, 'train_ucc_acc': 0.75, 'loss': 0.59565}\n",
            "Step 108140: {'train_ae_loss': 0.65634, 'train_ucc_loss': 0.39425, 'train_ucc_acc': 0.9375, 'loss': 0.5253}\n",
            "Step 108160: {'train_ae_loss': 0.65284, 'train_ucc_loss': 0.48619, 'train_ucc_acc': 0.8125, 'loss': 0.56951}\n",
            "Step 108180: {'train_ae_loss': 0.64756, 'train_ucc_loss': 0.56075, 'train_ucc_acc': 0.71875, 'loss': 0.60415}\n",
            "Step 108200: {'train_ae_loss': 0.6637, 'train_ucc_loss': 0.44805, 'train_ucc_acc': 0.84375, 'loss': 0.55588}\n",
            "Step 108220: {'train_ae_loss': 0.68567, 'train_ucc_loss': 0.44042, 'train_ucc_acc': 0.84375, 'loss': 0.56304}\n",
            "Step 108240: {'train_ae_loss': 0.64883, 'train_ucc_loss': 0.52407, 'train_ucc_acc': 0.78125, 'loss': 0.58645}\n",
            "Step 108260: {'train_ae_loss': 0.66887, 'train_ucc_loss': 0.38058, 'train_ucc_acc': 0.9375, 'loss': 0.52473}\n",
            "Step 108280: {'train_ae_loss': 0.65817, 'train_ucc_loss': 0.48729, 'train_ucc_acc': 0.84375, 'loss': 0.57273}\n",
            "Step 108300: {'train_ae_loss': 0.65421, 'train_ucc_loss': 0.41552, 'train_ucc_acc': 0.9375, 'loss': 0.53486}\n",
            "Step 108320: {'train_ae_loss': 0.66881, 'train_ucc_loss': 0.49652, 'train_ucc_acc': 0.78125, 'loss': 0.58266}\n",
            "Step 108340: {'train_ae_loss': 0.65084, 'train_ucc_loss': 0.45702, 'train_ucc_acc': 0.84375, 'loss': 0.55393}\n",
            "Step 108360: {'train_ae_loss': 0.65905, 'train_ucc_loss': 0.5445, 'train_ucc_acc': 0.75, 'loss': 0.60178}\n",
            "Step 108380: {'train_ae_loss': 0.66366, 'train_ucc_loss': 0.41945, 'train_ucc_acc': 0.90625, 'loss': 0.54156}\n",
            "Step 108400: {'train_ae_loss': 0.67017, 'train_ucc_loss': 0.43927, 'train_ucc_acc': 0.875, 'loss': 0.55472}\n",
            "Step 108420: {'train_ae_loss': 0.68785, 'train_ucc_loss': 0.35415, 'train_ucc_acc': 0.96875, 'loss': 0.521}\n",
            "Step 108440: {'train_ae_loss': 0.66661, 'train_ucc_loss': 0.42893, 'train_ucc_acc': 0.875, 'loss': 0.54777}\n",
            "Step 108460: {'train_ae_loss': 0.64408, 'train_ucc_loss': 0.36721, 'train_ucc_acc': 0.9375, 'loss': 0.50565}\n",
            "Step 108480: {'train_ae_loss': 0.65461, 'train_ucc_loss': 0.42649, 'train_ucc_acc': 0.875, 'loss': 0.54055}\n",
            "Step 108500: {'train_ae_loss': 0.64921, 'train_ucc_loss': 0.59539, 'train_ucc_acc': 0.71875, 'loss': 0.6223}\n",
            "Step 108520: {'train_ae_loss': 0.66173, 'train_ucc_loss': 0.40956, 'train_ucc_acc': 0.90625, 'loss': 0.53565}\n",
            "Step 108540: {'train_ae_loss': 0.66232, 'train_ucc_loss': 0.35064, 'train_ucc_acc': 0.9375, 'loss': 0.50648}\n",
            "Step 108560: {'train_ae_loss': 0.66572, 'train_ucc_loss': 0.48712, 'train_ucc_acc': 0.8125, 'loss': 0.57642}\n",
            "Step 108580: {'train_ae_loss': 0.65118, 'train_ucc_loss': 0.57722, 'train_ucc_acc': 0.75, 'loss': 0.6142}\n",
            "Step 108600: {'train_ae_loss': 0.66569, 'train_ucc_loss': 0.39669, 'train_ucc_acc': 0.90625, 'loss': 0.53119}\n",
            "Step 108620: {'train_ae_loss': 0.64837, 'train_ucc_loss': 0.45804, 'train_ucc_acc': 0.8125, 'loss': 0.5532}\n",
            "Step 108640: {'train_ae_loss': 0.68106, 'train_ucc_loss': 0.38157, 'train_ucc_acc': 0.9375, 'loss': 0.53131}\n",
            "Step 108660: {'train_ae_loss': 0.66592, 'train_ucc_loss': 0.4304, 'train_ucc_acc': 0.875, 'loss': 0.54816}\n",
            "Step 108680: {'train_ae_loss': 0.64453, 'train_ucc_loss': 0.42653, 'train_ucc_acc': 0.90625, 'loss': 0.53553}\n",
            "Step 108700: {'train_ae_loss': 0.64525, 'train_ucc_loss': 0.4499, 'train_ucc_acc': 0.84375, 'loss': 0.54757}\n",
            "Step 108720: {'train_ae_loss': 0.66282, 'train_ucc_loss': 0.47118, 'train_ucc_acc': 0.8125, 'loss': 0.567}\n",
            "Step 108740: {'train_ae_loss': 0.6518, 'train_ucc_loss': 0.35026, 'train_ucc_acc': 0.96875, 'loss': 0.50103}\n",
            "Step 108760: {'train_ae_loss': 0.65346, 'train_ucc_loss': 0.36391, 'train_ucc_acc': 0.9375, 'loss': 0.50868}\n",
            "Step 108780: {'train_ae_loss': 0.64883, 'train_ucc_loss': 0.49737, 'train_ucc_acc': 0.8125, 'loss': 0.5731}\n",
            "Step 108800: {'train_ae_loss': 0.64737, 'train_ucc_loss': 0.47685, 'train_ucc_acc': 0.8125, 'loss': 0.56211}\n",
            "Step 108820: {'train_ae_loss': 0.65609, 'train_ucc_loss': 0.57295, 'train_ucc_acc': 0.71875, 'loss': 0.61452}\n",
            "Step 108840: {'train_ae_loss': 0.64999, 'train_ucc_loss': 0.42474, 'train_ucc_acc': 0.90625, 'loss': 0.53736}\n",
            "Step 108860: {'train_ae_loss': 0.65948, 'train_ucc_loss': 0.44239, 'train_ucc_acc': 0.875, 'loss': 0.55093}\n",
            "Step 108880: {'train_ae_loss': 0.64644, 'train_ucc_loss': 0.5397, 'train_ucc_acc': 0.75, 'loss': 0.59307}\n",
            "Step 108900: {'train_ae_loss': 0.66517, 'train_ucc_loss': 0.43454, 'train_ucc_acc': 0.875, 'loss': 0.54985}\n",
            "Step 108920: {'train_ae_loss': 0.66528, 'train_ucc_loss': 0.49915, 'train_ucc_acc': 0.8125, 'loss': 0.58221}\n",
            "Step 108940: {'train_ae_loss': 0.66488, 'train_ucc_loss': 0.46204, 'train_ucc_acc': 0.84375, 'loss': 0.56346}\n",
            "Step 108960: {'train_ae_loss': 0.67569, 'train_ucc_loss': 0.409, 'train_ucc_acc': 0.90625, 'loss': 0.54234}\n",
            "Step 108980: {'train_ae_loss': 0.66037, 'train_ucc_loss': 0.48942, 'train_ucc_acc': 0.8125, 'loss': 0.57489}\n",
            "Step 109000: {'train_ae_loss': 0.67324, 'train_ucc_loss': 0.41303, 'train_ucc_acc': 0.875, 'loss': 0.54313}\n",
            "step: 109000,eval_ae_loss: 0.65712,eval_ucc_loss: 0.48093,eval_ucc_acc: 0.82422\n",
            "Step 109020: {'train_ae_loss': 0.67101, 'train_ucc_loss': 0.36591, 'train_ucc_acc': 0.9375, 'loss': 0.51846}\n",
            "Step 109040: {'train_ae_loss': 0.65971, 'train_ucc_loss': 0.37408, 'train_ucc_acc': 0.9375, 'loss': 0.5169}\n",
            "Step 109060: {'train_ae_loss': 0.65897, 'train_ucc_loss': 0.48407, 'train_ucc_acc': 0.8125, 'loss': 0.57152}\n",
            "Step 109080: {'train_ae_loss': 0.65817, 'train_ucc_loss': 0.33288, 'train_ucc_acc': 1.0, 'loss': 0.49553}\n",
            "Step 109100: {'train_ae_loss': 0.66469, 'train_ucc_loss': 0.46719, 'train_ucc_acc': 0.84375, 'loss': 0.56594}\n",
            "Step 109120: {'train_ae_loss': 0.66054, 'train_ucc_loss': 0.52496, 'train_ucc_acc': 0.75, 'loss': 0.59275}\n",
            "Step 109140: {'train_ae_loss': 0.68015, 'train_ucc_loss': 0.55048, 'train_ucc_acc': 0.78125, 'loss': 0.61531}\n",
            "Step 109160: {'train_ae_loss': 0.65767, 'train_ucc_loss': 0.40771, 'train_ucc_acc': 0.90625, 'loss': 0.53269}\n",
            "Step 109180: {'train_ae_loss': 0.65821, 'train_ucc_loss': 0.38325, 'train_ucc_acc': 0.9375, 'loss': 0.52073}\n",
            "Step 109200: {'train_ae_loss': 0.67351, 'train_ucc_loss': 0.45638, 'train_ucc_acc': 0.8125, 'loss': 0.56494}\n",
            "Step 109220: {'train_ae_loss': 0.67853, 'train_ucc_loss': 0.42012, 'train_ucc_acc': 0.90625, 'loss': 0.54932}\n",
            "Step 109240: {'train_ae_loss': 0.65916, 'train_ucc_loss': 0.48031, 'train_ucc_acc': 0.84375, 'loss': 0.56973}\n",
            "Step 109260: {'train_ae_loss': 0.64328, 'train_ucc_loss': 0.45065, 'train_ucc_acc': 0.84375, 'loss': 0.54697}\n",
            "Step 109280: {'train_ae_loss': 0.67017, 'train_ucc_loss': 0.4224, 'train_ucc_acc': 0.875, 'loss': 0.54628}\n",
            "Step 109300: {'train_ae_loss': 0.64423, 'train_ucc_loss': 0.40803, 'train_ucc_acc': 0.90625, 'loss': 0.52613}\n",
            "Step 109320: {'train_ae_loss': 0.66885, 'train_ucc_loss': 0.4209, 'train_ucc_acc': 0.90625, 'loss': 0.54488}\n",
            "Step 109340: {'train_ae_loss': 0.64595, 'train_ucc_loss': 0.47476, 'train_ucc_acc': 0.84375, 'loss': 0.56035}\n",
            "Step 109360: {'train_ae_loss': 0.67084, 'train_ucc_loss': 0.43207, 'train_ucc_acc': 0.84375, 'loss': 0.55145}\n",
            "Step 109380: {'train_ae_loss': 0.66525, 'train_ucc_loss': 0.42598, 'train_ucc_acc': 0.875, 'loss': 0.54562}\n",
            "Step 109400: {'train_ae_loss': 0.6433, 'train_ucc_loss': 0.50558, 'train_ucc_acc': 0.8125, 'loss': 0.57444}\n",
            "Step 109420: {'train_ae_loss': 0.6522, 'train_ucc_loss': 0.41377, 'train_ucc_acc': 0.90625, 'loss': 0.53299}\n",
            "Step 109440: {'train_ae_loss': 0.66138, 'train_ucc_loss': 0.50095, 'train_ucc_acc': 0.8125, 'loss': 0.58117}\n",
            "Step 109460: {'train_ae_loss': 0.67325, 'train_ucc_loss': 0.38815, 'train_ucc_acc': 0.9375, 'loss': 0.5307}\n",
            "Step 109480: {'train_ae_loss': 0.65082, 'train_ucc_loss': 0.40738, 'train_ucc_acc': 0.875, 'loss': 0.5291}\n",
            "Step 109500: {'train_ae_loss': 0.6629, 'train_ucc_loss': 0.40687, 'train_ucc_acc': 0.90625, 'loss': 0.53488}\n",
            "Step 109520: {'train_ae_loss': 0.64214, 'train_ucc_loss': 0.50601, 'train_ucc_acc': 0.78125, 'loss': 0.57408}\n",
            "Step 109540: {'train_ae_loss': 0.66595, 'train_ucc_loss': 0.51641, 'train_ucc_acc': 0.78125, 'loss': 0.59118}\n",
            "Step 109560: {'train_ae_loss': 0.66734, 'train_ucc_loss': 0.35994, 'train_ucc_acc': 0.96875, 'loss': 0.51364}\n",
            "Step 109580: {'train_ae_loss': 0.65539, 'train_ucc_loss': 0.43709, 'train_ucc_acc': 0.875, 'loss': 0.54624}\n",
            "Step 109600: {'train_ae_loss': 0.68216, 'train_ucc_loss': 0.46979, 'train_ucc_acc': 0.84375, 'loss': 0.57598}\n",
            "Step 109620: {'train_ae_loss': 0.65591, 'train_ucc_loss': 0.58633, 'train_ucc_acc': 0.6875, 'loss': 0.62112}\n",
            "Step 109640: {'train_ae_loss': 0.64254, 'train_ucc_loss': 0.45707, 'train_ucc_acc': 0.84375, 'loss': 0.54981}\n",
            "Step 109660: {'train_ae_loss': 0.63708, 'train_ucc_loss': 0.42637, 'train_ucc_acc': 0.875, 'loss': 0.53172}\n",
            "Step 109680: {'train_ae_loss': 0.66997, 'train_ucc_loss': 0.47819, 'train_ucc_acc': 0.875, 'loss': 0.57408}\n",
            "Step 109700: {'train_ae_loss': 0.65726, 'train_ucc_loss': 0.46051, 'train_ucc_acc': 0.84375, 'loss': 0.55889}\n",
            "Step 109720: {'train_ae_loss': 0.67133, 'train_ucc_loss': 0.42324, 'train_ucc_acc': 0.875, 'loss': 0.54728}\n",
            "Step 109740: {'train_ae_loss': 0.65965, 'train_ucc_loss': 0.49859, 'train_ucc_acc': 0.8125, 'loss': 0.57912}\n",
            "Step 109760: {'train_ae_loss': 0.66789, 'train_ucc_loss': 0.37099, 'train_ucc_acc': 0.9375, 'loss': 0.51944}\n",
            "Step 109780: {'train_ae_loss': 0.63872, 'train_ucc_loss': 0.48067, 'train_ucc_acc': 0.8125, 'loss': 0.5597}\n",
            "Step 109800: {'train_ae_loss': 0.65587, 'train_ucc_loss': 0.40612, 'train_ucc_acc': 0.90625, 'loss': 0.53099}\n",
            "Step 109820: {'train_ae_loss': 0.65546, 'train_ucc_loss': 0.41645, 'train_ucc_acc': 0.90625, 'loss': 0.53595}\n",
            "Step 109840: {'train_ae_loss': 0.6495, 'train_ucc_loss': 0.51895, 'train_ucc_acc': 0.78125, 'loss': 0.58422}\n",
            "Step 109860: {'train_ae_loss': 0.65259, 'train_ucc_loss': 0.44707, 'train_ucc_acc': 0.84375, 'loss': 0.54983}\n",
            "Step 109880: {'train_ae_loss': 0.65469, 'train_ucc_loss': 0.42826, 'train_ucc_acc': 0.875, 'loss': 0.54148}\n",
            "Step 109900: {'train_ae_loss': 0.66503, 'train_ucc_loss': 0.49159, 'train_ucc_acc': 0.8125, 'loss': 0.57831}\n",
            "Step 109920: {'train_ae_loss': 0.66962, 'train_ucc_loss': 0.4148, 'train_ucc_acc': 0.90625, 'loss': 0.54221}\n",
            "Step 109940: {'train_ae_loss': 0.66364, 'train_ucc_loss': 0.39652, 'train_ucc_acc': 0.90625, 'loss': 0.53008}\n",
            "Step 109960: {'train_ae_loss': 0.6648, 'train_ucc_loss': 0.47098, 'train_ucc_acc': 0.84375, 'loss': 0.56789}\n",
            "Step 109980: {'train_ae_loss': 0.64988, 'train_ucc_loss': 0.43909, 'train_ucc_acc': 0.875, 'loss': 0.54449}\n",
            "Step 110000: {'train_ae_loss': 0.65421, 'train_ucc_loss': 0.45359, 'train_ucc_acc': 0.8125, 'loss': 0.5539}\n",
            "step: 110000,eval_ae_loss: 0.64736,eval_ucc_loss: 0.488,eval_ucc_acc: 0.81836\n",
            "Step 110020: {'train_ae_loss': 0.66935, 'train_ucc_loss': 0.40341, 'train_ucc_acc': 0.90625, 'loss': 0.53638}\n",
            "Step 110040: {'train_ae_loss': 0.66743, 'train_ucc_loss': 0.37615, 'train_ucc_acc': 0.9375, 'loss': 0.52179}\n",
            "Step 110060: {'train_ae_loss': 0.67078, 'train_ucc_loss': 0.39512, 'train_ucc_acc': 0.9375, 'loss': 0.53295}\n",
            "Step 110080: {'train_ae_loss': 0.65423, 'train_ucc_loss': 0.40151, 'train_ucc_acc': 0.90625, 'loss': 0.52787}\n",
            "Step 110100: {'train_ae_loss': 0.66287, 'train_ucc_loss': 0.55481, 'train_ucc_acc': 0.75, 'loss': 0.60884}\n",
            "Step 110120: {'train_ae_loss': 0.67394, 'train_ucc_loss': 0.41914, 'train_ucc_acc': 0.875, 'loss': 0.54654}\n",
            "Step 110140: {'train_ae_loss': 0.65643, 'train_ucc_loss': 0.40886, 'train_ucc_acc': 0.90625, 'loss': 0.53264}\n",
            "Step 110160: {'train_ae_loss': 0.64663, 'train_ucc_loss': 0.43771, 'train_ucc_acc': 0.84375, 'loss': 0.54217}\n",
            "Step 110180: {'train_ae_loss': 0.65971, 'train_ucc_loss': 0.52156, 'train_ucc_acc': 0.78125, 'loss': 0.59064}\n",
            "Step 110200: {'train_ae_loss': 0.66499, 'train_ucc_loss': 0.37329, 'train_ucc_acc': 0.9375, 'loss': 0.51914}\n",
            "Step 110220: {'train_ae_loss': 0.66324, 'train_ucc_loss': 0.43069, 'train_ucc_acc': 0.875, 'loss': 0.54697}\n",
            "Step 110240: {'train_ae_loss': 0.66241, 'train_ucc_loss': 0.50164, 'train_ucc_acc': 0.78125, 'loss': 0.58202}\n",
            "Step 110260: {'train_ae_loss': 0.6649, 'train_ucc_loss': 0.49637, 'train_ucc_acc': 0.78125, 'loss': 0.58064}\n",
            "Step 110280: {'train_ae_loss': 0.66716, 'train_ucc_loss': 0.42821, 'train_ucc_acc': 0.90625, 'loss': 0.54768}\n",
            "Step 110300: {'train_ae_loss': 0.65381, 'train_ucc_loss': 0.49898, 'train_ucc_acc': 0.8125, 'loss': 0.57639}\n",
            "Step 110320: {'train_ae_loss': 0.67139, 'train_ucc_loss': 0.49926, 'train_ucc_acc': 0.8125, 'loss': 0.58533}\n",
            "Step 110340: {'train_ae_loss': 0.6549, 'train_ucc_loss': 0.46186, 'train_ucc_acc': 0.84375, 'loss': 0.55838}\n",
            "Step 110360: {'train_ae_loss': 0.6565, 'train_ucc_loss': 0.54168, 'train_ucc_acc': 0.75, 'loss': 0.59909}\n",
            "Step 110380: {'train_ae_loss': 0.67044, 'train_ucc_loss': 0.41512, 'train_ucc_acc': 0.875, 'loss': 0.54278}\n",
            "Step 110400: {'train_ae_loss': 0.65062, 'train_ucc_loss': 0.61257, 'train_ucc_acc': 0.65625, 'loss': 0.6316}\n",
            "Step 110420: {'train_ae_loss': 0.66427, 'train_ucc_loss': 0.53175, 'train_ucc_acc': 0.75, 'loss': 0.59801}\n",
            "Step 110440: {'train_ae_loss': 0.65853, 'train_ucc_loss': 0.38474, 'train_ucc_acc': 0.9375, 'loss': 0.52164}\n",
            "Step 110460: {'train_ae_loss': 0.65783, 'train_ucc_loss': 0.47049, 'train_ucc_acc': 0.8125, 'loss': 0.56416}\n",
            "Step 110480: {'train_ae_loss': 0.64644, 'train_ucc_loss': 0.47318, 'train_ucc_acc': 0.84375, 'loss': 0.55981}\n",
            "Step 110500: {'train_ae_loss': 0.64824, 'train_ucc_loss': 0.56513, 'train_ucc_acc': 0.75, 'loss': 0.60669}\n",
            "Step 110520: {'train_ae_loss': 0.65057, 'train_ucc_loss': 0.45847, 'train_ucc_acc': 0.84375, 'loss': 0.55452}\n",
            "Step 110540: {'train_ae_loss': 0.63621, 'train_ucc_loss': 0.47598, 'train_ucc_acc': 0.8125, 'loss': 0.55609}\n",
            "Step 110560: {'train_ae_loss': 0.6648, 'train_ucc_loss': 0.54555, 'train_ucc_acc': 0.75, 'loss': 0.60517}\n",
            "Step 110580: {'train_ae_loss': 0.65355, 'train_ucc_loss': 0.43842, 'train_ucc_acc': 0.84375, 'loss': 0.54598}\n",
            "Step 110600: {'train_ae_loss': 0.65645, 'train_ucc_loss': 0.4533, 'train_ucc_acc': 0.875, 'loss': 0.55488}\n",
            "Step 110620: {'train_ae_loss': 0.65738, 'train_ucc_loss': 0.50635, 'train_ucc_acc': 0.8125, 'loss': 0.58186}\n",
            "Step 110640: {'train_ae_loss': 0.66481, 'train_ucc_loss': 0.53601, 'train_ucc_acc': 0.78125, 'loss': 0.60041}\n",
            "Step 110660: {'train_ae_loss': 0.66742, 'train_ucc_loss': 0.43779, 'train_ucc_acc': 0.875, 'loss': 0.5526}\n",
            "Step 110680: {'train_ae_loss': 0.65982, 'train_ucc_loss': 0.40706, 'train_ucc_acc': 0.875, 'loss': 0.53344}\n",
            "Step 110700: {'train_ae_loss': 0.64768, 'train_ucc_loss': 0.51786, 'train_ucc_acc': 0.75, 'loss': 0.58277}\n",
            "Step 110720: {'train_ae_loss': 0.6744, 'train_ucc_loss': 0.42803, 'train_ucc_acc': 0.90625, 'loss': 0.55122}\n",
            "Step 110740: {'train_ae_loss': 0.67693, 'train_ucc_loss': 0.4087, 'train_ucc_acc': 0.90625, 'loss': 0.54282}\n",
            "Step 110760: {'train_ae_loss': 0.6615, 'train_ucc_loss': 0.52058, 'train_ucc_acc': 0.78125, 'loss': 0.59104}\n",
            "Step 110780: {'train_ae_loss': 0.65243, 'train_ucc_loss': 0.43433, 'train_ucc_acc': 0.90625, 'loss': 0.54338}\n",
            "Step 110800: {'train_ae_loss': 0.6767, 'train_ucc_loss': 0.44267, 'train_ucc_acc': 0.84375, 'loss': 0.55969}\n",
            "Step 110820: {'train_ae_loss': 0.66646, 'train_ucc_loss': 0.38513, 'train_ucc_acc': 0.9375, 'loss': 0.5258}\n",
            "Step 110840: {'train_ae_loss': 0.66062, 'train_ucc_loss': 0.42729, 'train_ucc_acc': 0.875, 'loss': 0.54395}\n",
            "Step 110860: {'train_ae_loss': 0.67412, 'train_ucc_loss': 0.39104, 'train_ucc_acc': 0.9375, 'loss': 0.53258}\n",
            "Step 110880: {'train_ae_loss': 0.66177, 'train_ucc_loss': 0.39544, 'train_ucc_acc': 0.90625, 'loss': 0.5286}\n",
            "Step 110900: {'train_ae_loss': 0.65719, 'train_ucc_loss': 0.34337, 'train_ucc_acc': 0.96875, 'loss': 0.50028}\n",
            "Step 110920: {'train_ae_loss': 0.66062, 'train_ucc_loss': 0.42711, 'train_ucc_acc': 0.875, 'loss': 0.54386}\n",
            "Step 110940: {'train_ae_loss': 0.65137, 'train_ucc_loss': 0.41752, 'train_ucc_acc': 0.90625, 'loss': 0.53445}\n",
            "Step 110960: {'train_ae_loss': 0.65599, 'train_ucc_loss': 0.37444, 'train_ucc_acc': 0.9375, 'loss': 0.51521}\n",
            "Step 110980: {'train_ae_loss': 0.64672, 'train_ucc_loss': 0.48579, 'train_ucc_acc': 0.75, 'loss': 0.56626}\n",
            "Step 111000: {'train_ae_loss': 0.67033, 'train_ucc_loss': 0.35924, 'train_ucc_acc': 0.9375, 'loss': 0.51479}\n",
            "step: 111000,eval_ae_loss: 0.64973,eval_ucc_loss: 0.48817,eval_ucc_acc: 0.81934\n",
            "Step 111020: {'train_ae_loss': 0.66553, 'train_ucc_loss': 0.42825, 'train_ucc_acc': 0.90625, 'loss': 0.54689}\n",
            "Step 111040: {'train_ae_loss': 0.66293, 'train_ucc_loss': 0.55797, 'train_ucc_acc': 0.6875, 'loss': 0.61045}\n",
            "Step 111060: {'train_ae_loss': 0.66189, 'train_ucc_loss': 0.51741, 'train_ucc_acc': 0.78125, 'loss': 0.58965}\n",
            "Step 111080: {'train_ae_loss': 0.65133, 'train_ucc_loss': 0.59974, 'train_ucc_acc': 0.65625, 'loss': 0.62554}\n",
            "Step 111100: {'train_ae_loss': 0.66308, 'train_ucc_loss': 0.46143, 'train_ucc_acc': 0.84375, 'loss': 0.56225}\n",
            "Step 111120: {'train_ae_loss': 0.64195, 'train_ucc_loss': 0.3991, 'train_ucc_acc': 0.90625, 'loss': 0.52052}\n",
            "Step 111140: {'train_ae_loss': 0.65286, 'train_ucc_loss': 0.46759, 'train_ucc_acc': 0.84375, 'loss': 0.56023}\n",
            "Step 111160: {'train_ae_loss': 0.69053, 'train_ucc_loss': 0.38256, 'train_ucc_acc': 0.9375, 'loss': 0.53654}\n",
            "Step 111180: {'train_ae_loss': 0.65921, 'train_ucc_loss': 0.34045, 'train_ucc_acc': 1.0, 'loss': 0.49983}\n",
            "Step 111200: {'train_ae_loss': 0.66284, 'train_ucc_loss': 0.36749, 'train_ucc_acc': 0.96875, 'loss': 0.51517}\n",
            "Step 111220: {'train_ae_loss': 0.66082, 'train_ucc_loss': 0.50016, 'train_ucc_acc': 0.8125, 'loss': 0.58049}\n",
            "Step 111240: {'train_ae_loss': 0.64289, 'train_ucc_loss': 0.41656, 'train_ucc_acc': 0.90625, 'loss': 0.52973}\n",
            "Step 111260: {'train_ae_loss': 0.65243, 'train_ucc_loss': 0.34811, 'train_ucc_acc': 0.96875, 'loss': 0.50027}\n",
            "Step 111280: {'train_ae_loss': 0.66303, 'train_ucc_loss': 0.47973, 'train_ucc_acc': 0.84375, 'loss': 0.57138}\n",
            "Step 111300: {'train_ae_loss': 0.66799, 'train_ucc_loss': 0.4692, 'train_ucc_acc': 0.8125, 'loss': 0.5686}\n",
            "Step 111320: {'train_ae_loss': 0.65586, 'train_ucc_loss': 0.45309, 'train_ucc_acc': 0.84375, 'loss': 0.55447}\n",
            "Step 111340: {'train_ae_loss': 0.66694, 'train_ucc_loss': 0.48956, 'train_ucc_acc': 0.84375, 'loss': 0.57825}\n",
            "Step 111360: {'train_ae_loss': 0.66914, 'train_ucc_loss': 0.41662, 'train_ucc_acc': 0.90625, 'loss': 0.54288}\n",
            "Step 111380: {'train_ae_loss': 0.67575, 'train_ucc_loss': 0.38768, 'train_ucc_acc': 0.9375, 'loss': 0.53171}\n",
            "Step 111400: {'train_ae_loss': 0.66616, 'train_ucc_loss': 0.46036, 'train_ucc_acc': 0.84375, 'loss': 0.56326}\n",
            "Step 111420: {'train_ae_loss': 0.66554, 'train_ucc_loss': 0.45972, 'train_ucc_acc': 0.84375, 'loss': 0.56263}\n",
            "Step 111440: {'train_ae_loss': 0.64991, 'train_ucc_loss': 0.53213, 'train_ucc_acc': 0.78125, 'loss': 0.59102}\n",
            "Step 111460: {'train_ae_loss': 0.66759, 'train_ucc_loss': 0.45308, 'train_ucc_acc': 0.84375, 'loss': 0.56034}\n",
            "Step 111480: {'train_ae_loss': 0.66514, 'train_ucc_loss': 0.49406, 'train_ucc_acc': 0.8125, 'loss': 0.5796}\n",
            "Step 111500: {'train_ae_loss': 0.654, 'train_ucc_loss': 0.44661, 'train_ucc_acc': 0.875, 'loss': 0.5503}\n",
            "Step 111520: {'train_ae_loss': 0.65404, 'train_ucc_loss': 0.39584, 'train_ucc_acc': 0.90625, 'loss': 0.52494}\n",
            "Step 111540: {'train_ae_loss': 0.65408, 'train_ucc_loss': 0.40961, 'train_ucc_acc': 0.90625, 'loss': 0.53184}\n",
            "Step 111560: {'train_ae_loss': 0.66766, 'train_ucc_loss': 0.42766, 'train_ucc_acc': 0.875, 'loss': 0.54766}\n",
            "Step 111580: {'train_ae_loss': 0.66343, 'train_ucc_loss': 0.43898, 'train_ucc_acc': 0.875, 'loss': 0.5512}\n",
            "Step 111600: {'train_ae_loss': 0.68796, 'train_ucc_loss': 0.43616, 'train_ucc_acc': 0.875, 'loss': 0.56206}\n",
            "Step 111620: {'train_ae_loss': 0.66642, 'train_ucc_loss': 0.38792, 'train_ucc_acc': 0.96875, 'loss': 0.52717}\n",
            "Step 111640: {'train_ae_loss': 0.65401, 'train_ucc_loss': 0.37633, 'train_ucc_acc': 0.9375, 'loss': 0.51517}\n",
            "Step 111660: {'train_ae_loss': 0.65573, 'train_ucc_loss': 0.41786, 'train_ucc_acc': 0.875, 'loss': 0.53679}\n",
            "Step 111680: {'train_ae_loss': 0.67858, 'train_ucc_loss': 0.50351, 'train_ucc_acc': 0.8125, 'loss': 0.59104}\n",
            "Step 111700: {'train_ae_loss': 0.67393, 'train_ucc_loss': 0.37237, 'train_ucc_acc': 0.96875, 'loss': 0.52315}\n",
            "Step 111720: {'train_ae_loss': 0.66948, 'train_ucc_loss': 0.46092, 'train_ucc_acc': 0.8125, 'loss': 0.5652}\n",
            "Step 111740: {'train_ae_loss': 0.6638, 'train_ucc_loss': 0.50759, 'train_ucc_acc': 0.8125, 'loss': 0.5857}\n",
            "Step 111760: {'train_ae_loss': 0.65168, 'train_ucc_loss': 0.49493, 'train_ucc_acc': 0.8125, 'loss': 0.57331}\n",
            "Step 111780: {'train_ae_loss': 0.66607, 'train_ucc_loss': 0.42556, 'train_ucc_acc': 0.875, 'loss': 0.54582}\n",
            "Step 111800: {'train_ae_loss': 0.68311, 'train_ucc_loss': 0.43883, 'train_ucc_acc': 0.875, 'loss': 0.56097}\n",
            "Step 111820: {'train_ae_loss': 0.66503, 'train_ucc_loss': 0.40311, 'train_ucc_acc': 0.875, 'loss': 0.53407}\n",
            "Step 111840: {'train_ae_loss': 0.66475, 'train_ucc_loss': 0.43217, 'train_ucc_acc': 0.84375, 'loss': 0.54846}\n",
            "Step 111860: {'train_ae_loss': 0.66472, 'train_ucc_loss': 0.34934, 'train_ucc_acc': 0.96875, 'loss': 0.50703}\n",
            "Step 111880: {'train_ae_loss': 0.66771, 'train_ucc_loss': 0.36669, 'train_ucc_acc': 0.9375, 'loss': 0.5172}\n",
            "Step 111900: {'train_ae_loss': 0.67483, 'train_ucc_loss': 0.4023, 'train_ucc_acc': 0.90625, 'loss': 0.53856}\n",
            "Step 111920: {'train_ae_loss': 0.67959, 'train_ucc_loss': 0.43623, 'train_ucc_acc': 0.875, 'loss': 0.55791}\n",
            "Step 111940: {'train_ae_loss': 0.66077, 'train_ucc_loss': 0.39916, 'train_ucc_acc': 0.90625, 'loss': 0.52997}\n",
            "Step 111960: {'train_ae_loss': 0.66008, 'train_ucc_loss': 0.34899, 'train_ucc_acc': 0.96875, 'loss': 0.50454}\n",
            "Step 111980: {'train_ae_loss': 0.66818, 'train_ucc_loss': 0.39923, 'train_ucc_acc': 0.9375, 'loss': 0.53371}\n",
            "Step 112000: {'train_ae_loss': 0.66956, 'train_ucc_loss': 0.44055, 'train_ucc_acc': 0.90625, 'loss': 0.55505}\n",
            "step: 112000,eval_ae_loss: 0.65682,eval_ucc_loss: 0.46337,eval_ucc_acc: 0.84473\n",
            "Step 112020: {'train_ae_loss': 0.67719, 'train_ucc_loss': 0.43967, 'train_ucc_acc': 0.84375, 'loss': 0.55843}\n",
            "Step 112040: {'train_ae_loss': 0.67475, 'train_ucc_loss': 0.43853, 'train_ucc_acc': 0.90625, 'loss': 0.55664}\n",
            "Step 112060: {'train_ae_loss': 0.66135, 'train_ucc_loss': 0.36745, 'train_ucc_acc': 0.96875, 'loss': 0.5144}\n",
            "Step 112080: {'train_ae_loss': 0.67134, 'train_ucc_loss': 0.52714, 'train_ucc_acc': 0.78125, 'loss': 0.59924}\n",
            "Step 112100: {'train_ae_loss': 0.66776, 'train_ucc_loss': 0.55877, 'train_ucc_acc': 0.78125, 'loss': 0.61327}\n",
            "Step 112120: {'train_ae_loss': 0.67807, 'train_ucc_loss': 0.40406, 'train_ucc_acc': 0.90625, 'loss': 0.54107}\n",
            "Step 112140: {'train_ae_loss': 0.66858, 'train_ucc_loss': 0.50742, 'train_ucc_acc': 0.8125, 'loss': 0.588}\n",
            "Step 112160: {'train_ae_loss': 0.65209, 'train_ucc_loss': 0.47125, 'train_ucc_acc': 0.84375, 'loss': 0.56167}\n",
            "Step 112180: {'train_ae_loss': 0.66552, 'train_ucc_loss': 0.36903, 'train_ucc_acc': 0.9375, 'loss': 0.51728}\n",
            "Step 112200: {'train_ae_loss': 0.66594, 'train_ucc_loss': 0.40736, 'train_ucc_acc': 0.90625, 'loss': 0.53665}\n",
            "Step 112220: {'train_ae_loss': 0.65715, 'train_ucc_loss': 0.52138, 'train_ucc_acc': 0.78125, 'loss': 0.58926}\n",
            "Step 112240: {'train_ae_loss': 0.66331, 'train_ucc_loss': 0.44388, 'train_ucc_acc': 0.84375, 'loss': 0.55359}\n",
            "Step 112260: {'train_ae_loss': 0.67215, 'train_ucc_loss': 0.47099, 'train_ucc_acc': 0.84375, 'loss': 0.57157}\n",
            "Step 112280: {'train_ae_loss': 0.66373, 'train_ucc_loss': 0.37375, 'train_ucc_acc': 0.9375, 'loss': 0.51874}\n",
            "Step 112300: {'train_ae_loss': 0.6671, 'train_ucc_loss': 0.46146, 'train_ucc_acc': 0.84375, 'loss': 0.56428}\n",
            "Step 112320: {'train_ae_loss': 0.65133, 'train_ucc_loss': 0.42233, 'train_ucc_acc': 0.90625, 'loss': 0.53683}\n",
            "Step 112340: {'train_ae_loss': 0.66519, 'train_ucc_loss': 0.44527, 'train_ucc_acc': 0.84375, 'loss': 0.55523}\n",
            "Step 112360: {'train_ae_loss': 0.64263, 'train_ucc_loss': 0.46215, 'train_ucc_acc': 0.84375, 'loss': 0.55239}\n",
            "Step 112380: {'train_ae_loss': 0.65195, 'train_ucc_loss': 0.4433, 'train_ucc_acc': 0.84375, 'loss': 0.54762}\n",
            "Step 112400: {'train_ae_loss': 0.66285, 'train_ucc_loss': 0.34228, 'train_ucc_acc': 0.96875, 'loss': 0.50256}\n",
            "Step 112420: {'train_ae_loss': 0.66548, 'train_ucc_loss': 0.42489, 'train_ucc_acc': 0.875, 'loss': 0.54519}\n",
            "Step 112440: {'train_ae_loss': 0.6506, 'train_ucc_loss': 0.49882, 'train_ucc_acc': 0.78125, 'loss': 0.57471}\n",
            "Step 112460: {'train_ae_loss': 0.66001, 'train_ucc_loss': 0.4888, 'train_ucc_acc': 0.8125, 'loss': 0.5744}\n",
            "Step 112480: {'train_ae_loss': 0.63171, 'train_ucc_loss': 0.44372, 'train_ucc_acc': 0.875, 'loss': 0.53772}\n",
            "Step 112500: {'train_ae_loss': 0.6531, 'train_ucc_loss': 0.41162, 'train_ucc_acc': 0.90625, 'loss': 0.53236}\n",
            "Step 112520: {'train_ae_loss': 0.65724, 'train_ucc_loss': 0.40917, 'train_ucc_acc': 0.90625, 'loss': 0.5332}\n",
            "Step 112540: {'train_ae_loss': 0.67254, 'train_ucc_loss': 0.42215, 'train_ucc_acc': 0.875, 'loss': 0.54734}\n",
            "Step 112560: {'train_ae_loss': 0.66929, 'train_ucc_loss': 0.38463, 'train_ucc_acc': 0.90625, 'loss': 0.52696}\n",
            "Step 112580: {'train_ae_loss': 0.6696, 'train_ucc_loss': 0.46084, 'train_ucc_acc': 0.84375, 'loss': 0.56522}\n",
            "Step 112600: {'train_ae_loss': 0.65541, 'train_ucc_loss': 0.32227, 'train_ucc_acc': 1.0, 'loss': 0.48884}\n",
            "Step 112620: {'train_ae_loss': 0.66478, 'train_ucc_loss': 0.40501, 'train_ucc_acc': 0.90625, 'loss': 0.5349}\n",
            "Step 112640: {'train_ae_loss': 0.66691, 'train_ucc_loss': 0.4004, 'train_ucc_acc': 0.90625, 'loss': 0.53365}\n",
            "Step 112660: {'train_ae_loss': 0.65627, 'train_ucc_loss': 0.41719, 'train_ucc_acc': 0.875, 'loss': 0.53673}\n",
            "Step 112680: {'train_ae_loss': 0.65911, 'train_ucc_loss': 0.48314, 'train_ucc_acc': 0.8125, 'loss': 0.57113}\n",
            "Step 112700: {'train_ae_loss': 0.65252, 'train_ucc_loss': 0.50761, 'train_ucc_acc': 0.78125, 'loss': 0.58007}\n",
            "Step 112720: {'train_ae_loss': 0.65309, 'train_ucc_loss': 0.45656, 'train_ucc_acc': 0.84375, 'loss': 0.55482}\n",
            "Step 112740: {'train_ae_loss': 0.64537, 'train_ucc_loss': 0.38525, 'train_ucc_acc': 0.9375, 'loss': 0.51531}\n",
            "Step 112760: {'train_ae_loss': 0.66962, 'train_ucc_loss': 0.43471, 'train_ucc_acc': 0.875, 'loss': 0.55217}\n",
            "Step 112780: {'train_ae_loss': 0.65664, 'train_ucc_loss': 0.41461, 'train_ucc_acc': 0.90625, 'loss': 0.53562}\n",
            "Step 112800: {'train_ae_loss': 0.66679, 'train_ucc_loss': 0.52982, 'train_ucc_acc': 0.71875, 'loss': 0.5983}\n",
            "Step 112820: {'train_ae_loss': 0.68179, 'train_ucc_loss': 0.40489, 'train_ucc_acc': 0.90625, 'loss': 0.54334}\n",
            "Step 112840: {'train_ae_loss': 0.66167, 'train_ucc_loss': 0.40342, 'train_ucc_acc': 0.90625, 'loss': 0.53254}\n",
            "Step 112860: {'train_ae_loss': 0.6542, 'train_ucc_loss': 0.61645, 'train_ucc_acc': 0.6875, 'loss': 0.63533}\n",
            "Step 112880: {'train_ae_loss': 0.66606, 'train_ucc_loss': 0.43231, 'train_ucc_acc': 0.84375, 'loss': 0.54919}\n",
            "Step 112900: {'train_ae_loss': 0.66542, 'train_ucc_loss': 0.41308, 'train_ucc_acc': 0.90625, 'loss': 0.53925}\n",
            "Step 112920: {'train_ae_loss': 0.6591, 'train_ucc_loss': 0.40893, 'train_ucc_acc': 0.90625, 'loss': 0.53401}\n",
            "Step 112940: {'train_ae_loss': 0.66764, 'train_ucc_loss': 0.39734, 'train_ucc_acc': 0.90625, 'loss': 0.53249}\n",
            "Step 112960: {'train_ae_loss': 0.65991, 'train_ucc_loss': 0.45319, 'train_ucc_acc': 0.84375, 'loss': 0.55655}\n",
            "Step 112980: {'train_ae_loss': 0.65132, 'train_ucc_loss': 0.41455, 'train_ucc_acc': 0.90625, 'loss': 0.53294}\n",
            "Step 113000: {'train_ae_loss': 0.64912, 'train_ucc_loss': 0.48972, 'train_ucc_acc': 0.8125, 'loss': 0.56942}\n",
            "step: 113000,eval_ae_loss: 0.65179,eval_ucc_loss: 0.48721,eval_ucc_acc: 0.81836\n",
            "Step 113020: {'train_ae_loss': 0.64796, 'train_ucc_loss': 0.54401, 'train_ucc_acc': 0.75, 'loss': 0.59599}\n",
            "Step 113040: {'train_ae_loss': 0.65647, 'train_ucc_loss': 0.43266, 'train_ucc_acc': 0.84375, 'loss': 0.54457}\n",
            "Step 113060: {'train_ae_loss': 0.65143, 'train_ucc_loss': 0.45247, 'train_ucc_acc': 0.875, 'loss': 0.55195}\n",
            "Step 113080: {'train_ae_loss': 0.67436, 'train_ucc_loss': 0.44993, 'train_ucc_acc': 0.875, 'loss': 0.56214}\n",
            "Step 113100: {'train_ae_loss': 0.64961, 'train_ucc_loss': 0.5123, 'train_ucc_acc': 0.78125, 'loss': 0.58096}\n",
            "Step 113120: {'train_ae_loss': 0.6504, 'train_ucc_loss': 0.45108, 'train_ucc_acc': 0.875, 'loss': 0.55074}\n",
            "Step 113140: {'train_ae_loss': 0.66695, 'train_ucc_loss': 0.39103, 'train_ucc_acc': 0.9375, 'loss': 0.52899}\n",
            "Step 113160: {'train_ae_loss': 0.6914, 'train_ucc_loss': 0.3848, 'train_ucc_acc': 0.9375, 'loss': 0.5381}\n",
            "Step 113180: {'train_ae_loss': 0.65824, 'train_ucc_loss': 0.53034, 'train_ucc_acc': 0.75, 'loss': 0.59429}\n",
            "Step 113200: {'train_ae_loss': 0.6571, 'train_ucc_loss': 0.49584, 'train_ucc_acc': 0.8125, 'loss': 0.57647}\n",
            "Step 113220: {'train_ae_loss': 0.67656, 'train_ucc_loss': 0.4077, 'train_ucc_acc': 0.90625, 'loss': 0.54213}\n",
            "Step 113240: {'train_ae_loss': 0.65662, 'train_ucc_loss': 0.49187, 'train_ucc_acc': 0.8125, 'loss': 0.57425}\n",
            "Step 113260: {'train_ae_loss': 0.65302, 'train_ucc_loss': 0.46855, 'train_ucc_acc': 0.8125, 'loss': 0.56078}\n",
            "Step 113280: {'train_ae_loss': 0.66488, 'train_ucc_loss': 0.40843, 'train_ucc_acc': 0.90625, 'loss': 0.53666}\n",
            "Step 113300: {'train_ae_loss': 0.67986, 'train_ucc_loss': 0.48584, 'train_ucc_acc': 0.78125, 'loss': 0.58285}\n",
            "Step 113320: {'train_ae_loss': 0.6711, 'train_ucc_loss': 0.41433, 'train_ucc_acc': 0.90625, 'loss': 0.54271}\n",
            "Step 113340: {'train_ae_loss': 0.65174, 'train_ucc_loss': 0.47266, 'train_ucc_acc': 0.84375, 'loss': 0.5622}\n",
            "Step 113360: {'train_ae_loss': 0.66444, 'train_ucc_loss': 0.35128, 'train_ucc_acc': 0.96875, 'loss': 0.50786}\n",
            "Step 113380: {'train_ae_loss': 0.65492, 'train_ucc_loss': 0.4005, 'train_ucc_acc': 0.90625, 'loss': 0.52771}\n",
            "Step 113400: {'train_ae_loss': 0.64564, 'train_ucc_loss': 0.49124, 'train_ucc_acc': 0.8125, 'loss': 0.56844}\n",
            "Step 113420: {'train_ae_loss': 0.65259, 'train_ucc_loss': 0.44805, 'train_ucc_acc': 0.875, 'loss': 0.55032}\n",
            "Step 113440: {'train_ae_loss': 0.65129, 'train_ucc_loss': 0.5225, 'train_ucc_acc': 0.75, 'loss': 0.58689}\n",
            "Step 113460: {'train_ae_loss': 0.66278, 'train_ucc_loss': 0.41891, 'train_ucc_acc': 0.875, 'loss': 0.54085}\n",
            "Step 113480: {'train_ae_loss': 0.65537, 'train_ucc_loss': 0.42872, 'train_ucc_acc': 0.84375, 'loss': 0.54205}\n",
            "Step 113500: {'train_ae_loss': 0.65905, 'train_ucc_loss': 0.3965, 'train_ucc_acc': 0.90625, 'loss': 0.52778}\n",
            "Step 113520: {'train_ae_loss': 0.64741, 'train_ucc_loss': 0.53242, 'train_ucc_acc': 0.75, 'loss': 0.58992}\n",
            "Step 113540: {'train_ae_loss': 0.66053, 'train_ucc_loss': 0.4316, 'train_ucc_acc': 0.875, 'loss': 0.54607}\n",
            "Step 113560: {'train_ae_loss': 0.65768, 'train_ucc_loss': 0.42404, 'train_ucc_acc': 0.875, 'loss': 0.54086}\n",
            "Step 113580: {'train_ae_loss': 0.65868, 'train_ucc_loss': 0.48061, 'train_ucc_acc': 0.8125, 'loss': 0.56964}\n",
            "Step 113600: {'train_ae_loss': 0.65529, 'train_ucc_loss': 0.40441, 'train_ucc_acc': 0.9375, 'loss': 0.52985}\n",
            "Step 113620: {'train_ae_loss': 0.65509, 'train_ucc_loss': 0.56628, 'train_ucc_acc': 0.71875, 'loss': 0.61068}\n",
            "Step 113640: {'train_ae_loss': 0.6699, 'train_ucc_loss': 0.42981, 'train_ucc_acc': 0.875, 'loss': 0.54985}\n",
            "Step 113660: {'train_ae_loss': 0.66633, 'train_ucc_loss': 0.45685, 'train_ucc_acc': 0.84375, 'loss': 0.56159}\n",
            "Step 113680: {'train_ae_loss': 0.65387, 'train_ucc_loss': 0.4467, 'train_ucc_acc': 0.875, 'loss': 0.55029}\n",
            "Step 113700: {'train_ae_loss': 0.6546, 'train_ucc_loss': 0.52049, 'train_ucc_acc': 0.78125, 'loss': 0.58754}\n",
            "Step 113720: {'train_ae_loss': 0.64324, 'train_ucc_loss': 0.58185, 'train_ucc_acc': 0.71875, 'loss': 0.61255}\n",
            "Step 113740: {'train_ae_loss': 0.66289, 'train_ucc_loss': 0.35071, 'train_ucc_acc': 0.96875, 'loss': 0.5068}\n",
            "Step 113760: {'train_ae_loss': 0.6601, 'train_ucc_loss': 0.48114, 'train_ucc_acc': 0.8125, 'loss': 0.57062}\n",
            "Step 113780: {'train_ae_loss': 0.66078, 'train_ucc_loss': 0.4753, 'train_ucc_acc': 0.84375, 'loss': 0.56804}\n",
            "Step 113800: {'train_ae_loss': 0.66104, 'train_ucc_loss': 0.38192, 'train_ucc_acc': 0.9375, 'loss': 0.52148}\n",
            "Step 113820: {'train_ae_loss': 0.65335, 'train_ucc_loss': 0.38924, 'train_ucc_acc': 0.90625, 'loss': 0.52129}\n",
            "Step 113840: {'train_ae_loss': 0.66235, 'train_ucc_loss': 0.45681, 'train_ucc_acc': 0.8125, 'loss': 0.55958}\n",
            "Step 113860: {'train_ae_loss': 0.68321, 'train_ucc_loss': 0.38378, 'train_ucc_acc': 0.9375, 'loss': 0.53349}\n",
            "Step 113880: {'train_ae_loss': 0.66478, 'train_ucc_loss': 0.43865, 'train_ucc_acc': 0.875, 'loss': 0.55172}\n",
            "Step 113900: {'train_ae_loss': 0.65055, 'train_ucc_loss': 0.46991, 'train_ucc_acc': 0.84375, 'loss': 0.56023}\n",
            "Step 113920: {'train_ae_loss': 0.66351, 'train_ucc_loss': 0.55659, 'train_ucc_acc': 0.75, 'loss': 0.61005}\n",
            "Step 113940: {'train_ae_loss': 0.64583, 'train_ucc_loss': 0.5436, 'train_ucc_acc': 0.75, 'loss': 0.59471}\n",
            "Step 113960: {'train_ae_loss': 0.64602, 'train_ucc_loss': 0.50012, 'train_ucc_acc': 0.78125, 'loss': 0.57307}\n",
            "Step 113980: {'train_ae_loss': 0.65736, 'train_ucc_loss': 0.41933, 'train_ucc_acc': 0.875, 'loss': 0.53834}\n",
            "Step 114000: {'train_ae_loss': 0.67559, 'train_ucc_loss': 0.46944, 'train_ucc_acc': 0.84375, 'loss': 0.57251}\n",
            "step: 114000,eval_ae_loss: 0.64287,eval_ucc_loss: 0.47291,eval_ucc_acc: 0.83594\n",
            "Step 114020: {'train_ae_loss': 0.66831, 'train_ucc_loss': 0.45059, 'train_ucc_acc': 0.84375, 'loss': 0.55945}\n",
            "Step 114040: {'train_ae_loss': 0.66563, 'train_ucc_loss': 0.51165, 'train_ucc_acc': 0.78125, 'loss': 0.58864}\n",
            "Step 114060: {'train_ae_loss': 0.66516, 'train_ucc_loss': 0.39204, 'train_ucc_acc': 0.90625, 'loss': 0.5286}\n",
            "Step 114080: {'train_ae_loss': 0.67122, 'train_ucc_loss': 0.4322, 'train_ucc_acc': 0.875, 'loss': 0.55171}\n",
            "Step 114100: {'train_ae_loss': 0.64989, 'train_ucc_loss': 0.499, 'train_ucc_acc': 0.84375, 'loss': 0.57444}\n",
            "Step 114120: {'train_ae_loss': 0.66781, 'train_ucc_loss': 0.47367, 'train_ucc_acc': 0.84375, 'loss': 0.57074}\n",
            "Step 114140: {'train_ae_loss': 0.68858, 'train_ucc_loss': 0.48188, 'train_ucc_acc': 0.8125, 'loss': 0.58523}\n",
            "Step 114160: {'train_ae_loss': 0.67877, 'train_ucc_loss': 0.37976, 'train_ucc_acc': 0.9375, 'loss': 0.52926}\n",
            "Step 114180: {'train_ae_loss': 0.66588, 'train_ucc_loss': 0.47513, 'train_ucc_acc': 0.84375, 'loss': 0.57051}\n",
            "Step 114200: {'train_ae_loss': 0.65859, 'train_ucc_loss': 0.43357, 'train_ucc_acc': 0.875, 'loss': 0.54608}\n",
            "Step 114220: {'train_ae_loss': 0.66701, 'train_ucc_loss': 0.4628, 'train_ucc_acc': 0.84375, 'loss': 0.5649}\n",
            "Step 114240: {'train_ae_loss': 0.65171, 'train_ucc_loss': 0.46133, 'train_ucc_acc': 0.84375, 'loss': 0.55652}\n",
            "Step 114260: {'train_ae_loss': 0.6699, 'train_ucc_loss': 0.45884, 'train_ucc_acc': 0.84375, 'loss': 0.56437}\n",
            "Step 114280: {'train_ae_loss': 0.67221, 'train_ucc_loss': 0.38537, 'train_ucc_acc': 0.9375, 'loss': 0.52879}\n",
            "Step 114300: {'train_ae_loss': 0.66833, 'train_ucc_loss': 0.37602, 'train_ucc_acc': 0.9375, 'loss': 0.52218}\n",
            "Step 114320: {'train_ae_loss': 0.66657, 'train_ucc_loss': 0.50601, 'train_ucc_acc': 0.8125, 'loss': 0.58629}\n",
            "Step 114340: {'train_ae_loss': 0.65789, 'train_ucc_loss': 0.48431, 'train_ucc_acc': 0.8125, 'loss': 0.5711}\n",
            "Step 114360: {'train_ae_loss': 0.68959, 'train_ucc_loss': 0.38874, 'train_ucc_acc': 0.90625, 'loss': 0.53916}\n",
            "Step 114380: {'train_ae_loss': 0.68688, 'train_ucc_loss': 0.44185, 'train_ucc_acc': 0.875, 'loss': 0.56437}\n",
            "Step 114400: {'train_ae_loss': 0.66653, 'train_ucc_loss': 0.519, 'train_ucc_acc': 0.78125, 'loss': 0.59276}\n",
            "Step 114420: {'train_ae_loss': 0.66974, 'train_ucc_loss': 0.48043, 'train_ucc_acc': 0.8125, 'loss': 0.57509}\n",
            "Step 114440: {'train_ae_loss': 0.6668, 'train_ucc_loss': 0.36579, 'train_ucc_acc': 0.96875, 'loss': 0.51629}\n",
            "Step 114460: {'train_ae_loss': 0.67551, 'train_ucc_loss': 0.50503, 'train_ucc_acc': 0.8125, 'loss': 0.59027}\n",
            "Step 114480: {'train_ae_loss': 0.66696, 'train_ucc_loss': 0.53587, 'train_ucc_acc': 0.71875, 'loss': 0.60142}\n",
            "Step 114500: {'train_ae_loss': 0.67403, 'train_ucc_loss': 0.39524, 'train_ucc_acc': 0.9375, 'loss': 0.53463}\n",
            "Step 114520: {'train_ae_loss': 0.65845, 'train_ucc_loss': 0.47071, 'train_ucc_acc': 0.875, 'loss': 0.56458}\n",
            "Step 114540: {'train_ae_loss': 0.67055, 'train_ucc_loss': 0.44114, 'train_ucc_acc': 0.84375, 'loss': 0.55585}\n",
            "Step 114560: {'train_ae_loss': 0.68648, 'train_ucc_loss': 0.43436, 'train_ucc_acc': 0.875, 'loss': 0.56042}\n",
            "Step 114580: {'train_ae_loss': 0.66562, 'train_ucc_loss': 0.40462, 'train_ucc_acc': 0.90625, 'loss': 0.53512}\n",
            "Step 114600: {'train_ae_loss': 0.65456, 'train_ucc_loss': 0.43199, 'train_ucc_acc': 0.875, 'loss': 0.54327}\n",
            "Step 114620: {'train_ae_loss': 0.67651, 'train_ucc_loss': 0.41987, 'train_ucc_acc': 0.875, 'loss': 0.54819}\n",
            "Step 114640: {'train_ae_loss': 0.66328, 'train_ucc_loss': 0.49138, 'train_ucc_acc': 0.84375, 'loss': 0.57733}\n",
            "Step 114660: {'train_ae_loss': 0.67242, 'train_ucc_loss': 0.52484, 'train_ucc_acc': 0.8125, 'loss': 0.59863}\n",
            "Step 114680: {'train_ae_loss': 0.66797, 'train_ucc_loss': 0.50003, 'train_ucc_acc': 0.78125, 'loss': 0.584}\n",
            "Step 114700: {'train_ae_loss': 0.67957, 'train_ucc_loss': 0.42218, 'train_ucc_acc': 0.875, 'loss': 0.55088}\n",
            "Step 114720: {'train_ae_loss': 0.67401, 'train_ucc_loss': 0.41969, 'train_ucc_acc': 0.90625, 'loss': 0.54685}\n",
            "Step 114740: {'train_ae_loss': 0.67807, 'train_ucc_loss': 0.40196, 'train_ucc_acc': 0.90625, 'loss': 0.54002}\n",
            "Step 114760: {'train_ae_loss': 0.67354, 'train_ucc_loss': 0.43455, 'train_ucc_acc': 0.875, 'loss': 0.55404}\n",
            "Step 114780: {'train_ae_loss': 0.64998, 'train_ucc_loss': 0.44371, 'train_ucc_acc': 0.875, 'loss': 0.54684}\n",
            "Step 114800: {'train_ae_loss': 0.66294, 'train_ucc_loss': 0.37249, 'train_ucc_acc': 0.9375, 'loss': 0.51772}\n",
            "Step 114820: {'train_ae_loss': 0.65065, 'train_ucc_loss': 0.45831, 'train_ucc_acc': 0.875, 'loss': 0.55448}\n",
            "Step 114840: {'train_ae_loss': 0.65161, 'train_ucc_loss': 0.4255, 'train_ucc_acc': 0.90625, 'loss': 0.53856}\n",
            "Step 114860: {'train_ae_loss': 0.6647, 'train_ucc_loss': 0.38, 'train_ucc_acc': 0.90625, 'loss': 0.52235}\n",
            "Step 114880: {'train_ae_loss': 0.68567, 'train_ucc_loss': 0.32012, 'train_ucc_acc': 1.0, 'loss': 0.50289}\n",
            "Step 114900: {'train_ae_loss': 0.65097, 'train_ucc_loss': 0.36594, 'train_ucc_acc': 0.96875, 'loss': 0.50846}\n",
            "Step 114920: {'train_ae_loss': 0.66886, 'train_ucc_loss': 0.36508, 'train_ucc_acc': 0.9375, 'loss': 0.51697}\n",
            "Step 114940: {'train_ae_loss': 0.65833, 'train_ucc_loss': 0.4763, 'train_ucc_acc': 0.8125, 'loss': 0.56732}\n",
            "Step 114960: {'train_ae_loss': 0.65702, 'train_ucc_loss': 0.50907, 'train_ucc_acc': 0.8125, 'loss': 0.58304}\n",
            "Step 114980: {'train_ae_loss': 0.66143, 'train_ucc_loss': 0.4045, 'train_ucc_acc': 0.875, 'loss': 0.53297}\n",
            "Step 115000: {'train_ae_loss': 0.68438, 'train_ucc_loss': 0.57423, 'train_ucc_acc': 0.71875, 'loss': 0.62931}\n",
            "step: 115000,eval_ae_loss: 0.65433,eval_ucc_loss: 0.53933,eval_ucc_acc: 0.76953\n",
            "Step 115020: {'train_ae_loss': 0.64666, 'train_ucc_loss': 0.42843, 'train_ucc_acc': 0.875, 'loss': 0.53754}\n",
            "Step 115040: {'train_ae_loss': 0.65426, 'train_ucc_loss': 0.41403, 'train_ucc_acc': 0.90625, 'loss': 0.53415}\n",
            "Step 115060: {'train_ae_loss': 0.65099, 'train_ucc_loss': 0.3813, 'train_ucc_acc': 0.9375, 'loss': 0.51614}\n",
            "Step 115080: {'train_ae_loss': 0.64527, 'train_ucc_loss': 0.4488, 'train_ucc_acc': 0.84375, 'loss': 0.54704}\n",
            "Step 115100: {'train_ae_loss': 0.67044, 'train_ucc_loss': 0.5461, 'train_ucc_acc': 0.78125, 'loss': 0.60827}\n",
            "Step 115120: {'train_ae_loss': 0.66624, 'train_ucc_loss': 0.42066, 'train_ucc_acc': 0.875, 'loss': 0.54345}\n",
            "Step 115140: {'train_ae_loss': 0.66366, 'train_ucc_loss': 0.4674, 'train_ucc_acc': 0.84375, 'loss': 0.56553}\n",
            "Step 115160: {'train_ae_loss': 0.67189, 'train_ucc_loss': 0.49083, 'train_ucc_acc': 0.78125, 'loss': 0.58136}\n",
            "Step 115180: {'train_ae_loss': 0.65217, 'train_ucc_loss': 0.39362, 'train_ucc_acc': 0.90625, 'loss': 0.5229}\n",
            "Step 115200: {'train_ae_loss': 0.66863, 'train_ucc_loss': 0.3968, 'train_ucc_acc': 0.90625, 'loss': 0.53272}\n",
            "Step 115220: {'train_ae_loss': 0.66962, 'train_ucc_loss': 0.42509, 'train_ucc_acc': 0.90625, 'loss': 0.54736}\n",
            "Step 115240: {'train_ae_loss': 0.67458, 'train_ucc_loss': 0.44655, 'train_ucc_acc': 0.84375, 'loss': 0.56057}\n",
            "Step 115260: {'train_ae_loss': 0.68823, 'train_ucc_loss': 0.46583, 'train_ucc_acc': 0.8125, 'loss': 0.57703}\n",
            "Step 115280: {'train_ae_loss': 0.67492, 'train_ucc_loss': 0.44589, 'train_ucc_acc': 0.84375, 'loss': 0.56041}\n",
            "Step 115300: {'train_ae_loss': 0.67478, 'train_ucc_loss': 0.50426, 'train_ucc_acc': 0.8125, 'loss': 0.58952}\n",
            "Step 115320: {'train_ae_loss': 0.66415, 'train_ucc_loss': 0.51858, 'train_ucc_acc': 0.8125, 'loss': 0.59136}\n",
            "Step 115340: {'train_ae_loss': 0.67382, 'train_ucc_loss': 0.41868, 'train_ucc_acc': 0.875, 'loss': 0.54625}\n",
            "Step 115360: {'train_ae_loss': 0.67013, 'train_ucc_loss': 0.49442, 'train_ucc_acc': 0.78125, 'loss': 0.58228}\n",
            "Step 115380: {'train_ae_loss': 0.67388, 'train_ucc_loss': 0.41482, 'train_ucc_acc': 0.875, 'loss': 0.54435}\n",
            "Step 115400: {'train_ae_loss': 0.66328, 'train_ucc_loss': 0.48504, 'train_ucc_acc': 0.84375, 'loss': 0.57416}\n",
            "Step 115420: {'train_ae_loss': 0.66898, 'train_ucc_loss': 0.41415, 'train_ucc_acc': 0.90625, 'loss': 0.54157}\n",
            "Step 115440: {'train_ae_loss': 0.67391, 'train_ucc_loss': 0.46497, 'train_ucc_acc': 0.84375, 'loss': 0.56944}\n",
            "Step 115460: {'train_ae_loss': 0.66044, 'train_ucc_loss': 0.44654, 'train_ucc_acc': 0.84375, 'loss': 0.55349}\n",
            "Step 115480: {'train_ae_loss': 0.67223, 'train_ucc_loss': 0.34987, 'train_ucc_acc': 0.96875, 'loss': 0.51105}\n",
            "Step 115500: {'train_ae_loss': 0.66523, 'train_ucc_loss': 0.51589, 'train_ucc_acc': 0.78125, 'loss': 0.59056}\n",
            "Step 115520: {'train_ae_loss': 0.6659, 'train_ucc_loss': 0.35282, 'train_ucc_acc': 0.96875, 'loss': 0.50936}\n",
            "Step 115540: {'train_ae_loss': 0.6796, 'train_ucc_loss': 0.45696, 'train_ucc_acc': 0.875, 'loss': 0.56828}\n",
            "Step 115560: {'train_ae_loss': 0.67708, 'train_ucc_loss': 0.46344, 'train_ucc_acc': 0.84375, 'loss': 0.57026}\n",
            "Step 115580: {'train_ae_loss': 0.66448, 'train_ucc_loss': 0.4191, 'train_ucc_acc': 0.90625, 'loss': 0.54179}\n",
            "Step 115600: {'train_ae_loss': 0.66257, 'train_ucc_loss': 0.43198, 'train_ucc_acc': 0.90625, 'loss': 0.54728}\n",
            "Step 115620: {'train_ae_loss': 0.68874, 'train_ucc_loss': 0.42343, 'train_ucc_acc': 0.875, 'loss': 0.55608}\n",
            "Step 115640: {'train_ae_loss': 0.67048, 'train_ucc_loss': 0.40599, 'train_ucc_acc': 0.90625, 'loss': 0.53824}\n",
            "Step 115660: {'train_ae_loss': 0.69249, 'train_ucc_loss': 0.44482, 'train_ucc_acc': 0.875, 'loss': 0.56865}\n",
            "Step 115680: {'train_ae_loss': 0.65586, 'train_ucc_loss': 0.43422, 'train_ucc_acc': 0.875, 'loss': 0.54504}\n",
            "Step 115700: {'train_ae_loss': 0.6626, 'train_ucc_loss': 0.41349, 'train_ucc_acc': 0.875, 'loss': 0.53804}\n",
            "Step 115720: {'train_ae_loss': 0.67666, 'train_ucc_loss': 0.51045, 'train_ucc_acc': 0.78125, 'loss': 0.59355}\n",
            "Step 115740: {'train_ae_loss': 0.65093, 'train_ucc_loss': 0.44848, 'train_ucc_acc': 0.84375, 'loss': 0.54971}\n",
            "Step 115760: {'train_ae_loss': 0.68218, 'train_ucc_loss': 0.35699, 'train_ucc_acc': 0.96875, 'loss': 0.51958}\n",
            "Step 115780: {'train_ae_loss': 0.6677, 'train_ucc_loss': 0.43409, 'train_ucc_acc': 0.875, 'loss': 0.55089}\n",
            "Step 115800: {'train_ae_loss': 0.65506, 'train_ucc_loss': 0.39953, 'train_ucc_acc': 0.9375, 'loss': 0.52729}\n",
            "Step 115820: {'train_ae_loss': 0.68199, 'train_ucc_loss': 0.35217, 'train_ucc_acc': 0.96875, 'loss': 0.51708}\n",
            "Step 115840: {'train_ae_loss': 0.65786, 'train_ucc_loss': 0.4432, 'train_ucc_acc': 0.875, 'loss': 0.55053}\n",
            "Step 115860: {'train_ae_loss': 0.6607, 'train_ucc_loss': 0.53518, 'train_ucc_acc': 0.75, 'loss': 0.59794}\n",
            "Step 115880: {'train_ae_loss': 0.66671, 'train_ucc_loss': 0.41004, 'train_ucc_acc': 0.875, 'loss': 0.53838}\n",
            "Step 115900: {'train_ae_loss': 0.68065, 'train_ucc_loss': 0.50514, 'train_ucc_acc': 0.8125, 'loss': 0.59289}\n",
            "Step 115920: {'train_ae_loss': 0.67545, 'train_ucc_loss': 0.50647, 'train_ucc_acc': 0.8125, 'loss': 0.59096}\n",
            "Step 115940: {'train_ae_loss': 0.66507, 'train_ucc_loss': 0.42708, 'train_ucc_acc': 0.875, 'loss': 0.54608}\n",
            "Step 115960: {'train_ae_loss': 0.6624, 'train_ucc_loss': 0.38557, 'train_ucc_acc': 0.9375, 'loss': 0.52398}\n",
            "Step 115980: {'train_ae_loss': 0.66843, 'train_ucc_loss': 0.3761, 'train_ucc_acc': 0.9375, 'loss': 0.52227}\n",
            "Step 116000: {'train_ae_loss': 0.67101, 'train_ucc_loss': 0.52693, 'train_ucc_acc': 0.75, 'loss': 0.59897}\n",
            "step: 116000,eval_ae_loss: 0.65017,eval_ucc_loss: 0.52068,eval_ucc_acc: 0.7832\n",
            "Step 116020: {'train_ae_loss': 0.67001, 'train_ucc_loss': 0.44049, 'train_ucc_acc': 0.875, 'loss': 0.55525}\n",
            "Step 116040: {'train_ae_loss': 0.64454, 'train_ucc_loss': 0.52009, 'train_ucc_acc': 0.78125, 'loss': 0.58231}\n",
            "Step 116060: {'train_ae_loss': 0.66934, 'train_ucc_loss': 0.41288, 'train_ucc_acc': 0.90625, 'loss': 0.54111}\n",
            "Step 116080: {'train_ae_loss': 0.65962, 'train_ucc_loss': 0.49397, 'train_ucc_acc': 0.8125, 'loss': 0.5768}\n",
            "Step 116100: {'train_ae_loss': 0.66767, 'train_ucc_loss': 0.56223, 'train_ucc_acc': 0.75, 'loss': 0.61495}\n",
            "Step 116120: {'train_ae_loss': 0.66682, 'train_ucc_loss': 0.53053, 'train_ucc_acc': 0.78125, 'loss': 0.59868}\n",
            "Step 116140: {'train_ae_loss': 0.66482, 'train_ucc_loss': 0.4811, 'train_ucc_acc': 0.8125, 'loss': 0.57296}\n",
            "Step 116160: {'train_ae_loss': 0.6754, 'train_ucc_loss': 0.36053, 'train_ucc_acc': 0.96875, 'loss': 0.51796}\n",
            "Step 116180: {'train_ae_loss': 0.65567, 'train_ucc_loss': 0.53961, 'train_ucc_acc': 0.78125, 'loss': 0.59764}\n",
            "Step 116200: {'train_ae_loss': 0.65185, 'train_ucc_loss': 0.46568, 'train_ucc_acc': 0.84375, 'loss': 0.55876}\n",
            "Step 116220: {'train_ae_loss': 0.66256, 'train_ucc_loss': 0.5497, 'train_ucc_acc': 0.75, 'loss': 0.60613}\n",
            "Step 116240: {'train_ae_loss': 0.66183, 'train_ucc_loss': 0.3704, 'train_ucc_acc': 0.96875, 'loss': 0.51611}\n",
            "Step 116260: {'train_ae_loss': 0.65922, 'train_ucc_loss': 0.40188, 'train_ucc_acc': 0.90625, 'loss': 0.53055}\n",
            "Step 116280: {'train_ae_loss': 0.65829, 'train_ucc_loss': 0.52094, 'train_ucc_acc': 0.78125, 'loss': 0.58961}\n",
            "Step 116300: {'train_ae_loss': 0.67244, 'train_ucc_loss': 0.37322, 'train_ucc_acc': 0.96875, 'loss': 0.52283}\n",
            "Step 116320: {'train_ae_loss': 0.66015, 'train_ucc_loss': 0.38062, 'train_ucc_acc': 0.9375, 'loss': 0.52038}\n",
            "Step 116340: {'train_ae_loss': 0.64431, 'train_ucc_loss': 0.40328, 'train_ucc_acc': 0.875, 'loss': 0.5238}\n",
            "Step 116360: {'train_ae_loss': 0.66094, 'train_ucc_loss': 0.45447, 'train_ucc_acc': 0.84375, 'loss': 0.5577}\n",
            "Step 116380: {'train_ae_loss': 0.66771, 'train_ucc_loss': 0.45987, 'train_ucc_acc': 0.84375, 'loss': 0.56379}\n",
            "Step 116400: {'train_ae_loss': 0.65465, 'train_ucc_loss': 0.42184, 'train_ucc_acc': 0.875, 'loss': 0.53824}\n",
            "Step 116420: {'train_ae_loss': 0.64501, 'train_ucc_loss': 0.55979, 'train_ucc_acc': 0.71875, 'loss': 0.6024}\n",
            "Step 116440: {'train_ae_loss': 0.67994, 'train_ucc_loss': 0.37944, 'train_ucc_acc': 0.9375, 'loss': 0.52969}\n",
            "Step 116460: {'train_ae_loss': 0.63654, 'train_ucc_loss': 0.49984, 'train_ucc_acc': 0.75, 'loss': 0.56819}\n",
            "Step 116480: {'train_ae_loss': 0.6646, 'train_ucc_loss': 0.45571, 'train_ucc_acc': 0.84375, 'loss': 0.56016}\n",
            "Step 116500: {'train_ae_loss': 0.65492, 'train_ucc_loss': 0.50142, 'train_ucc_acc': 0.84375, 'loss': 0.57817}\n",
            "Step 116520: {'train_ae_loss': 0.6728, 'train_ucc_loss': 0.33461, 'train_ucc_acc': 1.0, 'loss': 0.5037}\n",
            "Step 116540: {'train_ae_loss': 0.65116, 'train_ucc_loss': 0.43926, 'train_ucc_acc': 0.875, 'loss': 0.54521}\n",
            "Step 116560: {'train_ae_loss': 0.65413, 'train_ucc_loss': 0.44435, 'train_ucc_acc': 0.875, 'loss': 0.54924}\n",
            "Step 116580: {'train_ae_loss': 0.65685, 'train_ucc_loss': 0.40849, 'train_ucc_acc': 0.90625, 'loss': 0.53267}\n",
            "Step 116600: {'train_ae_loss': 0.6676, 'train_ucc_loss': 0.43161, 'train_ucc_acc': 0.90625, 'loss': 0.5496}\n",
            "Step 116620: {'train_ae_loss': 0.67905, 'train_ucc_loss': 0.41781, 'train_ucc_acc': 0.875, 'loss': 0.54843}\n",
            "Step 116640: {'train_ae_loss': 0.65315, 'train_ucc_loss': 0.46463, 'train_ucc_acc': 0.84375, 'loss': 0.55889}\n",
            "Step 116660: {'train_ae_loss': 0.66458, 'train_ucc_loss': 0.37798, 'train_ucc_acc': 0.9375, 'loss': 0.52128}\n",
            "Step 116680: {'train_ae_loss': 0.65262, 'train_ucc_loss': 0.42063, 'train_ucc_acc': 0.875, 'loss': 0.53663}\n",
            "Step 116700: {'train_ae_loss': 0.66255, 'train_ucc_loss': 0.40197, 'train_ucc_acc': 0.90625, 'loss': 0.53226}\n",
            "Step 116720: {'train_ae_loss': 0.66503, 'train_ucc_loss': 0.38942, 'train_ucc_acc': 0.90625, 'loss': 0.52722}\n",
            "Step 116740: {'train_ae_loss': 0.65736, 'train_ucc_loss': 0.46714, 'train_ucc_acc': 0.84375, 'loss': 0.56225}\n",
            "Step 116760: {'train_ae_loss': 0.63153, 'train_ucc_loss': 0.47043, 'train_ucc_acc': 0.84375, 'loss': 0.55098}\n",
            "Step 116780: {'train_ae_loss': 0.65681, 'train_ucc_loss': 0.37386, 'train_ucc_acc': 0.96875, 'loss': 0.51533}\n",
            "Step 116800: {'train_ae_loss': 0.65802, 'train_ucc_loss': 0.41185, 'train_ucc_acc': 0.90625, 'loss': 0.53493}\n",
            "Step 116820: {'train_ae_loss': 0.66722, 'train_ucc_loss': 0.47112, 'train_ucc_acc': 0.84375, 'loss': 0.56917}\n",
            "Step 116840: {'train_ae_loss': 0.64831, 'train_ucc_loss': 0.39657, 'train_ucc_acc': 0.9375, 'loss': 0.52244}\n",
            "Step 116860: {'train_ae_loss': 0.66478, 'train_ucc_loss': 0.45833, 'train_ucc_acc': 0.875, 'loss': 0.56155}\n",
            "Step 116880: {'train_ae_loss': 0.66885, 'train_ucc_loss': 0.55465, 'train_ucc_acc': 0.71875, 'loss': 0.61175}\n",
            "Step 116900: {'train_ae_loss': 0.65233, 'train_ucc_loss': 0.52446, 'train_ucc_acc': 0.8125, 'loss': 0.5884}\n",
            "Step 116920: {'train_ae_loss': 0.66286, 'train_ucc_loss': 0.4421, 'train_ucc_acc': 0.84375, 'loss': 0.55248}\n",
            "Step 116940: {'train_ae_loss': 0.6568, 'train_ucc_loss': 0.40455, 'train_ucc_acc': 0.90625, 'loss': 0.53068}\n",
            "Step 116960: {'train_ae_loss': 0.67118, 'train_ucc_loss': 0.50408, 'train_ucc_acc': 0.78125, 'loss': 0.58763}\n",
            "Step 116980: {'train_ae_loss': 0.67277, 'train_ucc_loss': 0.45983, 'train_ucc_acc': 0.84375, 'loss': 0.5663}\n",
            "Step 117000: {'train_ae_loss': 0.65911, 'train_ucc_loss': 0.50084, 'train_ucc_acc': 0.8125, 'loss': 0.57997}\n",
            "step: 117000,eval_ae_loss: 0.65067,eval_ucc_loss: 0.48893,eval_ucc_acc: 0.81445\n",
            "Step 117020: {'train_ae_loss': 0.66622, 'train_ucc_loss': 0.41042, 'train_ucc_acc': 0.9375, 'loss': 0.53832}\n",
            "Step 117040: {'train_ae_loss': 0.66522, 'train_ucc_loss': 0.39666, 'train_ucc_acc': 0.90625, 'loss': 0.53094}\n",
            "Step 117060: {'train_ae_loss': 0.65118, 'train_ucc_loss': 0.46012, 'train_ucc_acc': 0.84375, 'loss': 0.55565}\n",
            "Step 117080: {'train_ae_loss': 0.65076, 'train_ucc_loss': 0.38086, 'train_ucc_acc': 0.90625, 'loss': 0.51581}\n",
            "Step 117100: {'train_ae_loss': 0.67671, 'train_ucc_loss': 0.43735, 'train_ucc_acc': 0.875, 'loss': 0.55703}\n",
            "Step 117120: {'train_ae_loss': 0.66876, 'train_ucc_loss': 0.42519, 'train_ucc_acc': 0.875, 'loss': 0.54698}\n",
            "Step 117140: {'train_ae_loss': 0.66388, 'train_ucc_loss': 0.36718, 'train_ucc_acc': 0.9375, 'loss': 0.51553}\n",
            "Step 117160: {'train_ae_loss': 0.66734, 'train_ucc_loss': 0.47714, 'train_ucc_acc': 0.8125, 'loss': 0.57224}\n",
            "Step 117180: {'train_ae_loss': 0.65405, 'train_ucc_loss': 0.33118, 'train_ucc_acc': 0.96875, 'loss': 0.49261}\n",
            "Step 117200: {'train_ae_loss': 0.65238, 'train_ucc_loss': 0.43215, 'train_ucc_acc': 0.90625, 'loss': 0.54226}\n",
            "Step 117220: {'train_ae_loss': 0.65064, 'train_ucc_loss': 0.47579, 'train_ucc_acc': 0.84375, 'loss': 0.56322}\n",
            "Step 117240: {'train_ae_loss': 0.65833, 'train_ucc_loss': 0.37457, 'train_ucc_acc': 0.9375, 'loss': 0.51645}\n",
            "Step 117260: {'train_ae_loss': 0.6468, 'train_ucc_loss': 0.45679, 'train_ucc_acc': 0.875, 'loss': 0.55179}\n",
            "Step 117280: {'train_ae_loss': 0.65, 'train_ucc_loss': 0.47083, 'train_ucc_acc': 0.8125, 'loss': 0.56041}\n",
            "Step 117300: {'train_ae_loss': 0.65547, 'train_ucc_loss': 0.42827, 'train_ucc_acc': 0.875, 'loss': 0.54187}\n",
            "Step 117320: {'train_ae_loss': 0.67775, 'train_ucc_loss': 0.46586, 'train_ucc_acc': 0.8125, 'loss': 0.57181}\n",
            "Step 117340: {'train_ae_loss': 0.66939, 'train_ucc_loss': 0.46177, 'train_ucc_acc': 0.84375, 'loss': 0.56558}\n",
            "Step 117360: {'train_ae_loss': 0.67677, 'train_ucc_loss': 0.42093, 'train_ucc_acc': 0.90625, 'loss': 0.54885}\n",
            "Step 117380: {'train_ae_loss': 0.66325, 'train_ucc_loss': 0.49551, 'train_ucc_acc': 0.8125, 'loss': 0.57938}\n",
            "Step 117400: {'train_ae_loss': 0.67476, 'train_ucc_loss': 0.3935, 'train_ucc_acc': 0.9375, 'loss': 0.53413}\n",
            "Step 117420: {'train_ae_loss': 0.68628, 'train_ucc_loss': 0.46945, 'train_ucc_acc': 0.8125, 'loss': 0.57786}\n",
            "Step 117440: {'train_ae_loss': 0.65576, 'train_ucc_loss': 0.49677, 'train_ucc_acc': 0.8125, 'loss': 0.57627}\n",
            "Step 117460: {'train_ae_loss': 0.64912, 'train_ucc_loss': 0.40627, 'train_ucc_acc': 0.90625, 'loss': 0.5277}\n",
            "Step 117480: {'train_ae_loss': 0.66011, 'train_ucc_loss': 0.47243, 'train_ucc_acc': 0.8125, 'loss': 0.56627}\n",
            "Step 117500: {'train_ae_loss': 0.66278, 'train_ucc_loss': 0.45125, 'train_ucc_acc': 0.84375, 'loss': 0.55702}\n",
            "Step 117520: {'train_ae_loss': 0.66074, 'train_ucc_loss': 0.47803, 'train_ucc_acc': 0.84375, 'loss': 0.56938}\n",
            "Step 117540: {'train_ae_loss': 0.64843, 'train_ucc_loss': 0.37411, 'train_ucc_acc': 0.9375, 'loss': 0.51127}\n",
            "Step 117560: {'train_ae_loss': 0.63773, 'train_ucc_loss': 0.41779, 'train_ucc_acc': 0.875, 'loss': 0.52776}\n",
            "Step 117580: {'train_ae_loss': 0.66603, 'train_ucc_loss': 0.43172, 'train_ucc_acc': 0.875, 'loss': 0.54887}\n",
            "Step 117600: {'train_ae_loss': 0.67025, 'train_ucc_loss': 0.38268, 'train_ucc_acc': 0.9375, 'loss': 0.52647}\n",
            "Step 117620: {'train_ae_loss': 0.66883, 'train_ucc_loss': 0.45388, 'train_ucc_acc': 0.875, 'loss': 0.56136}\n",
            "Step 117640: {'train_ae_loss': 0.66572, 'train_ucc_loss': 0.44852, 'train_ucc_acc': 0.875, 'loss': 0.55712}\n",
            "Step 117660: {'train_ae_loss': 0.65717, 'train_ucc_loss': 0.48276, 'train_ucc_acc': 0.8125, 'loss': 0.56996}\n",
            "Step 117680: {'train_ae_loss': 0.65823, 'train_ucc_loss': 0.51209, 'train_ucc_acc': 0.8125, 'loss': 0.58516}\n",
            "Step 117700: {'train_ae_loss': 0.65343, 'train_ucc_loss': 0.49985, 'train_ucc_acc': 0.8125, 'loss': 0.57664}\n",
            "Step 117720: {'train_ae_loss': 0.64833, 'train_ucc_loss': 0.41184, 'train_ucc_acc': 0.90625, 'loss': 0.53009}\n",
            "Step 117740: {'train_ae_loss': 0.66023, 'train_ucc_loss': 0.42733, 'train_ucc_acc': 0.90625, 'loss': 0.54378}\n",
            "Step 117760: {'train_ae_loss': 0.67669, 'train_ucc_loss': 0.4658, 'train_ucc_acc': 0.84375, 'loss': 0.57124}\n",
            "Step 117780: {'train_ae_loss': 0.66707, 'train_ucc_loss': 0.4432, 'train_ucc_acc': 0.875, 'loss': 0.55513}\n",
            "Step 117800: {'train_ae_loss': 0.64961, 'train_ucc_loss': 0.42295, 'train_ucc_acc': 0.90625, 'loss': 0.53628}\n",
            "Step 117820: {'train_ae_loss': 0.64821, 'train_ucc_loss': 0.52681, 'train_ucc_acc': 0.78125, 'loss': 0.58751}\n",
            "Step 117840: {'train_ae_loss': 0.66755, 'train_ucc_loss': 0.40665, 'train_ucc_acc': 0.90625, 'loss': 0.5371}\n",
            "Step 117860: {'train_ae_loss': 0.64587, 'train_ucc_loss': 0.46998, 'train_ucc_acc': 0.84375, 'loss': 0.55793}\n",
            "Step 117880: {'train_ae_loss': 0.66951, 'train_ucc_loss': 0.45792, 'train_ucc_acc': 0.8125, 'loss': 0.56372}\n",
            "Step 117900: {'train_ae_loss': 0.67472, 'train_ucc_loss': 0.43537, 'train_ucc_acc': 0.875, 'loss': 0.55504}\n",
            "Step 117920: {'train_ae_loss': 0.66421, 'train_ucc_loss': 0.46301, 'train_ucc_acc': 0.84375, 'loss': 0.56361}\n",
            "Step 117940: {'train_ae_loss': 0.67245, 'train_ucc_loss': 0.37353, 'train_ucc_acc': 0.9375, 'loss': 0.52299}\n",
            "Step 117960: {'train_ae_loss': 0.66262, 'train_ucc_loss': 0.48579, 'train_ucc_acc': 0.8125, 'loss': 0.57421}\n",
            "Step 117980: {'train_ae_loss': 0.68207, 'train_ucc_loss': 0.53571, 'train_ucc_acc': 0.75, 'loss': 0.60889}\n",
            "Step 118000: {'train_ae_loss': 0.64995, 'train_ucc_loss': 0.45507, 'train_ucc_acc': 0.875, 'loss': 0.55251}\n",
            "step: 118000,eval_ae_loss: 0.65535,eval_ucc_loss: 0.47694,eval_ucc_acc: 0.82715\n",
            "Step 118020: {'train_ae_loss': 0.66984, 'train_ucc_loss': 0.4227, 'train_ucc_acc': 0.90625, 'loss': 0.54627}\n",
            "Step 118040: {'train_ae_loss': 0.65609, 'train_ucc_loss': 0.40049, 'train_ucc_acc': 0.90625, 'loss': 0.52829}\n",
            "Step 118060: {'train_ae_loss': 0.66714, 'train_ucc_loss': 0.45551, 'train_ucc_acc': 0.875, 'loss': 0.56133}\n",
            "Step 118080: {'train_ae_loss': 0.67653, 'train_ucc_loss': 0.34424, 'train_ucc_acc': 0.96875, 'loss': 0.51038}\n",
            "Step 118100: {'train_ae_loss': 0.67392, 'train_ucc_loss': 0.53001, 'train_ucc_acc': 0.78125, 'loss': 0.60197}\n",
            "Step 118120: {'train_ae_loss': 0.67601, 'train_ucc_loss': 0.38853, 'train_ucc_acc': 0.9375, 'loss': 0.53227}\n",
            "Step 118140: {'train_ae_loss': 0.66827, 'train_ucc_loss': 0.4184, 'train_ucc_acc': 0.90625, 'loss': 0.54334}\n",
            "Step 118160: {'train_ae_loss': 0.66292, 'train_ucc_loss': 0.41183, 'train_ucc_acc': 0.875, 'loss': 0.53737}\n",
            "Step 118180: {'train_ae_loss': 0.64437, 'train_ucc_loss': 0.39494, 'train_ucc_acc': 0.9375, 'loss': 0.51966}\n",
            "Step 118200: {'train_ae_loss': 0.66392, 'train_ucc_loss': 0.46098, 'train_ucc_acc': 0.8125, 'loss': 0.56245}\n",
            "Step 118220: {'train_ae_loss': 0.66397, 'train_ucc_loss': 0.46096, 'train_ucc_acc': 0.84375, 'loss': 0.56247}\n",
            "Step 118240: {'train_ae_loss': 0.6716, 'train_ucc_loss': 0.44938, 'train_ucc_acc': 0.84375, 'loss': 0.56049}\n",
            "Step 118260: {'train_ae_loss': 0.66319, 'train_ucc_loss': 0.4818, 'train_ucc_acc': 0.84375, 'loss': 0.5725}\n",
            "Step 118280: {'train_ae_loss': 0.65305, 'train_ucc_loss': 0.35063, 'train_ucc_acc': 0.96875, 'loss': 0.50184}\n",
            "Step 118300: {'train_ae_loss': 0.65206, 'train_ucc_loss': 0.50159, 'train_ucc_acc': 0.78125, 'loss': 0.57682}\n",
            "Step 118320: {'train_ae_loss': 0.66126, 'train_ucc_loss': 0.38767, 'train_ucc_acc': 0.9375, 'loss': 0.52447}\n",
            "Step 118340: {'train_ae_loss': 0.6835, 'train_ucc_loss': 0.44734, 'train_ucc_acc': 0.90625, 'loss': 0.56542}\n",
            "Step 118360: {'train_ae_loss': 0.66405, 'train_ucc_loss': 0.38402, 'train_ucc_acc': 0.9375, 'loss': 0.52404}\n",
            "Step 118380: {'train_ae_loss': 0.65034, 'train_ucc_loss': 0.49455, 'train_ucc_acc': 0.78125, 'loss': 0.57245}\n",
            "Step 118400: {'train_ae_loss': 0.6535, 'train_ucc_loss': 0.50525, 'train_ucc_acc': 0.78125, 'loss': 0.57938}\n",
            "Step 118420: {'train_ae_loss': 0.66066, 'train_ucc_loss': 0.37961, 'train_ucc_acc': 0.9375, 'loss': 0.52013}\n",
            "Step 118440: {'train_ae_loss': 0.65708, 'train_ucc_loss': 0.40107, 'train_ucc_acc': 0.875, 'loss': 0.52907}\n",
            "Step 118460: {'train_ae_loss': 0.65683, 'train_ucc_loss': 0.50651, 'train_ucc_acc': 0.8125, 'loss': 0.58167}\n",
            "Step 118480: {'train_ae_loss': 0.66345, 'train_ucc_loss': 0.39914, 'train_ucc_acc': 0.90625, 'loss': 0.5313}\n",
            "Step 118500: {'train_ae_loss': 0.67508, 'train_ucc_loss': 0.50704, 'train_ucc_acc': 0.8125, 'loss': 0.59106}\n",
            "Step 118520: {'train_ae_loss': 0.66606, 'train_ucc_loss': 0.44562, 'train_ucc_acc': 0.875, 'loss': 0.55584}\n",
            "Step 118540: {'train_ae_loss': 0.68452, 'train_ucc_loss': 0.35517, 'train_ucc_acc': 0.96875, 'loss': 0.51985}\n",
            "Step 118560: {'train_ae_loss': 0.68062, 'train_ucc_loss': 0.42033, 'train_ucc_acc': 0.90625, 'loss': 0.55047}\n",
            "Step 118580: {'train_ae_loss': 0.66803, 'train_ucc_loss': 0.41282, 'train_ucc_acc': 0.90625, 'loss': 0.54042}\n",
            "Step 118600: {'train_ae_loss': 0.66509, 'train_ucc_loss': 0.3969, 'train_ucc_acc': 0.90625, 'loss': 0.53099}\n",
            "Step 118620: {'train_ae_loss': 0.66289, 'train_ucc_loss': 0.42953, 'train_ucc_acc': 0.90625, 'loss': 0.54621}\n",
            "Step 118640: {'train_ae_loss': 0.67174, 'train_ucc_loss': 0.4506, 'train_ucc_acc': 0.875, 'loss': 0.56117}\n",
            "Step 118660: {'train_ae_loss': 0.66594, 'train_ucc_loss': 0.40027, 'train_ucc_acc': 0.90625, 'loss': 0.5331}\n",
            "Step 118680: {'train_ae_loss': 0.66216, 'train_ucc_loss': 0.42061, 'train_ucc_acc': 0.875, 'loss': 0.54139}\n",
            "Step 118700: {'train_ae_loss': 0.67671, 'train_ucc_loss': 0.49364, 'train_ucc_acc': 0.78125, 'loss': 0.58517}\n",
            "Step 118720: {'train_ae_loss': 0.639, 'train_ucc_loss': 0.41266, 'train_ucc_acc': 0.90625, 'loss': 0.52583}\n",
            "Step 118740: {'train_ae_loss': 0.65627, 'train_ucc_loss': 0.44711, 'train_ucc_acc': 0.875, 'loss': 0.55169}\n",
            "Step 118760: {'train_ae_loss': 0.6704, 'train_ucc_loss': 0.4123, 'train_ucc_acc': 0.875, 'loss': 0.54135}\n",
            "Step 118780: {'train_ae_loss': 0.66483, 'train_ucc_loss': 0.53197, 'train_ucc_acc': 0.75, 'loss': 0.5984}\n",
            "Step 118800: {'train_ae_loss': 0.64231, 'train_ucc_loss': 0.43845, 'train_ucc_acc': 0.875, 'loss': 0.54038}\n",
            "Step 118820: {'train_ae_loss': 0.66264, 'train_ucc_loss': 0.55407, 'train_ucc_acc': 0.75, 'loss': 0.60835}\n",
            "Step 118840: {'train_ae_loss': 0.66121, 'train_ucc_loss': 0.37916, 'train_ucc_acc': 0.9375, 'loss': 0.52018}\n",
            "Step 118860: {'train_ae_loss': 0.65275, 'train_ucc_loss': 0.47655, 'train_ucc_acc': 0.8125, 'loss': 0.56465}\n",
            "Step 118880: {'train_ae_loss': 0.65718, 'train_ucc_loss': 0.53496, 'train_ucc_acc': 0.75, 'loss': 0.59607}\n",
            "Step 118900: {'train_ae_loss': 0.64207, 'train_ucc_loss': 0.4484, 'train_ucc_acc': 0.84375, 'loss': 0.54523}\n",
            "Step 118920: {'train_ae_loss': 0.66335, 'train_ucc_loss': 0.4717, 'train_ucc_acc': 0.8125, 'loss': 0.56753}\n",
            "Step 118940: {'train_ae_loss': 0.67203, 'train_ucc_loss': 0.37483, 'train_ucc_acc': 0.9375, 'loss': 0.52343}\n",
            "Step 118960: {'train_ae_loss': 0.6799, 'train_ucc_loss': 0.4083, 'train_ucc_acc': 0.90625, 'loss': 0.5441}\n",
            "Step 118980: {'train_ae_loss': 0.63917, 'train_ucc_loss': 0.53758, 'train_ucc_acc': 0.78125, 'loss': 0.58838}\n",
            "Step 119000: {'train_ae_loss': 0.6518, 'train_ucc_loss': 0.42382, 'train_ucc_acc': 0.875, 'loss': 0.53781}\n",
            "step: 119000,eval_ae_loss: 0.64417,eval_ucc_loss: 0.49521,eval_ucc_acc: 0.80273\n",
            "Step 119020: {'train_ae_loss': 0.67176, 'train_ucc_loss': 0.4133, 'train_ucc_acc': 0.90625, 'loss': 0.54253}\n",
            "Step 119040: {'train_ae_loss': 0.65774, 'train_ucc_loss': 0.39838, 'train_ucc_acc': 0.90625, 'loss': 0.52806}\n",
            "Step 119060: {'train_ae_loss': 0.66443, 'train_ucc_loss': 0.50463, 'train_ucc_acc': 0.75, 'loss': 0.58453}\n",
            "Step 119080: {'train_ae_loss': 0.66564, 'train_ucc_loss': 0.40918, 'train_ucc_acc': 0.875, 'loss': 0.53741}\n",
            "Step 119100: {'train_ae_loss': 0.67494, 'train_ucc_loss': 0.41002, 'train_ucc_acc': 0.875, 'loss': 0.54248}\n",
            "Step 119120: {'train_ae_loss': 0.6755, 'train_ucc_loss': 0.46264, 'train_ucc_acc': 0.84375, 'loss': 0.56907}\n",
            "Step 119140: {'train_ae_loss': 0.67415, 'train_ucc_loss': 0.40391, 'train_ucc_acc': 0.90625, 'loss': 0.53903}\n",
            "Step 119160: {'train_ae_loss': 0.64362, 'train_ucc_loss': 0.51815, 'train_ucc_acc': 0.78125, 'loss': 0.58088}\n",
            "Step 119180: {'train_ae_loss': 0.68891, 'train_ucc_loss': 0.43163, 'train_ucc_acc': 0.84375, 'loss': 0.56027}\n",
            "Step 119200: {'train_ae_loss': 0.67867, 'train_ucc_loss': 0.42623, 'train_ucc_acc': 0.875, 'loss': 0.55245}\n",
            "Step 119220: {'train_ae_loss': 0.69884, 'train_ucc_loss': 0.44636, 'train_ucc_acc': 0.84375, 'loss': 0.5726}\n",
            "Step 119240: {'train_ae_loss': 0.68498, 'train_ucc_loss': 0.52183, 'train_ucc_acc': 0.78125, 'loss': 0.6034}\n",
            "Step 119260: {'train_ae_loss': 0.66007, 'train_ucc_loss': 0.48081, 'train_ucc_acc': 0.84375, 'loss': 0.57044}\n",
            "Step 119280: {'train_ae_loss': 0.65227, 'train_ucc_loss': 0.42316, 'train_ucc_acc': 0.875, 'loss': 0.53771}\n",
            "Step 119300: {'train_ae_loss': 0.66694, 'train_ucc_loss': 0.42959, 'train_ucc_acc': 0.90625, 'loss': 0.54826}\n",
            "Step 119320: {'train_ae_loss': 0.68012, 'train_ucc_loss': 0.53729, 'train_ucc_acc': 0.75, 'loss': 0.60871}\n",
            "Step 119340: {'train_ae_loss': 0.66356, 'train_ucc_loss': 0.44806, 'train_ucc_acc': 0.875, 'loss': 0.55581}\n",
            "Step 119360: {'train_ae_loss': 0.66784, 'train_ucc_loss': 0.43816, 'train_ucc_acc': 0.875, 'loss': 0.553}\n",
            "Step 119380: {'train_ae_loss': 0.65061, 'train_ucc_loss': 0.45738, 'train_ucc_acc': 0.875, 'loss': 0.55399}\n",
            "Step 119400: {'train_ae_loss': 0.65467, 'train_ucc_loss': 0.46975, 'train_ucc_acc': 0.84375, 'loss': 0.56221}\n",
            "Step 119420: {'train_ae_loss': 0.65544, 'train_ucc_loss': 0.55141, 'train_ucc_acc': 0.75, 'loss': 0.60342}\n",
            "Step 119440: {'train_ae_loss': 0.65718, 'train_ucc_loss': 0.42145, 'train_ucc_acc': 0.90625, 'loss': 0.53932}\n",
            "Step 119460: {'train_ae_loss': 0.67148, 'train_ucc_loss': 0.4424, 'train_ucc_acc': 0.875, 'loss': 0.55694}\n",
            "Step 119480: {'train_ae_loss': 0.67784, 'train_ucc_loss': 0.42353, 'train_ucc_acc': 0.90625, 'loss': 0.55069}\n",
            "Step 119500: {'train_ae_loss': 0.64707, 'train_ucc_loss': 0.55689, 'train_ucc_acc': 0.75, 'loss': 0.60198}\n",
            "Step 119520: {'train_ae_loss': 0.66115, 'train_ucc_loss': 0.38621, 'train_ucc_acc': 0.9375, 'loss': 0.52368}\n",
            "Step 119540: {'train_ae_loss': 0.67086, 'train_ucc_loss': 0.43579, 'train_ucc_acc': 0.875, 'loss': 0.55333}\n",
            "Step 119560: {'train_ae_loss': 0.6791, 'train_ucc_loss': 0.4629, 'train_ucc_acc': 0.84375, 'loss': 0.571}\n",
            "Step 119580: {'train_ae_loss': 0.66344, 'train_ucc_loss': 0.50446, 'train_ucc_acc': 0.8125, 'loss': 0.58395}\n",
            "Step 119600: {'train_ae_loss': 0.66648, 'train_ucc_loss': 0.40788, 'train_ucc_acc': 0.90625, 'loss': 0.53718}\n",
            "Step 119620: {'train_ae_loss': 0.67562, 'train_ucc_loss': 0.36183, 'train_ucc_acc': 0.96875, 'loss': 0.51872}\n",
            "Step 119640: {'train_ae_loss': 0.68141, 'train_ucc_loss': 0.48527, 'train_ucc_acc': 0.78125, 'loss': 0.58334}\n",
            "Step 119660: {'train_ae_loss': 0.6926, 'train_ucc_loss': 0.44723, 'train_ucc_acc': 0.875, 'loss': 0.56992}\n",
            "Step 119680: {'train_ae_loss': 0.6814, 'train_ucc_loss': 0.38546, 'train_ucc_acc': 0.9375, 'loss': 0.53343}\n",
            "Step 119700: {'train_ae_loss': 0.66992, 'train_ucc_loss': 0.43448, 'train_ucc_acc': 0.875, 'loss': 0.5522}\n",
            "Step 119720: {'train_ae_loss': 0.67447, 'train_ucc_loss': 0.3973, 'train_ucc_acc': 0.90625, 'loss': 0.53589}\n",
            "Step 119740: {'train_ae_loss': 0.69921, 'train_ucc_loss': 0.46959, 'train_ucc_acc': 0.78125, 'loss': 0.5844}\n",
            "Step 119760: {'train_ae_loss': 0.66133, 'train_ucc_loss': 0.39955, 'train_ucc_acc': 0.90625, 'loss': 0.53044}\n",
            "Step 119780: {'train_ae_loss': 0.69976, 'train_ucc_loss': 0.39312, 'train_ucc_acc': 0.90625, 'loss': 0.54644}\n",
            "Step 119800: {'train_ae_loss': 0.67747, 'train_ucc_loss': 0.50721, 'train_ucc_acc': 0.8125, 'loss': 0.59234}\n",
            "Step 119820: {'train_ae_loss': 0.67773, 'train_ucc_loss': 0.55723, 'train_ucc_acc': 0.75, 'loss': 0.61748}\n",
            "Step 119840: {'train_ae_loss': 0.67585, 'train_ucc_loss': 0.42709, 'train_ucc_acc': 0.875, 'loss': 0.55147}\n",
            "Step 119860: {'train_ae_loss': 0.66867, 'train_ucc_loss': 0.37955, 'train_ucc_acc': 0.90625, 'loss': 0.52411}\n",
            "Step 119880: {'train_ae_loss': 0.66319, 'train_ucc_loss': 0.5284, 'train_ucc_acc': 0.71875, 'loss': 0.5958}\n",
            "Step 119900: {'train_ae_loss': 0.68347, 'train_ucc_loss': 0.45479, 'train_ucc_acc': 0.84375, 'loss': 0.56913}\n",
            "Step 119920: {'train_ae_loss': 0.68088, 'train_ucc_loss': 0.45501, 'train_ucc_acc': 0.84375, 'loss': 0.56794}\n",
            "Step 119940: {'train_ae_loss': 0.67423, 'train_ucc_loss': 0.48985, 'train_ucc_acc': 0.8125, 'loss': 0.58204}\n",
            "Step 119960: {'train_ae_loss': 0.67963, 'train_ucc_loss': 0.39499, 'train_ucc_acc': 0.90625, 'loss': 0.53731}\n",
            "Step 119980: {'train_ae_loss': 0.67214, 'train_ucc_loss': 0.42164, 'train_ucc_acc': 0.875, 'loss': 0.54689}\n",
            "Step 120000: {'train_ae_loss': 0.66202, 'train_ucc_loss': 0.472, 'train_ucc_acc': 0.84375, 'loss': 0.56701}\n",
            "step: 120000,eval_ae_loss: 0.66846,eval_ucc_loss: 0.46869,eval_ucc_acc: 0.8418\n",
            "Step 120020: {'train_ae_loss': 0.67869, 'train_ucc_loss': 0.43715, 'train_ucc_acc': 0.875, 'loss': 0.55792}\n",
            "Step 120040: {'train_ae_loss': 0.66799, 'train_ucc_loss': 0.41371, 'train_ucc_acc': 0.90625, 'loss': 0.54085}\n",
            "Step 120060: {'train_ae_loss': 0.66605, 'train_ucc_loss': 0.33088, 'train_ucc_acc': 1.0, 'loss': 0.49847}\n",
            "Step 120080: {'train_ae_loss': 0.6729, 'train_ucc_loss': 0.46082, 'train_ucc_acc': 0.84375, 'loss': 0.56686}\n",
            "Step 120100: {'train_ae_loss': 0.68985, 'train_ucc_loss': 0.39612, 'train_ucc_acc': 0.875, 'loss': 0.54298}\n",
            "Step 120120: {'train_ae_loss': 0.66125, 'train_ucc_loss': 0.44303, 'train_ucc_acc': 0.875, 'loss': 0.55214}\n",
            "Step 120140: {'train_ae_loss': 0.66096, 'train_ucc_loss': 0.47184, 'train_ucc_acc': 0.84375, 'loss': 0.5664}\n",
            "Step 120160: {'train_ae_loss': 0.67338, 'train_ucc_loss': 0.4694, 'train_ucc_acc': 0.8125, 'loss': 0.57139}\n",
            "Step 120180: {'train_ae_loss': 0.65096, 'train_ucc_loss': 0.43482, 'train_ucc_acc': 0.84375, 'loss': 0.54289}\n",
            "Step 120200: {'train_ae_loss': 0.67857, 'train_ucc_loss': 0.42581, 'train_ucc_acc': 0.875, 'loss': 0.55219}\n",
            "Step 120220: {'train_ae_loss': 0.66275, 'train_ucc_loss': 0.50967, 'train_ucc_acc': 0.8125, 'loss': 0.58621}\n",
            "Step 120240: {'train_ae_loss': 0.66137, 'train_ucc_loss': 0.45264, 'train_ucc_acc': 0.84375, 'loss': 0.55701}\n",
            "Step 120260: {'train_ae_loss': 0.66388, 'train_ucc_loss': 0.51503, 'train_ucc_acc': 0.8125, 'loss': 0.58946}\n",
            "Step 120280: {'train_ae_loss': 0.64388, 'train_ucc_loss': 0.45596, 'train_ucc_acc': 0.875, 'loss': 0.54992}\n",
            "Step 120300: {'train_ae_loss': 0.64943, 'train_ucc_loss': 0.48238, 'train_ucc_acc': 0.8125, 'loss': 0.5659}\n",
            "Step 120320: {'train_ae_loss': 0.66587, 'train_ucc_loss': 0.39377, 'train_ucc_acc': 0.90625, 'loss': 0.52982}\n",
            "Step 120340: {'train_ae_loss': 0.67248, 'train_ucc_loss': 0.39449, 'train_ucc_acc': 0.90625, 'loss': 0.53348}\n",
            "Step 120360: {'train_ae_loss': 0.66869, 'train_ucc_loss': 0.58491, 'train_ucc_acc': 0.71875, 'loss': 0.6268}\n",
            "Step 120380: {'train_ae_loss': 0.65534, 'train_ucc_loss': 0.43722, 'train_ucc_acc': 0.875, 'loss': 0.54628}\n",
            "Step 120400: {'train_ae_loss': 0.65874, 'train_ucc_loss': 0.4334, 'train_ucc_acc': 0.875, 'loss': 0.54607}\n",
            "Step 120420: {'train_ae_loss': 0.65558, 'train_ucc_loss': 0.46446, 'train_ucc_acc': 0.84375, 'loss': 0.56002}\n",
            "Step 120440: {'train_ae_loss': 0.65767, 'train_ucc_loss': 0.51568, 'train_ucc_acc': 0.78125, 'loss': 0.58667}\n",
            "Step 120460: {'train_ae_loss': 0.64691, 'train_ucc_loss': 0.46, 'train_ucc_acc': 0.875, 'loss': 0.55345}\n",
            "Step 120480: {'train_ae_loss': 0.67171, 'train_ucc_loss': 0.42695, 'train_ucc_acc': 0.84375, 'loss': 0.54933}\n",
            "Step 120500: {'train_ae_loss': 0.67051, 'train_ucc_loss': 0.44163, 'train_ucc_acc': 0.875, 'loss': 0.55607}\n",
            "Step 120520: {'train_ae_loss': 0.66352, 'train_ucc_loss': 0.40929, 'train_ucc_acc': 0.90625, 'loss': 0.53641}\n",
            "Step 120540: {'train_ae_loss': 0.6726, 'train_ucc_loss': 0.36651, 'train_ucc_acc': 0.9375, 'loss': 0.51955}\n",
            "Step 120560: {'train_ae_loss': 0.66048, 'train_ucc_loss': 0.40202, 'train_ucc_acc': 0.90625, 'loss': 0.53125}\n",
            "Step 120580: {'train_ae_loss': 0.63987, 'train_ucc_loss': 0.50164, 'train_ucc_acc': 0.78125, 'loss': 0.57075}\n",
            "Step 120600: {'train_ae_loss': 0.65816, 'train_ucc_loss': 0.41174, 'train_ucc_acc': 0.90625, 'loss': 0.53495}\n",
            "Step 120620: {'train_ae_loss': 0.65466, 'train_ucc_loss': 0.4831, 'train_ucc_acc': 0.84375, 'loss': 0.56888}\n",
            "Step 120640: {'train_ae_loss': 0.65436, 'train_ucc_loss': 0.37407, 'train_ucc_acc': 0.90625, 'loss': 0.51421}\n",
            "Step 120660: {'train_ae_loss': 0.69702, 'train_ucc_loss': 0.32552, 'train_ucc_acc': 1.0, 'loss': 0.51127}\n",
            "Step 120680: {'train_ae_loss': 0.66363, 'train_ucc_loss': 0.44084, 'train_ucc_acc': 0.875, 'loss': 0.55224}\n",
            "Step 120700: {'train_ae_loss': 0.65335, 'train_ucc_loss': 0.44694, 'train_ucc_acc': 0.875, 'loss': 0.55014}\n",
            "Step 120720: {'train_ae_loss': 0.67049, 'train_ucc_loss': 0.44828, 'train_ucc_acc': 0.875, 'loss': 0.55938}\n",
            "Step 120740: {'train_ae_loss': 0.65095, 'train_ucc_loss': 0.45602, 'train_ucc_acc': 0.84375, 'loss': 0.55348}\n",
            "Step 120760: {'train_ae_loss': 0.65648, 'train_ucc_loss': 0.36528, 'train_ucc_acc': 0.9375, 'loss': 0.51088}\n",
            "Step 120780: {'train_ae_loss': 0.65955, 'train_ucc_loss': 0.37734, 'train_ucc_acc': 0.9375, 'loss': 0.51844}\n",
            "Step 120800: {'train_ae_loss': 0.6505, 'train_ucc_loss': 0.50693, 'train_ucc_acc': 0.8125, 'loss': 0.57872}\n",
            "Step 120820: {'train_ae_loss': 0.66238, 'train_ucc_loss': 0.35385, 'train_ucc_acc': 0.96875, 'loss': 0.50812}\n",
            "Step 120840: {'train_ae_loss': 0.65642, 'train_ucc_loss': 0.35395, 'train_ucc_acc': 0.96875, 'loss': 0.50519}\n",
            "Step 120860: {'train_ae_loss': 0.6666, 'train_ucc_loss': 0.48637, 'train_ucc_acc': 0.8125, 'loss': 0.57648}\n",
            "Step 120880: {'train_ae_loss': 0.66691, 'train_ucc_loss': 0.48067, 'train_ucc_acc': 0.84375, 'loss': 0.57379}\n",
            "Step 120900: {'train_ae_loss': 0.68053, 'train_ucc_loss': 0.42812, 'train_ucc_acc': 0.90625, 'loss': 0.55432}\n",
            "Step 120920: {'train_ae_loss': 0.67146, 'train_ucc_loss': 0.46615, 'train_ucc_acc': 0.8125, 'loss': 0.5688}\n",
            "Step 120940: {'train_ae_loss': 0.64859, 'train_ucc_loss': 0.45072, 'train_ucc_acc': 0.875, 'loss': 0.54965}\n",
            "Step 120960: {'train_ae_loss': 0.67341, 'train_ucc_loss': 0.52938, 'train_ucc_acc': 0.75, 'loss': 0.60139}\n",
            "Step 120980: {'train_ae_loss': 0.66813, 'train_ucc_loss': 0.51427, 'train_ucc_acc': 0.75, 'loss': 0.5912}\n",
            "Step 121000: {'train_ae_loss': 0.67775, 'train_ucc_loss': 0.42993, 'train_ucc_acc': 0.875, 'loss': 0.55384}\n",
            "step: 121000,eval_ae_loss: 0.65973,eval_ucc_loss: 0.47737,eval_ucc_acc: 0.8252\n",
            "Step 121020: {'train_ae_loss': 0.66305, 'train_ucc_loss': 0.4415, 'train_ucc_acc': 0.875, 'loss': 0.55227}\n",
            "Step 121040: {'train_ae_loss': 0.67929, 'train_ucc_loss': 0.43317, 'train_ucc_acc': 0.875, 'loss': 0.55623}\n",
            "Step 121060: {'train_ae_loss': 0.67945, 'train_ucc_loss': 0.43702, 'train_ucc_acc': 0.875, 'loss': 0.55823}\n",
            "Step 121080: {'train_ae_loss': 0.6724, 'train_ucc_loss': 0.42747, 'train_ucc_acc': 0.875, 'loss': 0.54993}\n",
            "Step 121100: {'train_ae_loss': 0.68414, 'train_ucc_loss': 0.50303, 'train_ucc_acc': 0.78125, 'loss': 0.59359}\n",
            "Step 121120: {'train_ae_loss': 0.67977, 'train_ucc_loss': 0.40059, 'train_ucc_acc': 0.90625, 'loss': 0.54018}\n",
            "Step 121140: {'train_ae_loss': 0.66979, 'train_ucc_loss': 0.43966, 'train_ucc_acc': 0.875, 'loss': 0.55473}\n",
            "Step 121160: {'train_ae_loss': 0.67733, 'train_ucc_loss': 0.4663, 'train_ucc_acc': 0.8125, 'loss': 0.57182}\n",
            "Step 121180: {'train_ae_loss': 0.67033, 'train_ucc_loss': 0.51329, 'train_ucc_acc': 0.78125, 'loss': 0.59181}\n",
            "Step 121200: {'train_ae_loss': 0.65899, 'train_ucc_loss': 0.3733, 'train_ucc_acc': 0.9375, 'loss': 0.51614}\n",
            "Step 121220: {'train_ae_loss': 0.68327, 'train_ucc_loss': 0.4562, 'train_ucc_acc': 0.84375, 'loss': 0.56973}\n",
            "Step 121240: {'train_ae_loss': 0.68863, 'train_ucc_loss': 0.37389, 'train_ucc_acc': 0.96875, 'loss': 0.53126}\n",
            "Step 121260: {'train_ae_loss': 0.65456, 'train_ucc_loss': 0.40317, 'train_ucc_acc': 0.90625, 'loss': 0.52887}\n",
            "Step 121280: {'train_ae_loss': 0.66535, 'train_ucc_loss': 0.40796, 'train_ucc_acc': 0.90625, 'loss': 0.53666}\n",
            "Step 121300: {'train_ae_loss': 0.68037, 'train_ucc_loss': 0.40801, 'train_ucc_acc': 0.875, 'loss': 0.54419}\n",
            "Step 121320: {'train_ae_loss': 0.66173, 'train_ucc_loss': 0.46025, 'train_ucc_acc': 0.84375, 'loss': 0.56099}\n",
            "Step 121340: {'train_ae_loss': 0.67832, 'train_ucc_loss': 0.42262, 'train_ucc_acc': 0.875, 'loss': 0.55047}\n",
            "Step 121360: {'train_ae_loss': 0.64788, 'train_ucc_loss': 0.45842, 'train_ucc_acc': 0.84375, 'loss': 0.55315}\n",
            "Step 121380: {'train_ae_loss': 0.64975, 'train_ucc_loss': 0.45621, 'train_ucc_acc': 0.78125, 'loss': 0.55298}\n",
            "Step 121400: {'train_ae_loss': 0.66774, 'train_ucc_loss': 0.49325, 'train_ucc_acc': 0.84375, 'loss': 0.5805}\n",
            "Step 121420: {'train_ae_loss': 0.65719, 'train_ucc_loss': 0.38498, 'train_ucc_acc': 0.90625, 'loss': 0.52109}\n",
            "Step 121440: {'train_ae_loss': 0.65921, 'train_ucc_loss': 0.49293, 'train_ucc_acc': 0.8125, 'loss': 0.57607}\n",
            "Step 121460: {'train_ae_loss': 0.66537, 'train_ucc_loss': 0.50668, 'train_ucc_acc': 0.8125, 'loss': 0.58603}\n",
            "Step 121480: {'train_ae_loss': 0.66116, 'train_ucc_loss': 0.39355, 'train_ucc_acc': 0.875, 'loss': 0.52735}\n",
            "Step 121500: {'train_ae_loss': 0.65877, 'train_ucc_loss': 0.50653, 'train_ucc_acc': 0.78125, 'loss': 0.58265}\n",
            "Step 121520: {'train_ae_loss': 0.6475, 'train_ucc_loss': 0.49703, 'train_ucc_acc': 0.8125, 'loss': 0.57227}\n",
            "Step 121540: {'train_ae_loss': 0.64099, 'train_ucc_loss': 0.51739, 'train_ucc_acc': 0.78125, 'loss': 0.57919}\n",
            "Step 121560: {'train_ae_loss': 0.65481, 'train_ucc_loss': 0.45027, 'train_ucc_acc': 0.84375, 'loss': 0.55254}\n",
            "Step 121580: {'train_ae_loss': 0.66759, 'train_ucc_loss': 0.57121, 'train_ucc_acc': 0.71875, 'loss': 0.6194}\n",
            "Step 121600: {'train_ae_loss': 0.65683, 'train_ucc_loss': 0.48417, 'train_ucc_acc': 0.8125, 'loss': 0.5705}\n",
            "Step 121620: {'train_ae_loss': 0.65186, 'train_ucc_loss': 0.44148, 'train_ucc_acc': 0.84375, 'loss': 0.54667}\n",
            "Step 121640: {'train_ae_loss': 0.65779, 'train_ucc_loss': 0.51786, 'train_ucc_acc': 0.8125, 'loss': 0.58783}\n",
            "Step 121660: {'train_ae_loss': 0.66808, 'train_ucc_loss': 0.46016, 'train_ucc_acc': 0.84375, 'loss': 0.56412}\n",
            "Step 121680: {'train_ae_loss': 0.66617, 'train_ucc_loss': 0.46968, 'train_ucc_acc': 0.84375, 'loss': 0.56793}\n",
            "Step 121700: {'train_ae_loss': 0.66631, 'train_ucc_loss': 0.49743, 'train_ucc_acc': 0.78125, 'loss': 0.58187}\n",
            "Step 121720: {'train_ae_loss': 0.67938, 'train_ucc_loss': 0.47889, 'train_ucc_acc': 0.78125, 'loss': 0.57914}\n",
            "Step 121740: {'train_ae_loss': 0.65907, 'train_ucc_loss': 0.46022, 'train_ucc_acc': 0.84375, 'loss': 0.55964}\n",
            "Step 121760: {'train_ae_loss': 0.64752, 'train_ucc_loss': 0.44143, 'train_ucc_acc': 0.875, 'loss': 0.54447}\n",
            "Step 121780: {'train_ae_loss': 0.66328, 'train_ucc_loss': 0.39559, 'train_ucc_acc': 0.90625, 'loss': 0.52944}\n",
            "Step 121800: {'train_ae_loss': 0.65549, 'train_ucc_loss': 0.45481, 'train_ucc_acc': 0.84375, 'loss': 0.55515}\n",
            "Step 121820: {'train_ae_loss': 0.65709, 'train_ucc_loss': 0.39011, 'train_ucc_acc': 0.9375, 'loss': 0.5236}\n",
            "Step 121840: {'train_ae_loss': 0.64941, 'train_ucc_loss': 0.35864, 'train_ucc_acc': 0.96875, 'loss': 0.50402}\n",
            "Step 121860: {'train_ae_loss': 0.64749, 'train_ucc_loss': 0.42217, 'train_ucc_acc': 0.90625, 'loss': 0.53483}\n",
            "Step 121880: {'train_ae_loss': 0.65964, 'train_ucc_loss': 0.44221, 'train_ucc_acc': 0.875, 'loss': 0.55093}\n",
            "Step 121900: {'train_ae_loss': 0.66665, 'train_ucc_loss': 0.41863, 'train_ucc_acc': 0.90625, 'loss': 0.54264}\n",
            "Step 121920: {'train_ae_loss': 0.66342, 'train_ucc_loss': 0.34712, 'train_ucc_acc': 0.96875, 'loss': 0.50527}\n",
            "Step 121940: {'train_ae_loss': 0.6665, 'train_ucc_loss': 0.47657, 'train_ucc_acc': 0.84375, 'loss': 0.57153}\n",
            "Step 121960: {'train_ae_loss': 0.65062, 'train_ucc_loss': 0.47409, 'train_ucc_acc': 0.84375, 'loss': 0.56235}\n",
            "Step 121980: {'train_ae_loss': 0.66475, 'train_ucc_loss': 0.43832, 'train_ucc_acc': 0.875, 'loss': 0.55153}\n",
            "Step 122000: {'train_ae_loss': 0.65676, 'train_ucc_loss': 0.48287, 'train_ucc_acc': 0.8125, 'loss': 0.56981}\n",
            "step: 122000,eval_ae_loss: 0.65136,eval_ucc_loss: 0.47571,eval_ucc_acc: 0.82617\n",
            "Step 122020: {'train_ae_loss': 0.66243, 'train_ucc_loss': 0.40723, 'train_ucc_acc': 0.90625, 'loss': 0.53483}\n",
            "Step 122040: {'train_ae_loss': 0.6574, 'train_ucc_loss': 0.44246, 'train_ucc_acc': 0.875, 'loss': 0.54993}\n",
            "Step 122060: {'train_ae_loss': 0.6454, 'train_ucc_loss': 0.47465, 'train_ucc_acc': 0.84375, 'loss': 0.56002}\n",
            "Step 122080: {'train_ae_loss': 0.65568, 'train_ucc_loss': 0.47513, 'train_ucc_acc': 0.8125, 'loss': 0.5654}\n",
            "Step 122100: {'train_ae_loss': 0.65189, 'train_ucc_loss': 0.47347, 'train_ucc_acc': 0.84375, 'loss': 0.56268}\n",
            "Step 122120: {'train_ae_loss': 0.66358, 'train_ucc_loss': 0.56574, 'train_ucc_acc': 0.6875, 'loss': 0.61466}\n",
            "Step 122140: {'train_ae_loss': 0.67758, 'train_ucc_loss': 0.38654, 'train_ucc_acc': 0.9375, 'loss': 0.53206}\n",
            "Step 122160: {'train_ae_loss': 0.6696, 'train_ucc_loss': 0.35067, 'train_ucc_acc': 0.96875, 'loss': 0.51014}\n",
            "Step 122180: {'train_ae_loss': 0.65355, 'train_ucc_loss': 0.45287, 'train_ucc_acc': 0.84375, 'loss': 0.55321}\n",
            "Step 122200: {'train_ae_loss': 0.65129, 'train_ucc_loss': 0.47458, 'train_ucc_acc': 0.84375, 'loss': 0.56293}\n",
            "Step 122220: {'train_ae_loss': 0.66364, 'train_ucc_loss': 0.44097, 'train_ucc_acc': 0.875, 'loss': 0.5523}\n",
            "Step 122240: {'train_ae_loss': 0.66064, 'train_ucc_loss': 0.36126, 'train_ucc_acc': 0.9375, 'loss': 0.51095}\n",
            "Step 122260: {'train_ae_loss': 0.6659, 'train_ucc_loss': 0.43585, 'train_ucc_acc': 0.875, 'loss': 0.55087}\n",
            "Step 122280: {'train_ae_loss': 0.65819, 'train_ucc_loss': 0.45424, 'train_ucc_acc': 0.875, 'loss': 0.55622}\n",
            "Step 122300: {'train_ae_loss': 0.66985, 'train_ucc_loss': 0.40987, 'train_ucc_acc': 0.875, 'loss': 0.53986}\n",
            "Step 122320: {'train_ae_loss': 0.68301, 'train_ucc_loss': 0.40086, 'train_ucc_acc': 0.9375, 'loss': 0.54193}\n",
            "Step 122340: {'train_ae_loss': 0.66082, 'train_ucc_loss': 0.41934, 'train_ucc_acc': 0.90625, 'loss': 0.54008}\n",
            "Step 122360: {'train_ae_loss': 0.6795, 'train_ucc_loss': 0.44594, 'train_ucc_acc': 0.875, 'loss': 0.56272}\n",
            "Step 122380: {'train_ae_loss': 0.66724, 'train_ucc_loss': 0.50576, 'train_ucc_acc': 0.8125, 'loss': 0.5865}\n",
            "Step 122400: {'train_ae_loss': 0.6653, 'train_ucc_loss': 0.42885, 'train_ucc_acc': 0.90625, 'loss': 0.54707}\n",
            "Step 122420: {'train_ae_loss': 0.65405, 'train_ucc_loss': 0.39449, 'train_ucc_acc': 0.90625, 'loss': 0.52427}\n",
            "Step 122440: {'train_ae_loss': 0.66707, 'train_ucc_loss': 0.47553, 'train_ucc_acc': 0.78125, 'loss': 0.5713}\n",
            "Step 122460: {'train_ae_loss': 0.65277, 'train_ucc_loss': 0.44719, 'train_ucc_acc': 0.84375, 'loss': 0.54998}\n",
            "Step 122480: {'train_ae_loss': 0.66667, 'train_ucc_loss': 0.42523, 'train_ucc_acc': 0.90625, 'loss': 0.54595}\n",
            "Step 122500: {'train_ae_loss': 0.67634, 'train_ucc_loss': 0.43684, 'train_ucc_acc': 0.875, 'loss': 0.55659}\n",
            "Step 122520: {'train_ae_loss': 0.67485, 'train_ucc_loss': 0.43311, 'train_ucc_acc': 0.875, 'loss': 0.55398}\n",
            "Step 122540: {'train_ae_loss': 0.66882, 'train_ucc_loss': 0.47151, 'train_ucc_acc': 0.8125, 'loss': 0.57017}\n",
            "Step 122560: {'train_ae_loss': 0.65822, 'train_ucc_loss': 0.49834, 'train_ucc_acc': 0.8125, 'loss': 0.57828}\n",
            "Step 122580: {'train_ae_loss': 0.67008, 'train_ucc_loss': 0.46547, 'train_ucc_acc': 0.8125, 'loss': 0.56778}\n",
            "Step 122600: {'train_ae_loss': 0.66139, 'train_ucc_loss': 0.54789, 'train_ucc_acc': 0.78125, 'loss': 0.60464}\n",
            "Step 122620: {'train_ae_loss': 0.64742, 'train_ucc_loss': 0.41081, 'train_ucc_acc': 0.90625, 'loss': 0.52912}\n",
            "Step 122640: {'train_ae_loss': 0.65788, 'train_ucc_loss': 0.51769, 'train_ucc_acc': 0.78125, 'loss': 0.58778}\n",
            "Step 122660: {'train_ae_loss': 0.66362, 'train_ucc_loss': 0.41599, 'train_ucc_acc': 0.875, 'loss': 0.5398}\n",
            "Step 122680: {'train_ae_loss': 0.66311, 'train_ucc_loss': 0.49735, 'train_ucc_acc': 0.8125, 'loss': 0.58023}\n",
            "Step 122700: {'train_ae_loss': 0.67131, 'train_ucc_loss': 0.46952, 'train_ucc_acc': 0.84375, 'loss': 0.57041}\n",
            "Step 122720: {'train_ae_loss': 0.65494, 'train_ucc_loss': 0.47378, 'train_ucc_acc': 0.84375, 'loss': 0.56436}\n",
            "Step 122740: {'train_ae_loss': 0.68017, 'train_ucc_loss': 0.41347, 'train_ucc_acc': 0.9375, 'loss': 0.54682}\n",
            "Step 122760: {'train_ae_loss': 0.66128, 'train_ucc_loss': 0.45583, 'train_ucc_acc': 0.84375, 'loss': 0.55855}\n",
            "Step 122780: {'train_ae_loss': 0.65952, 'train_ucc_loss': 0.47792, 'train_ucc_acc': 0.8125, 'loss': 0.56872}\n",
            "Step 122800: {'train_ae_loss': 0.65081, 'train_ucc_loss': 0.4785, 'train_ucc_acc': 0.84375, 'loss': 0.56465}\n",
            "Step 122820: {'train_ae_loss': 0.66868, 'train_ucc_loss': 0.43013, 'train_ucc_acc': 0.875, 'loss': 0.5494}\n",
            "Step 122840: {'train_ae_loss': 0.64018, 'train_ucc_loss': 0.40811, 'train_ucc_acc': 0.9375, 'loss': 0.52415}\n",
            "Step 122860: {'train_ae_loss': 0.65761, 'train_ucc_loss': 0.42474, 'train_ucc_acc': 0.875, 'loss': 0.54117}\n",
            "Step 122880: {'train_ae_loss': 0.65061, 'train_ucc_loss': 0.43827, 'train_ucc_acc': 0.875, 'loss': 0.54444}\n",
            "Step 122900: {'train_ae_loss': 0.66865, 'train_ucc_loss': 0.38033, 'train_ucc_acc': 0.9375, 'loss': 0.52449}\n",
            "Step 122920: {'train_ae_loss': 0.65942, 'train_ucc_loss': 0.39036, 'train_ucc_acc': 0.9375, 'loss': 0.52489}\n",
            "Step 122940: {'train_ae_loss': 0.66028, 'train_ucc_loss': 0.36917, 'train_ucc_acc': 0.9375, 'loss': 0.51473}\n",
            "Step 122960: {'train_ae_loss': 0.6617, 'train_ucc_loss': 0.452, 'train_ucc_acc': 0.875, 'loss': 0.55685}\n",
            "Step 122980: {'train_ae_loss': 0.66088, 'train_ucc_loss': 0.38916, 'train_ucc_acc': 0.90625, 'loss': 0.52502}\n",
            "Step 123000: {'train_ae_loss': 0.65802, 'train_ucc_loss': 0.36235, 'train_ucc_acc': 0.96875, 'loss': 0.51018}\n",
            "step: 123000,eval_ae_loss: 0.64808,eval_ucc_loss: 0.48165,eval_ucc_acc: 0.8252\n",
            "Step 123020: {'train_ae_loss': 0.67467, 'train_ucc_loss': 0.32583, 'train_ucc_acc': 1.0, 'loss': 0.50025}\n",
            "Step 123040: {'train_ae_loss': 0.65069, 'train_ucc_loss': 0.59435, 'train_ucc_acc': 0.6875, 'loss': 0.62252}\n",
            "Step 123060: {'train_ae_loss': 0.65911, 'train_ucc_loss': 0.43626, 'train_ucc_acc': 0.875, 'loss': 0.54768}\n",
            "Step 123080: {'train_ae_loss': 0.6679, 'train_ucc_loss': 0.45481, 'train_ucc_acc': 0.875, 'loss': 0.56136}\n",
            "Step 123100: {'train_ae_loss': 0.68012, 'train_ucc_loss': 0.45502, 'train_ucc_acc': 0.84375, 'loss': 0.56757}\n",
            "Step 123120: {'train_ae_loss': 0.65442, 'train_ucc_loss': 0.49553, 'train_ucc_acc': 0.8125, 'loss': 0.57497}\n",
            "Step 123140: {'train_ae_loss': 0.67557, 'train_ucc_loss': 0.45751, 'train_ucc_acc': 0.84375, 'loss': 0.56654}\n",
            "Step 123160: {'train_ae_loss': 0.65134, 'train_ucc_loss': 0.44306, 'train_ucc_acc': 0.875, 'loss': 0.5472}\n",
            "Step 123180: {'train_ae_loss': 0.63104, 'train_ucc_loss': 0.47229, 'train_ucc_acc': 0.84375, 'loss': 0.55166}\n",
            "Step 123200: {'train_ae_loss': 0.63556, 'train_ucc_loss': 0.39509, 'train_ucc_acc': 0.9375, 'loss': 0.51532}\n",
            "Step 123220: {'train_ae_loss': 0.65908, 'train_ucc_loss': 0.46455, 'train_ucc_acc': 0.84375, 'loss': 0.56181}\n",
            "Step 123240: {'train_ae_loss': 0.64225, 'train_ucc_loss': 0.4804, 'train_ucc_acc': 0.8125, 'loss': 0.56132}\n",
            "Step 123260: {'train_ae_loss': 0.65656, 'train_ucc_loss': 0.47274, 'train_ucc_acc': 0.84375, 'loss': 0.56465}\n",
            "Step 123280: {'train_ae_loss': 0.65197, 'train_ucc_loss': 0.47592, 'train_ucc_acc': 0.84375, 'loss': 0.56394}\n",
            "Step 123300: {'train_ae_loss': 0.67849, 'train_ucc_loss': 0.39918, 'train_ucc_acc': 0.90625, 'loss': 0.53883}\n",
            "Step 123320: {'train_ae_loss': 0.65772, 'train_ucc_loss': 0.54181, 'train_ucc_acc': 0.75, 'loss': 0.59977}\n",
            "Step 123340: {'train_ae_loss': 0.65682, 'train_ucc_loss': 0.48126, 'train_ucc_acc': 0.8125, 'loss': 0.56904}\n",
            "Step 123360: {'train_ae_loss': 0.67209, 'train_ucc_loss': 0.43722, 'train_ucc_acc': 0.875, 'loss': 0.55465}\n",
            "Step 123380: {'train_ae_loss': 0.65984, 'train_ucc_loss': 0.4594, 'train_ucc_acc': 0.8125, 'loss': 0.55962}\n",
            "Step 123400: {'train_ae_loss': 0.67441, 'train_ucc_loss': 0.39516, 'train_ucc_acc': 0.9375, 'loss': 0.53479}\n",
            "Step 123420: {'train_ae_loss': 0.66409, 'train_ucc_loss': 0.41703, 'train_ucc_acc': 0.875, 'loss': 0.54056}\n",
            "Step 123440: {'train_ae_loss': 0.67582, 'train_ucc_loss': 0.41282, 'train_ucc_acc': 0.90625, 'loss': 0.54432}\n",
            "Step 123460: {'train_ae_loss': 0.64583, 'train_ucc_loss': 0.44364, 'train_ucc_acc': 0.84375, 'loss': 0.54473}\n",
            "Step 123480: {'train_ae_loss': 0.66377, 'train_ucc_loss': 0.52935, 'train_ucc_acc': 0.75, 'loss': 0.59656}\n",
            "Step 123500: {'train_ae_loss': 0.64519, 'train_ucc_loss': 0.34734, 'train_ucc_acc': 0.96875, 'loss': 0.49627}\n",
            "Step 123520: {'train_ae_loss': 0.65202, 'train_ucc_loss': 0.36293, 'train_ucc_acc': 0.96875, 'loss': 0.50747}\n",
            "Step 123540: {'train_ae_loss': 0.66274, 'train_ucc_loss': 0.42318, 'train_ucc_acc': 0.875, 'loss': 0.54296}\n",
            "Step 123560: {'train_ae_loss': 0.65783, 'train_ucc_loss': 0.46124, 'train_ucc_acc': 0.84375, 'loss': 0.55953}\n",
            "Step 123580: {'train_ae_loss': 0.6492, 'train_ucc_loss': 0.41774, 'train_ucc_acc': 0.90625, 'loss': 0.53347}\n",
            "Step 123600: {'train_ae_loss': 0.66065, 'train_ucc_loss': 0.44694, 'train_ucc_acc': 0.84375, 'loss': 0.5538}\n",
            "Step 123620: {'train_ae_loss': 0.65994, 'train_ucc_loss': 0.42245, 'train_ucc_acc': 0.90625, 'loss': 0.54119}\n",
            "Step 123640: {'train_ae_loss': 0.66652, 'train_ucc_loss': 0.42646, 'train_ucc_acc': 0.875, 'loss': 0.54649}\n",
            "Step 123660: {'train_ae_loss': 0.66054, 'train_ucc_loss': 0.41755, 'train_ucc_acc': 0.875, 'loss': 0.53904}\n",
            "Step 123680: {'train_ae_loss': 0.64574, 'train_ucc_loss': 0.47308, 'train_ucc_acc': 0.8125, 'loss': 0.55941}\n",
            "Step 123700: {'train_ae_loss': 0.64502, 'train_ucc_loss': 0.44397, 'train_ucc_acc': 0.875, 'loss': 0.5445}\n",
            "Step 123720: {'train_ae_loss': 0.67971, 'train_ucc_loss': 0.39707, 'train_ucc_acc': 0.90625, 'loss': 0.53839}\n",
            "Step 123740: {'train_ae_loss': 0.66651, 'train_ucc_loss': 0.3892, 'train_ucc_acc': 0.90625, 'loss': 0.52785}\n",
            "Step 123760: {'train_ae_loss': 0.66226, 'train_ucc_loss': 0.47832, 'train_ucc_acc': 0.78125, 'loss': 0.57029}\n",
            "Step 123780: {'train_ae_loss': 0.66461, 'train_ucc_loss': 0.38628, 'train_ucc_acc': 0.9375, 'loss': 0.52544}\n",
            "Step 123800: {'train_ae_loss': 0.65656, 'train_ucc_loss': 0.51125, 'train_ucc_acc': 0.78125, 'loss': 0.58391}\n",
            "Step 123820: {'train_ae_loss': 0.67701, 'train_ucc_loss': 0.50019, 'train_ucc_acc': 0.75, 'loss': 0.5886}\n",
            "Step 123840: {'train_ae_loss': 0.68115, 'train_ucc_loss': 0.39569, 'train_ucc_acc': 0.9375, 'loss': 0.53842}\n",
            "Step 123860: {'train_ae_loss': 0.65619, 'train_ucc_loss': 0.41964, 'train_ucc_acc': 0.90625, 'loss': 0.53792}\n",
            "Step 123880: {'train_ae_loss': 0.6442, 'train_ucc_loss': 0.41494, 'train_ucc_acc': 0.90625, 'loss': 0.52957}\n",
            "Step 123900: {'train_ae_loss': 0.65776, 'train_ucc_loss': 0.4663, 'train_ucc_acc': 0.84375, 'loss': 0.56203}\n",
            "Step 123920: {'train_ae_loss': 0.64727, 'train_ucc_loss': 0.43398, 'train_ucc_acc': 0.875, 'loss': 0.54062}\n",
            "Step 123940: {'train_ae_loss': 0.64836, 'train_ucc_loss': 0.44783, 'train_ucc_acc': 0.875, 'loss': 0.5481}\n",
            "Step 123960: {'train_ae_loss': 0.67761, 'train_ucc_loss': 0.35077, 'train_ucc_acc': 0.96875, 'loss': 0.51419}\n",
            "Step 123980: {'train_ae_loss': 0.67676, 'train_ucc_loss': 0.56377, 'train_ucc_acc': 0.71875, 'loss': 0.62027}\n",
            "Step 124000: {'train_ae_loss': 0.65839, 'train_ucc_loss': 0.45823, 'train_ucc_acc': 0.875, 'loss': 0.55831}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/08 01:28:14 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 124000,eval_ae_loss: 0.65004,eval_ucc_loss: 0.442,eval_ucc_acc: 0.86719\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/09/08 01:28:20 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 124020: {'train_ae_loss': 0.65593, 'train_ucc_loss': 0.4889, 'train_ucc_acc': 0.8125, 'loss': 0.57242}\n",
            "Step 124040: {'train_ae_loss': 0.65736, 'train_ucc_loss': 0.40068, 'train_ucc_acc': 0.90625, 'loss': 0.52902}\n",
            "Step 124060: {'train_ae_loss': 0.65386, 'train_ucc_loss': 0.46182, 'train_ucc_acc': 0.84375, 'loss': 0.55784}\n",
            "Step 124080: {'train_ae_loss': 0.66573, 'train_ucc_loss': 0.42561, 'train_ucc_acc': 0.90625, 'loss': 0.54567}\n",
            "Step 124100: {'train_ae_loss': 0.65924, 'train_ucc_loss': 0.42172, 'train_ucc_acc': 0.90625, 'loss': 0.54048}\n",
            "Step 124120: {'train_ae_loss': 0.66568, 'train_ucc_loss': 0.48173, 'train_ucc_acc': 0.84375, 'loss': 0.57371}\n",
            "Step 124140: {'train_ae_loss': 0.64711, 'train_ucc_loss': 0.47957, 'train_ucc_acc': 0.84375, 'loss': 0.56334}\n",
            "Step 124160: {'train_ae_loss': 0.6753, 'train_ucc_loss': 0.45368, 'train_ucc_acc': 0.875, 'loss': 0.56449}\n",
            "Step 124180: {'train_ae_loss': 0.64438, 'train_ucc_loss': 0.4428, 'train_ucc_acc': 0.84375, 'loss': 0.54359}\n",
            "Step 124200: {'train_ae_loss': 0.6552, 'train_ucc_loss': 0.51361, 'train_ucc_acc': 0.78125, 'loss': 0.58441}\n",
            "Step 124220: {'train_ae_loss': 0.65891, 'train_ucc_loss': 0.38279, 'train_ucc_acc': 0.9375, 'loss': 0.52085}\n",
            "Step 124240: {'train_ae_loss': 0.66034, 'train_ucc_loss': 0.38158, 'train_ucc_acc': 0.9375, 'loss': 0.52096}\n",
            "Step 124260: {'train_ae_loss': 0.66574, 'train_ucc_loss': 0.48687, 'train_ucc_acc': 0.8125, 'loss': 0.57631}\n",
            "Step 124280: {'train_ae_loss': 0.66417, 'train_ucc_loss': 0.46945, 'train_ucc_acc': 0.84375, 'loss': 0.56681}\n",
            "Step 124300: {'train_ae_loss': 0.66766, 'train_ucc_loss': 0.56001, 'train_ucc_acc': 0.75, 'loss': 0.61384}\n",
            "Step 124320: {'train_ae_loss': 0.67556, 'train_ucc_loss': 0.36272, 'train_ucc_acc': 0.96875, 'loss': 0.51914}\n",
            "Step 124340: {'train_ae_loss': 0.65837, 'train_ucc_loss': 0.40035, 'train_ucc_acc': 0.90625, 'loss': 0.52936}\n",
            "Step 124360: {'train_ae_loss': 0.6478, 'train_ucc_loss': 0.38996, 'train_ucc_acc': 0.9375, 'loss': 0.51888}\n",
            "Step 124380: {'train_ae_loss': 0.66598, 'train_ucc_loss': 0.37758, 'train_ucc_acc': 0.9375, 'loss': 0.52178}\n",
            "Step 124400: {'train_ae_loss': 0.67097, 'train_ucc_loss': 0.3974, 'train_ucc_acc': 0.90625, 'loss': 0.53419}\n",
            "Step 124420: {'train_ae_loss': 0.66606, 'train_ucc_loss': 0.47248, 'train_ucc_acc': 0.84375, 'loss': 0.56927}\n",
            "Step 124440: {'train_ae_loss': 0.652, 'train_ucc_loss': 0.43283, 'train_ucc_acc': 0.84375, 'loss': 0.54241}\n",
            "Step 124460: {'train_ae_loss': 0.6675, 'train_ucc_loss': 0.40566, 'train_ucc_acc': 0.90625, 'loss': 0.53658}\n",
            "Step 124480: {'train_ae_loss': 0.66952, 'train_ucc_loss': 0.40376, 'train_ucc_acc': 0.90625, 'loss': 0.53664}\n",
            "Step 124500: {'train_ae_loss': 0.67106, 'train_ucc_loss': 0.45317, 'train_ucc_acc': 0.84375, 'loss': 0.56212}\n",
            "Step 124520: {'train_ae_loss': 0.63476, 'train_ucc_loss': 0.50038, 'train_ucc_acc': 0.8125, 'loss': 0.56757}\n",
            "Step 124540: {'train_ae_loss': 0.66805, 'train_ucc_loss': 0.4264, 'train_ucc_acc': 0.90625, 'loss': 0.54722}\n",
            "Step 124560: {'train_ae_loss': 0.66713, 'train_ucc_loss': 0.42312, 'train_ucc_acc': 0.90625, 'loss': 0.54512}\n",
            "Step 124580: {'train_ae_loss': 0.66415, 'train_ucc_loss': 0.43725, 'train_ucc_acc': 0.875, 'loss': 0.5507}\n",
            "Step 124600: {'train_ae_loss': 0.64683, 'train_ucc_loss': 0.43251, 'train_ucc_acc': 0.90625, 'loss': 0.53967}\n",
            "Step 124620: {'train_ae_loss': 0.64283, 'train_ucc_loss': 0.43393, 'train_ucc_acc': 0.84375, 'loss': 0.53838}\n",
            "Step 124640: {'train_ae_loss': 0.66763, 'train_ucc_loss': 0.393, 'train_ucc_acc': 0.90625, 'loss': 0.53032}\n",
            "Step 124660: {'train_ae_loss': 0.65126, 'train_ucc_loss': 0.45729, 'train_ucc_acc': 0.84375, 'loss': 0.55427}\n",
            "Step 124680: {'train_ae_loss': 0.6698, 'train_ucc_loss': 0.4165, 'train_ucc_acc': 0.90625, 'loss': 0.54315}\n",
            "Step 124700: {'train_ae_loss': 0.66992, 'train_ucc_loss': 0.48489, 'train_ucc_acc': 0.8125, 'loss': 0.57741}\n",
            "Step 124720: {'train_ae_loss': 0.65844, 'train_ucc_loss': 0.56571, 'train_ucc_acc': 0.71875, 'loss': 0.61207}\n",
            "Step 124740: {'train_ae_loss': 0.66323, 'train_ucc_loss': 0.46614, 'train_ucc_acc': 0.8125, 'loss': 0.56468}\n",
            "Step 124760: {'train_ae_loss': 0.65335, 'train_ucc_loss': 0.44992, 'train_ucc_acc': 0.84375, 'loss': 0.55163}\n",
            "Step 124780: {'train_ae_loss': 0.65048, 'train_ucc_loss': 0.51652, 'train_ucc_acc': 0.78125, 'loss': 0.5835}\n",
            "Step 124800: {'train_ae_loss': 0.6644, 'train_ucc_loss': 0.35001, 'train_ucc_acc': 1.0, 'loss': 0.5072}\n",
            "Step 124820: {'train_ae_loss': 0.66752, 'train_ucc_loss': 0.40772, 'train_ucc_acc': 0.90625, 'loss': 0.53762}\n",
            "Step 124840: {'train_ae_loss': 0.67022, 'train_ucc_loss': 0.42846, 'train_ucc_acc': 0.84375, 'loss': 0.54934}\n",
            "Step 124860: {'train_ae_loss': 0.65474, 'train_ucc_loss': 0.47737, 'train_ucc_acc': 0.8125, 'loss': 0.56605}\n",
            "Step 124880: {'train_ae_loss': 0.6715, 'train_ucc_loss': 0.47051, 'train_ucc_acc': 0.84375, 'loss': 0.571}\n",
            "Step 124900: {'train_ae_loss': 0.65997, 'train_ucc_loss': 0.4063, 'train_ucc_acc': 0.90625, 'loss': 0.53313}\n",
            "Step 124920: {'train_ae_loss': 0.66348, 'train_ucc_loss': 0.57564, 'train_ucc_acc': 0.75, 'loss': 0.61956}\n",
            "Step 124940: {'train_ae_loss': 0.66607, 'train_ucc_loss': 0.53272, 'train_ucc_acc': 0.75, 'loss': 0.59939}\n",
            "Step 124960: {'train_ae_loss': 0.67409, 'train_ucc_loss': 0.41989, 'train_ucc_acc': 0.90625, 'loss': 0.54699}\n",
            "Step 124980: {'train_ae_loss': 0.6714, 'train_ucc_loss': 0.43238, 'train_ucc_acc': 0.84375, 'loss': 0.55189}\n",
            "Step 125000: {'train_ae_loss': 0.66075, 'train_ucc_loss': 0.53261, 'train_ucc_acc': 0.75, 'loss': 0.59668}\n",
            "step: 125000,eval_ae_loss: 0.65306,eval_ucc_loss: 0.4845,eval_ucc_acc: 0.8125\n",
            "Step 125020: {'train_ae_loss': 0.66915, 'train_ucc_loss': 0.47333, 'train_ucc_acc': 0.8125, 'loss': 0.57124}\n",
            "Step 125040: {'train_ae_loss': 0.65451, 'train_ucc_loss': 0.41903, 'train_ucc_acc': 0.90625, 'loss': 0.53677}\n",
            "Step 125060: {'train_ae_loss': 0.6783, 'train_ucc_loss': 0.47192, 'train_ucc_acc': 0.8125, 'loss': 0.57511}\n",
            "Step 125080: {'train_ae_loss': 0.65449, 'train_ucc_loss': 0.49767, 'train_ucc_acc': 0.8125, 'loss': 0.57608}\n",
            "Step 125100: {'train_ae_loss': 0.65487, 'train_ucc_loss': 0.35884, 'train_ucc_acc': 0.96875, 'loss': 0.50686}\n",
            "Step 125120: {'train_ae_loss': 0.65447, 'train_ucc_loss': 0.44779, 'train_ucc_acc': 0.84375, 'loss': 0.55113}\n",
            "Step 125140: {'train_ae_loss': 0.66845, 'train_ucc_loss': 0.43148, 'train_ucc_acc': 0.875, 'loss': 0.54996}\n",
            "Step 125160: {'train_ae_loss': 0.65333, 'train_ucc_loss': 0.51724, 'train_ucc_acc': 0.8125, 'loss': 0.58528}\n",
            "Step 125180: {'train_ae_loss': 0.65778, 'train_ucc_loss': 0.47801, 'train_ucc_acc': 0.8125, 'loss': 0.56789}\n",
            "Step 125200: {'train_ae_loss': 0.67292, 'train_ucc_loss': 0.43306, 'train_ucc_acc': 0.875, 'loss': 0.55299}\n",
            "Step 125220: {'train_ae_loss': 0.66126, 'train_ucc_loss': 0.49221, 'train_ucc_acc': 0.8125, 'loss': 0.57674}\n",
            "Step 125240: {'train_ae_loss': 0.6528, 'train_ucc_loss': 0.3694, 'train_ucc_acc': 0.96875, 'loss': 0.5111}\n",
            "Step 125260: {'train_ae_loss': 0.64701, 'train_ucc_loss': 0.39961, 'train_ucc_acc': 0.875, 'loss': 0.52331}\n",
            "Step 125280: {'train_ae_loss': 0.63957, 'train_ucc_loss': 0.37454, 'train_ucc_acc': 0.9375, 'loss': 0.50705}\n",
            "Step 125300: {'train_ae_loss': 0.67007, 'train_ucc_loss': 0.37593, 'train_ucc_acc': 0.90625, 'loss': 0.523}\n",
            "Step 125320: {'train_ae_loss': 0.67022, 'train_ucc_loss': 0.42899, 'train_ucc_acc': 0.90625, 'loss': 0.54961}\n",
            "Step 125340: {'train_ae_loss': 0.65128, 'train_ucc_loss': 0.4551, 'train_ucc_acc': 0.84375, 'loss': 0.55319}\n",
            "Step 125360: {'train_ae_loss': 0.64991, 'train_ucc_loss': 0.38746, 'train_ucc_acc': 0.9375, 'loss': 0.51868}\n",
            "Step 125380: {'train_ae_loss': 0.64922, 'train_ucc_loss': 0.43417, 'train_ucc_acc': 0.875, 'loss': 0.54169}\n",
            "Step 125400: {'train_ae_loss': 0.64934, 'train_ucc_loss': 0.58413, 'train_ucc_acc': 0.75, 'loss': 0.61674}\n",
            "Step 125420: {'train_ae_loss': 0.6546, 'train_ucc_loss': 0.4297, 'train_ucc_acc': 0.875, 'loss': 0.54215}\n",
            "Step 125440: {'train_ae_loss': 0.66055, 'train_ucc_loss': 0.39744, 'train_ucc_acc': 0.90625, 'loss': 0.529}\n",
            "Step 125460: {'train_ae_loss': 0.64606, 'train_ucc_loss': 0.39928, 'train_ucc_acc': 0.90625, 'loss': 0.52267}\n",
            "Step 125480: {'train_ae_loss': 0.66089, 'train_ucc_loss': 0.3957, 'train_ucc_acc': 0.90625, 'loss': 0.52829}\n",
            "Step 125500: {'train_ae_loss': 0.65801, 'train_ucc_loss': 0.47116, 'train_ucc_acc': 0.8125, 'loss': 0.56458}\n",
            "Step 125520: {'train_ae_loss': 0.63839, 'train_ucc_loss': 0.53972, 'train_ucc_acc': 0.75, 'loss': 0.58905}\n",
            "Step 125540: {'train_ae_loss': 0.65525, 'train_ucc_loss': 0.40831, 'train_ucc_acc': 0.875, 'loss': 0.53178}\n",
            "Step 125560: {'train_ae_loss': 0.65397, 'train_ucc_loss': 0.48817, 'train_ucc_acc': 0.84375, 'loss': 0.57107}\n",
            "Step 125580: {'train_ae_loss': 0.6733, 'train_ucc_loss': 0.44321, 'train_ucc_acc': 0.875, 'loss': 0.55826}\n",
            "Step 125600: {'train_ae_loss': 0.64245, 'train_ucc_loss': 0.40524, 'train_ucc_acc': 0.90625, 'loss': 0.52384}\n",
            "Step 125620: {'train_ae_loss': 0.66443, 'train_ucc_loss': 0.408, 'train_ucc_acc': 0.875, 'loss': 0.53622}\n",
            "Step 125640: {'train_ae_loss': 0.65695, 'train_ucc_loss': 0.5755, 'train_ucc_acc': 0.71875, 'loss': 0.61623}\n",
            "Step 125660: {'train_ae_loss': 0.67086, 'train_ucc_loss': 0.42444, 'train_ucc_acc': 0.875, 'loss': 0.54765}\n",
            "Step 125680: {'train_ae_loss': 0.67006, 'train_ucc_loss': 0.58322, 'train_ucc_acc': 0.71875, 'loss': 0.62664}\n",
            "Step 125700: {'train_ae_loss': 0.66505, 'train_ucc_loss': 0.48093, 'train_ucc_acc': 0.84375, 'loss': 0.57299}\n",
            "Step 125720: {'train_ae_loss': 0.66996, 'train_ucc_loss': 0.48582, 'train_ucc_acc': 0.84375, 'loss': 0.57789}\n",
            "Step 125740: {'train_ae_loss': 0.65865, 'train_ucc_loss': 0.47008, 'train_ucc_acc': 0.84375, 'loss': 0.56437}\n",
            "Step 125760: {'train_ae_loss': 0.66659, 'train_ucc_loss': 0.42334, 'train_ucc_acc': 0.90625, 'loss': 0.54496}\n",
            "Step 125780: {'train_ae_loss': 0.65996, 'train_ucc_loss': 0.44445, 'train_ucc_acc': 0.875, 'loss': 0.5522}\n",
            "Step 125800: {'train_ae_loss': 0.6641, 'train_ucc_loss': 0.40106, 'train_ucc_acc': 0.875, 'loss': 0.53258}\n",
            "Step 125820: {'train_ae_loss': 0.67286, 'train_ucc_loss': 0.39248, 'train_ucc_acc': 0.9375, 'loss': 0.53267}\n",
            "Step 125840: {'train_ae_loss': 0.67577, 'train_ucc_loss': 0.45946, 'train_ucc_acc': 0.84375, 'loss': 0.56761}\n",
            "Step 125860: {'train_ae_loss': 0.6638, 'train_ucc_loss': 0.40323, 'train_ucc_acc': 0.90625, 'loss': 0.53352}\n",
            "Step 125880: {'train_ae_loss': 0.63082, 'train_ucc_loss': 0.48568, 'train_ucc_acc': 0.8125, 'loss': 0.55825}\n",
            "Step 125900: {'train_ae_loss': 0.64626, 'train_ucc_loss': 0.46374, 'train_ucc_acc': 0.84375, 'loss': 0.555}\n",
            "Step 125920: {'train_ae_loss': 0.65833, 'train_ucc_loss': 0.49466, 'train_ucc_acc': 0.84375, 'loss': 0.57649}\n",
            "Step 125940: {'train_ae_loss': 0.66091, 'train_ucc_loss': 0.37358, 'train_ucc_acc': 0.9375, 'loss': 0.51724}\n",
            "Step 125960: {'train_ae_loss': 0.63943, 'train_ucc_loss': 0.53637, 'train_ucc_acc': 0.75, 'loss': 0.5879}\n",
            "Step 125980: {'train_ae_loss': 0.63979, 'train_ucc_loss': 0.48227, 'train_ucc_acc': 0.8125, 'loss': 0.56103}\n",
            "Step 126000: {'train_ae_loss': 0.66507, 'train_ucc_loss': 0.41515, 'train_ucc_acc': 0.90625, 'loss': 0.54011}\n",
            "step: 126000,eval_ae_loss: 0.65439,eval_ucc_loss: 0.49217,eval_ucc_acc: 0.81738\n",
            "Step 126020: {'train_ae_loss': 0.67336, 'train_ucc_loss': 0.41707, 'train_ucc_acc': 0.90625, 'loss': 0.54521}\n",
            "Step 126040: {'train_ae_loss': 0.67983, 'train_ucc_loss': 0.44576, 'train_ucc_acc': 0.875, 'loss': 0.5628}\n",
            "Step 126060: {'train_ae_loss': 0.64087, 'train_ucc_loss': 0.45469, 'train_ucc_acc': 0.84375, 'loss': 0.54778}\n",
            "Step 126080: {'train_ae_loss': 0.67932, 'train_ucc_loss': 0.58597, 'train_ucc_acc': 0.71875, 'loss': 0.63265}\n",
            "Step 126100: {'train_ae_loss': 0.68393, 'train_ucc_loss': 0.45221, 'train_ucc_acc': 0.875, 'loss': 0.56807}\n",
            "Step 126120: {'train_ae_loss': 0.65813, 'train_ucc_loss': 0.4023, 'train_ucc_acc': 0.90625, 'loss': 0.53022}\n",
            "Step 126140: {'train_ae_loss': 0.67098, 'train_ucc_loss': 0.50521, 'train_ucc_acc': 0.78125, 'loss': 0.58809}\n",
            "Step 126160: {'train_ae_loss': 0.66031, 'train_ucc_loss': 0.40621, 'train_ucc_acc': 0.90625, 'loss': 0.53326}\n",
            "Step 126180: {'train_ae_loss': 0.65202, 'train_ucc_loss': 0.48059, 'train_ucc_acc': 0.84375, 'loss': 0.56631}\n",
            "Step 126200: {'train_ae_loss': 0.67023, 'train_ucc_loss': 0.42386, 'train_ucc_acc': 0.875, 'loss': 0.54705}\n",
            "Step 126220: {'train_ae_loss': 0.67161, 'train_ucc_loss': 0.4871, 'train_ucc_acc': 0.8125, 'loss': 0.57936}\n",
            "Step 126240: {'train_ae_loss': 0.66344, 'train_ucc_loss': 0.4566, 'train_ucc_acc': 0.875, 'loss': 0.56002}\n",
            "Step 126260: {'train_ae_loss': 0.6472, 'train_ucc_loss': 0.45571, 'train_ucc_acc': 0.84375, 'loss': 0.55146}\n",
            "Step 126280: {'train_ae_loss': 0.68029, 'train_ucc_loss': 0.4243, 'train_ucc_acc': 0.875, 'loss': 0.5523}\n",
            "Step 126300: {'train_ae_loss': 0.67242, 'train_ucc_loss': 0.5085, 'train_ucc_acc': 0.78125, 'loss': 0.59046}\n",
            "Step 126320: {'train_ae_loss': 0.66269, 'train_ucc_loss': 0.35431, 'train_ucc_acc': 0.96875, 'loss': 0.5085}\n",
            "Step 126340: {'train_ae_loss': 0.66598, 'train_ucc_loss': 0.42233, 'train_ucc_acc': 0.875, 'loss': 0.54415}\n",
            "Step 126360: {'train_ae_loss': 0.67374, 'train_ucc_loss': 0.46644, 'train_ucc_acc': 0.84375, 'loss': 0.57009}\n",
            "Step 126380: {'train_ae_loss': 0.65117, 'train_ucc_loss': 0.35094, 'train_ucc_acc': 0.96875, 'loss': 0.50105}\n",
            "Step 126400: {'train_ae_loss': 0.67157, 'train_ucc_loss': 0.34301, 'train_ucc_acc': 1.0, 'loss': 0.50729}\n",
            "Step 126420: {'train_ae_loss': 0.66148, 'train_ucc_loss': 0.47877, 'train_ucc_acc': 0.8125, 'loss': 0.57013}\n",
            "Step 126440: {'train_ae_loss': 0.65302, 'train_ucc_loss': 0.41511, 'train_ucc_acc': 0.90625, 'loss': 0.53406}\n",
            "Step 126460: {'train_ae_loss': 0.65873, 'train_ucc_loss': 0.36508, 'train_ucc_acc': 0.9375, 'loss': 0.51191}\n",
            "Step 126480: {'train_ae_loss': 0.65537, 'train_ucc_loss': 0.5631, 'train_ucc_acc': 0.75, 'loss': 0.60923}\n",
            "Step 126500: {'train_ae_loss': 0.67959, 'train_ucc_loss': 0.37493, 'train_ucc_acc': 0.9375, 'loss': 0.52726}\n",
            "Step 126520: {'train_ae_loss': 0.64086, 'train_ucc_loss': 0.4678, 'train_ucc_acc': 0.84375, 'loss': 0.55433}\n",
            "Step 126540: {'train_ae_loss': 0.67968, 'train_ucc_loss': 0.47678, 'train_ucc_acc': 0.8125, 'loss': 0.57823}\n",
            "Step 126560: {'train_ae_loss': 0.66946, 'train_ucc_loss': 0.46968, 'train_ucc_acc': 0.84375, 'loss': 0.56957}\n",
            "Step 126580: {'train_ae_loss': 0.70161, 'train_ucc_loss': 0.41059, 'train_ucc_acc': 0.875, 'loss': 0.5561}\n",
            "Step 126600: {'train_ae_loss': 0.65409, 'train_ucc_loss': 0.41567, 'train_ucc_acc': 0.84375, 'loss': 0.53488}\n",
            "Step 126620: {'train_ae_loss': 0.63721, 'train_ucc_loss': 0.43568, 'train_ucc_acc': 0.875, 'loss': 0.53644}\n",
            "Step 126640: {'train_ae_loss': 0.64854, 'train_ucc_loss': 0.45755, 'train_ucc_acc': 0.84375, 'loss': 0.55304}\n",
            "Step 126660: {'train_ae_loss': 0.68642, 'train_ucc_loss': 0.42342, 'train_ucc_acc': 0.84375, 'loss': 0.55492}\n",
            "Step 126680: {'train_ae_loss': 0.67653, 'train_ucc_loss': 0.4727, 'train_ucc_acc': 0.84375, 'loss': 0.57462}\n",
            "Step 126700: {'train_ae_loss': 0.67897, 'train_ucc_loss': 0.41433, 'train_ucc_acc': 0.875, 'loss': 0.54665}\n",
            "Step 126720: {'train_ae_loss': 0.64318, 'train_ucc_loss': 0.44558, 'train_ucc_acc': 0.84375, 'loss': 0.54438}\n",
            "Step 126740: {'train_ae_loss': 0.65834, 'train_ucc_loss': 0.3643, 'train_ucc_acc': 0.96875, 'loss': 0.51132}\n",
            "Step 126760: {'train_ae_loss': 0.66612, 'train_ucc_loss': 0.4338, 'train_ucc_acc': 0.875, 'loss': 0.54996}\n",
            "Step 126780: {'train_ae_loss': 0.6574, 'train_ucc_loss': 0.42431, 'train_ucc_acc': 0.875, 'loss': 0.54086}\n",
            "Step 126800: {'train_ae_loss': 0.67949, 'train_ucc_loss': 0.40467, 'train_ucc_acc': 0.90625, 'loss': 0.54208}\n",
            "Step 126820: {'train_ae_loss': 0.67165, 'train_ucc_loss': 0.37296, 'train_ucc_acc': 0.9375, 'loss': 0.52231}\n",
            "Step 126840: {'train_ae_loss': 0.66417, 'train_ucc_loss': 0.4906, 'train_ucc_acc': 0.78125, 'loss': 0.57739}\n",
            "Step 126860: {'train_ae_loss': 0.66166, 'train_ucc_loss': 0.45946, 'train_ucc_acc': 0.875, 'loss': 0.56056}\n",
            "Step 126880: {'train_ae_loss': 0.6597, 'train_ucc_loss': 0.48958, 'train_ucc_acc': 0.78125, 'loss': 0.57464}\n",
            "Step 126900: {'train_ae_loss': 0.64624, 'train_ucc_loss': 0.44906, 'train_ucc_acc': 0.84375, 'loss': 0.54765}\n",
            "Step 126920: {'train_ae_loss': 0.65882, 'train_ucc_loss': 0.48285, 'train_ucc_acc': 0.8125, 'loss': 0.57084}\n",
            "Step 126940: {'train_ae_loss': 0.6663, 'train_ucc_loss': 0.4345, 'train_ucc_acc': 0.875, 'loss': 0.5504}\n",
            "Step 126960: {'train_ae_loss': 0.65614, 'train_ucc_loss': 0.48135, 'train_ucc_acc': 0.84375, 'loss': 0.56875}\n",
            "Step 126980: {'train_ae_loss': 0.66328, 'train_ucc_loss': 0.37219, 'train_ucc_acc': 0.9375, 'loss': 0.51774}\n",
            "Step 127000: {'train_ae_loss': 0.66836, 'train_ucc_loss': 0.4897, 'train_ucc_acc': 0.8125, 'loss': 0.57903}\n",
            "step: 127000,eval_ae_loss: 0.65259,eval_ucc_loss: 0.47384,eval_ucc_acc: 0.83301\n",
            "Step 127020: {'train_ae_loss': 0.66592, 'train_ucc_loss': 0.52766, 'train_ucc_acc': 0.78125, 'loss': 0.59679}\n",
            "Step 127040: {'train_ae_loss': 0.66813, 'train_ucc_loss': 0.4048, 'train_ucc_acc': 0.90625, 'loss': 0.53647}\n",
            "Step 127060: {'train_ae_loss': 0.66138, 'train_ucc_loss': 0.40853, 'train_ucc_acc': 0.90625, 'loss': 0.53495}\n",
            "Step 127080: {'train_ae_loss': 0.67653, 'train_ucc_loss': 0.43077, 'train_ucc_acc': 0.875, 'loss': 0.55365}\n",
            "Step 127100: {'train_ae_loss': 0.6688, 'train_ucc_loss': 0.48554, 'train_ucc_acc': 0.8125, 'loss': 0.57717}\n",
            "Step 127120: {'train_ae_loss': 0.68299, 'train_ucc_loss': 0.48412, 'train_ucc_acc': 0.8125, 'loss': 0.58356}\n",
            "Step 127140: {'train_ae_loss': 0.66028, 'train_ucc_loss': 0.51014, 'train_ucc_acc': 0.8125, 'loss': 0.58521}\n",
            "Step 127160: {'train_ae_loss': 0.6662, 'train_ucc_loss': 0.46694, 'train_ucc_acc': 0.84375, 'loss': 0.56657}\n",
            "Step 127180: {'train_ae_loss': 0.65887, 'train_ucc_loss': 0.43581, 'train_ucc_acc': 0.875, 'loss': 0.54734}\n",
            "Step 127200: {'train_ae_loss': 0.68654, 'train_ucc_loss': 0.46045, 'train_ucc_acc': 0.875, 'loss': 0.5735}\n",
            "Step 127220: {'train_ae_loss': 0.65774, 'train_ucc_loss': 0.48312, 'train_ucc_acc': 0.78125, 'loss': 0.57043}\n",
            "Step 127240: {'train_ae_loss': 0.66215, 'train_ucc_loss': 0.47452, 'train_ucc_acc': 0.84375, 'loss': 0.56833}\n",
            "Step 127260: {'train_ae_loss': 0.66061, 'train_ucc_loss': 0.43376, 'train_ucc_acc': 0.875, 'loss': 0.54719}\n",
            "Step 127280: {'train_ae_loss': 0.67549, 'train_ucc_loss': 0.37349, 'train_ucc_acc': 0.9375, 'loss': 0.52449}\n",
            "Step 127300: {'train_ae_loss': 0.67335, 'train_ucc_loss': 0.49773, 'train_ucc_acc': 0.84375, 'loss': 0.58554}\n",
            "Step 127320: {'train_ae_loss': 0.68203, 'train_ucc_loss': 0.4986, 'train_ucc_acc': 0.8125, 'loss': 0.59032}\n",
            "Step 127340: {'train_ae_loss': 0.67662, 'train_ucc_loss': 0.52293, 'train_ucc_acc': 0.78125, 'loss': 0.59977}\n",
            "Step 127360: {'train_ae_loss': 0.69497, 'train_ucc_loss': 0.37121, 'train_ucc_acc': 0.9375, 'loss': 0.53309}\n",
            "Step 127380: {'train_ae_loss': 0.66383, 'train_ucc_loss': 0.54396, 'train_ucc_acc': 0.78125, 'loss': 0.6039}\n",
            "Step 127400: {'train_ae_loss': 0.6471, 'train_ucc_loss': 0.423, 'train_ucc_acc': 0.875, 'loss': 0.53505}\n",
            "Step 127420: {'train_ae_loss': 0.67135, 'train_ucc_loss': 0.43725, 'train_ucc_acc': 0.875, 'loss': 0.5543}\n",
            "Step 127440: {'train_ae_loss': 0.68536, 'train_ucc_loss': 0.44927, 'train_ucc_acc': 0.84375, 'loss': 0.56732}\n",
            "Step 127460: {'train_ae_loss': 0.70079, 'train_ucc_loss': 0.37848, 'train_ucc_acc': 0.9375, 'loss': 0.53964}\n",
            "Step 127480: {'train_ae_loss': 0.66087, 'train_ucc_loss': 0.41547, 'train_ucc_acc': 0.90625, 'loss': 0.53817}\n",
            "Step 127500: {'train_ae_loss': 0.67086, 'train_ucc_loss': 0.44459, 'train_ucc_acc': 0.875, 'loss': 0.55772}\n",
            "Step 127520: {'train_ae_loss': 0.64954, 'train_ucc_loss': 0.38528, 'train_ucc_acc': 0.90625, 'loss': 0.51741}\n",
            "Step 127540: {'train_ae_loss': 0.66181, 'train_ucc_loss': 0.5251, 'train_ucc_acc': 0.78125, 'loss': 0.59345}\n",
            "Step 127560: {'train_ae_loss': 0.66151, 'train_ucc_loss': 0.48141, 'train_ucc_acc': 0.8125, 'loss': 0.57146}\n",
            "Step 127580: {'train_ae_loss': 0.65353, 'train_ucc_loss': 0.5119, 'train_ucc_acc': 0.78125, 'loss': 0.58271}\n",
            "Step 127600: {'train_ae_loss': 0.67973, 'train_ucc_loss': 0.47856, 'train_ucc_acc': 0.8125, 'loss': 0.57915}\n",
            "Step 127620: {'train_ae_loss': 0.65094, 'train_ucc_loss': 0.44145, 'train_ucc_acc': 0.875, 'loss': 0.54619}\n",
            "Step 127640: {'train_ae_loss': 0.65692, 'train_ucc_loss': 0.43793, 'train_ucc_acc': 0.875, 'loss': 0.54743}\n",
            "Step 127660: {'train_ae_loss': 0.67737, 'train_ucc_loss': 0.4797, 'train_ucc_acc': 0.8125, 'loss': 0.57854}\n",
            "Step 127680: {'train_ae_loss': 0.66795, 'train_ucc_loss': 0.42858, 'train_ucc_acc': 0.875, 'loss': 0.54826}\n",
            "Step 127700: {'train_ae_loss': 0.65041, 'train_ucc_loss': 0.5653, 'train_ucc_acc': 0.71875, 'loss': 0.60785}\n",
            "Step 127720: {'train_ae_loss': 0.65473, 'train_ucc_loss': 0.46415, 'train_ucc_acc': 0.84375, 'loss': 0.55944}\n",
            "Step 127740: {'train_ae_loss': 0.65589, 'train_ucc_loss': 0.48746, 'train_ucc_acc': 0.8125, 'loss': 0.57168}\n",
            "Step 127760: {'train_ae_loss': 0.64573, 'train_ucc_loss': 0.38961, 'train_ucc_acc': 0.90625, 'loss': 0.51767}\n",
            "Step 127780: {'train_ae_loss': 0.63642, 'train_ucc_loss': 0.41496, 'train_ucc_acc': 0.90625, 'loss': 0.52569}\n",
            "Step 127800: {'train_ae_loss': 0.67585, 'train_ucc_loss': 0.39455, 'train_ucc_acc': 0.9375, 'loss': 0.5352}\n",
            "Step 127820: {'train_ae_loss': 0.65685, 'train_ucc_loss': 0.42998, 'train_ucc_acc': 0.90625, 'loss': 0.54341}\n",
            "Step 127840: {'train_ae_loss': 0.66135, 'train_ucc_loss': 0.44296, 'train_ucc_acc': 0.875, 'loss': 0.55215}\n",
            "Step 127860: {'train_ae_loss': 0.68307, 'train_ucc_loss': 0.41651, 'train_ucc_acc': 0.875, 'loss': 0.54979}\n",
            "Step 127880: {'train_ae_loss': 0.65119, 'train_ucc_loss': 0.44293, 'train_ucc_acc': 0.84375, 'loss': 0.54706}\n",
            "Step 127900: {'train_ae_loss': 0.66417, 'train_ucc_loss': 0.55934, 'train_ucc_acc': 0.75, 'loss': 0.61175}\n",
            "Step 127920: {'train_ae_loss': 0.66411, 'train_ucc_loss': 0.42828, 'train_ucc_acc': 0.875, 'loss': 0.5462}\n",
            "Step 127940: {'train_ae_loss': 0.66109, 'train_ucc_loss': 0.39991, 'train_ucc_acc': 0.90625, 'loss': 0.5305}\n",
            "Step 127960: {'train_ae_loss': 0.68139, 'train_ucc_loss': 0.5135, 'train_ucc_acc': 0.78125, 'loss': 0.59744}\n",
            "Step 127980: {'train_ae_loss': 0.67681, 'train_ucc_loss': 0.47395, 'train_ucc_acc': 0.84375, 'loss': 0.57538}\n",
            "Step 128000: {'train_ae_loss': 0.65409, 'train_ucc_loss': 0.50864, 'train_ucc_acc': 0.78125, 'loss': 0.58136}\n",
            "step: 128000,eval_ae_loss: 0.6481,eval_ucc_loss: 0.49635,eval_ucc_acc: 0.80566\n",
            "Step 128020: {'train_ae_loss': 0.65842, 'train_ucc_loss': 0.50651, 'train_ucc_acc': 0.8125, 'loss': 0.58246}\n",
            "Step 128040: {'train_ae_loss': 0.67164, 'train_ucc_loss': 0.48808, 'train_ucc_acc': 0.8125, 'loss': 0.57986}\n",
            "Step 128060: {'train_ae_loss': 0.65557, 'train_ucc_loss': 0.5022, 'train_ucc_acc': 0.75, 'loss': 0.57888}\n",
            "Step 128080: {'train_ae_loss': 0.64643, 'train_ucc_loss': 0.62441, 'train_ucc_acc': 0.65625, 'loss': 0.63542}\n",
            "Step 128100: {'train_ae_loss': 0.65467, 'train_ucc_loss': 0.46644, 'train_ucc_acc': 0.84375, 'loss': 0.56056}\n",
            "Step 128120: {'train_ae_loss': 0.65985, 'train_ucc_loss': 0.32865, 'train_ucc_acc': 1.0, 'loss': 0.49425}\n",
            "Step 128140: {'train_ae_loss': 0.68768, 'train_ucc_loss': 0.5134, 'train_ucc_acc': 0.8125, 'loss': 0.60054}\n",
            "Step 128160: {'train_ae_loss': 0.63827, 'train_ucc_loss': 0.35688, 'train_ucc_acc': 0.96875, 'loss': 0.49757}\n",
            "Step 128180: {'train_ae_loss': 0.67509, 'train_ucc_loss': 0.33931, 'train_ucc_acc': 1.0, 'loss': 0.5072}\n",
            "Step 128200: {'train_ae_loss': 0.67913, 'train_ucc_loss': 0.41426, 'train_ucc_acc': 0.90625, 'loss': 0.54669}\n",
            "Step 128220: {'train_ae_loss': 0.65566, 'train_ucc_loss': 0.45845, 'train_ucc_acc': 0.84375, 'loss': 0.55705}\n",
            "Step 128240: {'train_ae_loss': 0.64859, 'train_ucc_loss': 0.49688, 'train_ucc_acc': 0.78125, 'loss': 0.57274}\n",
            "Step 128260: {'train_ae_loss': 0.66668, 'train_ucc_loss': 0.47133, 'train_ucc_acc': 0.84375, 'loss': 0.56901}\n",
            "Step 128280: {'train_ae_loss': 0.67187, 'train_ucc_loss': 0.39294, 'train_ucc_acc': 0.9375, 'loss': 0.53241}\n",
            "Step 128300: {'train_ae_loss': 0.6743, 'train_ucc_loss': 0.3845, 'train_ucc_acc': 0.9375, 'loss': 0.5294}\n",
            "Step 128320: {'train_ae_loss': 0.65698, 'train_ucc_loss': 0.51697, 'train_ucc_acc': 0.75, 'loss': 0.58697}\n",
            "Step 128340: {'train_ae_loss': 0.6654, 'train_ucc_loss': 0.43558, 'train_ucc_acc': 0.875, 'loss': 0.55049}\n",
            "Step 128360: {'train_ae_loss': 0.6562, 'train_ucc_loss': 0.49355, 'train_ucc_acc': 0.8125, 'loss': 0.57487}\n",
            "Step 128380: {'train_ae_loss': 0.65123, 'train_ucc_loss': 0.49138, 'train_ucc_acc': 0.8125, 'loss': 0.57131}\n",
            "Step 128400: {'train_ae_loss': 0.65406, 'train_ucc_loss': 0.5369, 'train_ucc_acc': 0.75, 'loss': 0.59548}\n",
            "Step 128420: {'train_ae_loss': 0.6618, 'train_ucc_loss': 0.44911, 'train_ucc_acc': 0.875, 'loss': 0.55546}\n",
            "Step 128440: {'train_ae_loss': 0.65128, 'train_ucc_loss': 0.38908, 'train_ucc_acc': 0.9375, 'loss': 0.52018}\n",
            "Step 128460: {'train_ae_loss': 0.65976, 'train_ucc_loss': 0.36331, 'train_ucc_acc': 0.9375, 'loss': 0.51154}\n",
            "Step 128480: {'train_ae_loss': 0.66579, 'train_ucc_loss': 0.39577, 'train_ucc_acc': 0.875, 'loss': 0.53078}\n",
            "Step 128500: {'train_ae_loss': 0.66512, 'train_ucc_loss': 0.5018, 'train_ucc_acc': 0.8125, 'loss': 0.58346}\n",
            "Step 128520: {'train_ae_loss': 0.6557, 'train_ucc_loss': 0.54108, 'train_ucc_acc': 0.75, 'loss': 0.59839}\n",
            "Step 128540: {'train_ae_loss': 0.66309, 'train_ucc_loss': 0.43696, 'train_ucc_acc': 0.875, 'loss': 0.55002}\n",
            "Step 128560: {'train_ae_loss': 0.64777, 'train_ucc_loss': 0.49116, 'train_ucc_acc': 0.78125, 'loss': 0.56946}\n",
            "Step 128580: {'train_ae_loss': 0.66433, 'train_ucc_loss': 0.39245, 'train_ucc_acc': 0.9375, 'loss': 0.52839}\n",
            "Step 128600: {'train_ae_loss': 0.66914, 'train_ucc_loss': 0.40182, 'train_ucc_acc': 0.90625, 'loss': 0.53548}\n",
            "Step 128620: {'train_ae_loss': 0.67769, 'train_ucc_loss': 0.41141, 'train_ucc_acc': 0.90625, 'loss': 0.54455}\n",
            "Step 128640: {'train_ae_loss': 0.65966, 'train_ucc_loss': 0.36058, 'train_ucc_acc': 0.9375, 'loss': 0.51012}\n",
            "Step 128660: {'train_ae_loss': 0.64327, 'train_ucc_loss': 0.42286, 'train_ucc_acc': 0.875, 'loss': 0.53307}\n",
            "Step 128680: {'train_ae_loss': 0.65422, 'train_ucc_loss': 0.42023, 'train_ucc_acc': 0.875, 'loss': 0.53722}\n",
            "Step 128700: {'train_ae_loss': 0.65891, 'train_ucc_loss': 0.5268, 'train_ucc_acc': 0.78125, 'loss': 0.59286}\n",
            "Step 128720: {'train_ae_loss': 0.67569, 'train_ucc_loss': 0.50914, 'train_ucc_acc': 0.78125, 'loss': 0.59242}\n",
            "Step 128740: {'train_ae_loss': 0.66056, 'train_ucc_loss': 0.40482, 'train_ucc_acc': 0.90625, 'loss': 0.53269}\n",
            "Step 128760: {'train_ae_loss': 0.65616, 'train_ucc_loss': 0.56698, 'train_ucc_acc': 0.71875, 'loss': 0.61157}\n",
            "Step 128780: {'train_ae_loss': 0.66587, 'train_ucc_loss': 0.53028, 'train_ucc_acc': 0.78125, 'loss': 0.59808}\n",
            "Step 128800: {'train_ae_loss': 0.65453, 'train_ucc_loss': 0.45341, 'train_ucc_acc': 0.875, 'loss': 0.55397}\n",
            "Step 128820: {'train_ae_loss': 0.65179, 'train_ucc_loss': 0.49007, 'train_ucc_acc': 0.78125, 'loss': 0.57093}\n",
            "Step 128840: {'train_ae_loss': 0.65289, 'train_ucc_loss': 0.48797, 'train_ucc_acc': 0.84375, 'loss': 0.57043}\n",
            "Step 128860: {'train_ae_loss': 0.66116, 'train_ucc_loss': 0.41669, 'train_ucc_acc': 0.90625, 'loss': 0.53893}\n",
            "Step 128880: {'train_ae_loss': 0.64849, 'train_ucc_loss': 0.50997, 'train_ucc_acc': 0.78125, 'loss': 0.57923}\n",
            "Step 128900: {'train_ae_loss': 0.66224, 'train_ucc_loss': 0.47724, 'train_ucc_acc': 0.8125, 'loss': 0.56974}\n",
            "Step 128920: {'train_ae_loss': 0.65151, 'train_ucc_loss': 0.36408, 'train_ucc_acc': 0.96875, 'loss': 0.50779}\n",
            "Step 128940: {'train_ae_loss': 0.65785, 'train_ucc_loss': 0.51099, 'train_ucc_acc': 0.78125, 'loss': 0.58442}\n",
            "Step 128960: {'train_ae_loss': 0.65493, 'train_ucc_loss': 0.47719, 'train_ucc_acc': 0.8125, 'loss': 0.56606}\n",
            "Step 128980: {'train_ae_loss': 0.66059, 'train_ucc_loss': 0.57307, 'train_ucc_acc': 0.71875, 'loss': 0.61683}\n",
            "Step 129000: {'train_ae_loss': 0.64448, 'train_ucc_loss': 0.53854, 'train_ucc_acc': 0.78125, 'loss': 0.59151}\n",
            "step: 129000,eval_ae_loss: 0.65706,eval_ucc_loss: 0.51393,eval_ucc_acc: 0.79199\n",
            "Step 129020: {'train_ae_loss': 0.66872, 'train_ucc_loss': 0.47114, 'train_ucc_acc': 0.84375, 'loss': 0.56993}\n",
            "Step 129040: {'train_ae_loss': 0.65245, 'train_ucc_loss': 0.43279, 'train_ucc_acc': 0.875, 'loss': 0.54262}\n",
            "Step 129060: {'train_ae_loss': 0.66299, 'train_ucc_loss': 0.41852, 'train_ucc_acc': 0.90625, 'loss': 0.54075}\n",
            "Step 129080: {'train_ae_loss': 0.65451, 'train_ucc_loss': 0.35049, 'train_ucc_acc': 0.9375, 'loss': 0.5025}\n",
            "Step 129100: {'train_ae_loss': 0.65374, 'train_ucc_loss': 0.43984, 'train_ucc_acc': 0.875, 'loss': 0.54679}\n",
            "Step 129120: {'train_ae_loss': 0.65915, 'train_ucc_loss': 0.39046, 'train_ucc_acc': 0.9375, 'loss': 0.52481}\n",
            "Step 129140: {'train_ae_loss': 0.66915, 'train_ucc_loss': 0.49332, 'train_ucc_acc': 0.78125, 'loss': 0.58124}\n",
            "Step 129160: {'train_ae_loss': 0.66743, 'train_ucc_loss': 0.41662, 'train_ucc_acc': 0.90625, 'loss': 0.54202}\n",
            "Step 129180: {'train_ae_loss': 0.67048, 'train_ucc_loss': 0.38841, 'train_ucc_acc': 0.9375, 'loss': 0.52944}\n",
            "Step 129200: {'train_ae_loss': 0.6445, 'train_ucc_loss': 0.48633, 'train_ucc_acc': 0.8125, 'loss': 0.56541}\n",
            "Step 129220: {'train_ae_loss': 0.6663, 'train_ucc_loss': 0.41456, 'train_ucc_acc': 0.90625, 'loss': 0.54043}\n",
            "Step 129240: {'train_ae_loss': 0.652, 'train_ucc_loss': 0.48454, 'train_ucc_acc': 0.8125, 'loss': 0.56827}\n",
            "Step 129260: {'train_ae_loss': 0.67096, 'train_ucc_loss': 0.41593, 'train_ucc_acc': 0.875, 'loss': 0.54344}\n",
            "Step 129280: {'train_ae_loss': 0.63437, 'train_ucc_loss': 0.44067, 'train_ucc_acc': 0.875, 'loss': 0.53752}\n",
            "Step 129300: {'train_ae_loss': 0.65435, 'train_ucc_loss': 0.40662, 'train_ucc_acc': 0.90625, 'loss': 0.53049}\n",
            "Step 129320: {'train_ae_loss': 0.67545, 'train_ucc_loss': 0.49084, 'train_ucc_acc': 0.84375, 'loss': 0.58315}\n",
            "Step 129340: {'train_ae_loss': 0.66279, 'train_ucc_loss': 0.43557, 'train_ucc_acc': 0.875, 'loss': 0.54918}\n",
            "Step 129360: {'train_ae_loss': 0.65828, 'train_ucc_loss': 0.4599, 'train_ucc_acc': 0.8125, 'loss': 0.55909}\n",
            "Step 129380: {'train_ae_loss': 0.65744, 'train_ucc_loss': 0.3475, 'train_ucc_acc': 0.9375, 'loss': 0.50247}\n",
            "Step 129400: {'train_ae_loss': 0.66349, 'train_ucc_loss': 0.63053, 'train_ucc_acc': 0.65625, 'loss': 0.64701}\n",
            "Step 129420: {'train_ae_loss': 0.65337, 'train_ucc_loss': 0.40538, 'train_ucc_acc': 0.90625, 'loss': 0.52938}\n",
            "Step 129440: {'train_ae_loss': 0.67968, 'train_ucc_loss': 0.45907, 'train_ucc_acc': 0.84375, 'loss': 0.56938}\n",
            "Step 129460: {'train_ae_loss': 0.64013, 'train_ucc_loss': 0.4116, 'train_ucc_acc': 0.90625, 'loss': 0.52586}\n",
            "Step 129480: {'train_ae_loss': 0.66247, 'train_ucc_loss': 0.47691, 'train_ucc_acc': 0.84375, 'loss': 0.56969}\n",
            "Step 129500: {'train_ae_loss': 0.64542, 'train_ucc_loss': 0.40173, 'train_ucc_acc': 0.90625, 'loss': 0.52358}\n",
            "Step 129520: {'train_ae_loss': 0.64854, 'train_ucc_loss': 0.48034, 'train_ucc_acc': 0.8125, 'loss': 0.56444}\n",
            "Step 129540: {'train_ae_loss': 0.65706, 'train_ucc_loss': 0.49972, 'train_ucc_acc': 0.8125, 'loss': 0.57839}\n",
            "Step 129560: {'train_ae_loss': 0.65198, 'train_ucc_loss': 0.39624, 'train_ucc_acc': 0.90625, 'loss': 0.52411}\n",
            "Step 129580: {'train_ae_loss': 0.64297, 'train_ucc_loss': 0.36414, 'train_ucc_acc': 0.9375, 'loss': 0.50356}\n",
            "Step 129600: {'train_ae_loss': 0.67027, 'train_ucc_loss': 0.525, 'train_ucc_acc': 0.71875, 'loss': 0.59763}\n",
            "Step 129620: {'train_ae_loss': 0.65415, 'train_ucc_loss': 0.47735, 'train_ucc_acc': 0.8125, 'loss': 0.56575}\n",
            "Step 129640: {'train_ae_loss': 0.6543, 'train_ucc_loss': 0.51199, 'train_ucc_acc': 0.78125, 'loss': 0.58315}\n",
            "Step 129660: {'train_ae_loss': 0.65466, 'train_ucc_loss': 0.45995, 'train_ucc_acc': 0.84375, 'loss': 0.55731}\n",
            "Step 129680: {'train_ae_loss': 0.66154, 'train_ucc_loss': 0.40797, 'train_ucc_acc': 0.90625, 'loss': 0.53476}\n",
            "Step 129700: {'train_ae_loss': 0.65762, 'train_ucc_loss': 0.45287, 'train_ucc_acc': 0.84375, 'loss': 0.55524}\n",
            "Step 129720: {'train_ae_loss': 0.65285, 'train_ucc_loss': 0.43031, 'train_ucc_acc': 0.875, 'loss': 0.54158}\n",
            "Step 129740: {'train_ae_loss': 0.67877, 'train_ucc_loss': 0.40312, 'train_ucc_acc': 0.90625, 'loss': 0.54094}\n",
            "Step 129760: {'train_ae_loss': 0.66236, 'train_ucc_loss': 0.36985, 'train_ucc_acc': 0.9375, 'loss': 0.51611}\n",
            "Step 129780: {'train_ae_loss': 0.65293, 'train_ucc_loss': 0.41761, 'train_ucc_acc': 0.90625, 'loss': 0.53527}\n",
            "Step 129800: {'train_ae_loss': 0.66196, 'train_ucc_loss': 0.41467, 'train_ucc_acc': 0.90625, 'loss': 0.53831}\n",
            "Step 129820: {'train_ae_loss': 0.65125, 'train_ucc_loss': 0.45207, 'train_ucc_acc': 0.84375, 'loss': 0.55166}\n",
            "Step 129840: {'train_ae_loss': 0.65026, 'train_ucc_loss': 0.4415, 'train_ucc_acc': 0.875, 'loss': 0.54588}\n",
            "Step 129860: {'train_ae_loss': 0.65265, 'train_ucc_loss': 0.38834, 'train_ucc_acc': 0.90625, 'loss': 0.5205}\n",
            "Step 129880: {'train_ae_loss': 0.6619, 'train_ucc_loss': 0.39556, 'train_ucc_acc': 0.9375, 'loss': 0.52873}\n",
            "Step 129900: {'train_ae_loss': 0.65676, 'train_ucc_loss': 0.43073, 'train_ucc_acc': 0.90625, 'loss': 0.54375}\n",
            "Step 129920: {'train_ae_loss': 0.66111, 'train_ucc_loss': 0.47559, 'train_ucc_acc': 0.8125, 'loss': 0.56835}\n",
            "Step 129940: {'train_ae_loss': 0.65038, 'train_ucc_loss': 0.43418, 'train_ucc_acc': 0.875, 'loss': 0.54228}\n",
            "Step 129960: {'train_ae_loss': 0.66677, 'train_ucc_loss': 0.45694, 'train_ucc_acc': 0.84375, 'loss': 0.56186}\n",
            "Step 129980: {'train_ae_loss': 0.65832, 'train_ucc_loss': 0.40855, 'train_ucc_acc': 0.875, 'loss': 0.53343}\n",
            "Step 130000: {'train_ae_loss': 0.64062, 'train_ucc_loss': 0.41844, 'train_ucc_acc': 0.875, 'loss': 0.52953}\n",
            "step: 130000,eval_ae_loss: 0.64647,eval_ucc_loss: 0.50308,eval_ucc_acc: 0.80273\n",
            "Step 130020: {'train_ae_loss': 0.66175, 'train_ucc_loss': 0.4105, 'train_ucc_acc': 0.90625, 'loss': 0.53613}\n",
            "Step 130040: {'train_ae_loss': 0.66364, 'train_ucc_loss': 0.48663, 'train_ucc_acc': 0.8125, 'loss': 0.57513}\n",
            "Step 130060: {'train_ae_loss': 0.66256, 'train_ucc_loss': 0.46763, 'train_ucc_acc': 0.84375, 'loss': 0.5651}\n",
            "Step 130080: {'train_ae_loss': 0.65528, 'train_ucc_loss': 0.40447, 'train_ucc_acc': 0.90625, 'loss': 0.52987}\n",
            "Step 130100: {'train_ae_loss': 0.66428, 'train_ucc_loss': 0.45434, 'train_ucc_acc': 0.84375, 'loss': 0.55931}\n",
            "Step 130120: {'train_ae_loss': 0.67434, 'train_ucc_loss': 0.36601, 'train_ucc_acc': 0.9375, 'loss': 0.52017}\n",
            "Step 130140: {'train_ae_loss': 0.6578, 'train_ucc_loss': 0.46881, 'train_ucc_acc': 0.84375, 'loss': 0.56331}\n",
            "Step 130160: {'train_ae_loss': 0.66468, 'train_ucc_loss': 0.39699, 'train_ucc_acc': 0.9375, 'loss': 0.53083}\n",
            "Step 130180: {'train_ae_loss': 0.66493, 'train_ucc_loss': 0.38809, 'train_ucc_acc': 0.90625, 'loss': 0.52651}\n",
            "Step 130200: {'train_ae_loss': 0.66757, 'train_ucc_loss': 0.44157, 'train_ucc_acc': 0.875, 'loss': 0.55457}\n",
            "Step 130220: {'train_ae_loss': 0.65441, 'train_ucc_loss': 0.43259, 'train_ucc_acc': 0.8125, 'loss': 0.5435}\n",
            "Step 130240: {'train_ae_loss': 0.65603, 'train_ucc_loss': 0.39318, 'train_ucc_acc': 0.90625, 'loss': 0.5246}\n",
            "Step 130260: {'train_ae_loss': 0.6685, 'train_ucc_loss': 0.46315, 'train_ucc_acc': 0.84375, 'loss': 0.56582}\n",
            "Step 130280: {'train_ae_loss': 0.65214, 'train_ucc_loss': 0.43605, 'train_ucc_acc': 0.90625, 'loss': 0.54409}\n",
            "Step 130300: {'train_ae_loss': 0.64914, 'train_ucc_loss': 0.35683, 'train_ucc_acc': 0.96875, 'loss': 0.50299}\n",
            "Step 130320: {'train_ae_loss': 0.65142, 'train_ucc_loss': 0.50808, 'train_ucc_acc': 0.8125, 'loss': 0.57975}\n",
            "Step 130340: {'train_ae_loss': 0.65991, 'train_ucc_loss': 0.41684, 'train_ucc_acc': 0.90625, 'loss': 0.53838}\n",
            "Step 130360: {'train_ae_loss': 0.67599, 'train_ucc_loss': 0.47547, 'train_ucc_acc': 0.875, 'loss': 0.57573}\n",
            "Step 130380: {'train_ae_loss': 0.66973, 'train_ucc_loss': 0.32859, 'train_ucc_acc': 1.0, 'loss': 0.49916}\n",
            "Step 130400: {'train_ae_loss': 0.65436, 'train_ucc_loss': 0.41228, 'train_ucc_acc': 0.90625, 'loss': 0.53332}\n",
            "Step 130420: {'train_ae_loss': 0.66903, 'train_ucc_loss': 0.40498, 'train_ucc_acc': 0.90625, 'loss': 0.537}\n",
            "Step 130440: {'train_ae_loss': 0.66096, 'train_ucc_loss': 0.45554, 'train_ucc_acc': 0.8125, 'loss': 0.55825}\n",
            "Step 130460: {'train_ae_loss': 0.66077, 'train_ucc_loss': 0.53157, 'train_ucc_acc': 0.78125, 'loss': 0.59617}\n",
            "Step 130480: {'train_ae_loss': 0.66658, 'train_ucc_loss': 0.44596, 'train_ucc_acc': 0.875, 'loss': 0.55627}\n",
            "Step 130500: {'train_ae_loss': 0.66458, 'train_ucc_loss': 0.42791, 'train_ucc_acc': 0.84375, 'loss': 0.54625}\n",
            "Step 130520: {'train_ae_loss': 0.66688, 'train_ucc_loss': 0.58139, 'train_ucc_acc': 0.71875, 'loss': 0.62414}\n",
            "Step 130540: {'train_ae_loss': 0.65406, 'train_ucc_loss': 0.51891, 'train_ucc_acc': 0.8125, 'loss': 0.58648}\n",
            "Step 130560: {'train_ae_loss': 0.65546, 'train_ucc_loss': 0.36217, 'train_ucc_acc': 0.9375, 'loss': 0.50881}\n",
            "Step 130580: {'train_ae_loss': 0.66962, 'train_ucc_loss': 0.40798, 'train_ucc_acc': 0.90625, 'loss': 0.5388}\n",
            "Step 130600: {'train_ae_loss': 0.66941, 'train_ucc_loss': 0.45039, 'train_ucc_acc': 0.875, 'loss': 0.5599}\n",
            "Step 130620: {'train_ae_loss': 0.644, 'train_ucc_loss': 0.3588, 'train_ucc_acc': 0.96875, 'loss': 0.5014}\n",
            "Step 130640: {'train_ae_loss': 0.67367, 'train_ucc_loss': 0.35926, 'train_ucc_acc': 0.96875, 'loss': 0.51647}\n",
            "Step 130660: {'train_ae_loss': 0.66409, 'train_ucc_loss': 0.45945, 'train_ucc_acc': 0.875, 'loss': 0.56177}\n",
            "Step 130680: {'train_ae_loss': 0.67739, 'train_ucc_loss': 0.51341, 'train_ucc_acc': 0.8125, 'loss': 0.5954}\n",
            "Step 130700: {'train_ae_loss': 0.6507, 'train_ucc_loss': 0.48619, 'train_ucc_acc': 0.8125, 'loss': 0.56845}\n",
            "Step 130720: {'train_ae_loss': 0.66185, 'train_ucc_loss': 0.40063, 'train_ucc_acc': 0.90625, 'loss': 0.53124}\n",
            "Step 130740: {'train_ae_loss': 0.66151, 'train_ucc_loss': 0.48008, 'train_ucc_acc': 0.8125, 'loss': 0.5708}\n",
            "Step 130760: {'train_ae_loss': 0.63945, 'train_ucc_loss': 0.41536, 'train_ucc_acc': 0.90625, 'loss': 0.5274}\n",
            "Step 130780: {'train_ae_loss': 0.65776, 'train_ucc_loss': 0.43558, 'train_ucc_acc': 0.875, 'loss': 0.54667}\n",
            "Step 130800: {'train_ae_loss': 0.65741, 'train_ucc_loss': 0.5469, 'train_ucc_acc': 0.75, 'loss': 0.60216}\n",
            "Step 130820: {'train_ae_loss': 0.6662, 'train_ucc_loss': 0.41286, 'train_ucc_acc': 0.875, 'loss': 0.53953}\n",
            "Step 130840: {'train_ae_loss': 0.66312, 'train_ucc_loss': 0.44374, 'train_ucc_acc': 0.875, 'loss': 0.55343}\n",
            "Step 130860: {'train_ae_loss': 0.6486, 'train_ucc_loss': 0.39328, 'train_ucc_acc': 0.90625, 'loss': 0.52094}\n",
            "Step 130880: {'train_ae_loss': 0.67007, 'train_ucc_loss': 0.44047, 'train_ucc_acc': 0.84375, 'loss': 0.55527}\n",
            "Step 130900: {'train_ae_loss': 0.65006, 'train_ucc_loss': 0.35395, 'train_ucc_acc': 0.96875, 'loss': 0.502}\n",
            "Step 130920: {'train_ae_loss': 0.65848, 'train_ucc_loss': 0.38893, 'train_ucc_acc': 0.9375, 'loss': 0.5237}\n",
            "Step 130940: {'train_ae_loss': 0.65654, 'train_ucc_loss': 0.47698, 'train_ucc_acc': 0.84375, 'loss': 0.56676}\n",
            "Step 130960: {'train_ae_loss': 0.66478, 'train_ucc_loss': 0.49243, 'train_ucc_acc': 0.8125, 'loss': 0.57861}\n",
            "Step 130980: {'train_ae_loss': 0.66656, 'train_ucc_loss': 0.40432, 'train_ucc_acc': 0.90625, 'loss': 0.53544}\n",
            "Step 131000: {'train_ae_loss': 0.66772, 'train_ucc_loss': 0.43637, 'train_ucc_acc': 0.875, 'loss': 0.55204}\n",
            "step: 131000,eval_ae_loss: 0.66705,eval_ucc_loss: 0.48098,eval_ucc_acc: 0.83008\n",
            "Step 131020: {'train_ae_loss': 0.66419, 'train_ucc_loss': 0.41563, 'train_ucc_acc': 0.90625, 'loss': 0.53991}\n",
            "Step 131040: {'train_ae_loss': 0.65462, 'train_ucc_loss': 0.37952, 'train_ucc_acc': 0.90625, 'loss': 0.51707}\n",
            "Step 131060: {'train_ae_loss': 0.66688, 'train_ucc_loss': 0.41076, 'train_ucc_acc': 0.9375, 'loss': 0.53882}\n",
            "Step 131080: {'train_ae_loss': 0.67115, 'train_ucc_loss': 0.39898, 'train_ucc_acc': 0.90625, 'loss': 0.53506}\n",
            "Step 131100: {'train_ae_loss': 0.65994, 'train_ucc_loss': 0.41569, 'train_ucc_acc': 0.90625, 'loss': 0.53782}\n",
            "Step 131120: {'train_ae_loss': 0.69033, 'train_ucc_loss': 0.45272, 'train_ucc_acc': 0.8125, 'loss': 0.57153}\n",
            "Step 131140: {'train_ae_loss': 0.67252, 'train_ucc_loss': 0.43575, 'train_ucc_acc': 0.875, 'loss': 0.55413}\n",
            "Step 131160: {'train_ae_loss': 0.66177, 'train_ucc_loss': 0.40308, 'train_ucc_acc': 0.90625, 'loss': 0.53243}\n",
            "Step 131180: {'train_ae_loss': 0.679, 'train_ucc_loss': 0.37602, 'train_ucc_acc': 0.9375, 'loss': 0.52751}\n",
            "Step 131200: {'train_ae_loss': 0.6664, 'train_ucc_loss': 0.44769, 'train_ucc_acc': 0.84375, 'loss': 0.55704}\n",
            "Step 131220: {'train_ae_loss': 0.68215, 'train_ucc_loss': 0.4653, 'train_ucc_acc': 0.84375, 'loss': 0.57373}\n",
            "Step 131240: {'train_ae_loss': 0.67301, 'train_ucc_loss': 0.43646, 'train_ucc_acc': 0.875, 'loss': 0.55473}\n",
            "Step 131260: {'train_ae_loss': 0.66737, 'train_ucc_loss': 0.4181, 'train_ucc_acc': 0.875, 'loss': 0.54274}\n",
            "Step 131280: {'train_ae_loss': 0.6784, 'train_ucc_loss': 0.37211, 'train_ucc_acc': 0.9375, 'loss': 0.52526}\n",
            "Step 131300: {'train_ae_loss': 0.6677, 'train_ucc_loss': 0.47444, 'train_ucc_acc': 0.84375, 'loss': 0.57107}\n",
            "Step 131320: {'train_ae_loss': 0.6559, 'train_ucc_loss': 0.42453, 'train_ucc_acc': 0.90625, 'loss': 0.54022}\n",
            "Step 131340: {'train_ae_loss': 0.6678, 'train_ucc_loss': 0.46541, 'train_ucc_acc': 0.84375, 'loss': 0.56661}\n",
            "Step 131360: {'train_ae_loss': 0.64912, 'train_ucc_loss': 0.3931, 'train_ucc_acc': 0.9375, 'loss': 0.52111}\n",
            "Step 131380: {'train_ae_loss': 0.65541, 'train_ucc_loss': 0.38669, 'train_ucc_acc': 0.9375, 'loss': 0.52105}\n",
            "Step 131400: {'train_ae_loss': 0.66398, 'train_ucc_loss': 0.50232, 'train_ucc_acc': 0.78125, 'loss': 0.58315}\n",
            "Step 131420: {'train_ae_loss': 0.66631, 'train_ucc_loss': 0.41604, 'train_ucc_acc': 0.875, 'loss': 0.54118}\n",
            "Step 131440: {'train_ae_loss': 0.66066, 'train_ucc_loss': 0.44979, 'train_ucc_acc': 0.84375, 'loss': 0.55523}\n",
            "Step 131460: {'train_ae_loss': 0.65295, 'train_ucc_loss': 0.46741, 'train_ucc_acc': 0.84375, 'loss': 0.56018}\n",
            "Step 131480: {'train_ae_loss': 0.66937, 'train_ucc_loss': 0.4686, 'train_ucc_acc': 0.84375, 'loss': 0.56898}\n",
            "Step 131500: {'train_ae_loss': 0.64342, 'train_ucc_loss': 0.39994, 'train_ucc_acc': 0.90625, 'loss': 0.52168}\n",
            "Step 131520: {'train_ae_loss': 0.66349, 'train_ucc_loss': 0.47999, 'train_ucc_acc': 0.8125, 'loss': 0.57174}\n",
            "Step 131540: {'train_ae_loss': 0.68289, 'train_ucc_loss': 0.47151, 'train_ucc_acc': 0.8125, 'loss': 0.5772}\n",
            "Step 131560: {'train_ae_loss': 0.67678, 'train_ucc_loss': 0.42554, 'train_ucc_acc': 0.84375, 'loss': 0.55116}\n",
            "Step 131580: {'train_ae_loss': 0.65819, 'train_ucc_loss': 0.4046, 'train_ucc_acc': 0.90625, 'loss': 0.5314}\n",
            "Step 131600: {'train_ae_loss': 0.67678, 'train_ucc_loss': 0.4413, 'train_ucc_acc': 0.875, 'loss': 0.55904}\n",
            "Step 131620: {'train_ae_loss': 0.67108, 'train_ucc_loss': 0.38426, 'train_ucc_acc': 0.90625, 'loss': 0.52767}\n",
            "Step 131640: {'train_ae_loss': 0.67458, 'train_ucc_loss': 0.39965, 'train_ucc_acc': 0.90625, 'loss': 0.53712}\n",
            "Step 131660: {'train_ae_loss': 0.67345, 'train_ucc_loss': 0.55657, 'train_ucc_acc': 0.78125, 'loss': 0.61501}\n",
            "Step 131680: {'train_ae_loss': 0.67021, 'train_ucc_loss': 0.43299, 'train_ucc_acc': 0.875, 'loss': 0.5516}\n",
            "Step 131700: {'train_ae_loss': 0.69046, 'train_ucc_loss': 0.39279, 'train_ucc_acc': 0.9375, 'loss': 0.54162}\n",
            "Step 131720: {'train_ae_loss': 0.67688, 'train_ucc_loss': 0.38597, 'train_ucc_acc': 0.90625, 'loss': 0.53142}\n",
            "Step 131740: {'train_ae_loss': 0.66737, 'train_ucc_loss': 0.49477, 'train_ucc_acc': 0.8125, 'loss': 0.58107}\n",
            "Step 131760: {'train_ae_loss': 0.66565, 'train_ucc_loss': 0.38087, 'train_ucc_acc': 0.9375, 'loss': 0.52326}\n",
            "Step 131780: {'train_ae_loss': 0.65981, 'train_ucc_loss': 0.49047, 'train_ucc_acc': 0.78125, 'loss': 0.57514}\n",
            "Step 131800: {'train_ae_loss': 0.66517, 'train_ucc_loss': 0.43523, 'train_ucc_acc': 0.875, 'loss': 0.5502}\n",
            "Step 131820: {'train_ae_loss': 0.67787, 'train_ucc_loss': 0.50322, 'train_ucc_acc': 0.78125, 'loss': 0.59055}\n",
            "Step 131840: {'train_ae_loss': 0.66677, 'train_ucc_loss': 0.42348, 'train_ucc_acc': 0.875, 'loss': 0.54512}\n",
            "Step 131860: {'train_ae_loss': 0.66586, 'train_ucc_loss': 0.53276, 'train_ucc_acc': 0.78125, 'loss': 0.59931}\n",
            "Step 131880: {'train_ae_loss': 0.66848, 'train_ucc_loss': 0.442, 'train_ucc_acc': 0.84375, 'loss': 0.55524}\n",
            "Step 131900: {'train_ae_loss': 0.66317, 'train_ucc_loss': 0.46702, 'train_ucc_acc': 0.84375, 'loss': 0.5651}\n",
            "Step 131920: {'train_ae_loss': 0.65685, 'train_ucc_loss': 0.52267, 'train_ucc_acc': 0.78125, 'loss': 0.58976}\n",
            "Step 131940: {'train_ae_loss': 0.67412, 'train_ucc_loss': 0.44627, 'train_ucc_acc': 0.84375, 'loss': 0.56019}\n",
            "Step 131960: {'train_ae_loss': 0.66848, 'train_ucc_loss': 0.42874, 'train_ucc_acc': 0.875, 'loss': 0.54861}\n",
            "Step 131980: {'train_ae_loss': 0.66842, 'train_ucc_loss': 0.4842, 'train_ucc_acc': 0.8125, 'loss': 0.57631}\n",
            "Step 132000: {'train_ae_loss': 0.66765, 'train_ucc_loss': 0.39659, 'train_ucc_acc': 0.90625, 'loss': 0.53212}\n",
            "step: 132000,eval_ae_loss: 0.65327,eval_ucc_loss: 0.47519,eval_ucc_acc: 0.8291\n",
            "Step 132020: {'train_ae_loss': 0.65941, 'train_ucc_loss': 0.44904, 'train_ucc_acc': 0.84375, 'loss': 0.55422}\n",
            "Step 132040: {'train_ae_loss': 0.67797, 'train_ucc_loss': 0.48807, 'train_ucc_acc': 0.8125, 'loss': 0.58302}\n",
            "Step 132060: {'train_ae_loss': 0.68291, 'train_ucc_loss': 0.35534, 'train_ucc_acc': 0.96875, 'loss': 0.51913}\n",
            "Step 132080: {'train_ae_loss': 0.6704, 'train_ucc_loss': 0.54057, 'train_ucc_acc': 0.75, 'loss': 0.60549}\n",
            "Step 132100: {'train_ae_loss': 0.66565, 'train_ucc_loss': 0.587, 'train_ucc_acc': 0.71875, 'loss': 0.62633}\n",
            "Step 132120: {'train_ae_loss': 0.65976, 'train_ucc_loss': 0.58857, 'train_ucc_acc': 0.71875, 'loss': 0.62417}\n",
            "Step 132140: {'train_ae_loss': 0.65263, 'train_ucc_loss': 0.51827, 'train_ucc_acc': 0.78125, 'loss': 0.58545}\n",
            "Step 132160: {'train_ae_loss': 0.6649, 'train_ucc_loss': 0.3785, 'train_ucc_acc': 0.9375, 'loss': 0.5217}\n",
            "Step 132180: {'train_ae_loss': 0.67621, 'train_ucc_loss': 0.48164, 'train_ucc_acc': 0.8125, 'loss': 0.57892}\n",
            "Step 132200: {'train_ae_loss': 0.66946, 'train_ucc_loss': 0.4199, 'train_ucc_acc': 0.90625, 'loss': 0.54468}\n",
            "Step 132220: {'train_ae_loss': 0.67236, 'train_ucc_loss': 0.43253, 'train_ucc_acc': 0.875, 'loss': 0.55244}\n",
            "Step 132240: {'train_ae_loss': 0.67868, 'train_ucc_loss': 0.4294, 'train_ucc_acc': 0.90625, 'loss': 0.55404}\n",
            "Step 132260: {'train_ae_loss': 0.67243, 'train_ucc_loss': 0.47358, 'train_ucc_acc': 0.8125, 'loss': 0.573}\n",
            "Step 132280: {'train_ae_loss': 0.63145, 'train_ucc_loss': 0.45243, 'train_ucc_acc': 0.875, 'loss': 0.54194}\n",
            "Step 132300: {'train_ae_loss': 0.67659, 'train_ucc_loss': 0.39536, 'train_ucc_acc': 0.90625, 'loss': 0.53598}\n",
            "Step 132320: {'train_ae_loss': 0.6674, 'train_ucc_loss': 0.46281, 'train_ucc_acc': 0.84375, 'loss': 0.5651}\n",
            "Step 132340: {'train_ae_loss': 0.64159, 'train_ucc_loss': 0.48057, 'train_ucc_acc': 0.84375, 'loss': 0.56108}\n",
            "Step 132360: {'train_ae_loss': 0.64991, 'train_ucc_loss': 0.50801, 'train_ucc_acc': 0.78125, 'loss': 0.57896}\n",
            "Step 132380: {'train_ae_loss': 0.66098, 'train_ucc_loss': 0.43444, 'train_ucc_acc': 0.875, 'loss': 0.54771}\n",
            "Step 132400: {'train_ae_loss': 0.6664, 'train_ucc_loss': 0.38731, 'train_ucc_acc': 0.9375, 'loss': 0.52685}\n",
            "Step 132420: {'train_ae_loss': 0.66444, 'train_ucc_loss': 0.42371, 'train_ucc_acc': 0.90625, 'loss': 0.54408}\n",
            "Step 132440: {'train_ae_loss': 0.67035, 'train_ucc_loss': 0.37409, 'train_ucc_acc': 0.9375, 'loss': 0.52222}\n",
            "Step 132460: {'train_ae_loss': 0.67239, 'train_ucc_loss': 0.51352, 'train_ucc_acc': 0.8125, 'loss': 0.59295}\n",
            "Step 132480: {'train_ae_loss': 0.65101, 'train_ucc_loss': 0.41383, 'train_ucc_acc': 0.90625, 'loss': 0.53242}\n",
            "Step 132500: {'train_ae_loss': 0.67758, 'train_ucc_loss': 0.40567, 'train_ucc_acc': 0.90625, 'loss': 0.54162}\n",
            "Step 132520: {'train_ae_loss': 0.66065, 'train_ucc_loss': 0.47865, 'train_ucc_acc': 0.84375, 'loss': 0.56965}\n",
            "Step 132540: {'train_ae_loss': 0.65512, 'train_ucc_loss': 0.45784, 'train_ucc_acc': 0.84375, 'loss': 0.55648}\n",
            "Step 132560: {'train_ae_loss': 0.65525, 'train_ucc_loss': 0.39056, 'train_ucc_acc': 0.90625, 'loss': 0.52291}\n",
            "Step 132580: {'train_ae_loss': 0.646, 'train_ucc_loss': 0.52195, 'train_ucc_acc': 0.78125, 'loss': 0.58397}\n",
            "Step 132600: {'train_ae_loss': 0.68347, 'train_ucc_loss': 0.38007, 'train_ucc_acc': 0.9375, 'loss': 0.53177}\n",
            "Step 132620: {'train_ae_loss': 0.65209, 'train_ucc_loss': 0.42225, 'train_ucc_acc': 0.90625, 'loss': 0.53717}\n",
            "Step 132640: {'train_ae_loss': 0.66126, 'train_ucc_loss': 0.38404, 'train_ucc_acc': 0.9375, 'loss': 0.52265}\n",
            "Step 132660: {'train_ae_loss': 0.66188, 'train_ucc_loss': 0.40001, 'train_ucc_acc': 0.90625, 'loss': 0.53095}\n",
            "Step 132680: {'train_ae_loss': 0.67395, 'train_ucc_loss': 0.43366, 'train_ucc_acc': 0.875, 'loss': 0.5538}\n",
            "Step 132700: {'train_ae_loss': 0.65864, 'train_ucc_loss': 0.43025, 'train_ucc_acc': 0.875, 'loss': 0.54445}\n",
            "Step 132720: {'train_ae_loss': 0.66951, 'train_ucc_loss': 0.41544, 'train_ucc_acc': 0.90625, 'loss': 0.54248}\n",
            "Step 132740: {'train_ae_loss': 0.68308, 'train_ucc_loss': 0.46453, 'train_ucc_acc': 0.84375, 'loss': 0.5738}\n",
            "Step 132760: {'train_ae_loss': 0.68549, 'train_ucc_loss': 0.49748, 'train_ucc_acc': 0.8125, 'loss': 0.59148}\n",
            "Step 132780: {'train_ae_loss': 0.66542, 'train_ucc_loss': 0.40681, 'train_ucc_acc': 0.90625, 'loss': 0.53611}\n",
            "Step 132800: {'train_ae_loss': 0.66281, 'train_ucc_loss': 0.4724, 'train_ucc_acc': 0.84375, 'loss': 0.56761}\n",
            "Step 132820: {'train_ae_loss': 0.67087, 'train_ucc_loss': 0.45138, 'train_ucc_acc': 0.84375, 'loss': 0.56113}\n",
            "Step 132840: {'train_ae_loss': 0.65476, 'train_ucc_loss': 0.46466, 'train_ucc_acc': 0.8125, 'loss': 0.55971}\n",
            "Step 132860: {'train_ae_loss': 0.64552, 'train_ucc_loss': 0.42713, 'train_ucc_acc': 0.875, 'loss': 0.53632}\n",
            "Step 132880: {'train_ae_loss': 0.66021, 'train_ucc_loss': 0.47428, 'train_ucc_acc': 0.84375, 'loss': 0.56725}\n",
            "Step 132900: {'train_ae_loss': 0.67794, 'train_ucc_loss': 0.43148, 'train_ucc_acc': 0.875, 'loss': 0.55471}\n",
            "Step 132920: {'train_ae_loss': 0.66035, 'train_ucc_loss': 0.38968, 'train_ucc_acc': 0.9375, 'loss': 0.52501}\n",
            "Step 132940: {'train_ae_loss': 0.65562, 'train_ucc_loss': 0.38151, 'train_ucc_acc': 0.9375, 'loss': 0.51857}\n",
            "Step 132960: {'train_ae_loss': 0.66213, 'train_ucc_loss': 0.41681, 'train_ucc_acc': 0.90625, 'loss': 0.53947}\n",
            "Step 132980: {'train_ae_loss': 0.6689, 'train_ucc_loss': 0.48318, 'train_ucc_acc': 0.8125, 'loss': 0.57604}\n",
            "Step 133000: {'train_ae_loss': 0.65089, 'train_ucc_loss': 0.50471, 'train_ucc_acc': 0.8125, 'loss': 0.5778}\n",
            "step: 133000,eval_ae_loss: 0.65167,eval_ucc_loss: 0.46647,eval_ucc_acc: 0.83984\n",
            "Step 133020: {'train_ae_loss': 0.66087, 'train_ucc_loss': 0.39824, 'train_ucc_acc': 0.90625, 'loss': 0.52956}\n",
            "Step 133040: {'train_ae_loss': 0.67305, 'train_ucc_loss': 0.39173, 'train_ucc_acc': 0.9375, 'loss': 0.53239}\n",
            "Step 133060: {'train_ae_loss': 0.66548, 'train_ucc_loss': 0.38077, 'train_ucc_acc': 0.9375, 'loss': 0.52312}\n",
            "Step 133080: {'train_ae_loss': 0.66818, 'train_ucc_loss': 0.48663, 'train_ucc_acc': 0.8125, 'loss': 0.5774}\n",
            "Step 133100: {'train_ae_loss': 0.66496, 'train_ucc_loss': 0.45283, 'train_ucc_acc': 0.84375, 'loss': 0.5589}\n",
            "Step 133120: {'train_ae_loss': 0.65311, 'train_ucc_loss': 0.38586, 'train_ucc_acc': 0.90625, 'loss': 0.51949}\n",
            "Step 133140: {'train_ae_loss': 0.65526, 'train_ucc_loss': 0.35299, 'train_ucc_acc': 0.9375, 'loss': 0.50412}\n",
            "Step 133160: {'train_ae_loss': 0.66373, 'train_ucc_loss': 0.41295, 'train_ucc_acc': 0.90625, 'loss': 0.53834}\n",
            "Step 133180: {'train_ae_loss': 0.63749, 'train_ucc_loss': 0.41801, 'train_ucc_acc': 0.90625, 'loss': 0.52775}\n",
            "Step 133200: {'train_ae_loss': 0.67066, 'train_ucc_loss': 0.394, 'train_ucc_acc': 0.84375, 'loss': 0.53233}\n",
            "Step 133220: {'train_ae_loss': 0.63591, 'train_ucc_loss': 0.55614, 'train_ucc_acc': 0.71875, 'loss': 0.59603}\n",
            "Step 133240: {'train_ae_loss': 0.66324, 'train_ucc_loss': 0.41229, 'train_ucc_acc': 0.90625, 'loss': 0.53777}\n",
            "Step 133260: {'train_ae_loss': 0.66751, 'train_ucc_loss': 0.42668, 'train_ucc_acc': 0.90625, 'loss': 0.54709}\n",
            "Step 133280: {'train_ae_loss': 0.66334, 'train_ucc_loss': 0.40144, 'train_ucc_acc': 0.90625, 'loss': 0.53239}\n",
            "Step 133300: {'train_ae_loss': 0.63972, 'train_ucc_loss': 0.46625, 'train_ucc_acc': 0.875, 'loss': 0.55298}\n",
            "Step 133320: {'train_ae_loss': 0.6614, 'train_ucc_loss': 0.40834, 'train_ucc_acc': 0.90625, 'loss': 0.53487}\n",
            "Step 133340: {'train_ae_loss': 0.68, 'train_ucc_loss': 0.40247, 'train_ucc_acc': 0.90625, 'loss': 0.54123}\n",
            "Step 133360: {'train_ae_loss': 0.65383, 'train_ucc_loss': 0.43966, 'train_ucc_acc': 0.875, 'loss': 0.54675}\n",
            "Step 133380: {'train_ae_loss': 0.65024, 'train_ucc_loss': 0.46455, 'train_ucc_acc': 0.84375, 'loss': 0.55739}\n",
            "Step 133400: {'train_ae_loss': 0.66262, 'train_ucc_loss': 0.42476, 'train_ucc_acc': 0.90625, 'loss': 0.54369}\n",
            "Step 133420: {'train_ae_loss': 0.68192, 'train_ucc_loss': 0.46564, 'train_ucc_acc': 0.84375, 'loss': 0.57378}\n",
            "Step 133440: {'train_ae_loss': 0.65481, 'train_ucc_loss': 0.569, 'train_ucc_acc': 0.75, 'loss': 0.6119}\n",
            "Step 133460: {'train_ae_loss': 0.6514, 'train_ucc_loss': 0.47278, 'train_ucc_acc': 0.84375, 'loss': 0.56209}\n",
            "Step 133480: {'train_ae_loss': 0.66655, 'train_ucc_loss': 0.37973, 'train_ucc_acc': 0.9375, 'loss': 0.52314}\n",
            "Step 133500: {'train_ae_loss': 0.6615, 'train_ucc_loss': 0.41755, 'train_ucc_acc': 0.875, 'loss': 0.53953}\n",
            "Step 133520: {'train_ae_loss': 0.6639, 'train_ucc_loss': 0.40934, 'train_ucc_acc': 0.875, 'loss': 0.53662}\n",
            "Step 133540: {'train_ae_loss': 0.65589, 'train_ucc_loss': 0.43226, 'train_ucc_acc': 0.875, 'loss': 0.54407}\n",
            "Step 133560: {'train_ae_loss': 0.66457, 'train_ucc_loss': 0.4692, 'train_ucc_acc': 0.8125, 'loss': 0.56689}\n",
            "Step 133580: {'train_ae_loss': 0.65916, 'train_ucc_loss': 0.55541, 'train_ucc_acc': 0.71875, 'loss': 0.60729}\n",
            "Step 133600: {'train_ae_loss': 0.65137, 'train_ucc_loss': 0.54855, 'train_ucc_acc': 0.75, 'loss': 0.59996}\n",
            "Step 133620: {'train_ae_loss': 0.68414, 'train_ucc_loss': 0.33736, 'train_ucc_acc': 0.96875, 'loss': 0.51075}\n",
            "Step 133640: {'train_ae_loss': 0.65983, 'train_ucc_loss': 0.40694, 'train_ucc_acc': 0.90625, 'loss': 0.53339}\n",
            "Step 133660: {'train_ae_loss': 0.66366, 'train_ucc_loss': 0.33243, 'train_ucc_acc': 1.0, 'loss': 0.49804}\n",
            "Step 133680: {'train_ae_loss': 0.66729, 'train_ucc_loss': 0.45351, 'train_ucc_acc': 0.875, 'loss': 0.5604}\n",
            "Step 133700: {'train_ae_loss': 0.66326, 'train_ucc_loss': 0.41432, 'train_ucc_acc': 0.875, 'loss': 0.53879}\n",
            "Step 133720: {'train_ae_loss': 0.67115, 'train_ucc_loss': 0.43437, 'train_ucc_acc': 0.875, 'loss': 0.55276}\n",
            "Step 133740: {'train_ae_loss': 0.66976, 'train_ucc_loss': 0.44762, 'train_ucc_acc': 0.875, 'loss': 0.55869}\n",
            "Step 133760: {'train_ae_loss': 0.6654, 'train_ucc_loss': 0.43445, 'train_ucc_acc': 0.84375, 'loss': 0.54992}\n",
            "Step 133780: {'train_ae_loss': 0.65646, 'train_ucc_loss': 0.5351, 'train_ucc_acc': 0.78125, 'loss': 0.59578}\n",
            "Step 133800: {'train_ae_loss': 0.64538, 'train_ucc_loss': 0.48249, 'train_ucc_acc': 0.84375, 'loss': 0.56394}\n",
            "Step 133820: {'train_ae_loss': 0.66426, 'train_ucc_loss': 0.46701, 'train_ucc_acc': 0.84375, 'loss': 0.56563}\n",
            "Step 133840: {'train_ae_loss': 0.66032, 'train_ucc_loss': 0.47922, 'train_ucc_acc': 0.8125, 'loss': 0.56977}\n",
            "Step 133860: {'train_ae_loss': 0.64153, 'train_ucc_loss': 0.53972, 'train_ucc_acc': 0.75, 'loss': 0.59062}\n",
            "Step 133880: {'train_ae_loss': 0.68103, 'train_ucc_loss': 0.43222, 'train_ucc_acc': 0.875, 'loss': 0.55663}\n",
            "Step 133900: {'train_ae_loss': 0.65499, 'train_ucc_loss': 0.47858, 'train_ucc_acc': 0.8125, 'loss': 0.56678}\n",
            "Step 133920: {'train_ae_loss': 0.64893, 'train_ucc_loss': 0.44808, 'train_ucc_acc': 0.875, 'loss': 0.5485}\n",
            "Step 133940: {'train_ae_loss': 0.6622, 'train_ucc_loss': 0.46573, 'train_ucc_acc': 0.84375, 'loss': 0.56396}\n",
            "Step 133960: {'train_ae_loss': 0.66632, 'train_ucc_loss': 0.40235, 'train_ucc_acc': 0.90625, 'loss': 0.53434}\n",
            "Step 133980: {'train_ae_loss': 0.66797, 'train_ucc_loss': 0.35218, 'train_ucc_acc': 0.96875, 'loss': 0.51007}\n",
            "Step 134000: {'train_ae_loss': 0.66786, 'train_ucc_loss': 0.37751, 'train_ucc_acc': 0.9375, 'loss': 0.52268}\n",
            "step: 134000,eval_ae_loss: 0.65109,eval_ucc_loss: 0.47036,eval_ucc_acc: 0.83984\n",
            "Step 134020: {'train_ae_loss': 0.66347, 'train_ucc_loss': 0.46415, 'train_ucc_acc': 0.875, 'loss': 0.56381}\n",
            "Step 134040: {'train_ae_loss': 0.65658, 'train_ucc_loss': 0.45774, 'train_ucc_acc': 0.8125, 'loss': 0.55716}\n",
            "Step 134060: {'train_ae_loss': 0.66309, 'train_ucc_loss': 0.40658, 'train_ucc_acc': 0.90625, 'loss': 0.53484}\n",
            "Step 134080: {'train_ae_loss': 0.65489, 'train_ucc_loss': 0.38032, 'train_ucc_acc': 0.90625, 'loss': 0.51761}\n",
            "Step 134100: {'train_ae_loss': 0.67311, 'train_ucc_loss': 0.387, 'train_ucc_acc': 0.90625, 'loss': 0.53005}\n",
            "Step 134120: {'train_ae_loss': 0.65564, 'train_ucc_loss': 0.51653, 'train_ucc_acc': 0.8125, 'loss': 0.58609}\n",
            "Step 134140: {'train_ae_loss': 0.65267, 'train_ucc_loss': 0.45225, 'train_ucc_acc': 0.875, 'loss': 0.55246}\n",
            "Step 134160: {'train_ae_loss': 0.66388, 'train_ucc_loss': 0.56879, 'train_ucc_acc': 0.75, 'loss': 0.61634}\n",
            "Step 134180: {'train_ae_loss': 0.67478, 'train_ucc_loss': 0.43732, 'train_ucc_acc': 0.84375, 'loss': 0.55605}\n",
            "Step 134200: {'train_ae_loss': 0.66071, 'train_ucc_loss': 0.42738, 'train_ucc_acc': 0.875, 'loss': 0.54404}\n",
            "Step 134220: {'train_ae_loss': 0.66041, 'train_ucc_loss': 0.37449, 'train_ucc_acc': 0.9375, 'loss': 0.51745}\n",
            "Step 134240: {'train_ae_loss': 0.66915, 'train_ucc_loss': 0.44607, 'train_ucc_acc': 0.84375, 'loss': 0.55761}\n",
            "Step 134260: {'train_ae_loss': 0.66902, 'train_ucc_loss': 0.32615, 'train_ucc_acc': 1.0, 'loss': 0.49758}\n",
            "Step 134280: {'train_ae_loss': 0.66149, 'train_ucc_loss': 0.42228, 'train_ucc_acc': 0.875, 'loss': 0.54189}\n",
            "Step 134300: {'train_ae_loss': 0.68299, 'train_ucc_loss': 0.41463, 'train_ucc_acc': 0.90625, 'loss': 0.54881}\n",
            "Step 134320: {'train_ae_loss': 0.65163, 'train_ucc_loss': 0.41974, 'train_ucc_acc': 0.875, 'loss': 0.53568}\n",
            "Step 134340: {'train_ae_loss': 0.66934, 'train_ucc_loss': 0.41391, 'train_ucc_acc': 0.90625, 'loss': 0.54163}\n",
            "Step 134360: {'train_ae_loss': 0.68408, 'train_ucc_loss': 0.42399, 'train_ucc_acc': 0.875, 'loss': 0.55404}\n",
            "Step 134380: {'train_ae_loss': 0.65808, 'train_ucc_loss': 0.46368, 'train_ucc_acc': 0.8125, 'loss': 0.56088}\n",
            "Step 134400: {'train_ae_loss': 0.67242, 'train_ucc_loss': 0.46909, 'train_ucc_acc': 0.84375, 'loss': 0.57075}\n",
            "Step 134420: {'train_ae_loss': 0.64649, 'train_ucc_loss': 0.4559, 'train_ucc_acc': 0.84375, 'loss': 0.55119}\n",
            "Step 134440: {'train_ae_loss': 0.66803, 'train_ucc_loss': 0.47543, 'train_ucc_acc': 0.8125, 'loss': 0.57173}\n",
            "Step 134460: {'train_ae_loss': 0.66818, 'train_ucc_loss': 0.43384, 'train_ucc_acc': 0.875, 'loss': 0.55101}\n",
            "Step 134480: {'train_ae_loss': 0.6592, 'train_ucc_loss': 0.47158, 'train_ucc_acc': 0.84375, 'loss': 0.56539}\n",
            "Step 134500: {'train_ae_loss': 0.64969, 'train_ucc_loss': 0.47868, 'train_ucc_acc': 0.8125, 'loss': 0.56419}\n",
            "Step 134520: {'train_ae_loss': 0.64889, 'train_ucc_loss': 0.44265, 'train_ucc_acc': 0.875, 'loss': 0.54577}\n",
            "Step 134540: {'train_ae_loss': 0.6605, 'train_ucc_loss': 0.37451, 'train_ucc_acc': 0.9375, 'loss': 0.5175}\n",
            "Step 134560: {'train_ae_loss': 0.66459, 'train_ucc_loss': 0.46402, 'train_ucc_acc': 0.875, 'loss': 0.5643}\n",
            "Step 134580: {'train_ae_loss': 0.66697, 'train_ucc_loss': 0.37218, 'train_ucc_acc': 0.90625, 'loss': 0.51958}\n",
            "Step 134600: {'train_ae_loss': 0.6806, 'train_ucc_loss': 0.34481, 'train_ucc_acc': 0.96875, 'loss': 0.5127}\n",
            "Step 134620: {'train_ae_loss': 0.6439, 'train_ucc_loss': 0.45551, 'train_ucc_acc': 0.875, 'loss': 0.5497}\n",
            "Step 134640: {'train_ae_loss': 0.6649, 'train_ucc_loss': 0.45573, 'train_ucc_acc': 0.84375, 'loss': 0.56031}\n",
            "Step 134660: {'train_ae_loss': 0.65504, 'train_ucc_loss': 0.36431, 'train_ucc_acc': 0.9375, 'loss': 0.50967}\n",
            "Step 134680: {'train_ae_loss': 0.66757, 'train_ucc_loss': 0.38743, 'train_ucc_acc': 0.9375, 'loss': 0.5275}\n",
            "Step 134700: {'train_ae_loss': 0.65291, 'train_ucc_loss': 0.45231, 'train_ucc_acc': 0.84375, 'loss': 0.55261}\n",
            "Step 134720: {'train_ae_loss': 0.64704, 'train_ucc_loss': 0.39648, 'train_ucc_acc': 0.90625, 'loss': 0.52176}\n",
            "Step 134740: {'train_ae_loss': 0.66122, 'train_ucc_loss': 0.32714, 'train_ucc_acc': 1.0, 'loss': 0.49418}\n",
            "Step 134760: {'train_ae_loss': 0.66716, 'train_ucc_loss': 0.45357, 'train_ucc_acc': 0.875, 'loss': 0.56036}\n",
            "Step 134780: {'train_ae_loss': 0.64828, 'train_ucc_loss': 0.32042, 'train_ucc_acc': 1.0, 'loss': 0.48435}\n",
            "Step 134800: {'train_ae_loss': 0.65706, 'train_ucc_loss': 0.45595, 'train_ucc_acc': 0.84375, 'loss': 0.5565}\n",
            "Step 134820: {'train_ae_loss': 0.64852, 'train_ucc_loss': 0.38221, 'train_ucc_acc': 0.9375, 'loss': 0.51537}\n",
            "Step 134840: {'train_ae_loss': 0.65874, 'train_ucc_loss': 0.40345, 'train_ucc_acc': 0.9375, 'loss': 0.53109}\n",
            "Step 134860: {'train_ae_loss': 0.63799, 'train_ucc_loss': 0.41128, 'train_ucc_acc': 0.90625, 'loss': 0.52464}\n",
            "Step 134880: {'train_ae_loss': 0.65331, 'train_ucc_loss': 0.4557, 'train_ucc_acc': 0.8125, 'loss': 0.5545}\n",
            "Step 134900: {'train_ae_loss': 0.64548, 'train_ucc_loss': 0.40874, 'train_ucc_acc': 0.90625, 'loss': 0.52711}\n",
            "Step 134920: {'train_ae_loss': 0.67195, 'train_ucc_loss': 0.40361, 'train_ucc_acc': 0.90625, 'loss': 0.53778}\n",
            "Step 134940: {'train_ae_loss': 0.66851, 'train_ucc_loss': 0.41166, 'train_ucc_acc': 0.90625, 'loss': 0.54009}\n",
            "Step 134960: {'train_ae_loss': 0.67264, 'train_ucc_loss': 0.35661, 'train_ucc_acc': 0.96875, 'loss': 0.51463}\n",
            "Step 134980: {'train_ae_loss': 0.65594, 'train_ucc_loss': 0.48765, 'train_ucc_acc': 0.8125, 'loss': 0.57179}\n",
            "Step 135000: {'train_ae_loss': 0.66249, 'train_ucc_loss': 0.4623, 'train_ucc_acc': 0.84375, 'loss': 0.56239}\n",
            "step: 135000,eval_ae_loss: 0.64485,eval_ucc_loss: 0.49645,eval_ucc_acc: 0.80566\n",
            "Step 135020: {'train_ae_loss': 0.66713, 'train_ucc_loss': 0.48359, 'train_ucc_acc': 0.8125, 'loss': 0.57536}\n",
            "Step 135040: {'train_ae_loss': 0.63317, 'train_ucc_loss': 0.43777, 'train_ucc_acc': 0.875, 'loss': 0.53547}\n",
            "Step 135060: {'train_ae_loss': 0.67008, 'train_ucc_loss': 0.5263, 'train_ucc_acc': 0.78125, 'loss': 0.59819}\n",
            "Step 135080: {'train_ae_loss': 0.66894, 'train_ucc_loss': 0.36025, 'train_ucc_acc': 0.96875, 'loss': 0.51459}\n",
            "Step 135100: {'train_ae_loss': 0.65165, 'train_ucc_loss': 0.45497, 'train_ucc_acc': 0.84375, 'loss': 0.55331}\n",
            "Step 135120: {'train_ae_loss': 0.66263, 'train_ucc_loss': 0.55296, 'train_ucc_acc': 0.75, 'loss': 0.60779}\n",
            "Step 135140: {'train_ae_loss': 0.65333, 'train_ucc_loss': 0.40174, 'train_ucc_acc': 0.90625, 'loss': 0.52753}\n",
            "Step 135160: {'train_ae_loss': 0.67461, 'train_ucc_loss': 0.41566, 'train_ucc_acc': 0.90625, 'loss': 0.54513}\n",
            "Step 135180: {'train_ae_loss': 0.65934, 'train_ucc_loss': 0.39333, 'train_ucc_acc': 0.9375, 'loss': 0.52634}\n",
            "Step 135200: {'train_ae_loss': 0.66415, 'train_ucc_loss': 0.44786, 'train_ucc_acc': 0.875, 'loss': 0.55601}\n",
            "Step 135220: {'train_ae_loss': 0.66111, 'train_ucc_loss': 0.41249, 'train_ucc_acc': 0.90625, 'loss': 0.5368}\n",
            "Step 135240: {'train_ae_loss': 0.6591, 'train_ucc_loss': 0.34529, 'train_ucc_acc': 0.96875, 'loss': 0.50219}\n",
            "Step 135260: {'train_ae_loss': 0.64598, 'train_ucc_loss': 0.53594, 'train_ucc_acc': 0.75, 'loss': 0.59096}\n",
            "Step 135280: {'train_ae_loss': 0.66874, 'train_ucc_loss': 0.49733, 'train_ucc_acc': 0.8125, 'loss': 0.58303}\n",
            "Step 135300: {'train_ae_loss': 0.65684, 'train_ucc_loss': 0.40673, 'train_ucc_acc': 0.90625, 'loss': 0.53179}\n",
            "Step 135320: {'train_ae_loss': 0.67077, 'train_ucc_loss': 0.38696, 'train_ucc_acc': 0.90625, 'loss': 0.52886}\n",
            "Step 135340: {'train_ae_loss': 0.65727, 'train_ucc_loss': 0.46005, 'train_ucc_acc': 0.875, 'loss': 0.55866}\n",
            "Step 135360: {'train_ae_loss': 0.67143, 'train_ucc_loss': 0.48837, 'train_ucc_acc': 0.84375, 'loss': 0.5799}\n",
            "Step 135380: {'train_ae_loss': 0.66428, 'train_ucc_loss': 0.41567, 'train_ucc_acc': 0.90625, 'loss': 0.53997}\n",
            "Step 135400: {'train_ae_loss': 0.6736, 'train_ucc_loss': 0.46026, 'train_ucc_acc': 0.84375, 'loss': 0.56693}\n",
            "Step 135420: {'train_ae_loss': 0.65575, 'train_ucc_loss': 0.49644, 'train_ucc_acc': 0.8125, 'loss': 0.57609}\n",
            "Step 135440: {'train_ae_loss': 0.65852, 'train_ucc_loss': 0.52131, 'train_ucc_acc': 0.75, 'loss': 0.58992}\n",
            "Step 135460: {'train_ae_loss': 0.65087, 'train_ucc_loss': 0.42853, 'train_ucc_acc': 0.875, 'loss': 0.5397}\n",
            "Step 135480: {'train_ae_loss': 0.64149, 'train_ucc_loss': 0.43924, 'train_ucc_acc': 0.84375, 'loss': 0.54036}\n",
            "Step 135500: {'train_ae_loss': 0.64245, 'train_ucc_loss': 0.40053, 'train_ucc_acc': 0.90625, 'loss': 0.52149}\n",
            "Step 135520: {'train_ae_loss': 0.6682, 'train_ucc_loss': 0.38268, 'train_ucc_acc': 0.90625, 'loss': 0.52544}\n",
            "Step 135540: {'train_ae_loss': 0.65365, 'train_ucc_loss': 0.42163, 'train_ucc_acc': 0.90625, 'loss': 0.53764}\n",
            "Step 135560: {'train_ae_loss': 0.65658, 'train_ucc_loss': 0.37478, 'train_ucc_acc': 0.9375, 'loss': 0.51568}\n",
            "Step 135580: {'train_ae_loss': 0.65615, 'train_ucc_loss': 0.43018, 'train_ucc_acc': 0.875, 'loss': 0.54316}\n",
            "Step 135600: {'train_ae_loss': 0.66117, 'train_ucc_loss': 0.45345, 'train_ucc_acc': 0.84375, 'loss': 0.55731}\n",
            "Step 135620: {'train_ae_loss': 0.6741, 'train_ucc_loss': 0.46681, 'train_ucc_acc': 0.8125, 'loss': 0.57046}\n",
            "Step 135640: {'train_ae_loss': 0.64286, 'train_ucc_loss': 0.5625, 'train_ucc_acc': 0.71875, 'loss': 0.60268}\n",
            "Step 135660: {'train_ae_loss': 0.66713, 'train_ucc_loss': 0.52463, 'train_ucc_acc': 0.78125, 'loss': 0.59588}\n",
            "Step 135680: {'train_ae_loss': 0.65855, 'train_ucc_loss': 0.49982, 'train_ucc_acc': 0.8125, 'loss': 0.57918}\n",
            "Step 135700: {'train_ae_loss': 0.66141, 'train_ucc_loss': 0.43048, 'train_ucc_acc': 0.875, 'loss': 0.54594}\n",
            "Step 135720: {'train_ae_loss': 0.64909, 'train_ucc_loss': 0.39009, 'train_ucc_acc': 0.9375, 'loss': 0.51959}\n",
            "Step 135740: {'train_ae_loss': 0.66867, 'train_ucc_loss': 0.60621, 'train_ucc_acc': 0.6875, 'loss': 0.63744}\n",
            "Step 135760: {'train_ae_loss': 0.67239, 'train_ucc_loss': 0.39328, 'train_ucc_acc': 0.90625, 'loss': 0.53283}\n",
            "Step 135780: {'train_ae_loss': 0.66389, 'train_ucc_loss': 0.45786, 'train_ucc_acc': 0.875, 'loss': 0.56087}\n",
            "Step 135800: {'train_ae_loss': 0.65822, 'train_ucc_loss': 0.36648, 'train_ucc_acc': 0.9375, 'loss': 0.51235}\n",
            "Step 135820: {'train_ae_loss': 0.65306, 'train_ucc_loss': 0.45378, 'train_ucc_acc': 0.84375, 'loss': 0.55342}\n",
            "Step 135840: {'train_ae_loss': 0.65789, 'train_ucc_loss': 0.43671, 'train_ucc_acc': 0.875, 'loss': 0.5473}\n",
            "Step 135860: {'train_ae_loss': 0.67888, 'train_ucc_loss': 0.34942, 'train_ucc_acc': 0.96875, 'loss': 0.51415}\n",
            "Step 135880: {'train_ae_loss': 0.6579, 'train_ucc_loss': 0.50635, 'train_ucc_acc': 0.8125, 'loss': 0.58212}\n",
            "Step 135900: {'train_ae_loss': 0.65978, 'train_ucc_loss': 0.39024, 'train_ucc_acc': 0.9375, 'loss': 0.52501}\n",
            "Step 135920: {'train_ae_loss': 0.66034, 'train_ucc_loss': 0.4162, 'train_ucc_acc': 0.90625, 'loss': 0.53827}\n",
            "Step 135940: {'train_ae_loss': 0.66553, 'train_ucc_loss': 0.37984, 'train_ucc_acc': 0.9375, 'loss': 0.52268}\n",
            "Step 135960: {'train_ae_loss': 0.65981, 'train_ucc_loss': 0.4175, 'train_ucc_acc': 0.90625, 'loss': 0.53866}\n",
            "Step 135980: {'train_ae_loss': 0.67536, 'train_ucc_loss': 0.42721, 'train_ucc_acc': 0.90625, 'loss': 0.55129}\n",
            "Step 136000: {'train_ae_loss': 0.6762, 'train_ucc_loss': 0.45133, 'train_ucc_acc': 0.84375, 'loss': 0.56376}\n",
            "step: 136000,eval_ae_loss: 0.65032,eval_ucc_loss: 0.50806,eval_ucc_acc: 0.79785\n",
            "Step 136020: {'train_ae_loss': 0.66254, 'train_ucc_loss': 0.34614, 'train_ucc_acc': 1.0, 'loss': 0.50434}\n",
            "Step 136040: {'train_ae_loss': 0.66348, 'train_ucc_loss': 0.41635, 'train_ucc_acc': 0.90625, 'loss': 0.53992}\n",
            "Step 136060: {'train_ae_loss': 0.66415, 'train_ucc_loss': 0.41867, 'train_ucc_acc': 0.90625, 'loss': 0.54141}\n",
            "Step 136080: {'train_ae_loss': 0.66827, 'train_ucc_loss': 0.3775, 'train_ucc_acc': 0.9375, 'loss': 0.52288}\n",
            "Step 136100: {'train_ae_loss': 0.65377, 'train_ucc_loss': 0.48673, 'train_ucc_acc': 0.84375, 'loss': 0.57025}\n",
            "Step 136120: {'train_ae_loss': 0.66622, 'train_ucc_loss': 0.4437, 'train_ucc_acc': 0.8125, 'loss': 0.55496}\n",
            "Step 136140: {'train_ae_loss': 0.65145, 'train_ucc_loss': 0.43557, 'train_ucc_acc': 0.875, 'loss': 0.54351}\n",
            "Step 136160: {'train_ae_loss': 0.65257, 'train_ucc_loss': 0.40999, 'train_ucc_acc': 0.90625, 'loss': 0.53128}\n",
            "Step 136180: {'train_ae_loss': 0.6618, 'train_ucc_loss': 0.39409, 'train_ucc_acc': 0.90625, 'loss': 0.52794}\n",
            "Step 136200: {'train_ae_loss': 0.65655, 'train_ucc_loss': 0.49219, 'train_ucc_acc': 0.8125, 'loss': 0.57437}\n",
            "Step 136220: {'train_ae_loss': 0.65466, 'train_ucc_loss': 0.35313, 'train_ucc_acc': 0.96875, 'loss': 0.50389}\n",
            "Step 136240: {'train_ae_loss': 0.65282, 'train_ucc_loss': 0.34583, 'train_ucc_acc': 1.0, 'loss': 0.49932}\n",
            "Step 136260: {'train_ae_loss': 0.65437, 'train_ucc_loss': 0.42762, 'train_ucc_acc': 0.875, 'loss': 0.541}\n",
            "Step 136280: {'train_ae_loss': 0.65664, 'train_ucc_loss': 0.37651, 'train_ucc_acc': 0.9375, 'loss': 0.51657}\n",
            "Step 136300: {'train_ae_loss': 0.62872, 'train_ucc_loss': 0.5421, 'train_ucc_acc': 0.78125, 'loss': 0.58541}\n",
            "Step 136320: {'train_ae_loss': 0.67346, 'train_ucc_loss': 0.47125, 'train_ucc_acc': 0.84375, 'loss': 0.57235}\n",
            "Step 136340: {'train_ae_loss': 0.66207, 'train_ucc_loss': 0.44188, 'train_ucc_acc': 0.875, 'loss': 0.55198}\n",
            "Step 136360: {'train_ae_loss': 0.6466, 'train_ucc_loss': 0.44983, 'train_ucc_acc': 0.84375, 'loss': 0.54821}\n",
            "Step 136380: {'train_ae_loss': 0.64451, 'train_ucc_loss': 0.47615, 'train_ucc_acc': 0.84375, 'loss': 0.56033}\n",
            "Step 136400: {'train_ae_loss': 0.6441, 'train_ucc_loss': 0.37906, 'train_ucc_acc': 0.9375, 'loss': 0.51158}\n",
            "Step 136420: {'train_ae_loss': 0.66975, 'train_ucc_loss': 0.36536, 'train_ucc_acc': 0.96875, 'loss': 0.51756}\n",
            "Step 136440: {'train_ae_loss': 0.66154, 'train_ucc_loss': 0.41599, 'train_ucc_acc': 0.875, 'loss': 0.53877}\n",
            "Step 136460: {'train_ae_loss': 0.65584, 'train_ucc_loss': 0.42182, 'train_ucc_acc': 0.90625, 'loss': 0.53883}\n",
            "Step 136480: {'train_ae_loss': 0.65537, 'train_ucc_loss': 0.3148, 'train_ucc_acc': 1.0, 'loss': 0.48508}\n",
            "Step 136500: {'train_ae_loss': 0.6645, 'train_ucc_loss': 0.44208, 'train_ucc_acc': 0.875, 'loss': 0.55329}\n",
            "Step 136520: {'train_ae_loss': 0.63703, 'train_ucc_loss': 0.44434, 'train_ucc_acc': 0.875, 'loss': 0.54069}\n",
            "Step 136540: {'train_ae_loss': 0.65578, 'train_ucc_loss': 0.43302, 'train_ucc_acc': 0.875, 'loss': 0.5444}\n",
            "Step 136560: {'train_ae_loss': 0.65987, 'train_ucc_loss': 0.40848, 'train_ucc_acc': 0.90625, 'loss': 0.53417}\n",
            "Step 136580: {'train_ae_loss': 0.66158, 'train_ucc_loss': 0.49038, 'train_ucc_acc': 0.8125, 'loss': 0.57598}\n",
            "Step 136600: {'train_ae_loss': 0.66182, 'train_ucc_loss': 0.49438, 'train_ucc_acc': 0.8125, 'loss': 0.5781}\n",
            "Step 136620: {'train_ae_loss': 0.6632, 'train_ucc_loss': 0.36529, 'train_ucc_acc': 0.9375, 'loss': 0.51425}\n",
            "Step 136640: {'train_ae_loss': 0.65017, 'train_ucc_loss': 0.4101, 'train_ucc_acc': 0.90625, 'loss': 0.53014}\n",
            "Step 136660: {'train_ae_loss': 0.66977, 'train_ucc_loss': 0.49973, 'train_ucc_acc': 0.8125, 'loss': 0.58475}\n",
            "Step 136680: {'train_ae_loss': 0.66848, 'train_ucc_loss': 0.46824, 'train_ucc_acc': 0.84375, 'loss': 0.56836}\n",
            "Step 136700: {'train_ae_loss': 0.66045, 'train_ucc_loss': 0.44958, 'train_ucc_acc': 0.875, 'loss': 0.55502}\n",
            "Step 136720: {'train_ae_loss': 0.66615, 'train_ucc_loss': 0.38061, 'train_ucc_acc': 0.90625, 'loss': 0.52338}\n",
            "Step 136740: {'train_ae_loss': 0.66358, 'train_ucc_loss': 0.52566, 'train_ucc_acc': 0.78125, 'loss': 0.59462}\n",
            "Step 136760: {'train_ae_loss': 0.6607, 'train_ucc_loss': 0.4459, 'train_ucc_acc': 0.84375, 'loss': 0.5533}\n",
            "Step 136780: {'train_ae_loss': 0.65376, 'train_ucc_loss': 0.53444, 'train_ucc_acc': 0.78125, 'loss': 0.5941}\n",
            "Step 136800: {'train_ae_loss': 0.64784, 'train_ucc_loss': 0.42521, 'train_ucc_acc': 0.875, 'loss': 0.53652}\n",
            "Step 136820: {'train_ae_loss': 0.64612, 'train_ucc_loss': 0.55337, 'train_ucc_acc': 0.75, 'loss': 0.59975}\n",
            "Step 136840: {'train_ae_loss': 0.67232, 'train_ucc_loss': 0.41033, 'train_ucc_acc': 0.90625, 'loss': 0.54133}\n",
            "Step 136860: {'train_ae_loss': 0.6557, 'train_ucc_loss': 0.50178, 'train_ucc_acc': 0.8125, 'loss': 0.57874}\n",
            "Step 136880: {'train_ae_loss': 0.66801, 'train_ucc_loss': 0.48476, 'train_ucc_acc': 0.8125, 'loss': 0.57639}\n",
            "Step 136900: {'train_ae_loss': 0.66429, 'train_ucc_loss': 0.46179, 'train_ucc_acc': 0.84375, 'loss': 0.56304}\n",
            "Step 136920: {'train_ae_loss': 0.63264, 'train_ucc_loss': 0.54876, 'train_ucc_acc': 0.71875, 'loss': 0.5907}\n",
            "Step 136940: {'train_ae_loss': 0.64699, 'train_ucc_loss': 0.46615, 'train_ucc_acc': 0.875, 'loss': 0.55657}\n",
            "Step 136960: {'train_ae_loss': 0.65742, 'train_ucc_loss': 0.48679, 'train_ucc_acc': 0.8125, 'loss': 0.57211}\n",
            "Step 136980: {'train_ae_loss': 0.66328, 'train_ucc_loss': 0.52967, 'train_ucc_acc': 0.8125, 'loss': 0.59648}\n",
            "Step 137000: {'train_ae_loss': 0.65261, 'train_ucc_loss': 0.47236, 'train_ucc_acc': 0.8125, 'loss': 0.56248}\n",
            "step: 137000,eval_ae_loss: 0.65125,eval_ucc_loss: 0.48789,eval_ucc_acc: 0.81055\n",
            "Step 137020: {'train_ae_loss': 0.66467, 'train_ucc_loss': 0.40202, 'train_ucc_acc': 0.90625, 'loss': 0.53334}\n",
            "Step 137040: {'train_ae_loss': 0.65369, 'train_ucc_loss': 0.46919, 'train_ucc_acc': 0.84375, 'loss': 0.56144}\n",
            "Step 137060: {'train_ae_loss': 0.64605, 'train_ucc_loss': 0.47081, 'train_ucc_acc': 0.84375, 'loss': 0.55843}\n",
            "Step 137080: {'train_ae_loss': 0.67013, 'train_ucc_loss': 0.38796, 'train_ucc_acc': 0.9375, 'loss': 0.52904}\n",
            "Step 137100: {'train_ae_loss': 0.66691, 'train_ucc_loss': 0.4511, 'train_ucc_acc': 0.875, 'loss': 0.559}\n",
            "Step 137120: {'train_ae_loss': 0.66368, 'train_ucc_loss': 0.528, 'train_ucc_acc': 0.71875, 'loss': 0.59584}\n",
            "Step 137140: {'train_ae_loss': 0.65714, 'train_ucc_loss': 0.42455, 'train_ucc_acc': 0.875, 'loss': 0.54084}\n",
            "Step 137160: {'train_ae_loss': 0.66124, 'train_ucc_loss': 0.41269, 'train_ucc_acc': 0.90625, 'loss': 0.53696}\n",
            "Step 137180: {'train_ae_loss': 0.64248, 'train_ucc_loss': 0.40881, 'train_ucc_acc': 0.90625, 'loss': 0.52564}\n",
            "Step 137200: {'train_ae_loss': 0.65293, 'train_ucc_loss': 0.44391, 'train_ucc_acc': 0.84375, 'loss': 0.54842}\n",
            "Step 137220: {'train_ae_loss': 0.66555, 'train_ucc_loss': 0.37344, 'train_ucc_acc': 0.9375, 'loss': 0.51949}\n",
            "Step 137240: {'train_ae_loss': 0.66443, 'train_ucc_loss': 0.46846, 'train_ucc_acc': 0.84375, 'loss': 0.56645}\n",
            "Step 137260: {'train_ae_loss': 0.66416, 'train_ucc_loss': 0.59434, 'train_ucc_acc': 0.71875, 'loss': 0.62925}\n",
            "Step 137280: {'train_ae_loss': 0.65631, 'train_ucc_loss': 0.3833, 'train_ucc_acc': 0.9375, 'loss': 0.5198}\n",
            "Step 137300: {'train_ae_loss': 0.66616, 'train_ucc_loss': 0.44299, 'train_ucc_acc': 0.875, 'loss': 0.55458}\n",
            "Step 137320: {'train_ae_loss': 0.65231, 'train_ucc_loss': 0.42777, 'train_ucc_acc': 0.875, 'loss': 0.54004}\n",
            "Step 137340: {'train_ae_loss': 0.66697, 'train_ucc_loss': 0.45095, 'train_ucc_acc': 0.875, 'loss': 0.55896}\n",
            "Step 137360: {'train_ae_loss': 0.65157, 'train_ucc_loss': 0.37804, 'train_ucc_acc': 0.9375, 'loss': 0.5148}\n",
            "Step 137380: {'train_ae_loss': 0.66481, 'train_ucc_loss': 0.3839, 'train_ucc_acc': 0.9375, 'loss': 0.52436}\n",
            "Step 137400: {'train_ae_loss': 0.65973, 'train_ucc_loss': 0.44801, 'train_ucc_acc': 0.875, 'loss': 0.55387}\n",
            "Step 137420: {'train_ae_loss': 0.65712, 'train_ucc_loss': 0.48431, 'train_ucc_acc': 0.8125, 'loss': 0.57071}\n",
            "Step 137440: {'train_ae_loss': 0.66338, 'train_ucc_loss': 0.53412, 'train_ucc_acc': 0.78125, 'loss': 0.59875}\n",
            "Step 137460: {'train_ae_loss': 0.68288, 'train_ucc_loss': 0.42619, 'train_ucc_acc': 0.875, 'loss': 0.55453}\n",
            "Step 137480: {'train_ae_loss': 0.64747, 'train_ucc_loss': 0.46308, 'train_ucc_acc': 0.84375, 'loss': 0.55527}\n",
            "Step 137500: {'train_ae_loss': 0.66007, 'train_ucc_loss': 0.45375, 'train_ucc_acc': 0.875, 'loss': 0.55691}\n",
            "Step 137520: {'train_ae_loss': 0.66456, 'train_ucc_loss': 0.49536, 'train_ucc_acc': 0.8125, 'loss': 0.57996}\n",
            "Step 137540: {'train_ae_loss': 0.65695, 'train_ucc_loss': 0.42041, 'train_ucc_acc': 0.875, 'loss': 0.53868}\n",
            "Step 137560: {'train_ae_loss': 0.64887, 'train_ucc_loss': 0.5566, 'train_ucc_acc': 0.71875, 'loss': 0.60274}\n",
            "Step 137580: {'train_ae_loss': 0.64899, 'train_ucc_loss': 0.44711, 'train_ucc_acc': 0.84375, 'loss': 0.54805}\n",
            "Step 137600: {'train_ae_loss': 0.66208, 'train_ucc_loss': 0.40019, 'train_ucc_acc': 0.90625, 'loss': 0.53114}\n",
            "Step 137620: {'train_ae_loss': 0.6693, 'train_ucc_loss': 0.46227, 'train_ucc_acc': 0.875, 'loss': 0.56579}\n",
            "Step 137640: {'train_ae_loss': 0.66711, 'train_ucc_loss': 0.47735, 'train_ucc_acc': 0.84375, 'loss': 0.57223}\n",
            "Step 137660: {'train_ae_loss': 0.65484, 'train_ucc_loss': 0.55972, 'train_ucc_acc': 0.71875, 'loss': 0.60728}\n",
            "Step 137680: {'train_ae_loss': 0.684, 'train_ucc_loss': 0.33515, 'train_ucc_acc': 1.0, 'loss': 0.50958}\n",
            "Step 137700: {'train_ae_loss': 0.66687, 'train_ucc_loss': 0.54247, 'train_ucc_acc': 0.75, 'loss': 0.60467}\n",
            "Step 137720: {'train_ae_loss': 0.66388, 'train_ucc_loss': 0.37063, 'train_ucc_acc': 0.9375, 'loss': 0.51725}\n",
            "Step 137740: {'train_ae_loss': 0.65931, 'train_ucc_loss': 0.44826, 'train_ucc_acc': 0.84375, 'loss': 0.55379}\n",
            "Step 137760: {'train_ae_loss': 0.66016, 'train_ucc_loss': 0.45891, 'train_ucc_acc': 0.84375, 'loss': 0.55954}\n",
            "Step 137780: {'train_ae_loss': 0.648, 'train_ucc_loss': 0.46408, 'train_ucc_acc': 0.84375, 'loss': 0.55604}\n",
            "Step 137800: {'train_ae_loss': 0.65666, 'train_ucc_loss': 0.51121, 'train_ucc_acc': 0.8125, 'loss': 0.58394}\n",
            "Step 137820: {'train_ae_loss': 0.65804, 'train_ucc_loss': 0.47793, 'train_ucc_acc': 0.8125, 'loss': 0.56799}\n",
            "Step 137840: {'train_ae_loss': 0.64438, 'train_ucc_loss': 0.50601, 'train_ucc_acc': 0.78125, 'loss': 0.5752}\n",
            "Step 137860: {'train_ae_loss': 0.6627, 'train_ucc_loss': 0.502, 'train_ucc_acc': 0.8125, 'loss': 0.58235}\n",
            "Step 137880: {'train_ae_loss': 0.65535, 'train_ucc_loss': 0.4049, 'train_ucc_acc': 0.90625, 'loss': 0.53013}\n",
            "Step 137900: {'train_ae_loss': 0.67812, 'train_ucc_loss': 0.36463, 'train_ucc_acc': 0.9375, 'loss': 0.52137}\n",
            "Step 137920: {'train_ae_loss': 0.66668, 'train_ucc_loss': 0.45223, 'train_ucc_acc': 0.84375, 'loss': 0.55945}\n",
            "Step 137940: {'train_ae_loss': 0.66034, 'train_ucc_loss': 0.39313, 'train_ucc_acc': 0.9375, 'loss': 0.52674}\n",
            "Step 137960: {'train_ae_loss': 0.64262, 'train_ucc_loss': 0.3706, 'train_ucc_acc': 0.96875, 'loss': 0.50661}\n",
            "Step 137980: {'train_ae_loss': 0.6571, 'train_ucc_loss': 0.43637, 'train_ucc_acc': 0.875, 'loss': 0.54674}\n",
            "Step 138000: {'train_ae_loss': 0.64786, 'train_ucc_loss': 0.48477, 'train_ucc_acc': 0.8125, 'loss': 0.56632}\n",
            "step: 138000,eval_ae_loss: 0.65193,eval_ucc_loss: 0.44857,eval_ucc_acc: 0.86328\n",
            "Step 138020: {'train_ae_loss': 0.67124, 'train_ucc_loss': 0.32359, 'train_ucc_acc': 1.0, 'loss': 0.49742}\n",
            "Step 138040: {'train_ae_loss': 0.66286, 'train_ucc_loss': 0.44588, 'train_ucc_acc': 0.84375, 'loss': 0.55437}\n",
            "Step 138060: {'train_ae_loss': 0.64639, 'train_ucc_loss': 0.54276, 'train_ucc_acc': 0.75, 'loss': 0.59458}\n",
            "Step 138080: {'train_ae_loss': 0.66638, 'train_ucc_loss': 0.50906, 'train_ucc_acc': 0.78125, 'loss': 0.58772}\n",
            "Step 138100: {'train_ae_loss': 0.65921, 'train_ucc_loss': 0.51639, 'train_ucc_acc': 0.75, 'loss': 0.5878}\n",
            "Step 138120: {'train_ae_loss': 0.64215, 'train_ucc_loss': 0.4978, 'train_ucc_acc': 0.75, 'loss': 0.56997}\n",
            "Step 138140: {'train_ae_loss': 0.65584, 'train_ucc_loss': 0.42763, 'train_ucc_acc': 0.90625, 'loss': 0.54173}\n",
            "Step 138160: {'train_ae_loss': 0.66014, 'train_ucc_loss': 0.42093, 'train_ucc_acc': 0.875, 'loss': 0.54053}\n",
            "Step 138180: {'train_ae_loss': 0.65574, 'train_ucc_loss': 0.45615, 'train_ucc_acc': 0.84375, 'loss': 0.55594}\n",
            "Step 138200: {'train_ae_loss': 0.66784, 'train_ucc_loss': 0.48909, 'train_ucc_acc': 0.8125, 'loss': 0.57846}\n",
            "Step 138220: {'train_ae_loss': 0.65643, 'train_ucc_loss': 0.53814, 'train_ucc_acc': 0.78125, 'loss': 0.59728}\n",
            "Step 138240: {'train_ae_loss': 0.65716, 'train_ucc_loss': 0.38988, 'train_ucc_acc': 0.9375, 'loss': 0.52352}\n",
            "Step 138260: {'train_ae_loss': 0.65769, 'train_ucc_loss': 0.38811, 'train_ucc_acc': 0.9375, 'loss': 0.5229}\n",
            "Step 138280: {'train_ae_loss': 0.63358, 'train_ucc_loss': 0.39879, 'train_ucc_acc': 0.90625, 'loss': 0.51618}\n",
            "Step 138300: {'train_ae_loss': 0.66485, 'train_ucc_loss': 0.36262, 'train_ucc_acc': 0.9375, 'loss': 0.51374}\n",
            "Step 138320: {'train_ae_loss': 0.68453, 'train_ucc_loss': 0.45895, 'train_ucc_acc': 0.875, 'loss': 0.57174}\n",
            "Step 138340: {'train_ae_loss': 0.65564, 'train_ucc_loss': 0.41726, 'train_ucc_acc': 0.90625, 'loss': 0.53645}\n",
            "Step 138360: {'train_ae_loss': 0.64404, 'train_ucc_loss': 0.47133, 'train_ucc_acc': 0.78125, 'loss': 0.55769}\n",
            "Step 138380: {'train_ae_loss': 0.64591, 'train_ucc_loss': 0.46959, 'train_ucc_acc': 0.84375, 'loss': 0.55775}\n",
            "Step 138400: {'train_ae_loss': 0.65062, 'train_ucc_loss': 0.51261, 'train_ucc_acc': 0.78125, 'loss': 0.58161}\n",
            "Step 138420: {'train_ae_loss': 0.66433, 'train_ucc_loss': 0.34646, 'train_ucc_acc': 0.96875, 'loss': 0.5054}\n",
            "Step 138440: {'train_ae_loss': 0.65362, 'train_ucc_loss': 0.45976, 'train_ucc_acc': 0.84375, 'loss': 0.55669}\n",
            "Step 138460: {'train_ae_loss': 0.66389, 'train_ucc_loss': 0.51085, 'train_ucc_acc': 0.78125, 'loss': 0.58737}\n",
            "Step 138480: {'train_ae_loss': 0.64783, 'train_ucc_loss': 0.50639, 'train_ucc_acc': 0.78125, 'loss': 0.57711}\n",
            "Step 138500: {'train_ae_loss': 0.66389, 'train_ucc_loss': 0.40281, 'train_ucc_acc': 0.90625, 'loss': 0.53335}\n",
            "Step 138520: {'train_ae_loss': 0.65077, 'train_ucc_loss': 0.44276, 'train_ucc_acc': 0.875, 'loss': 0.54677}\n",
            "Step 138540: {'train_ae_loss': 0.65873, 'train_ucc_loss': 0.58757, 'train_ucc_acc': 0.6875, 'loss': 0.62315}\n",
            "Step 138560: {'train_ae_loss': 0.64873, 'train_ucc_loss': 0.50928, 'train_ucc_acc': 0.8125, 'loss': 0.57901}\n",
            "Step 138580: {'train_ae_loss': 0.6709, 'train_ucc_loss': 0.3432, 'train_ucc_acc': 0.96875, 'loss': 0.50705}\n",
            "Step 138600: {'train_ae_loss': 0.65313, 'train_ucc_loss': 0.38406, 'train_ucc_acc': 0.9375, 'loss': 0.5186}\n",
            "Step 138620: {'train_ae_loss': 0.65447, 'train_ucc_loss': 0.48073, 'train_ucc_acc': 0.84375, 'loss': 0.5676}\n",
            "Step 138640: {'train_ae_loss': 0.65638, 'train_ucc_loss': 0.43235, 'train_ucc_acc': 0.875, 'loss': 0.54436}\n",
            "Step 138660: {'train_ae_loss': 0.65635, 'train_ucc_loss': 0.47834, 'train_ucc_acc': 0.84375, 'loss': 0.56734}\n",
            "Step 138680: {'train_ae_loss': 0.65414, 'train_ucc_loss': 0.33488, 'train_ucc_acc': 0.96875, 'loss': 0.49451}\n",
            "Step 138700: {'train_ae_loss': 0.66531, 'train_ucc_loss': 0.42496, 'train_ucc_acc': 0.875, 'loss': 0.54513}\n",
            "Step 138720: {'train_ae_loss': 0.67286, 'train_ucc_loss': 0.41323, 'train_ucc_acc': 0.90625, 'loss': 0.54304}\n",
            "Step 138740: {'train_ae_loss': 0.65999, 'train_ucc_loss': 0.41273, 'train_ucc_acc': 0.875, 'loss': 0.53636}\n",
            "Step 138760: {'train_ae_loss': 0.65486, 'train_ucc_loss': 0.46495, 'train_ucc_acc': 0.84375, 'loss': 0.5599}\n",
            "Step 138780: {'train_ae_loss': 0.66689, 'train_ucc_loss': 0.41717, 'train_ucc_acc': 0.90625, 'loss': 0.54203}\n",
            "Step 138800: {'train_ae_loss': 0.65769, 'train_ucc_loss': 0.38656, 'train_ucc_acc': 0.9375, 'loss': 0.52213}\n",
            "Step 138820: {'train_ae_loss': 0.64371, 'train_ucc_loss': 0.45056, 'train_ucc_acc': 0.875, 'loss': 0.54713}\n",
            "Step 138840: {'train_ae_loss': 0.6658, 'train_ucc_loss': 0.39136, 'train_ucc_acc': 0.9375, 'loss': 0.52858}\n",
            "Step 138860: {'train_ae_loss': 0.64734, 'train_ucc_loss': 0.45808, 'train_ucc_acc': 0.84375, 'loss': 0.55271}\n",
            "Step 138880: {'train_ae_loss': 0.653, 'train_ucc_loss': 0.3795, 'train_ucc_acc': 0.9375, 'loss': 0.51625}\n",
            "Step 138900: {'train_ae_loss': 0.66665, 'train_ucc_loss': 0.43987, 'train_ucc_acc': 0.875, 'loss': 0.55326}\n",
            "Step 138920: {'train_ae_loss': 0.66818, 'train_ucc_loss': 0.42691, 'train_ucc_acc': 0.875, 'loss': 0.54754}\n",
            "Step 138940: {'train_ae_loss': 0.67385, 'train_ucc_loss': 0.50424, 'train_ucc_acc': 0.8125, 'loss': 0.58905}\n",
            "Step 138960: {'train_ae_loss': 0.67732, 'train_ucc_loss': 0.4788, 'train_ucc_acc': 0.8125, 'loss': 0.57806}\n",
            "Step 138980: {'train_ae_loss': 0.65864, 'train_ucc_loss': 0.53159, 'train_ucc_acc': 0.75, 'loss': 0.59512}\n",
            "Step 139000: {'train_ae_loss': 0.66112, 'train_ucc_loss': 0.47287, 'train_ucc_acc': 0.84375, 'loss': 0.567}\n",
            "step: 139000,eval_ae_loss: 0.64857,eval_ucc_loss: 0.46869,eval_ucc_acc: 0.8418\n",
            "Step 139020: {'train_ae_loss': 0.66079, 'train_ucc_loss': 0.42362, 'train_ucc_acc': 0.875, 'loss': 0.54221}\n",
            "Step 139040: {'train_ae_loss': 0.66739, 'train_ucc_loss': 0.40124, 'train_ucc_acc': 0.90625, 'loss': 0.53431}\n",
            "Step 139060: {'train_ae_loss': 0.66601, 'train_ucc_loss': 0.40567, 'train_ucc_acc': 0.90625, 'loss': 0.53584}\n",
            "Step 139080: {'train_ae_loss': 0.65939, 'train_ucc_loss': 0.44563, 'train_ucc_acc': 0.875, 'loss': 0.55251}\n",
            "Step 139100: {'train_ae_loss': 0.65309, 'train_ucc_loss': 0.45924, 'train_ucc_acc': 0.84375, 'loss': 0.55617}\n",
            "Step 139120: {'train_ae_loss': 0.66382, 'train_ucc_loss': 0.46271, 'train_ucc_acc': 0.8125, 'loss': 0.56327}\n",
            "Step 139140: {'train_ae_loss': 0.65222, 'train_ucc_loss': 0.43415, 'train_ucc_acc': 0.875, 'loss': 0.54319}\n",
            "Step 139160: {'train_ae_loss': 0.6674, 'train_ucc_loss': 0.43313, 'train_ucc_acc': 0.875, 'loss': 0.55026}\n",
            "Step 139180: {'train_ae_loss': 0.66126, 'train_ucc_loss': 0.37864, 'train_ucc_acc': 0.9375, 'loss': 0.51995}\n",
            "Step 139200: {'train_ae_loss': 0.66691, 'train_ucc_loss': 0.40428, 'train_ucc_acc': 0.90625, 'loss': 0.53559}\n",
            "Step 139220: {'train_ae_loss': 0.65598, 'train_ucc_loss': 0.43057, 'train_ucc_acc': 0.875, 'loss': 0.54328}\n",
            "Step 139240: {'train_ae_loss': 0.64197, 'train_ucc_loss': 0.36714, 'train_ucc_acc': 0.9375, 'loss': 0.50456}\n",
            "Step 139260: {'train_ae_loss': 0.67226, 'train_ucc_loss': 0.34649, 'train_ucc_acc': 0.96875, 'loss': 0.50937}\n",
            "Step 139280: {'train_ae_loss': 0.63775, 'train_ucc_loss': 0.46605, 'train_ucc_acc': 0.84375, 'loss': 0.5519}\n",
            "Step 139300: {'train_ae_loss': 0.65786, 'train_ucc_loss': 0.41317, 'train_ucc_acc': 0.90625, 'loss': 0.53551}\n",
            "Step 139320: {'train_ae_loss': 0.65663, 'train_ucc_loss': 0.32003, 'train_ucc_acc': 1.0, 'loss': 0.48833}\n",
            "Step 139340: {'train_ae_loss': 0.65082, 'train_ucc_loss': 0.44157, 'train_ucc_acc': 0.875, 'loss': 0.5462}\n",
            "Step 139360: {'train_ae_loss': 0.66863, 'train_ucc_loss': 0.36643, 'train_ucc_acc': 0.96875, 'loss': 0.51753}\n",
            "Step 139380: {'train_ae_loss': 0.65902, 'train_ucc_loss': 0.46944, 'train_ucc_acc': 0.84375, 'loss': 0.56423}\n",
            "Step 139400: {'train_ae_loss': 0.66301, 'train_ucc_loss': 0.43849, 'train_ucc_acc': 0.875, 'loss': 0.55075}\n",
            "Step 139420: {'train_ae_loss': 0.6583, 'train_ucc_loss': 0.44841, 'train_ucc_acc': 0.875, 'loss': 0.55336}\n",
            "Step 139440: {'train_ae_loss': 0.64402, 'train_ucc_loss': 0.49602, 'train_ucc_acc': 0.8125, 'loss': 0.57002}\n",
            "Step 139460: {'train_ae_loss': 0.66431, 'train_ucc_loss': 0.4256, 'train_ucc_acc': 0.875, 'loss': 0.54495}\n",
            "Step 139480: {'train_ae_loss': 0.67031, 'train_ucc_loss': 0.39836, 'train_ucc_acc': 0.90625, 'loss': 0.53434}\n",
            "Step 139500: {'train_ae_loss': 0.665, 'train_ucc_loss': 0.38381, 'train_ucc_acc': 0.90625, 'loss': 0.52441}\n",
            "Step 139520: {'train_ae_loss': 0.64693, 'train_ucc_loss': 0.45063, 'train_ucc_acc': 0.875, 'loss': 0.54878}\n",
            "Step 139540: {'train_ae_loss': 0.6529, 'train_ucc_loss': 0.38571, 'train_ucc_acc': 0.9375, 'loss': 0.51931}\n",
            "Step 139560: {'train_ae_loss': 0.66244, 'train_ucc_loss': 0.40821, 'train_ucc_acc': 0.90625, 'loss': 0.53533}\n",
            "Step 139580: {'train_ae_loss': 0.66466, 'train_ucc_loss': 0.43533, 'train_ucc_acc': 0.875, 'loss': 0.55}\n",
            "Step 139600: {'train_ae_loss': 0.66106, 'train_ucc_loss': 0.36978, 'train_ucc_acc': 0.96875, 'loss': 0.51542}\n",
            "Step 139620: {'train_ae_loss': 0.66874, 'train_ucc_loss': 0.44183, 'train_ucc_acc': 0.875, 'loss': 0.55529}\n",
            "Step 139640: {'train_ae_loss': 0.65334, 'train_ucc_loss': 0.41229, 'train_ucc_acc': 0.90625, 'loss': 0.53281}\n",
            "Step 139660: {'train_ae_loss': 0.65876, 'train_ucc_loss': 0.43389, 'train_ucc_acc': 0.90625, 'loss': 0.54632}\n",
            "Step 139680: {'train_ae_loss': 0.65985, 'train_ucc_loss': 0.37912, 'train_ucc_acc': 0.9375, 'loss': 0.51949}\n",
            "Step 139700: {'train_ae_loss': 0.65392, 'train_ucc_loss': 0.35021, 'train_ucc_acc': 0.96875, 'loss': 0.50207}\n",
            "Step 139720: {'train_ae_loss': 0.65364, 'train_ucc_loss': 0.49587, 'train_ucc_acc': 0.8125, 'loss': 0.57475}\n",
            "Step 139740: {'train_ae_loss': 0.65302, 'train_ucc_loss': 0.36177, 'train_ucc_acc': 0.96875, 'loss': 0.50739}\n",
            "Step 139760: {'train_ae_loss': 0.67104, 'train_ucc_loss': 0.4282, 'train_ucc_acc': 0.875, 'loss': 0.54962}\n",
            "Step 139780: {'train_ae_loss': 0.66751, 'train_ucc_loss': 0.40666, 'train_ucc_acc': 0.90625, 'loss': 0.53709}\n",
            "Step 139800: {'train_ae_loss': 0.6707, 'train_ucc_loss': 0.38683, 'train_ucc_acc': 0.90625, 'loss': 0.52876}\n",
            "Step 139820: {'train_ae_loss': 0.64608, 'train_ucc_loss': 0.38469, 'train_ucc_acc': 0.9375, 'loss': 0.51538}\n",
            "Step 139840: {'train_ae_loss': 0.65987, 'train_ucc_loss': 0.48346, 'train_ucc_acc': 0.84375, 'loss': 0.57167}\n",
            "Step 139860: {'train_ae_loss': 0.65138, 'train_ucc_loss': 0.486, 'train_ucc_acc': 0.84375, 'loss': 0.56869}\n",
            "Step 139880: {'train_ae_loss': 0.66519, 'train_ucc_loss': 0.49905, 'train_ucc_acc': 0.78125, 'loss': 0.58212}\n",
            "Step 139900: {'train_ae_loss': 0.67388, 'train_ucc_loss': 0.33591, 'train_ucc_acc': 1.0, 'loss': 0.5049}\n",
            "Step 139920: {'train_ae_loss': 0.64911, 'train_ucc_loss': 0.44751, 'train_ucc_acc': 0.84375, 'loss': 0.54831}\n",
            "Step 139940: {'train_ae_loss': 0.66944, 'train_ucc_loss': 0.5057, 'train_ucc_acc': 0.78125, 'loss': 0.58757}\n",
            "Step 139960: {'train_ae_loss': 0.65809, 'train_ucc_loss': 0.44401, 'train_ucc_acc': 0.875, 'loss': 0.55105}\n",
            "Step 139980: {'train_ae_loss': 0.66321, 'train_ucc_loss': 0.4638, 'train_ucc_acc': 0.84375, 'loss': 0.56351}\n",
            "Step 140000: {'train_ae_loss': 0.68222, 'train_ucc_loss': 0.34563, 'train_ucc_acc': 0.96875, 'loss': 0.51392}\n",
            "step: 140000,eval_ae_loss: 0.65307,eval_ucc_loss: 0.48133,eval_ucc_acc: 0.82227\n",
            "Step 140020: {'train_ae_loss': 0.67221, 'train_ucc_loss': 0.3927, 'train_ucc_acc': 0.90625, 'loss': 0.53245}\n",
            "Step 140040: {'train_ae_loss': 0.65553, 'train_ucc_loss': 0.41106, 'train_ucc_acc': 0.90625, 'loss': 0.53329}\n",
            "Step 140060: {'train_ae_loss': 0.64864, 'train_ucc_loss': 0.4352, 'train_ucc_acc': 0.90625, 'loss': 0.54192}\n",
            "Step 140080: {'train_ae_loss': 0.65242, 'train_ucc_loss': 0.42074, 'train_ucc_acc': 0.90625, 'loss': 0.53658}\n",
            "Step 140100: {'train_ae_loss': 0.6482, 'train_ucc_loss': 0.41589, 'train_ucc_acc': 0.875, 'loss': 0.53204}\n",
            "Step 140120: {'train_ae_loss': 0.66628, 'train_ucc_loss': 0.41755, 'train_ucc_acc': 0.90625, 'loss': 0.54192}\n",
            "Step 140140: {'train_ae_loss': 0.67337, 'train_ucc_loss': 0.3948, 'train_ucc_acc': 0.90625, 'loss': 0.53409}\n",
            "Step 140160: {'train_ae_loss': 0.66781, 'train_ucc_loss': 0.38105, 'train_ucc_acc': 0.9375, 'loss': 0.52443}\n",
            "Step 140180: {'train_ae_loss': 0.66974, 'train_ucc_loss': 0.44526, 'train_ucc_acc': 0.8125, 'loss': 0.5575}\n",
            "Step 140200: {'train_ae_loss': 0.67786, 'train_ucc_loss': 0.33978, 'train_ucc_acc': 1.0, 'loss': 0.50882}\n",
            "Step 140220: {'train_ae_loss': 0.66203, 'train_ucc_loss': 0.58804, 'train_ucc_acc': 0.6875, 'loss': 0.62504}\n",
            "Step 140240: {'train_ae_loss': 0.66524, 'train_ucc_loss': 0.48802, 'train_ucc_acc': 0.8125, 'loss': 0.57663}\n",
            "Step 140260: {'train_ae_loss': 0.65258, 'train_ucc_loss': 0.49274, 'train_ucc_acc': 0.8125, 'loss': 0.57266}\n",
            "Step 140280: {'train_ae_loss': 0.65357, 'train_ucc_loss': 0.4253, 'train_ucc_acc': 0.875, 'loss': 0.53944}\n",
            "Step 140300: {'train_ae_loss': 0.68125, 'train_ucc_loss': 0.41364, 'train_ucc_acc': 0.90625, 'loss': 0.54744}\n",
            "Step 140320: {'train_ae_loss': 0.66321, 'train_ucc_loss': 0.48063, 'train_ucc_acc': 0.84375, 'loss': 0.57192}\n",
            "Step 140340: {'train_ae_loss': 0.67215, 'train_ucc_loss': 0.46927, 'train_ucc_acc': 0.84375, 'loss': 0.57071}\n",
            "Step 140360: {'train_ae_loss': 0.65359, 'train_ucc_loss': 0.41735, 'train_ucc_acc': 0.90625, 'loss': 0.53547}\n",
            "Step 140380: {'train_ae_loss': 0.67168, 'train_ucc_loss': 0.48056, 'train_ucc_acc': 0.8125, 'loss': 0.57612}\n",
            "Step 140400: {'train_ae_loss': 0.6589, 'train_ucc_loss': 0.35206, 'train_ucc_acc': 0.96875, 'loss': 0.50548}\n",
            "Step 140420: {'train_ae_loss': 0.6548, 'train_ucc_loss': 0.41981, 'train_ucc_acc': 0.90625, 'loss': 0.5373}\n",
            "Step 140440: {'train_ae_loss': 0.65149, 'train_ucc_loss': 0.41294, 'train_ucc_acc': 0.90625, 'loss': 0.53221}\n",
            "Step 140460: {'train_ae_loss': 0.66103, 'train_ucc_loss': 0.47421, 'train_ucc_acc': 0.84375, 'loss': 0.56762}\n",
            "Step 140480: {'train_ae_loss': 0.67753, 'train_ucc_loss': 0.51124, 'train_ucc_acc': 0.8125, 'loss': 0.59438}\n",
            "Step 140500: {'train_ae_loss': 0.65436, 'train_ucc_loss': 0.35438, 'train_ucc_acc': 0.96875, 'loss': 0.50437}\n",
            "Step 140520: {'train_ae_loss': 0.67036, 'train_ucc_loss': 0.41027, 'train_ucc_acc': 0.90625, 'loss': 0.54032}\n",
            "Step 140540: {'train_ae_loss': 0.65804, 'train_ucc_loss': 0.48489, 'train_ucc_acc': 0.8125, 'loss': 0.57147}\n",
            "Step 140560: {'train_ae_loss': 0.66036, 'train_ucc_loss': 0.51824, 'train_ucc_acc': 0.78125, 'loss': 0.5893}\n",
            "Step 140580: {'train_ae_loss': 0.66056, 'train_ucc_loss': 0.40248, 'train_ucc_acc': 0.90625, 'loss': 0.53152}\n",
            "Step 140600: {'train_ae_loss': 0.66332, 'train_ucc_loss': 0.44242, 'train_ucc_acc': 0.875, 'loss': 0.55287}\n",
            "Step 140620: {'train_ae_loss': 0.69092, 'train_ucc_loss': 0.43507, 'train_ucc_acc': 0.875, 'loss': 0.56299}\n",
            "Step 140640: {'train_ae_loss': 0.64982, 'train_ucc_loss': 0.36611, 'train_ucc_acc': 0.96875, 'loss': 0.50796}\n",
            "Step 140660: {'train_ae_loss': 0.65659, 'train_ucc_loss': 0.41757, 'train_ucc_acc': 0.90625, 'loss': 0.53708}\n",
            "Step 140680: {'train_ae_loss': 0.65185, 'train_ucc_loss': 0.46224, 'train_ucc_acc': 0.84375, 'loss': 0.55705}\n",
            "Step 140700: {'train_ae_loss': 0.64234, 'train_ucc_loss': 0.45317, 'train_ucc_acc': 0.875, 'loss': 0.54776}\n",
            "Step 140720: {'train_ae_loss': 0.6605, 'train_ucc_loss': 0.36132, 'train_ucc_acc': 0.9375, 'loss': 0.51091}\n",
            "Step 140740: {'train_ae_loss': 0.66983, 'train_ucc_loss': 0.37845, 'train_ucc_acc': 0.9375, 'loss': 0.52414}\n",
            "Step 140760: {'train_ae_loss': 0.65564, 'train_ucc_loss': 0.51788, 'train_ucc_acc': 0.75, 'loss': 0.58676}\n",
            "Step 140780: {'train_ae_loss': 0.67157, 'train_ucc_loss': 0.39457, 'train_ucc_acc': 0.90625, 'loss': 0.53307}\n",
            "Step 140800: {'train_ae_loss': 0.66708, 'train_ucc_loss': 0.42747, 'train_ucc_acc': 0.875, 'loss': 0.54728}\n",
            "Step 140820: {'train_ae_loss': 0.66289, 'train_ucc_loss': 0.41371, 'train_ucc_acc': 0.90625, 'loss': 0.5383}\n",
            "Step 140840: {'train_ae_loss': 0.65926, 'train_ucc_loss': 0.44194, 'train_ucc_acc': 0.875, 'loss': 0.5506}\n",
            "Step 140860: {'train_ae_loss': 0.67607, 'train_ucc_loss': 0.37711, 'train_ucc_acc': 0.9375, 'loss': 0.52659}\n",
            "Step 140880: {'train_ae_loss': 0.65237, 'train_ucc_loss': 0.47437, 'train_ucc_acc': 0.8125, 'loss': 0.56337}\n",
            "Step 140900: {'train_ae_loss': 0.67901, 'train_ucc_loss': 0.41979, 'train_ucc_acc': 0.875, 'loss': 0.5494}\n",
            "Step 140920: {'train_ae_loss': 0.66075, 'train_ucc_loss': 0.35558, 'train_ucc_acc': 0.96875, 'loss': 0.50817}\n",
            "Step 140940: {'train_ae_loss': 0.67972, 'train_ucc_loss': 0.43386, 'train_ucc_acc': 0.875, 'loss': 0.55679}\n",
            "Step 140960: {'train_ae_loss': 0.67085, 'train_ucc_loss': 0.43368, 'train_ucc_acc': 0.875, 'loss': 0.55226}\n",
            "Step 140980: {'train_ae_loss': 0.64911, 'train_ucc_loss': 0.48478, 'train_ucc_acc': 0.84375, 'loss': 0.56694}\n",
            "Step 141000: {'train_ae_loss': 0.6753, 'train_ucc_loss': 0.41923, 'train_ucc_acc': 0.90625, 'loss': 0.54726}\n",
            "step: 141000,eval_ae_loss: 0.64988,eval_ucc_loss: 0.49208,eval_ucc_acc: 0.80957\n",
            "Step 141020: {'train_ae_loss': 0.66537, 'train_ucc_loss': 0.44068, 'train_ucc_acc': 0.875, 'loss': 0.55303}\n",
            "Step 141040: {'train_ae_loss': 0.66724, 'train_ucc_loss': 0.4244, 'train_ucc_acc': 0.875, 'loss': 0.54582}\n",
            "Step 141060: {'train_ae_loss': 0.65247, 'train_ucc_loss': 0.44375, 'train_ucc_acc': 0.875, 'loss': 0.54811}\n",
            "Step 141080: {'train_ae_loss': 0.67908, 'train_ucc_loss': 0.47694, 'train_ucc_acc': 0.84375, 'loss': 0.57801}\n",
            "Step 141100: {'train_ae_loss': 0.66282, 'train_ucc_loss': 0.36872, 'train_ucc_acc': 0.9375, 'loss': 0.51577}\n",
            "Step 141120: {'train_ae_loss': 0.66199, 'train_ucc_loss': 0.41004, 'train_ucc_acc': 0.90625, 'loss': 0.53602}\n",
            "Step 141140: {'train_ae_loss': 0.66509, 'train_ucc_loss': 0.4131, 'train_ucc_acc': 0.90625, 'loss': 0.53909}\n",
            "Step 141160: {'train_ae_loss': 0.66812, 'train_ucc_loss': 0.40685, 'train_ucc_acc': 0.90625, 'loss': 0.53748}\n",
            "Step 141180: {'train_ae_loss': 0.66472, 'train_ucc_loss': 0.53153, 'train_ucc_acc': 0.75, 'loss': 0.59813}\n",
            "Step 141200: {'train_ae_loss': 0.65454, 'train_ucc_loss': 0.39595, 'train_ucc_acc': 0.90625, 'loss': 0.52525}\n",
            "Step 141220: {'train_ae_loss': 0.6557, 'train_ucc_loss': 0.43382, 'train_ucc_acc': 0.875, 'loss': 0.54476}\n",
            "Step 141240: {'train_ae_loss': 0.65286, 'train_ucc_loss': 0.50876, 'train_ucc_acc': 0.8125, 'loss': 0.58081}\n",
            "Step 141260: {'train_ae_loss': 0.66857, 'train_ucc_loss': 0.37487, 'train_ucc_acc': 0.9375, 'loss': 0.52172}\n",
            "Step 141280: {'train_ae_loss': 0.66792, 'train_ucc_loss': 0.36902, 'train_ucc_acc': 0.9375, 'loss': 0.51847}\n",
            "Step 141300: {'train_ae_loss': 0.68384, 'train_ucc_loss': 0.56508, 'train_ucc_acc': 0.71875, 'loss': 0.62446}\n",
            "Step 141320: {'train_ae_loss': 0.66359, 'train_ucc_loss': 0.42923, 'train_ucc_acc': 0.875, 'loss': 0.54641}\n",
            "Step 141340: {'train_ae_loss': 0.65459, 'train_ucc_loss': 0.33268, 'train_ucc_acc': 1.0, 'loss': 0.49363}\n",
            "Step 141360: {'train_ae_loss': 0.65757, 'train_ucc_loss': 0.50894, 'train_ucc_acc': 0.78125, 'loss': 0.58326}\n",
            "Step 141380: {'train_ae_loss': 0.6526, 'train_ucc_loss': 0.47132, 'train_ucc_acc': 0.84375, 'loss': 0.56196}\n",
            "Step 141400: {'train_ae_loss': 0.66159, 'train_ucc_loss': 0.37897, 'train_ucc_acc': 0.9375, 'loss': 0.52028}\n",
            "Step 141420: {'train_ae_loss': 0.68179, 'train_ucc_loss': 0.47292, 'train_ucc_acc': 0.8125, 'loss': 0.57735}\n",
            "Step 141440: {'train_ae_loss': 0.66007, 'train_ucc_loss': 0.43411, 'train_ucc_acc': 0.875, 'loss': 0.54709}\n",
            "Step 141460: {'train_ae_loss': 0.64866, 'train_ucc_loss': 0.44581, 'train_ucc_acc': 0.875, 'loss': 0.54723}\n",
            "Step 141480: {'train_ae_loss': 0.67092, 'train_ucc_loss': 0.52194, 'train_ucc_acc': 0.78125, 'loss': 0.59643}\n",
            "Step 141500: {'train_ae_loss': 0.66599, 'train_ucc_loss': 0.36244, 'train_ucc_acc': 0.9375, 'loss': 0.51421}\n",
            "Step 141520: {'train_ae_loss': 0.65867, 'train_ucc_loss': 0.36814, 'train_ucc_acc': 0.96875, 'loss': 0.5134}\n",
            "Step 141540: {'train_ae_loss': 0.64149, 'train_ucc_loss': 0.47592, 'train_ucc_acc': 0.84375, 'loss': 0.5587}\n",
            "Step 141560: {'train_ae_loss': 0.66141, 'train_ucc_loss': 0.39268, 'train_ucc_acc': 0.9375, 'loss': 0.52704}\n",
            "Step 141580: {'train_ae_loss': 0.67081, 'train_ucc_loss': 0.44892, 'train_ucc_acc': 0.875, 'loss': 0.55987}\n",
            "Step 141600: {'train_ae_loss': 0.66772, 'train_ucc_loss': 0.42203, 'train_ucc_acc': 0.90625, 'loss': 0.54488}\n",
            "Step 141620: {'train_ae_loss': 0.6553, 'train_ucc_loss': 0.44337, 'train_ucc_acc': 0.84375, 'loss': 0.54933}\n",
            "Step 141640: {'train_ae_loss': 0.66617, 'train_ucc_loss': 0.4443, 'train_ucc_acc': 0.875, 'loss': 0.55524}\n",
            "Step 141660: {'train_ae_loss': 0.67637, 'train_ucc_loss': 0.46941, 'train_ucc_acc': 0.84375, 'loss': 0.57289}\n",
            "Step 141680: {'train_ae_loss': 0.65938, 'train_ucc_loss': 0.52574, 'train_ucc_acc': 0.78125, 'loss': 0.59256}\n",
            "Step 141700: {'train_ae_loss': 0.66417, 'train_ucc_loss': 0.58665, 'train_ucc_acc': 0.75, 'loss': 0.62541}\n",
            "Step 141720: {'train_ae_loss': 0.6705, 'train_ucc_loss': 0.34395, 'train_ucc_acc': 0.96875, 'loss': 0.50722}\n",
            "Step 141740: {'train_ae_loss': 0.66712, 'train_ucc_loss': 0.35786, 'train_ucc_acc': 0.96875, 'loss': 0.51249}\n",
            "Step 141760: {'train_ae_loss': 0.65871, 'train_ucc_loss': 0.53322, 'train_ucc_acc': 0.78125, 'loss': 0.59597}\n",
            "Step 141780: {'train_ae_loss': 0.67409, 'train_ucc_loss': 0.48044, 'train_ucc_acc': 0.78125, 'loss': 0.57726}\n",
            "Step 141800: {'train_ae_loss': 0.6717, 'train_ucc_loss': 0.41698, 'train_ucc_acc': 0.90625, 'loss': 0.54434}\n",
            "Step 141820: {'train_ae_loss': 0.65711, 'train_ucc_loss': 0.46587, 'train_ucc_acc': 0.875, 'loss': 0.56149}\n",
            "Step 141840: {'train_ae_loss': 0.67672, 'train_ucc_loss': 0.51744, 'train_ucc_acc': 0.78125, 'loss': 0.59708}\n",
            "Step 141860: {'train_ae_loss': 0.66613, 'train_ucc_loss': 0.42289, 'train_ucc_acc': 0.90625, 'loss': 0.54451}\n",
            "Step 141880: {'train_ae_loss': 0.66504, 'train_ucc_loss': 0.51239, 'train_ucc_acc': 0.78125, 'loss': 0.58871}\n",
            "Step 141900: {'train_ae_loss': 0.66998, 'train_ucc_loss': 0.51322, 'train_ucc_acc': 0.8125, 'loss': 0.5916}\n",
            "Step 141920: {'train_ae_loss': 0.66381, 'train_ucc_loss': 0.55655, 'train_ucc_acc': 0.71875, 'loss': 0.61018}\n",
            "Step 141940: {'train_ae_loss': 0.65742, 'train_ucc_loss': 0.4246, 'train_ucc_acc': 0.875, 'loss': 0.54101}\n",
            "Step 141960: {'train_ae_loss': 0.67101, 'train_ucc_loss': 0.43944, 'train_ucc_acc': 0.875, 'loss': 0.55522}\n",
            "Step 141980: {'train_ae_loss': 0.67109, 'train_ucc_loss': 0.48084, 'train_ucc_acc': 0.8125, 'loss': 0.57596}\n",
            "Step 142000: {'train_ae_loss': 0.68121, 'train_ucc_loss': 0.42639, 'train_ucc_acc': 0.875, 'loss': 0.5538}\n",
            "step: 142000,eval_ae_loss: 0.65737,eval_ucc_loss: 0.47418,eval_ucc_acc: 0.83398\n",
            "Step 142020: {'train_ae_loss': 0.66253, 'train_ucc_loss': 0.5502, 'train_ucc_acc': 0.71875, 'loss': 0.60637}\n",
            "Step 142040: {'train_ae_loss': 0.67763, 'train_ucc_loss': 0.4097, 'train_ucc_acc': 0.90625, 'loss': 0.54366}\n",
            "Step 142060: {'train_ae_loss': 0.67612, 'train_ucc_loss': 0.43374, 'train_ucc_acc': 0.875, 'loss': 0.55493}\n",
            "Step 142080: {'train_ae_loss': 0.66014, 'train_ucc_loss': 0.47705, 'train_ucc_acc': 0.875, 'loss': 0.5686}\n",
            "Step 142100: {'train_ae_loss': 0.65959, 'train_ucc_loss': 0.40133, 'train_ucc_acc': 0.90625, 'loss': 0.53046}\n",
            "Step 142120: {'train_ae_loss': 0.65637, 'train_ucc_loss': 0.43669, 'train_ucc_acc': 0.875, 'loss': 0.54653}\n",
            "Step 142140: {'train_ae_loss': 0.67384, 'train_ucc_loss': 0.3406, 'train_ucc_acc': 0.96875, 'loss': 0.50722}\n",
            "Step 142160: {'train_ae_loss': 0.65153, 'train_ucc_loss': 0.46376, 'train_ucc_acc': 0.84375, 'loss': 0.55764}\n",
            "Step 142180: {'train_ae_loss': 0.66437, 'train_ucc_loss': 0.51653, 'train_ucc_acc': 0.78125, 'loss': 0.59045}\n",
            "Step 142200: {'train_ae_loss': 0.66162, 'train_ucc_loss': 0.53911, 'train_ucc_acc': 0.75, 'loss': 0.60037}\n",
            "Step 142220: {'train_ae_loss': 0.66973, 'train_ucc_loss': 0.44881, 'train_ucc_acc': 0.875, 'loss': 0.55927}\n",
            "Step 142240: {'train_ae_loss': 0.66288, 'train_ucc_loss': 0.37857, 'train_ucc_acc': 0.9375, 'loss': 0.52073}\n",
            "Step 142260: {'train_ae_loss': 0.66429, 'train_ucc_loss': 0.50816, 'train_ucc_acc': 0.8125, 'loss': 0.58623}\n",
            "Step 142280: {'train_ae_loss': 0.67336, 'train_ucc_loss': 0.34803, 'train_ucc_acc': 0.96875, 'loss': 0.51069}\n",
            "Step 142300: {'train_ae_loss': 0.6656, 'train_ucc_loss': 0.45009, 'train_ucc_acc': 0.875, 'loss': 0.55784}\n",
            "Step 142320: {'train_ae_loss': 0.66907, 'train_ucc_loss': 0.52682, 'train_ucc_acc': 0.75, 'loss': 0.59794}\n",
            "Step 142340: {'train_ae_loss': 0.66238, 'train_ucc_loss': 0.44396, 'train_ucc_acc': 0.84375, 'loss': 0.55317}\n",
            "Step 142360: {'train_ae_loss': 0.66128, 'train_ucc_loss': 0.4127, 'train_ucc_acc': 0.875, 'loss': 0.53699}\n",
            "Step 142380: {'train_ae_loss': 0.66054, 'train_ucc_loss': 0.40184, 'train_ucc_acc': 0.90625, 'loss': 0.53119}\n",
            "Step 142400: {'train_ae_loss': 0.65299, 'train_ucc_loss': 0.47043, 'train_ucc_acc': 0.84375, 'loss': 0.56171}\n",
            "Step 142420: {'train_ae_loss': 0.66656, 'train_ucc_loss': 0.38816, 'train_ucc_acc': 0.9375, 'loss': 0.52736}\n",
            "Step 142440: {'train_ae_loss': 0.64947, 'train_ucc_loss': 0.44076, 'train_ucc_acc': 0.875, 'loss': 0.54512}\n",
            "Step 142460: {'train_ae_loss': 0.67021, 'train_ucc_loss': 0.4268, 'train_ucc_acc': 0.90625, 'loss': 0.5485}\n",
            "Step 142480: {'train_ae_loss': 0.65118, 'train_ucc_loss': 0.38685, 'train_ucc_acc': 0.9375, 'loss': 0.51901}\n",
            "Step 142500: {'train_ae_loss': 0.64532, 'train_ucc_loss': 0.43281, 'train_ucc_acc': 0.875, 'loss': 0.53906}\n",
            "Step 142520: {'train_ae_loss': 0.65651, 'train_ucc_loss': 0.46182, 'train_ucc_acc': 0.84375, 'loss': 0.55916}\n",
            "Step 142540: {'train_ae_loss': 0.66381, 'train_ucc_loss': 0.47987, 'train_ucc_acc': 0.84375, 'loss': 0.57184}\n",
            "Step 142560: {'train_ae_loss': 0.65683, 'train_ucc_loss': 0.46485, 'train_ucc_acc': 0.84375, 'loss': 0.56084}\n",
            "Step 142580: {'train_ae_loss': 0.66063, 'train_ucc_loss': 0.42758, 'train_ucc_acc': 0.90625, 'loss': 0.5441}\n",
            "Step 142600: {'train_ae_loss': 0.6631, 'train_ucc_loss': 0.41286, 'train_ucc_acc': 0.90625, 'loss': 0.53798}\n",
            "Step 142620: {'train_ae_loss': 0.65092, 'train_ucc_loss': 0.4284, 'train_ucc_acc': 0.875, 'loss': 0.53966}\n",
            "Step 142640: {'train_ae_loss': 0.66132, 'train_ucc_loss': 0.45592, 'train_ucc_acc': 0.8125, 'loss': 0.55862}\n",
            "Step 142660: {'train_ae_loss': 0.65388, 'train_ucc_loss': 0.45403, 'train_ucc_acc': 0.84375, 'loss': 0.55396}\n",
            "Step 142680: {'train_ae_loss': 0.66415, 'train_ucc_loss': 0.44401, 'train_ucc_acc': 0.84375, 'loss': 0.55408}\n",
            "Step 142700: {'train_ae_loss': 0.65548, 'train_ucc_loss': 0.31647, 'train_ucc_acc': 1.0, 'loss': 0.48597}\n",
            "Step 142720: {'train_ae_loss': 0.65038, 'train_ucc_loss': 0.4114, 'train_ucc_acc': 0.875, 'loss': 0.53089}\n",
            "Step 142740: {'train_ae_loss': 0.64626, 'train_ucc_loss': 0.48299, 'train_ucc_acc': 0.84375, 'loss': 0.56463}\n",
            "Step 142760: {'train_ae_loss': 0.66449, 'train_ucc_loss': 0.45158, 'train_ucc_acc': 0.84375, 'loss': 0.55804}\n",
            "Step 142780: {'train_ae_loss': 0.66133, 'train_ucc_loss': 0.55852, 'train_ucc_acc': 0.71875, 'loss': 0.60993}\n",
            "Step 142800: {'train_ae_loss': 0.63935, 'train_ucc_loss': 0.60296, 'train_ucc_acc': 0.6875, 'loss': 0.62116}\n",
            "Step 142820: {'train_ae_loss': 0.67027, 'train_ucc_loss': 0.38749, 'train_ucc_acc': 0.9375, 'loss': 0.52888}\n",
            "Step 142840: {'train_ae_loss': 0.65975, 'train_ucc_loss': 0.53892, 'train_ucc_acc': 0.78125, 'loss': 0.59933}\n",
            "Step 142860: {'train_ae_loss': 0.66189, 'train_ucc_loss': 0.4913, 'train_ucc_acc': 0.8125, 'loss': 0.57659}\n",
            "Step 142880: {'train_ae_loss': 0.65367, 'train_ucc_loss': 0.32227, 'train_ucc_acc': 1.0, 'loss': 0.48797}\n",
            "Step 142900: {'train_ae_loss': 0.65497, 'train_ucc_loss': 0.64736, 'train_ucc_acc': 0.625, 'loss': 0.65116}\n",
            "Step 142920: {'train_ae_loss': 0.65875, 'train_ucc_loss': 0.45028, 'train_ucc_acc': 0.84375, 'loss': 0.55452}\n",
            "Step 142940: {'train_ae_loss': 0.65848, 'train_ucc_loss': 0.56347, 'train_ucc_acc': 0.75, 'loss': 0.61097}\n",
            "Step 142960: {'train_ae_loss': 0.6647, 'train_ucc_loss': 0.5004, 'train_ucc_acc': 0.8125, 'loss': 0.58255}\n",
            "Step 142980: {'train_ae_loss': 0.66554, 'train_ucc_loss': 0.4486, 'train_ucc_acc': 0.875, 'loss': 0.55707}\n",
            "Step 143000: {'train_ae_loss': 0.65953, 'train_ucc_loss': 0.38852, 'train_ucc_acc': 0.90625, 'loss': 0.52402}\n",
            "step: 143000,eval_ae_loss: 0.64765,eval_ucc_loss: 0.48291,eval_ucc_acc: 0.82324\n",
            "Step 143020: {'train_ae_loss': 0.65698, 'train_ucc_loss': 0.39291, 'train_ucc_acc': 0.9375, 'loss': 0.52495}\n",
            "Step 143040: {'train_ae_loss': 0.6664, 'train_ucc_loss': 0.42468, 'train_ucc_acc': 0.875, 'loss': 0.54554}\n",
            "Step 143060: {'train_ae_loss': 0.66129, 'train_ucc_loss': 0.4959, 'train_ucc_acc': 0.84375, 'loss': 0.5786}\n",
            "Step 143080: {'train_ae_loss': 0.66356, 'train_ucc_loss': 0.47824, 'train_ucc_acc': 0.84375, 'loss': 0.5709}\n",
            "Step 143100: {'train_ae_loss': 0.65916, 'train_ucc_loss': 0.3786, 'train_ucc_acc': 0.9375, 'loss': 0.51888}\n",
            "Step 143120: {'train_ae_loss': 0.66051, 'train_ucc_loss': 0.46642, 'train_ucc_acc': 0.84375, 'loss': 0.56347}\n",
            "Step 143140: {'train_ae_loss': 0.64247, 'train_ucc_loss': 0.41563, 'train_ucc_acc': 0.90625, 'loss': 0.52905}\n",
            "Step 143160: {'train_ae_loss': 0.64442, 'train_ucc_loss': 0.49128, 'train_ucc_acc': 0.8125, 'loss': 0.56785}\n",
            "Step 143180: {'train_ae_loss': 0.6703, 'train_ucc_loss': 0.49788, 'train_ucc_acc': 0.8125, 'loss': 0.58409}\n",
            "Step 143200: {'train_ae_loss': 0.66648, 'train_ucc_loss': 0.40995, 'train_ucc_acc': 0.90625, 'loss': 0.53822}\n",
            "Step 143220: {'train_ae_loss': 0.66595, 'train_ucc_loss': 0.36844, 'train_ucc_acc': 0.9375, 'loss': 0.51719}\n",
            "Step 143240: {'train_ae_loss': 0.64922, 'train_ucc_loss': 0.46069, 'train_ucc_acc': 0.84375, 'loss': 0.55496}\n",
            "Step 143260: {'train_ae_loss': 0.63969, 'train_ucc_loss': 0.40731, 'train_ucc_acc': 0.90625, 'loss': 0.5235}\n",
            "Step 143280: {'train_ae_loss': 0.64879, 'train_ucc_loss': 0.49156, 'train_ucc_acc': 0.78125, 'loss': 0.57017}\n",
            "Step 143300: {'train_ae_loss': 0.6548, 'train_ucc_loss': 0.49392, 'train_ucc_acc': 0.8125, 'loss': 0.57436}\n",
            "Step 143320: {'train_ae_loss': 0.66158, 'train_ucc_loss': 0.41711, 'train_ucc_acc': 0.875, 'loss': 0.53934}\n",
            "Step 143340: {'train_ae_loss': 0.66658, 'train_ucc_loss': 0.50382, 'train_ucc_acc': 0.78125, 'loss': 0.5852}\n",
            "Step 143360: {'train_ae_loss': 0.67345, 'train_ucc_loss': 0.39455, 'train_ucc_acc': 0.9375, 'loss': 0.534}\n",
            "Step 143380: {'train_ae_loss': 0.65731, 'train_ucc_loss': 0.51349, 'train_ucc_acc': 0.78125, 'loss': 0.5854}\n",
            "Step 143400: {'train_ae_loss': 0.65943, 'train_ucc_loss': 0.38886, 'train_ucc_acc': 0.9375, 'loss': 0.52415}\n",
            "Step 143420: {'train_ae_loss': 0.66043, 'train_ucc_loss': 0.43733, 'train_ucc_acc': 0.875, 'loss': 0.54888}\n",
            "Step 143440: {'train_ae_loss': 0.64784, 'train_ucc_loss': 0.54799, 'train_ucc_acc': 0.78125, 'loss': 0.59792}\n",
            "Step 143460: {'train_ae_loss': 0.66013, 'train_ucc_loss': 0.34693, 'train_ucc_acc': 0.96875, 'loss': 0.50353}\n",
            "Step 143480: {'train_ae_loss': 0.67165, 'train_ucc_loss': 0.41092, 'train_ucc_acc': 0.90625, 'loss': 0.54128}\n",
            "Step 143500: {'train_ae_loss': 0.66473, 'train_ucc_loss': 0.41569, 'train_ucc_acc': 0.90625, 'loss': 0.54021}\n",
            "Step 143520: {'train_ae_loss': 0.64698, 'train_ucc_loss': 0.39671, 'train_ucc_acc': 0.90625, 'loss': 0.52184}\n",
            "Step 143540: {'train_ae_loss': 0.64431, 'train_ucc_loss': 0.464, 'train_ucc_acc': 0.8125, 'loss': 0.55415}\n",
            "Step 143560: {'train_ae_loss': 0.65069, 'train_ucc_loss': 0.58396, 'train_ucc_acc': 0.71875, 'loss': 0.61733}\n",
            "Step 143580: {'train_ae_loss': 0.67564, 'train_ucc_loss': 0.50282, 'train_ucc_acc': 0.8125, 'loss': 0.58923}\n",
            "Step 143600: {'train_ae_loss': 0.65621, 'train_ucc_loss': 0.49518, 'train_ucc_acc': 0.84375, 'loss': 0.5757}\n",
            "Step 143620: {'train_ae_loss': 0.66772, 'train_ucc_loss': 0.44078, 'train_ucc_acc': 0.875, 'loss': 0.55425}\n",
            "Step 143640: {'train_ae_loss': 0.663, 'train_ucc_loss': 0.38551, 'train_ucc_acc': 0.90625, 'loss': 0.52426}\n",
            "Step 143660: {'train_ae_loss': 0.6561, 'train_ucc_loss': 0.40323, 'train_ucc_acc': 0.90625, 'loss': 0.52966}\n",
            "Step 143680: {'train_ae_loss': 0.66027, 'train_ucc_loss': 0.46569, 'train_ucc_acc': 0.8125, 'loss': 0.56298}\n",
            "Step 143700: {'train_ae_loss': 0.66551, 'train_ucc_loss': 0.53717, 'train_ucc_acc': 0.78125, 'loss': 0.60134}\n",
            "Step 143720: {'train_ae_loss': 0.64765, 'train_ucc_loss': 0.46896, 'train_ucc_acc': 0.84375, 'loss': 0.55831}\n",
            "Step 143740: {'train_ae_loss': 0.65723, 'train_ucc_loss': 0.47643, 'train_ucc_acc': 0.8125, 'loss': 0.56683}\n",
            "Step 143760: {'train_ae_loss': 0.67685, 'train_ucc_loss': 0.46136, 'train_ucc_acc': 0.84375, 'loss': 0.5691}\n",
            "Step 143780: {'train_ae_loss': 0.66052, 'train_ucc_loss': 0.44473, 'train_ucc_acc': 0.875, 'loss': 0.55262}\n",
            "Step 143800: {'train_ae_loss': 0.6514, 'train_ucc_loss': 0.41818, 'train_ucc_acc': 0.90625, 'loss': 0.53479}\n",
            "Step 143820: {'train_ae_loss': 0.64445, 'train_ucc_loss': 0.39769, 'train_ucc_acc': 0.90625, 'loss': 0.52107}\n",
            "Step 143840: {'train_ae_loss': 0.66626, 'train_ucc_loss': 0.39227, 'train_ucc_acc': 0.9375, 'loss': 0.52927}\n",
            "Step 143860: {'train_ae_loss': 0.6665, 'train_ucc_loss': 0.34653, 'train_ucc_acc': 0.96875, 'loss': 0.50651}\n",
            "Step 143880: {'train_ae_loss': 0.66836, 'train_ucc_loss': 0.44381, 'train_ucc_acc': 0.875, 'loss': 0.55608}\n",
            "Step 143900: {'train_ae_loss': 0.65636, 'train_ucc_loss': 0.42425, 'train_ucc_acc': 0.875, 'loss': 0.5403}\n",
            "Step 143920: {'train_ae_loss': 0.64123, 'train_ucc_loss': 0.47603, 'train_ucc_acc': 0.8125, 'loss': 0.55863}\n",
            "Step 143940: {'train_ae_loss': 0.65825, 'train_ucc_loss': 0.35673, 'train_ucc_acc': 0.96875, 'loss': 0.50749}\n",
            "Step 143960: {'train_ae_loss': 0.66215, 'train_ucc_loss': 0.34542, 'train_ucc_acc': 0.9375, 'loss': 0.50378}\n",
            "Step 143980: {'train_ae_loss': 0.6672, 'train_ucc_loss': 0.42701, 'train_ucc_acc': 0.875, 'loss': 0.5471}\n",
            "Step 144000: {'train_ae_loss': 0.66443, 'train_ucc_loss': 0.49791, 'train_ucc_acc': 0.78125, 'loss': 0.58117}\n",
            "step: 144000,eval_ae_loss: 0.65061,eval_ucc_loss: 0.51421,eval_ucc_acc: 0.7959\n",
            "Step 144020: {'train_ae_loss': 0.66231, 'train_ucc_loss': 0.58334, 'train_ucc_acc': 0.6875, 'loss': 0.62282}\n",
            "Step 144040: {'train_ae_loss': 0.66051, 'train_ucc_loss': 0.42509, 'train_ucc_acc': 0.90625, 'loss': 0.5428}\n",
            "Step 144060: {'train_ae_loss': 0.64705, 'train_ucc_loss': 0.4256, 'train_ucc_acc': 0.875, 'loss': 0.53633}\n",
            "Step 144080: {'train_ae_loss': 0.67853, 'train_ucc_loss': 0.44215, 'train_ucc_acc': 0.84375, 'loss': 0.56034}\n",
            "Step 144100: {'train_ae_loss': 0.6697, 'train_ucc_loss': 0.48171, 'train_ucc_acc': 0.8125, 'loss': 0.5757}\n",
            "Step 144120: {'train_ae_loss': 0.67055, 'train_ucc_loss': 0.41362, 'train_ucc_acc': 0.90625, 'loss': 0.54209}\n",
            "Step 144140: {'train_ae_loss': 0.65353, 'train_ucc_loss': 0.42718, 'train_ucc_acc': 0.90625, 'loss': 0.54036}\n",
            "Step 144160: {'train_ae_loss': 0.66647, 'train_ucc_loss': 0.36317, 'train_ucc_acc': 0.9375, 'loss': 0.51482}\n",
            "Step 144180: {'train_ae_loss': 0.66161, 'train_ucc_loss': 0.41533, 'train_ucc_acc': 0.90625, 'loss': 0.53847}\n",
            "Step 144200: {'train_ae_loss': 0.66379, 'train_ucc_loss': 0.43778, 'train_ucc_acc': 0.875, 'loss': 0.55079}\n",
            "Step 144220: {'train_ae_loss': 0.66753, 'train_ucc_loss': 0.36734, 'train_ucc_acc': 0.9375, 'loss': 0.51744}\n",
            "Step 144240: {'train_ae_loss': 0.65929, 'train_ucc_loss': 0.38898, 'train_ucc_acc': 0.9375, 'loss': 0.52414}\n",
            "Step 144260: {'train_ae_loss': 0.6642, 'train_ucc_loss': 0.63275, 'train_ucc_acc': 0.65625, 'loss': 0.64848}\n",
            "Step 144280: {'train_ae_loss': 0.6708, 'train_ucc_loss': 0.38358, 'train_ucc_acc': 0.9375, 'loss': 0.52719}\n",
            "Step 144300: {'train_ae_loss': 0.66688, 'train_ucc_loss': 0.37082, 'train_ucc_acc': 0.9375, 'loss': 0.51885}\n",
            "Step 144320: {'train_ae_loss': 0.66315, 'train_ucc_loss': 0.46795, 'train_ucc_acc': 0.84375, 'loss': 0.56555}\n",
            "Step 144340: {'train_ae_loss': 0.68705, 'train_ucc_loss': 0.39447, 'train_ucc_acc': 0.90625, 'loss': 0.54076}\n",
            "Step 144360: {'train_ae_loss': 0.66256, 'train_ucc_loss': 0.4363, 'train_ucc_acc': 0.84375, 'loss': 0.54943}\n",
            "Step 144380: {'train_ae_loss': 0.66422, 'train_ucc_loss': 0.50201, 'train_ucc_acc': 0.78125, 'loss': 0.58312}\n",
            "Step 144400: {'train_ae_loss': 0.66633, 'train_ucc_loss': 0.35963, 'train_ucc_acc': 0.9375, 'loss': 0.51298}\n",
            "Step 144420: {'train_ae_loss': 0.66704, 'train_ucc_loss': 0.407, 'train_ucc_acc': 0.90625, 'loss': 0.53702}\n",
            "Step 144440: {'train_ae_loss': 0.65203, 'train_ucc_loss': 0.52349, 'train_ucc_acc': 0.78125, 'loss': 0.58776}\n",
            "Step 144460: {'train_ae_loss': 0.65485, 'train_ucc_loss': 0.43319, 'train_ucc_acc': 0.875, 'loss': 0.54402}\n",
            "Step 144480: {'train_ae_loss': 0.66315, 'train_ucc_loss': 0.43506, 'train_ucc_acc': 0.875, 'loss': 0.5491}\n",
            "Step 144500: {'train_ae_loss': 0.67503, 'train_ucc_loss': 0.4314, 'train_ucc_acc': 0.875, 'loss': 0.55322}\n",
            "Step 144520: {'train_ae_loss': 0.65114, 'train_ucc_loss': 0.50193, 'train_ucc_acc': 0.75, 'loss': 0.57653}\n",
            "Step 144540: {'train_ae_loss': 0.65535, 'train_ucc_loss': 0.41638, 'train_ucc_acc': 0.875, 'loss': 0.53587}\n",
            "Step 144560: {'train_ae_loss': 0.66501, 'train_ucc_loss': 0.39524, 'train_ucc_acc': 0.9375, 'loss': 0.53012}\n",
            "Step 144580: {'train_ae_loss': 0.657, 'train_ucc_loss': 0.40782, 'train_ucc_acc': 0.90625, 'loss': 0.53241}\n",
            "Step 144600: {'train_ae_loss': 0.67308, 'train_ucc_loss': 0.56332, 'train_ucc_acc': 0.75, 'loss': 0.6182}\n",
            "Step 144620: {'train_ae_loss': 0.65144, 'train_ucc_loss': 0.42968, 'train_ucc_acc': 0.875, 'loss': 0.54056}\n",
            "Step 144640: {'train_ae_loss': 0.67228, 'train_ucc_loss': 0.41774, 'train_ucc_acc': 0.90625, 'loss': 0.54501}\n",
            "Step 144660: {'train_ae_loss': 0.66255, 'train_ucc_loss': 0.48489, 'train_ucc_acc': 0.84375, 'loss': 0.57372}\n",
            "Step 144680: {'train_ae_loss': 0.65509, 'train_ucc_loss': 0.46095, 'train_ucc_acc': 0.84375, 'loss': 0.55802}\n",
            "Step 144700: {'train_ae_loss': 0.66524, 'train_ucc_loss': 0.40514, 'train_ucc_acc': 0.875, 'loss': 0.53519}\n",
            "Step 144720: {'train_ae_loss': 0.66149, 'train_ucc_loss': 0.31552, 'train_ucc_acc': 1.0, 'loss': 0.48851}\n",
            "Step 144740: {'train_ae_loss': 0.66677, 'train_ucc_loss': 0.39179, 'train_ucc_acc': 0.9375, 'loss': 0.52928}\n",
            "Step 144760: {'train_ae_loss': 0.67631, 'train_ucc_loss': 0.37876, 'train_ucc_acc': 0.9375, 'loss': 0.52754}\n",
            "Step 144780: {'train_ae_loss': 0.68037, 'train_ucc_loss': 0.47888, 'train_ucc_acc': 0.84375, 'loss': 0.57962}\n",
            "Step 144800: {'train_ae_loss': 0.66957, 'train_ucc_loss': 0.57464, 'train_ucc_acc': 0.71875, 'loss': 0.6221}\n",
            "Step 144820: {'train_ae_loss': 0.66335, 'train_ucc_loss': 0.47466, 'train_ucc_acc': 0.84375, 'loss': 0.569}\n",
            "Step 144840: {'train_ae_loss': 0.64499, 'train_ucc_loss': 0.37537, 'train_ucc_acc': 0.9375, 'loss': 0.51018}\n",
            "Step 144860: {'train_ae_loss': 0.65428, 'train_ucc_loss': 0.38701, 'train_ucc_acc': 0.9375, 'loss': 0.52065}\n",
            "Step 144880: {'train_ae_loss': 0.67703, 'train_ucc_loss': 0.37611, 'train_ucc_acc': 0.9375, 'loss': 0.52657}\n",
            "Step 144900: {'train_ae_loss': 0.64277, 'train_ucc_loss': 0.39933, 'train_ucc_acc': 0.90625, 'loss': 0.52105}\n",
            "Step 144920: {'train_ae_loss': 0.66404, 'train_ucc_loss': 0.44691, 'train_ucc_acc': 0.875, 'loss': 0.55547}\n",
            "Step 144940: {'train_ae_loss': 0.64908, 'train_ucc_loss': 0.46755, 'train_ucc_acc': 0.84375, 'loss': 0.55831}\n",
            "Step 144960: {'train_ae_loss': 0.64895, 'train_ucc_loss': 0.44127, 'train_ucc_acc': 0.875, 'loss': 0.54511}\n",
            "Step 144980: {'train_ae_loss': 0.65659, 'train_ucc_loss': 0.39838, 'train_ucc_acc': 0.90625, 'loss': 0.52748}\n",
            "Step 145000: {'train_ae_loss': 0.64773, 'train_ucc_loss': 0.38851, 'train_ucc_acc': 0.90625, 'loss': 0.51812}\n",
            "step: 145000,eval_ae_loss: 0.64637,eval_ucc_loss: 0.50432,eval_ucc_acc: 0.79883\n",
            "Step 145020: {'train_ae_loss': 0.65032, 'train_ucc_loss': 0.50294, 'train_ucc_acc': 0.8125, 'loss': 0.57663}\n",
            "Step 145040: {'train_ae_loss': 0.66746, 'train_ucc_loss': 0.45368, 'train_ucc_acc': 0.84375, 'loss': 0.56057}\n",
            "Step 145060: {'train_ae_loss': 0.66123, 'train_ucc_loss': 0.45486, 'train_ucc_acc': 0.875, 'loss': 0.55804}\n",
            "Step 145080: {'train_ae_loss': 0.63224, 'train_ucc_loss': 0.44321, 'train_ucc_acc': 0.875, 'loss': 0.53772}\n",
            "Step 145100: {'train_ae_loss': 0.66264, 'train_ucc_loss': 0.36426, 'train_ucc_acc': 0.9375, 'loss': 0.51345}\n",
            "Step 145120: {'train_ae_loss': 0.65356, 'train_ucc_loss': 0.34684, 'train_ucc_acc': 0.96875, 'loss': 0.5002}\n",
            "Step 145140: {'train_ae_loss': 0.667, 'train_ucc_loss': 0.38025, 'train_ucc_acc': 0.9375, 'loss': 0.52362}\n",
            "Step 145160: {'train_ae_loss': 0.65907, 'train_ucc_loss': 0.38349, 'train_ucc_acc': 0.9375, 'loss': 0.52128}\n",
            "Step 145180: {'train_ae_loss': 0.6728, 'train_ucc_loss': 0.43401, 'train_ucc_acc': 0.875, 'loss': 0.5534}\n",
            "Step 145200: {'train_ae_loss': 0.66323, 'train_ucc_loss': 0.46185, 'train_ucc_acc': 0.84375, 'loss': 0.56254}\n",
            "Step 145220: {'train_ae_loss': 0.656, 'train_ucc_loss': 0.43254, 'train_ucc_acc': 0.875, 'loss': 0.54427}\n",
            "Step 145240: {'train_ae_loss': 0.65323, 'train_ucc_loss': 0.43805, 'train_ucc_acc': 0.875, 'loss': 0.54564}\n",
            "Step 145260: {'train_ae_loss': 0.64782, 'train_ucc_loss': 0.51471, 'train_ucc_acc': 0.78125, 'loss': 0.58126}\n",
            "Step 145280: {'train_ae_loss': 0.6794, 'train_ucc_loss': 0.47739, 'train_ucc_acc': 0.84375, 'loss': 0.5784}\n",
            "Step 145300: {'train_ae_loss': 0.66188, 'train_ucc_loss': 0.37593, 'train_ucc_acc': 0.9375, 'loss': 0.51891}\n",
            "Step 145320: {'train_ae_loss': 0.65926, 'train_ucc_loss': 0.42087, 'train_ucc_acc': 0.90625, 'loss': 0.54007}\n",
            "Step 145340: {'train_ae_loss': 0.66606, 'train_ucc_loss': 0.50148, 'train_ucc_acc': 0.8125, 'loss': 0.58377}\n",
            "Step 145360: {'train_ae_loss': 0.65784, 'train_ucc_loss': 0.45296, 'train_ucc_acc': 0.875, 'loss': 0.5554}\n",
            "Step 145380: {'train_ae_loss': 0.65222, 'train_ucc_loss': 0.54817, 'train_ucc_acc': 0.75, 'loss': 0.6002}\n",
            "Step 145400: {'train_ae_loss': 0.65437, 'train_ucc_loss': 0.49728, 'train_ucc_acc': 0.8125, 'loss': 0.57582}\n",
            "Step 145420: {'train_ae_loss': 0.63556, 'train_ucc_loss': 0.39131, 'train_ucc_acc': 0.90625, 'loss': 0.51343}\n",
            "Step 145440: {'train_ae_loss': 0.64654, 'train_ucc_loss': 0.45769, 'train_ucc_acc': 0.875, 'loss': 0.55211}\n",
            "Step 145460: {'train_ae_loss': 0.63672, 'train_ucc_loss': 0.54704, 'train_ucc_acc': 0.75, 'loss': 0.59188}\n",
            "Step 145480: {'train_ae_loss': 0.66115, 'train_ucc_loss': 0.34263, 'train_ucc_acc': 0.96875, 'loss': 0.50189}\n",
            "Step 145500: {'train_ae_loss': 0.6482, 'train_ucc_loss': 0.49168, 'train_ucc_acc': 0.8125, 'loss': 0.56994}\n",
            "Step 145520: {'train_ae_loss': 0.66375, 'train_ucc_loss': 0.3834, 'train_ucc_acc': 0.90625, 'loss': 0.52357}\n",
            "Step 145540: {'train_ae_loss': 0.66194, 'train_ucc_loss': 0.39157, 'train_ucc_acc': 0.875, 'loss': 0.52675}\n",
            "Step 145560: {'train_ae_loss': 0.67065, 'train_ucc_loss': 0.36138, 'train_ucc_acc': 0.96875, 'loss': 0.51601}\n",
            "Step 145580: {'train_ae_loss': 0.65746, 'train_ucc_loss': 0.34411, 'train_ucc_acc': 1.0, 'loss': 0.50079}\n",
            "Step 145600: {'train_ae_loss': 0.64475, 'train_ucc_loss': 0.49129, 'train_ucc_acc': 0.8125, 'loss': 0.56802}\n",
            "Step 145620: {'train_ae_loss': 0.65647, 'train_ucc_loss': 0.4923, 'train_ucc_acc': 0.8125, 'loss': 0.57438}\n",
            "Step 145640: {'train_ae_loss': 0.67236, 'train_ucc_loss': 0.40119, 'train_ucc_acc': 0.90625, 'loss': 0.53678}\n",
            "Step 145660: {'train_ae_loss': 0.66383, 'train_ucc_loss': 0.47989, 'train_ucc_acc': 0.8125, 'loss': 0.57186}\n",
            "Step 145680: {'train_ae_loss': 0.64803, 'train_ucc_loss': 0.57381, 'train_ucc_acc': 0.71875, 'loss': 0.61092}\n",
            "Step 145700: {'train_ae_loss': 0.67486, 'train_ucc_loss': 0.4379, 'train_ucc_acc': 0.875, 'loss': 0.55638}\n",
            "Step 145720: {'train_ae_loss': 0.64835, 'train_ucc_loss': 0.52003, 'train_ucc_acc': 0.78125, 'loss': 0.58419}\n",
            "Step 145740: {'train_ae_loss': 0.66488, 'train_ucc_loss': 0.34875, 'train_ucc_acc': 0.96875, 'loss': 0.50682}\n",
            "Step 145760: {'train_ae_loss': 0.65255, 'train_ucc_loss': 0.49298, 'train_ucc_acc': 0.8125, 'loss': 0.57276}\n",
            "Step 145780: {'train_ae_loss': 0.67521, 'train_ucc_loss': 0.36133, 'train_ucc_acc': 0.9375, 'loss': 0.51827}\n",
            "Step 145800: {'train_ae_loss': 0.66365, 'train_ucc_loss': 0.44475, 'train_ucc_acc': 0.875, 'loss': 0.5542}\n",
            "Step 145820: {'train_ae_loss': 0.65987, 'train_ucc_loss': 0.45012, 'train_ucc_acc': 0.84375, 'loss': 0.55499}\n",
            "Step 145840: {'train_ae_loss': 0.66461, 'train_ucc_loss': 0.47969, 'train_ucc_acc': 0.8125, 'loss': 0.57215}\n",
            "Step 145860: {'train_ae_loss': 0.67493, 'train_ucc_loss': 0.40168, 'train_ucc_acc': 0.90625, 'loss': 0.5383}\n",
            "Step 145880: {'train_ae_loss': 0.68445, 'train_ucc_loss': 0.38613, 'train_ucc_acc': 0.9375, 'loss': 0.53529}\n",
            "Step 145900: {'train_ae_loss': 0.67069, 'train_ucc_loss': 0.38112, 'train_ucc_acc': 0.9375, 'loss': 0.52591}\n",
            "Step 145920: {'train_ae_loss': 0.65711, 'train_ucc_loss': 0.53325, 'train_ucc_acc': 0.78125, 'loss': 0.59518}\n",
            "Step 145940: {'train_ae_loss': 0.64008, 'train_ucc_loss': 0.51774, 'train_ucc_acc': 0.78125, 'loss': 0.57891}\n",
            "Step 145960: {'train_ae_loss': 0.65631, 'train_ucc_loss': 0.39081, 'train_ucc_acc': 0.9375, 'loss': 0.52356}\n",
            "Step 145980: {'train_ae_loss': 0.66581, 'train_ucc_loss': 0.53407, 'train_ucc_acc': 0.75, 'loss': 0.59994}\n",
            "Step 146000: {'train_ae_loss': 0.66019, 'train_ucc_loss': 0.45575, 'train_ucc_acc': 0.875, 'loss': 0.55797}\n",
            "step: 146000,eval_ae_loss: 0.65599,eval_ucc_loss: 0.46047,eval_ucc_acc: 0.84375\n",
            "Step 146020: {'train_ae_loss': 0.64624, 'train_ucc_loss': 0.38246, 'train_ucc_acc': 0.9375, 'loss': 0.51435}\n",
            "Step 146040: {'train_ae_loss': 0.66844, 'train_ucc_loss': 0.51607, 'train_ucc_acc': 0.78125, 'loss': 0.59226}\n",
            "Step 146060: {'train_ae_loss': 0.66619, 'train_ucc_loss': 0.49938, 'train_ucc_acc': 0.75, 'loss': 0.58278}\n",
            "Step 146080: {'train_ae_loss': 0.6718, 'train_ucc_loss': 0.44685, 'train_ucc_acc': 0.875, 'loss': 0.55933}\n",
            "Step 146100: {'train_ae_loss': 0.65382, 'train_ucc_loss': 0.52822, 'train_ucc_acc': 0.78125, 'loss': 0.59102}\n",
            "Step 146120: {'train_ae_loss': 0.67595, 'train_ucc_loss': 0.41571, 'train_ucc_acc': 0.90625, 'loss': 0.54583}\n",
            "Step 146140: {'train_ae_loss': 0.65779, 'train_ucc_loss': 0.4872, 'train_ucc_acc': 0.8125, 'loss': 0.57249}\n",
            "Step 146160: {'train_ae_loss': 0.67296, 'train_ucc_loss': 0.37219, 'train_ucc_acc': 0.9375, 'loss': 0.52258}\n",
            "Step 146180: {'train_ae_loss': 0.66256, 'train_ucc_loss': 0.42199, 'train_ucc_acc': 0.90625, 'loss': 0.54227}\n",
            "Step 146200: {'train_ae_loss': 0.67098, 'train_ucc_loss': 0.46314, 'train_ucc_acc': 0.84375, 'loss': 0.56706}\n",
            "Step 146220: {'train_ae_loss': 0.66991, 'train_ucc_loss': 0.60378, 'train_ucc_acc': 0.65625, 'loss': 0.63684}\n",
            "Step 146240: {'train_ae_loss': 0.66566, 'train_ucc_loss': 0.49245, 'train_ucc_acc': 0.84375, 'loss': 0.57905}\n",
            "Step 146260: {'train_ae_loss': 0.66234, 'train_ucc_loss': 0.42976, 'train_ucc_acc': 0.84375, 'loss': 0.54605}\n",
            "Step 146280: {'train_ae_loss': 0.65247, 'train_ucc_loss': 0.37617, 'train_ucc_acc': 0.9375, 'loss': 0.51432}\n",
            "Step 146300: {'train_ae_loss': 0.67913, 'train_ucc_loss': 0.4866, 'train_ucc_acc': 0.8125, 'loss': 0.58287}\n",
            "Step 146320: {'train_ae_loss': 0.64508, 'train_ucc_loss': 0.4481, 'train_ucc_acc': 0.84375, 'loss': 0.54659}\n",
            "Step 146340: {'train_ae_loss': 0.65241, 'train_ucc_loss': 0.40001, 'train_ucc_acc': 0.9375, 'loss': 0.52621}\n",
            "Step 146360: {'train_ae_loss': 0.67185, 'train_ucc_loss': 0.4185, 'train_ucc_acc': 0.875, 'loss': 0.54517}\n",
            "Step 146380: {'train_ae_loss': 0.66997, 'train_ucc_loss': 0.37033, 'train_ucc_acc': 0.90625, 'loss': 0.52015}\n",
            "Step 146400: {'train_ae_loss': 0.65513, 'train_ucc_loss': 0.42574, 'train_ucc_acc': 0.90625, 'loss': 0.54043}\n",
            "Step 146420: {'train_ae_loss': 0.6705, 'train_ucc_loss': 0.47024, 'train_ucc_acc': 0.8125, 'loss': 0.57037}\n",
            "Step 146440: {'train_ae_loss': 0.66259, 'train_ucc_loss': 0.33397, 'train_ucc_acc': 1.0, 'loss': 0.49828}\n",
            "Step 146460: {'train_ae_loss': 0.6802, 'train_ucc_loss': 0.35371, 'train_ucc_acc': 0.96875, 'loss': 0.51696}\n",
            "Step 146480: {'train_ae_loss': 0.65863, 'train_ucc_loss': 0.43774, 'train_ucc_acc': 0.875, 'loss': 0.54819}\n",
            "Step 146500: {'train_ae_loss': 0.65227, 'train_ucc_loss': 0.37176, 'train_ucc_acc': 0.9375, 'loss': 0.51201}\n",
            "Step 146520: {'train_ae_loss': 0.6599, 'train_ucc_loss': 0.43296, 'train_ucc_acc': 0.875, 'loss': 0.54643}\n",
            "Step 146540: {'train_ae_loss': 0.65851, 'train_ucc_loss': 0.42571, 'train_ucc_acc': 0.875, 'loss': 0.54211}\n",
            "Step 146560: {'train_ae_loss': 0.66881, 'train_ucc_loss': 0.38253, 'train_ucc_acc': 0.9375, 'loss': 0.52567}\n",
            "Step 146580: {'train_ae_loss': 0.66697, 'train_ucc_loss': 0.44497, 'train_ucc_acc': 0.875, 'loss': 0.55597}\n",
            "Step 146600: {'train_ae_loss': 0.66407, 'train_ucc_loss': 0.41878, 'train_ucc_acc': 0.90625, 'loss': 0.54142}\n",
            "Step 146620: {'train_ae_loss': 0.64733, 'train_ucc_loss': 0.46973, 'train_ucc_acc': 0.8125, 'loss': 0.55853}\n",
            "Step 146640: {'train_ae_loss': 0.67375, 'train_ucc_loss': 0.54852, 'train_ucc_acc': 0.75, 'loss': 0.61113}\n",
            "Step 146660: {'train_ae_loss': 0.66176, 'train_ucc_loss': 0.4205, 'train_ucc_acc': 0.875, 'loss': 0.54113}\n",
            "Step 146680: {'train_ae_loss': 0.67415, 'train_ucc_loss': 0.40995, 'train_ucc_acc': 0.90625, 'loss': 0.54205}\n",
            "Step 146700: {'train_ae_loss': 0.68598, 'train_ucc_loss': 0.4865, 'train_ucc_acc': 0.84375, 'loss': 0.58624}\n",
            "Step 146720: {'train_ae_loss': 0.65054, 'train_ucc_loss': 0.52646, 'train_ucc_acc': 0.75, 'loss': 0.5885}\n",
            "Step 146740: {'train_ae_loss': 0.65774, 'train_ucc_loss': 0.43875, 'train_ucc_acc': 0.84375, 'loss': 0.54825}\n",
            "Step 146760: {'train_ae_loss': 0.66997, 'train_ucc_loss': 0.43764, 'train_ucc_acc': 0.90625, 'loss': 0.5538}\n",
            "Step 146780: {'train_ae_loss': 0.66723, 'train_ucc_loss': 0.36792, 'train_ucc_acc': 0.9375, 'loss': 0.51758}\n",
            "Step 146800: {'train_ae_loss': 0.67692, 'train_ucc_loss': 0.45953, 'train_ucc_acc': 0.875, 'loss': 0.56822}\n",
            "Step 146820: {'train_ae_loss': 0.64789, 'train_ucc_loss': 0.47311, 'train_ucc_acc': 0.78125, 'loss': 0.5605}\n",
            "Step 146840: {'train_ae_loss': 0.64587, 'train_ucc_loss': 0.41101, 'train_ucc_acc': 0.90625, 'loss': 0.52844}\n",
            "Step 146860: {'train_ae_loss': 0.66116, 'train_ucc_loss': 0.38008, 'train_ucc_acc': 0.90625, 'loss': 0.52062}\n",
            "Step 146880: {'train_ae_loss': 0.6573, 'train_ucc_loss': 0.54698, 'train_ucc_acc': 0.71875, 'loss': 0.60214}\n",
            "Step 146900: {'train_ae_loss': 0.67753, 'train_ucc_loss': 0.47391, 'train_ucc_acc': 0.84375, 'loss': 0.57572}\n",
            "Step 146920: {'train_ae_loss': 0.65181, 'train_ucc_loss': 0.43583, 'train_ucc_acc': 0.875, 'loss': 0.54382}\n",
            "Step 146940: {'train_ae_loss': 0.66798, 'train_ucc_loss': 0.46645, 'train_ucc_acc': 0.84375, 'loss': 0.56721}\n",
            "Step 146960: {'train_ae_loss': 0.66455, 'train_ucc_loss': 0.36198, 'train_ucc_acc': 0.96875, 'loss': 0.51327}\n",
            "Step 146980: {'train_ae_loss': 0.65958, 'train_ucc_loss': 0.45312, 'train_ucc_acc': 0.875, 'loss': 0.55635}\n",
            "Step 147000: {'train_ae_loss': 0.6675, 'train_ucc_loss': 0.47212, 'train_ucc_acc': 0.78125, 'loss': 0.56981}\n",
            "step: 147000,eval_ae_loss: 0.65415,eval_ucc_loss: 0.49259,eval_ucc_acc: 0.81055\n",
            "Step 147020: {'train_ae_loss': 0.671, 'train_ucc_loss': 0.4294, 'train_ucc_acc': 0.875, 'loss': 0.5502}\n",
            "Step 147040: {'train_ae_loss': 0.67216, 'train_ucc_loss': 0.46149, 'train_ucc_acc': 0.84375, 'loss': 0.56683}\n",
            "Step 147060: {'train_ae_loss': 0.67979, 'train_ucc_loss': 0.43751, 'train_ucc_acc': 0.875, 'loss': 0.55865}\n",
            "Step 147080: {'train_ae_loss': 0.67578, 'train_ucc_loss': 0.42873, 'train_ucc_acc': 0.90625, 'loss': 0.55226}\n",
            "Step 147100: {'train_ae_loss': 0.64636, 'train_ucc_loss': 0.43181, 'train_ucc_acc': 0.875, 'loss': 0.53908}\n",
            "Step 147120: {'train_ae_loss': 0.67043, 'train_ucc_loss': 0.42345, 'train_ucc_acc': 0.875, 'loss': 0.54694}\n",
            "Step 147140: {'train_ae_loss': 0.66802, 'train_ucc_loss': 0.38611, 'train_ucc_acc': 0.9375, 'loss': 0.52707}\n",
            "Step 147160: {'train_ae_loss': 0.68879, 'train_ucc_loss': 0.40258, 'train_ucc_acc': 0.90625, 'loss': 0.54569}\n",
            "Step 147180: {'train_ae_loss': 0.66688, 'train_ucc_loss': 0.48257, 'train_ucc_acc': 0.8125, 'loss': 0.57473}\n",
            "Step 147200: {'train_ae_loss': 0.66557, 'train_ucc_loss': 0.40094, 'train_ucc_acc': 0.90625, 'loss': 0.53325}\n",
            "Step 147220: {'train_ae_loss': 0.673, 'train_ucc_loss': 0.38677, 'train_ucc_acc': 0.90625, 'loss': 0.52988}\n",
            "Step 147240: {'train_ae_loss': 0.66462, 'train_ucc_loss': 0.44299, 'train_ucc_acc': 0.84375, 'loss': 0.5538}\n",
            "Step 147260: {'train_ae_loss': 0.66599, 'train_ucc_loss': 0.48208, 'train_ucc_acc': 0.84375, 'loss': 0.57403}\n",
            "Step 147280: {'train_ae_loss': 0.64443, 'train_ucc_loss': 0.48317, 'train_ucc_acc': 0.84375, 'loss': 0.5638}\n",
            "Step 147300: {'train_ae_loss': 0.65453, 'train_ucc_loss': 0.38428, 'train_ucc_acc': 0.9375, 'loss': 0.51941}\n",
            "Step 147320: {'train_ae_loss': 0.65581, 'train_ucc_loss': 0.42501, 'train_ucc_acc': 0.875, 'loss': 0.54041}\n",
            "Step 147340: {'train_ae_loss': 0.66752, 'train_ucc_loss': 0.49434, 'train_ucc_acc': 0.8125, 'loss': 0.58093}\n",
            "Step 147360: {'train_ae_loss': 0.65294, 'train_ucc_loss': 0.44344, 'train_ucc_acc': 0.875, 'loss': 0.54819}\n",
            "Step 147380: {'train_ae_loss': 0.64974, 'train_ucc_loss': 0.41829, 'train_ucc_acc': 0.875, 'loss': 0.53401}\n",
            "Step 147400: {'train_ae_loss': 0.66234, 'train_ucc_loss': 0.47137, 'train_ucc_acc': 0.8125, 'loss': 0.56685}\n",
            "Step 147420: {'train_ae_loss': 0.64858, 'train_ucc_loss': 0.41054, 'train_ucc_acc': 0.90625, 'loss': 0.52956}\n",
            "Step 147440: {'train_ae_loss': 0.6695, 'train_ucc_loss': 0.41578, 'train_ucc_acc': 0.90625, 'loss': 0.54264}\n",
            "Step 147460: {'train_ae_loss': 0.66024, 'train_ucc_loss': 0.38098, 'train_ucc_acc': 0.90625, 'loss': 0.52061}\n",
            "Step 147480: {'train_ae_loss': 0.64628, 'train_ucc_loss': 0.40296, 'train_ucc_acc': 0.875, 'loss': 0.52462}\n",
            "Step 147500: {'train_ae_loss': 0.66758, 'train_ucc_loss': 0.40155, 'train_ucc_acc': 0.90625, 'loss': 0.53456}\n",
            "Step 147520: {'train_ae_loss': 0.68088, 'train_ucc_loss': 0.37368, 'train_ucc_acc': 0.9375, 'loss': 0.52728}\n",
            "Step 147540: {'train_ae_loss': 0.64571, 'train_ucc_loss': 0.48649, 'train_ucc_acc': 0.8125, 'loss': 0.5661}\n",
            "Step 147560: {'train_ae_loss': 0.66889, 'train_ucc_loss': 0.43996, 'train_ucc_acc': 0.875, 'loss': 0.55442}\n",
            "Step 147580: {'train_ae_loss': 0.66338, 'train_ucc_loss': 0.417, 'train_ucc_acc': 0.875, 'loss': 0.54019}\n",
            "Step 147600: {'train_ae_loss': 0.66121, 'train_ucc_loss': 0.52345, 'train_ucc_acc': 0.78125, 'loss': 0.59233}\n",
            "Step 147620: {'train_ae_loss': 0.65105, 'train_ucc_loss': 0.42607, 'train_ucc_acc': 0.875, 'loss': 0.53856}\n",
            "Step 147640: {'train_ae_loss': 0.6479, 'train_ucc_loss': 0.4006, 'train_ucc_acc': 0.90625, 'loss': 0.52425}\n",
            "Step 147660: {'train_ae_loss': 0.65473, 'train_ucc_loss': 0.4976, 'train_ucc_acc': 0.8125, 'loss': 0.57616}\n",
            "Step 147680: {'train_ae_loss': 0.66787, 'train_ucc_loss': 0.44233, 'train_ucc_acc': 0.84375, 'loss': 0.5551}\n",
            "Step 147700: {'train_ae_loss': 0.68411, 'train_ucc_loss': 0.43687, 'train_ucc_acc': 0.875, 'loss': 0.56049}\n",
            "Step 147720: {'train_ae_loss': 0.66007, 'train_ucc_loss': 0.41023, 'train_ucc_acc': 0.90625, 'loss': 0.53515}\n",
            "Step 147740: {'train_ae_loss': 0.66001, 'train_ucc_loss': 0.40136, 'train_ucc_acc': 0.90625, 'loss': 0.53069}\n",
            "Step 147760: {'train_ae_loss': 0.65792, 'train_ucc_loss': 0.41505, 'train_ucc_acc': 0.90625, 'loss': 0.53648}\n",
            "Step 147780: {'train_ae_loss': 0.67499, 'train_ucc_loss': 0.43144, 'train_ucc_acc': 0.875, 'loss': 0.55322}\n",
            "Step 147800: {'train_ae_loss': 0.66443, 'train_ucc_loss': 0.44235, 'train_ucc_acc': 0.875, 'loss': 0.55339}\n",
            "Step 147820: {'train_ae_loss': 0.66099, 'train_ucc_loss': 0.54825, 'train_ucc_acc': 0.71875, 'loss': 0.60462}\n",
            "Step 147840: {'train_ae_loss': 0.64043, 'train_ucc_loss': 0.51674, 'train_ucc_acc': 0.78125, 'loss': 0.57858}\n",
            "Step 147860: {'train_ae_loss': 0.67054, 'train_ucc_loss': 0.47214, 'train_ucc_acc': 0.84375, 'loss': 0.57134}\n",
            "Step 147880: {'train_ae_loss': 0.66545, 'train_ucc_loss': 0.48858, 'train_ucc_acc': 0.8125, 'loss': 0.57701}\n",
            "Step 147900: {'train_ae_loss': 0.65336, 'train_ucc_loss': 0.46233, 'train_ucc_acc': 0.84375, 'loss': 0.55785}\n",
            "Step 147920: {'train_ae_loss': 0.67236, 'train_ucc_loss': 0.4887, 'train_ucc_acc': 0.8125, 'loss': 0.58053}\n",
            "Step 147940: {'train_ae_loss': 0.66108, 'train_ucc_loss': 0.39833, 'train_ucc_acc': 0.90625, 'loss': 0.52971}\n",
            "Step 147960: {'train_ae_loss': 0.65523, 'train_ucc_loss': 0.53839, 'train_ucc_acc': 0.75, 'loss': 0.59681}\n",
            "Step 147980: {'train_ae_loss': 0.65708, 'train_ucc_loss': 0.42738, 'train_ucc_acc': 0.875, 'loss': 0.54223}\n",
            "Step 148000: {'train_ae_loss': 0.6645, 'train_ucc_loss': 0.46523, 'train_ucc_acc': 0.84375, 'loss': 0.56487}\n",
            "step: 148000,eval_ae_loss: 0.6491,eval_ucc_loss: 0.48301,eval_ucc_acc: 0.82324\n",
            "Step 148020: {'train_ae_loss': 0.64693, 'train_ucc_loss': 0.37911, 'train_ucc_acc': 0.90625, 'loss': 0.51302}\n",
            "Step 148040: {'train_ae_loss': 0.66189, 'train_ucc_loss': 0.36931, 'train_ucc_acc': 0.96875, 'loss': 0.5156}\n",
            "Step 148060: {'train_ae_loss': 0.66481, 'train_ucc_loss': 0.45268, 'train_ucc_acc': 0.875, 'loss': 0.55875}\n",
            "Step 148080: {'train_ae_loss': 0.65374, 'train_ucc_loss': 0.41947, 'train_ucc_acc': 0.875, 'loss': 0.5366}\n",
            "Step 148100: {'train_ae_loss': 0.66077, 'train_ucc_loss': 0.4543, 'train_ucc_acc': 0.875, 'loss': 0.55754}\n",
            "Step 148120: {'train_ae_loss': 0.66539, 'train_ucc_loss': 0.52627, 'train_ucc_acc': 0.78125, 'loss': 0.59583}\n",
            "Step 148140: {'train_ae_loss': 0.66765, 'train_ucc_loss': 0.35833, 'train_ucc_acc': 0.96875, 'loss': 0.51299}\n",
            "Step 148160: {'train_ae_loss': 0.65798, 'train_ucc_loss': 0.47935, 'train_ucc_acc': 0.84375, 'loss': 0.56866}\n",
            "Step 148180: {'train_ae_loss': 0.66479, 'train_ucc_loss': 0.40025, 'train_ucc_acc': 0.90625, 'loss': 0.53252}\n",
            "Step 148200: {'train_ae_loss': 0.6601, 'train_ucc_loss': 0.44033, 'train_ucc_acc': 0.875, 'loss': 0.55022}\n",
            "Step 148220: {'train_ae_loss': 0.65705, 'train_ucc_loss': 0.5184, 'train_ucc_acc': 0.78125, 'loss': 0.58773}\n",
            "Step 148240: {'train_ae_loss': 0.65713, 'train_ucc_loss': 0.37602, 'train_ucc_acc': 0.9375, 'loss': 0.51658}\n",
            "Step 148260: {'train_ae_loss': 0.66189, 'train_ucc_loss': 0.49675, 'train_ucc_acc': 0.78125, 'loss': 0.57932}\n",
            "Step 148280: {'train_ae_loss': 0.66855, 'train_ucc_loss': 0.51528, 'train_ucc_acc': 0.78125, 'loss': 0.59192}\n",
            "Step 148300: {'train_ae_loss': 0.65002, 'train_ucc_loss': 0.4443, 'train_ucc_acc': 0.84375, 'loss': 0.54716}\n",
            "Step 148320: {'train_ae_loss': 0.66068, 'train_ucc_loss': 0.39485, 'train_ucc_acc': 0.90625, 'loss': 0.52777}\n",
            "Step 148340: {'train_ae_loss': 0.67077, 'train_ucc_loss': 0.46116, 'train_ucc_acc': 0.875, 'loss': 0.56597}\n",
            "Step 148360: {'train_ae_loss': 0.66818, 'train_ucc_loss': 0.39706, 'train_ucc_acc': 0.90625, 'loss': 0.53262}\n",
            "Step 148380: {'train_ae_loss': 0.66185, 'train_ucc_loss': 0.37639, 'train_ucc_acc': 0.9375, 'loss': 0.51912}\n",
            "Step 148400: {'train_ae_loss': 0.66414, 'train_ucc_loss': 0.48605, 'train_ucc_acc': 0.8125, 'loss': 0.57509}\n",
            "Step 148420: {'train_ae_loss': 0.67037, 'train_ucc_loss': 0.39763, 'train_ucc_acc': 0.90625, 'loss': 0.534}\n",
            "Step 148440: {'train_ae_loss': 0.65349, 'train_ucc_loss': 0.49435, 'train_ucc_acc': 0.78125, 'loss': 0.57392}\n",
            "Step 148460: {'train_ae_loss': 0.68762, 'train_ucc_loss': 0.4611, 'train_ucc_acc': 0.875, 'loss': 0.57436}\n",
            "Step 148480: {'train_ae_loss': 0.66847, 'train_ucc_loss': 0.44705, 'train_ucc_acc': 0.84375, 'loss': 0.55776}\n",
            "Step 148500: {'train_ae_loss': 0.66957, 'train_ucc_loss': 0.3675, 'train_ucc_acc': 0.9375, 'loss': 0.51853}\n",
            "Step 148520: {'train_ae_loss': 0.66347, 'train_ucc_loss': 0.43977, 'train_ucc_acc': 0.84375, 'loss': 0.55162}\n",
            "Step 148540: {'train_ae_loss': 0.66982, 'train_ucc_loss': 0.45501, 'train_ucc_acc': 0.875, 'loss': 0.56241}\n",
            "Step 148560: {'train_ae_loss': 0.66911, 'train_ucc_loss': 0.53161, 'train_ucc_acc': 0.75, 'loss': 0.60036}\n",
            "Step 148580: {'train_ae_loss': 0.6897, 'train_ucc_loss': 0.41614, 'train_ucc_acc': 0.875, 'loss': 0.55292}\n",
            "Step 148600: {'train_ae_loss': 0.65605, 'train_ucc_loss': 0.38819, 'train_ucc_acc': 0.90625, 'loss': 0.52212}\n",
            "Step 148620: {'train_ae_loss': 0.65619, 'train_ucc_loss': 0.46152, 'train_ucc_acc': 0.84375, 'loss': 0.55886}\n",
            "Step 148640: {'train_ae_loss': 0.6693, 'train_ucc_loss': 0.35157, 'train_ucc_acc': 0.96875, 'loss': 0.51044}\n",
            "Step 148660: {'train_ae_loss': 0.65418, 'train_ucc_loss': 0.42377, 'train_ucc_acc': 0.875, 'loss': 0.53897}\n",
            "Step 148680: {'train_ae_loss': 0.66556, 'train_ucc_loss': 0.38698, 'train_ucc_acc': 0.9375, 'loss': 0.52627}\n",
            "Step 148700: {'train_ae_loss': 0.6443, 'train_ucc_loss': 0.38012, 'train_ucc_acc': 0.9375, 'loss': 0.51221}\n",
            "Step 148720: {'train_ae_loss': 0.64812, 'train_ucc_loss': 0.40498, 'train_ucc_acc': 0.9375, 'loss': 0.52655}\n",
            "Step 148740: {'train_ae_loss': 0.66544, 'train_ucc_loss': 0.48755, 'train_ucc_acc': 0.78125, 'loss': 0.57649}\n",
            "Step 148760: {'train_ae_loss': 0.66835, 'train_ucc_loss': 0.39242, 'train_ucc_acc': 0.90625, 'loss': 0.53039}\n",
            "Step 148780: {'train_ae_loss': 0.6535, 'train_ucc_loss': 0.4337, 'train_ucc_acc': 0.875, 'loss': 0.5436}\n",
            "Step 148800: {'train_ae_loss': 0.66159, 'train_ucc_loss': 0.57398, 'train_ucc_acc': 0.71875, 'loss': 0.61779}\n",
            "Step 148820: {'train_ae_loss': 0.66292, 'train_ucc_loss': 0.33748, 'train_ucc_acc': 0.96875, 'loss': 0.5002}\n",
            "Step 148840: {'train_ae_loss': 0.65607, 'train_ucc_loss': 0.39765, 'train_ucc_acc': 0.90625, 'loss': 0.52686}\n",
            "Step 148860: {'train_ae_loss': 0.64587, 'train_ucc_loss': 0.59232, 'train_ucc_acc': 0.6875, 'loss': 0.61909}\n",
            "Step 148880: {'train_ae_loss': 0.6657, 'train_ucc_loss': 0.40772, 'train_ucc_acc': 0.90625, 'loss': 0.53671}\n",
            "Step 148900: {'train_ae_loss': 0.64618, 'train_ucc_loss': 0.43919, 'train_ucc_acc': 0.90625, 'loss': 0.54269}\n",
            "Step 148920: {'train_ae_loss': 0.65867, 'train_ucc_loss': 0.52684, 'train_ucc_acc': 0.78125, 'loss': 0.59276}\n",
            "Step 148940: {'train_ae_loss': 0.66261, 'train_ucc_loss': 0.39838, 'train_ucc_acc': 0.9375, 'loss': 0.53049}\n",
            "Step 148960: {'train_ae_loss': 0.65857, 'train_ucc_loss': 0.46697, 'train_ucc_acc': 0.84375, 'loss': 0.56277}\n",
            "Step 148980: {'train_ae_loss': 0.65264, 'train_ucc_loss': 0.47134, 'train_ucc_acc': 0.8125, 'loss': 0.56199}\n",
            "Step 149000: {'train_ae_loss': 0.66301, 'train_ucc_loss': 0.61261, 'train_ucc_acc': 0.65625, 'loss': 0.63781}\n",
            "step: 149000,eval_ae_loss: 0.6586,eval_ucc_loss: 0.48506,eval_ucc_acc: 0.82227\n",
            "Step 149020: {'train_ae_loss': 0.64316, 'train_ucc_loss': 0.43487, 'train_ucc_acc': 0.875, 'loss': 0.53902}\n",
            "Step 149040: {'train_ae_loss': 0.67282, 'train_ucc_loss': 0.47366, 'train_ucc_acc': 0.84375, 'loss': 0.57324}\n",
            "Step 149060: {'train_ae_loss': 0.65073, 'train_ucc_loss': 0.52112, 'train_ucc_acc': 0.78125, 'loss': 0.58593}\n",
            "Step 149080: {'train_ae_loss': 0.67545, 'train_ucc_loss': 0.46883, 'train_ucc_acc': 0.8125, 'loss': 0.57214}\n",
            "Step 149100: {'train_ae_loss': 0.66472, 'train_ucc_loss': 0.3662, 'train_ucc_acc': 0.9375, 'loss': 0.51546}\n",
            "Step 149120: {'train_ae_loss': 0.66812, 'train_ucc_loss': 0.43005, 'train_ucc_acc': 0.875, 'loss': 0.54908}\n",
            "Step 149140: {'train_ae_loss': 0.64752, 'train_ucc_loss': 0.38179, 'train_ucc_acc': 0.9375, 'loss': 0.51466}\n",
            "Step 149160: {'train_ae_loss': 0.66459, 'train_ucc_loss': 0.41013, 'train_ucc_acc': 0.875, 'loss': 0.53736}\n",
            "Step 149180: {'train_ae_loss': 0.66295, 'train_ucc_loss': 0.34868, 'train_ucc_acc': 0.96875, 'loss': 0.50581}\n",
            "Step 149200: {'train_ae_loss': 0.65291, 'train_ucc_loss': 0.43472, 'train_ucc_acc': 0.875, 'loss': 0.54381}\n",
            "Step 149220: {'train_ae_loss': 0.64701, 'train_ucc_loss': 0.52311, 'train_ucc_acc': 0.78125, 'loss': 0.58506}\n",
            "Step 149240: {'train_ae_loss': 0.67381, 'train_ucc_loss': 0.37633, 'train_ucc_acc': 0.9375, 'loss': 0.52507}\n",
            "Step 149260: {'train_ae_loss': 0.64815, 'train_ucc_loss': 0.44464, 'train_ucc_acc': 0.875, 'loss': 0.54639}\n",
            "Step 149280: {'train_ae_loss': 0.65813, 'train_ucc_loss': 0.50043, 'train_ucc_acc': 0.78125, 'loss': 0.57928}\n",
            "Step 149300: {'train_ae_loss': 0.64876, 'train_ucc_loss': 0.42909, 'train_ucc_acc': 0.875, 'loss': 0.53892}\n",
            "Step 149320: {'train_ae_loss': 0.6541, 'train_ucc_loss': 0.4821, 'train_ucc_acc': 0.8125, 'loss': 0.5681}\n",
            "Step 149340: {'train_ae_loss': 0.66127, 'train_ucc_loss': 0.42038, 'train_ucc_acc': 0.875, 'loss': 0.54083}\n",
            "Step 149360: {'train_ae_loss': 0.66525, 'train_ucc_loss': 0.49996, 'train_ucc_acc': 0.78125, 'loss': 0.58261}\n",
            "Step 149380: {'train_ae_loss': 0.67225, 'train_ucc_loss': 0.49708, 'train_ucc_acc': 0.8125, 'loss': 0.58466}\n",
            "Step 149400: {'train_ae_loss': 0.67301, 'train_ucc_loss': 0.42091, 'train_ucc_acc': 0.875, 'loss': 0.54696}\n",
            "Step 149420: {'train_ae_loss': 0.67304, 'train_ucc_loss': 0.39774, 'train_ucc_acc': 0.875, 'loss': 0.53539}\n",
            "Step 149440: {'train_ae_loss': 0.6611, 'train_ucc_loss': 0.46226, 'train_ucc_acc': 0.84375, 'loss': 0.56168}\n",
            "Step 149460: {'train_ae_loss': 0.66335, 'train_ucc_loss': 0.43526, 'train_ucc_acc': 0.84375, 'loss': 0.54931}\n",
            "Step 149480: {'train_ae_loss': 0.67595, 'train_ucc_loss': 0.46799, 'train_ucc_acc': 0.84375, 'loss': 0.57197}\n",
            "Step 149500: {'train_ae_loss': 0.66725, 'train_ucc_loss': 0.44574, 'train_ucc_acc': 0.84375, 'loss': 0.55649}\n",
            "Step 149520: {'train_ae_loss': 0.67394, 'train_ucc_loss': 0.49872, 'train_ucc_acc': 0.8125, 'loss': 0.58633}\n",
            "Step 149540: {'train_ae_loss': 0.65086, 'train_ucc_loss': 0.42743, 'train_ucc_acc': 0.875, 'loss': 0.53915}\n",
            "Step 149560: {'train_ae_loss': 0.66447, 'train_ucc_loss': 0.46425, 'train_ucc_acc': 0.84375, 'loss': 0.56436}\n",
            "Step 149580: {'train_ae_loss': 0.68182, 'train_ucc_loss': 0.43545, 'train_ucc_acc': 0.84375, 'loss': 0.55864}\n",
            "Step 149600: {'train_ae_loss': 0.68464, 'train_ucc_loss': 0.52681, 'train_ucc_acc': 0.75, 'loss': 0.60573}\n",
            "Step 149620: {'train_ae_loss': 0.68226, 'train_ucc_loss': 0.4108, 'train_ucc_acc': 0.9375, 'loss': 0.54653}\n",
            "Step 149640: {'train_ae_loss': 0.64757, 'train_ucc_loss': 0.45745, 'train_ucc_acc': 0.875, 'loss': 0.55251}\n",
            "Step 149660: {'train_ae_loss': 0.66184, 'train_ucc_loss': 0.39171, 'train_ucc_acc': 0.9375, 'loss': 0.52678}\n",
            "Step 149680: {'train_ae_loss': 0.65587, 'train_ucc_loss': 0.53995, 'train_ucc_acc': 0.75, 'loss': 0.59791}\n",
            "Step 149700: {'train_ae_loss': 0.6638, 'train_ucc_loss': 0.35384, 'train_ucc_acc': 0.96875, 'loss': 0.50882}\n",
            "Step 149720: {'train_ae_loss': 0.64304, 'train_ucc_loss': 0.50442, 'train_ucc_acc': 0.8125, 'loss': 0.57373}\n",
            "Step 149740: {'train_ae_loss': 0.66789, 'train_ucc_loss': 0.37988, 'train_ucc_acc': 0.9375, 'loss': 0.52388}\n",
            "Step 149760: {'train_ae_loss': 0.65363, 'train_ucc_loss': 0.35705, 'train_ucc_acc': 0.96875, 'loss': 0.50534}\n",
            "Step 149780: {'train_ae_loss': 0.65321, 'train_ucc_loss': 0.4285, 'train_ucc_acc': 0.875, 'loss': 0.54085}\n",
            "Step 149800: {'train_ae_loss': 0.65579, 'train_ucc_loss': 0.48743, 'train_ucc_acc': 0.8125, 'loss': 0.57161}\n",
            "Step 149820: {'train_ae_loss': 0.65865, 'train_ucc_loss': 0.41397, 'train_ucc_acc': 0.875, 'loss': 0.53631}\n",
            "Step 149840: {'train_ae_loss': 0.65677, 'train_ucc_loss': 0.53105, 'train_ucc_acc': 0.78125, 'loss': 0.59391}\n",
            "Step 149860: {'train_ae_loss': 0.66598, 'train_ucc_loss': 0.44338, 'train_ucc_acc': 0.84375, 'loss': 0.55468}\n",
            "Step 149880: {'train_ae_loss': 0.65214, 'train_ucc_loss': 0.39083, 'train_ucc_acc': 0.9375, 'loss': 0.52149}\n",
            "Step 149900: {'train_ae_loss': 0.64304, 'train_ucc_loss': 0.38389, 'train_ucc_acc': 0.9375, 'loss': 0.51347}\n",
            "Step 149920: {'train_ae_loss': 0.64976, 'train_ucc_loss': 0.48425, 'train_ucc_acc': 0.84375, 'loss': 0.567}\n",
            "Step 149940: {'train_ae_loss': 0.66709, 'train_ucc_loss': 0.45298, 'train_ucc_acc': 0.8125, 'loss': 0.56004}\n",
            "Step 149960: {'train_ae_loss': 0.65872, 'train_ucc_loss': 0.50384, 'train_ucc_acc': 0.8125, 'loss': 0.58128}\n",
            "Step 149980: {'train_ae_loss': 0.64025, 'train_ucc_loss': 0.37189, 'train_ucc_acc': 0.9375, 'loss': 0.50607}\n",
            "Step 150000: {'train_ae_loss': 0.65945, 'train_ucc_loss': 0.40253, 'train_ucc_acc': 0.90625, 'loss': 0.53099}\n",
            "step: 150000,eval_ae_loss: 0.64867,eval_ucc_loss: 0.47865,eval_ucc_acc: 0.8291\n",
            "Step 150020: {'train_ae_loss': 0.66158, 'train_ucc_loss': 0.53667, 'train_ucc_acc': 0.75, 'loss': 0.59913}\n",
            "Step 150040: {'train_ae_loss': 0.65185, 'train_ucc_loss': 0.5287, 'train_ucc_acc': 0.78125, 'loss': 0.59028}\n",
            "Step 150060: {'train_ae_loss': 0.64363, 'train_ucc_loss': 0.42117, 'train_ucc_acc': 0.90625, 'loss': 0.5324}\n",
            "Step 150080: {'train_ae_loss': 0.66298, 'train_ucc_loss': 0.44323, 'train_ucc_acc': 0.84375, 'loss': 0.55311}\n",
            "Step 150100: {'train_ae_loss': 0.65034, 'train_ucc_loss': 0.52103, 'train_ucc_acc': 0.78125, 'loss': 0.58568}\n",
            "Step 150120: {'train_ae_loss': 0.65865, 'train_ucc_loss': 0.40705, 'train_ucc_acc': 0.9375, 'loss': 0.53285}\n",
            "Step 150140: {'train_ae_loss': 0.66854, 'train_ucc_loss': 0.57642, 'train_ucc_acc': 0.71875, 'loss': 0.62248}\n",
            "Step 150160: {'train_ae_loss': 0.65534, 'train_ucc_loss': 0.463, 'train_ucc_acc': 0.84375, 'loss': 0.55917}\n",
            "Step 150180: {'train_ae_loss': 0.65953, 'train_ucc_loss': 0.45442, 'train_ucc_acc': 0.84375, 'loss': 0.55698}\n",
            "Step 150200: {'train_ae_loss': 0.65914, 'train_ucc_loss': 0.51246, 'train_ucc_acc': 0.8125, 'loss': 0.5858}\n",
            "Step 150220: {'train_ae_loss': 0.65567, 'train_ucc_loss': 0.4553, 'train_ucc_acc': 0.84375, 'loss': 0.55548}\n",
            "Step 150240: {'train_ae_loss': 0.65529, 'train_ucc_loss': 0.46356, 'train_ucc_acc': 0.875, 'loss': 0.55942}\n",
            "Step 150260: {'train_ae_loss': 0.66141, 'train_ucc_loss': 0.42146, 'train_ucc_acc': 0.875, 'loss': 0.54144}\n",
            "Step 150280: {'train_ae_loss': 0.65061, 'train_ucc_loss': 0.51462, 'train_ucc_acc': 0.78125, 'loss': 0.58261}\n",
            "Step 150300: {'train_ae_loss': 0.66423, 'train_ucc_loss': 0.39436, 'train_ucc_acc': 0.9375, 'loss': 0.52929}\n",
            "Step 150320: {'train_ae_loss': 0.65925, 'train_ucc_loss': 0.38711, 'train_ucc_acc': 0.9375, 'loss': 0.52318}\n",
            "Step 150340: {'train_ae_loss': 0.65316, 'train_ucc_loss': 0.45029, 'train_ucc_acc': 0.84375, 'loss': 0.55173}\n",
            "Step 150360: {'train_ae_loss': 0.66462, 'train_ucc_loss': 0.38627, 'train_ucc_acc': 0.9375, 'loss': 0.52545}\n",
            "Step 150380: {'train_ae_loss': 0.65727, 'train_ucc_loss': 0.54684, 'train_ucc_acc': 0.75, 'loss': 0.60206}\n",
            "Step 150400: {'train_ae_loss': 0.65974, 'train_ucc_loss': 0.40442, 'train_ucc_acc': 0.90625, 'loss': 0.53208}\n",
            "Step 150420: {'train_ae_loss': 0.65436, 'train_ucc_loss': 0.48849, 'train_ucc_acc': 0.8125, 'loss': 0.57142}\n",
            "Step 150440: {'train_ae_loss': 0.67238, 'train_ucc_loss': 0.51202, 'train_ucc_acc': 0.78125, 'loss': 0.5922}\n",
            "Step 150460: {'train_ae_loss': 0.65273, 'train_ucc_loss': 0.40828, 'train_ucc_acc': 0.90625, 'loss': 0.53051}\n",
            "Step 150480: {'train_ae_loss': 0.65093, 'train_ucc_loss': 0.45022, 'train_ucc_acc': 0.84375, 'loss': 0.55058}\n",
            "Step 150500: {'train_ae_loss': 0.66584, 'train_ucc_loss': 0.39262, 'train_ucc_acc': 0.90625, 'loss': 0.52923}\n",
            "Step 150520: {'train_ae_loss': 0.65669, 'train_ucc_loss': 0.48231, 'train_ucc_acc': 0.78125, 'loss': 0.5695}\n",
            "Step 150540: {'train_ae_loss': 0.64855, 'train_ucc_loss': 0.42929, 'train_ucc_acc': 0.84375, 'loss': 0.53892}\n",
            "Step 150560: {'train_ae_loss': 0.6635, 'train_ucc_loss': 0.38262, 'train_ucc_acc': 0.90625, 'loss': 0.52306}\n",
            "Step 150580: {'train_ae_loss': 0.66539, 'train_ucc_loss': 0.53872, 'train_ucc_acc': 0.75, 'loss': 0.60205}\n",
            "Step 150600: {'train_ae_loss': 0.65335, 'train_ucc_loss': 0.42348, 'train_ucc_acc': 0.84375, 'loss': 0.53841}\n",
            "Step 150620: {'train_ae_loss': 0.6476, 'train_ucc_loss': 0.35856, 'train_ucc_acc': 0.96875, 'loss': 0.50308}\n",
            "Step 150640: {'train_ae_loss': 0.67656, 'train_ucc_loss': 0.37834, 'train_ucc_acc': 0.90625, 'loss': 0.52745}\n",
            "Step 150660: {'train_ae_loss': 0.67591, 'train_ucc_loss': 0.35456, 'train_ucc_acc': 0.96875, 'loss': 0.51524}\n",
            "Step 150680: {'train_ae_loss': 0.6418, 'train_ucc_loss': 0.51906, 'train_ucc_acc': 0.78125, 'loss': 0.58043}\n",
            "Step 150700: {'train_ae_loss': 0.65971, 'train_ucc_loss': 0.46225, 'train_ucc_acc': 0.875, 'loss': 0.56098}\n",
            "Step 150720: {'train_ae_loss': 0.66869, 'train_ucc_loss': 0.37399, 'train_ucc_acc': 0.9375, 'loss': 0.52134}\n",
            "Step 150740: {'train_ae_loss': 0.64323, 'train_ucc_loss': 0.40829, 'train_ucc_acc': 0.90625, 'loss': 0.52576}\n",
            "Step 150760: {'train_ae_loss': 0.67482, 'train_ucc_loss': 0.41, 'train_ucc_acc': 0.875, 'loss': 0.54241}\n",
            "Step 150780: {'train_ae_loss': 0.64789, 'train_ucc_loss': 0.41548, 'train_ucc_acc': 0.90625, 'loss': 0.53169}\n",
            "Step 150800: {'train_ae_loss': 0.6614, 'train_ucc_loss': 0.42307, 'train_ucc_acc': 0.875, 'loss': 0.54223}\n",
            "Step 150820: {'train_ae_loss': 0.65808, 'train_ucc_loss': 0.39946, 'train_ucc_acc': 0.9375, 'loss': 0.52877}\n",
            "Step 150840: {'train_ae_loss': 0.65591, 'train_ucc_loss': 0.38976, 'train_ucc_acc': 0.9375, 'loss': 0.52284}\n",
            "Step 150860: {'train_ae_loss': 0.65396, 'train_ucc_loss': 0.43309, 'train_ucc_acc': 0.875, 'loss': 0.54353}\n",
            "Step 150880: {'train_ae_loss': 0.67165, 'train_ucc_loss': 0.37254, 'train_ucc_acc': 0.9375, 'loss': 0.5221}\n",
            "Step 150900: {'train_ae_loss': 0.64762, 'train_ucc_loss': 0.40833, 'train_ucc_acc': 0.90625, 'loss': 0.52798}\n",
            "Step 150920: {'train_ae_loss': 0.65318, 'train_ucc_loss': 0.42581, 'train_ucc_acc': 0.875, 'loss': 0.5395}\n",
            "Step 150940: {'train_ae_loss': 0.64812, 'train_ucc_loss': 0.44932, 'train_ucc_acc': 0.875, 'loss': 0.54872}\n",
            "Step 150960: {'train_ae_loss': 0.67593, 'train_ucc_loss': 0.59107, 'train_ucc_acc': 0.71875, 'loss': 0.6335}\n",
            "Step 150980: {'train_ae_loss': 0.6832, 'train_ucc_loss': 0.50354, 'train_ucc_acc': 0.78125, 'loss': 0.59337}\n",
            "Step 151000: {'train_ae_loss': 0.68075, 'train_ucc_loss': 0.443, 'train_ucc_acc': 0.875, 'loss': 0.56188}\n",
            "step: 151000,eval_ae_loss: 0.6558,eval_ucc_loss: 0.48807,eval_ucc_acc: 0.81445\n",
            "Step 151020: {'train_ae_loss': 0.64811, 'train_ucc_loss': 0.50955, 'train_ucc_acc': 0.78125, 'loss': 0.57883}\n",
            "Step 151040: {'train_ae_loss': 0.67213, 'train_ucc_loss': 0.4477, 'train_ucc_acc': 0.84375, 'loss': 0.55991}\n",
            "Step 151060: {'train_ae_loss': 0.66773, 'train_ucc_loss': 0.43208, 'train_ucc_acc': 0.875, 'loss': 0.5499}\n",
            "Step 151080: {'train_ae_loss': 0.65584, 'train_ucc_loss': 0.42371, 'train_ucc_acc': 0.875, 'loss': 0.53977}\n",
            "Step 151100: {'train_ae_loss': 0.64647, 'train_ucc_loss': 0.40628, 'train_ucc_acc': 0.90625, 'loss': 0.52638}\n",
            "Step 151120: {'train_ae_loss': 0.6604, 'train_ucc_loss': 0.55638, 'train_ucc_acc': 0.71875, 'loss': 0.60839}\n",
            "Step 151140: {'train_ae_loss': 0.64154, 'train_ucc_loss': 0.47334, 'train_ucc_acc': 0.8125, 'loss': 0.55744}\n",
            "Step 151160: {'train_ae_loss': 0.64981, 'train_ucc_loss': 0.46163, 'train_ucc_acc': 0.84375, 'loss': 0.55572}\n",
            "Step 151180: {'train_ae_loss': 0.64112, 'train_ucc_loss': 0.42978, 'train_ucc_acc': 0.875, 'loss': 0.53545}\n",
            "Step 151200: {'train_ae_loss': 0.67236, 'train_ucc_loss': 0.3735, 'train_ucc_acc': 0.9375, 'loss': 0.52293}\n",
            "Step 151220: {'train_ae_loss': 0.64747, 'train_ucc_loss': 0.42046, 'train_ucc_acc': 0.90625, 'loss': 0.53396}\n",
            "Step 151240: {'train_ae_loss': 0.65486, 'train_ucc_loss': 0.37636, 'train_ucc_acc': 0.9375, 'loss': 0.51561}\n",
            "Step 151260: {'train_ae_loss': 0.6432, 'train_ucc_loss': 0.4182, 'train_ucc_acc': 0.90625, 'loss': 0.5307}\n",
            "Step 151280: {'train_ae_loss': 0.66026, 'train_ucc_loss': 0.37642, 'train_ucc_acc': 0.9375, 'loss': 0.51834}\n",
            "Step 151300: {'train_ae_loss': 0.64449, 'train_ucc_loss': 0.38259, 'train_ucc_acc': 0.90625, 'loss': 0.51354}\n",
            "Step 151320: {'train_ae_loss': 0.64573, 'train_ucc_loss': 0.42061, 'train_ucc_acc': 0.90625, 'loss': 0.53317}\n",
            "Step 151340: {'train_ae_loss': 0.66863, 'train_ucc_loss': 0.331, 'train_ucc_acc': 1.0, 'loss': 0.49982}\n",
            "Step 151360: {'train_ae_loss': 0.67796, 'train_ucc_loss': 0.43161, 'train_ucc_acc': 0.90625, 'loss': 0.55478}\n",
            "Step 151380: {'train_ae_loss': 0.65556, 'train_ucc_loss': 0.39575, 'train_ucc_acc': 0.90625, 'loss': 0.52566}\n",
            "Step 151400: {'train_ae_loss': 0.64105, 'train_ucc_loss': 0.3948, 'train_ucc_acc': 0.90625, 'loss': 0.51792}\n",
            "Step 151420: {'train_ae_loss': 0.66311, 'train_ucc_loss': 0.4681, 'train_ucc_acc': 0.8125, 'loss': 0.5656}\n",
            "Step 151440: {'train_ae_loss': 0.6546, 'train_ucc_loss': 0.4598, 'train_ucc_acc': 0.84375, 'loss': 0.5572}\n",
            "Step 151460: {'train_ae_loss': 0.64096, 'train_ucc_loss': 0.48517, 'train_ucc_acc': 0.8125, 'loss': 0.56306}\n",
            "Step 151480: {'train_ae_loss': 0.66365, 'train_ucc_loss': 0.41263, 'train_ucc_acc': 0.90625, 'loss': 0.53814}\n",
            "Step 151500: {'train_ae_loss': 0.65151, 'train_ucc_loss': 0.48226, 'train_ucc_acc': 0.8125, 'loss': 0.56688}\n",
            "Step 151520: {'train_ae_loss': 0.66822, 'train_ucc_loss': 0.42334, 'train_ucc_acc': 0.875, 'loss': 0.54578}\n",
            "Step 151540: {'train_ae_loss': 0.66493, 'train_ucc_loss': 0.39031, 'train_ucc_acc': 0.9375, 'loss': 0.52762}\n",
            "Step 151560: {'train_ae_loss': 0.66385, 'train_ucc_loss': 0.45306, 'train_ucc_acc': 0.875, 'loss': 0.55846}\n",
            "Step 151580: {'train_ae_loss': 0.64079, 'train_ucc_loss': 0.37581, 'train_ucc_acc': 0.9375, 'loss': 0.5083}\n",
            "Step 151600: {'train_ae_loss': 0.66699, 'train_ucc_loss': 0.50178, 'train_ucc_acc': 0.84375, 'loss': 0.58439}\n",
            "Step 151620: {'train_ae_loss': 0.65851, 'train_ucc_loss': 0.4572, 'train_ucc_acc': 0.84375, 'loss': 0.55785}\n",
            "Step 151640: {'train_ae_loss': 0.66796, 'train_ucc_loss': 0.38001, 'train_ucc_acc': 0.9375, 'loss': 0.52399}\n",
            "Step 151660: {'train_ae_loss': 0.67005, 'train_ucc_loss': 0.4112, 'train_ucc_acc': 0.90625, 'loss': 0.54062}\n",
            "Step 151680: {'train_ae_loss': 0.64117, 'train_ucc_loss': 0.55023, 'train_ucc_acc': 0.75, 'loss': 0.5957}\n",
            "Step 151700: {'train_ae_loss': 0.65771, 'train_ucc_loss': 0.41048, 'train_ucc_acc': 0.90625, 'loss': 0.53409}\n",
            "Step 151720: {'train_ae_loss': 0.65056, 'train_ucc_loss': 0.46739, 'train_ucc_acc': 0.84375, 'loss': 0.55898}\n",
            "Step 151740: {'train_ae_loss': 0.67227, 'train_ucc_loss': 0.35781, 'train_ucc_acc': 0.9375, 'loss': 0.51504}\n",
            "Step 151760: {'train_ae_loss': 0.66812, 'train_ucc_loss': 0.45563, 'train_ucc_acc': 0.84375, 'loss': 0.56188}\n",
            "Step 151780: {'train_ae_loss': 0.64384, 'train_ucc_loss': 0.52953, 'train_ucc_acc': 0.75, 'loss': 0.58669}\n",
            "Step 151800: {'train_ae_loss': 0.65299, 'train_ucc_loss': 0.50415, 'train_ucc_acc': 0.8125, 'loss': 0.57857}\n",
            "Step 151820: {'train_ae_loss': 0.66438, 'train_ucc_loss': 0.49826, 'train_ucc_acc': 0.8125, 'loss': 0.58132}\n",
            "Step 151840: {'train_ae_loss': 0.64692, 'train_ucc_loss': 0.50438, 'train_ucc_acc': 0.8125, 'loss': 0.57565}\n",
            "Step 151860: {'train_ae_loss': 0.68504, 'train_ucc_loss': 0.43066, 'train_ucc_acc': 0.875, 'loss': 0.55785}\n",
            "Step 151880: {'train_ae_loss': 0.67138, 'train_ucc_loss': 0.41152, 'train_ucc_acc': 0.90625, 'loss': 0.54145}\n",
            "Step 151900: {'train_ae_loss': 0.66906, 'train_ucc_loss': 0.38988, 'train_ucc_acc': 0.90625, 'loss': 0.52947}\n",
            "Step 151920: {'train_ae_loss': 0.65235, 'train_ucc_loss': 0.37376, 'train_ucc_acc': 0.9375, 'loss': 0.51306}\n",
            "Step 151940: {'train_ae_loss': 0.66114, 'train_ucc_loss': 0.42354, 'train_ucc_acc': 0.875, 'loss': 0.54234}\n",
            "Step 151960: {'train_ae_loss': 0.64793, 'train_ucc_loss': 0.54528, 'train_ucc_acc': 0.71875, 'loss': 0.59661}\n",
            "Step 151980: {'train_ae_loss': 0.65971, 'train_ucc_loss': 0.38884, 'train_ucc_acc': 0.90625, 'loss': 0.52428}\n",
            "Step 152000: {'train_ae_loss': 0.63899, 'train_ucc_loss': 0.44194, 'train_ucc_acc': 0.875, 'loss': 0.54047}\n",
            "step: 152000,eval_ae_loss: 0.65209,eval_ucc_loss: 0.49199,eval_ucc_acc: 0.82129\n",
            "Step 152020: {'train_ae_loss': 0.65756, 'train_ucc_loss': 0.42009, 'train_ucc_acc': 0.90625, 'loss': 0.53883}\n",
            "Step 152040: {'train_ae_loss': 0.65603, 'train_ucc_loss': 0.46824, 'train_ucc_acc': 0.8125, 'loss': 0.56213}\n",
            "Step 152060: {'train_ae_loss': 0.6602, 'train_ucc_loss': 0.38946, 'train_ucc_acc': 0.90625, 'loss': 0.52483}\n",
            "Step 152080: {'train_ae_loss': 0.67802, 'train_ucc_loss': 0.43978, 'train_ucc_acc': 0.8125, 'loss': 0.5589}\n",
            "Step 152100: {'train_ae_loss': 0.6544, 'train_ucc_loss': 0.46659, 'train_ucc_acc': 0.84375, 'loss': 0.5605}\n",
            "Step 152120: {'train_ae_loss': 0.66837, 'train_ucc_loss': 0.47073, 'train_ucc_acc': 0.84375, 'loss': 0.56955}\n",
            "Step 152140: {'train_ae_loss': 0.65318, 'train_ucc_loss': 0.40881, 'train_ucc_acc': 0.90625, 'loss': 0.53099}\n",
            "Step 152160: {'train_ae_loss': 0.66795, 'train_ucc_loss': 0.39587, 'train_ucc_acc': 0.9375, 'loss': 0.53191}\n",
            "Step 152180: {'train_ae_loss': 0.67615, 'train_ucc_loss': 0.43184, 'train_ucc_acc': 0.875, 'loss': 0.55399}\n",
            "Step 152200: {'train_ae_loss': 0.65799, 'train_ucc_loss': 0.37938, 'train_ucc_acc': 0.9375, 'loss': 0.51868}\n",
            "Step 152220: {'train_ae_loss': 0.66646, 'train_ucc_loss': 0.45692, 'train_ucc_acc': 0.84375, 'loss': 0.56169}\n",
            "Step 152240: {'train_ae_loss': 0.68141, 'train_ucc_loss': 0.36658, 'train_ucc_acc': 0.96875, 'loss': 0.52399}\n",
            "Step 152260: {'train_ae_loss': 0.66554, 'train_ucc_loss': 0.41586, 'train_ucc_acc': 0.875, 'loss': 0.5407}\n",
            "Step 152280: {'train_ae_loss': 0.66306, 'train_ucc_loss': 0.40449, 'train_ucc_acc': 0.90625, 'loss': 0.53378}\n",
            "Step 152300: {'train_ae_loss': 0.66025, 'train_ucc_loss': 0.40007, 'train_ucc_acc': 0.875, 'loss': 0.53016}\n",
            "Step 152320: {'train_ae_loss': 0.66114, 'train_ucc_loss': 0.42106, 'train_ucc_acc': 0.90625, 'loss': 0.5411}\n",
            "Step 152340: {'train_ae_loss': 0.65206, 'train_ucc_loss': 0.40081, 'train_ucc_acc': 0.90625, 'loss': 0.52644}\n",
            "Step 152360: {'train_ae_loss': 0.68377, 'train_ucc_loss': 0.43444, 'train_ucc_acc': 0.84375, 'loss': 0.55911}\n",
            "Step 152380: {'train_ae_loss': 0.65822, 'train_ucc_loss': 0.35727, 'train_ucc_acc': 0.96875, 'loss': 0.50775}\n",
            "Step 152400: {'train_ae_loss': 0.67815, 'train_ucc_loss': 0.3994, 'train_ucc_acc': 0.90625, 'loss': 0.53877}\n",
            "Step 152420: {'train_ae_loss': 0.65895, 'train_ucc_loss': 0.40735, 'train_ucc_acc': 0.9375, 'loss': 0.53315}\n",
            "Step 152440: {'train_ae_loss': 0.65814, 'train_ucc_loss': 0.36544, 'train_ucc_acc': 0.9375, 'loss': 0.51179}\n",
            "Step 152460: {'train_ae_loss': 0.66646, 'train_ucc_loss': 0.44371, 'train_ucc_acc': 0.875, 'loss': 0.55508}\n",
            "Step 152480: {'train_ae_loss': 0.66427, 'train_ucc_loss': 0.3657, 'train_ucc_acc': 0.9375, 'loss': 0.51499}\n",
            "Step 152500: {'train_ae_loss': 0.6605, 'train_ucc_loss': 0.49342, 'train_ucc_acc': 0.8125, 'loss': 0.57696}\n",
            "Step 152520: {'train_ae_loss': 0.67523, 'train_ucc_loss': 0.33527, 'train_ucc_acc': 0.96875, 'loss': 0.50525}\n",
            "Step 152540: {'train_ae_loss': 0.66207, 'train_ucc_loss': 0.35526, 'train_ucc_acc': 0.96875, 'loss': 0.50867}\n",
            "Step 152560: {'train_ae_loss': 0.65517, 'train_ucc_loss': 0.46637, 'train_ucc_acc': 0.84375, 'loss': 0.56077}\n",
            "Step 152580: {'train_ae_loss': 0.66369, 'train_ucc_loss': 0.46416, 'train_ucc_acc': 0.84375, 'loss': 0.56392}\n",
            "Step 152600: {'train_ae_loss': 0.64615, 'train_ucc_loss': 0.41344, 'train_ucc_acc': 0.90625, 'loss': 0.5298}\n",
            "Step 152620: {'train_ae_loss': 0.66345, 'train_ucc_loss': 0.45464, 'train_ucc_acc': 0.84375, 'loss': 0.55905}\n",
            "Step 152640: {'train_ae_loss': 0.6855, 'train_ucc_loss': 0.38191, 'train_ucc_acc': 0.9375, 'loss': 0.53371}\n",
            "Step 152660: {'train_ae_loss': 0.66472, 'train_ucc_loss': 0.42251, 'train_ucc_acc': 0.875, 'loss': 0.54361}\n",
            "Step 152680: {'train_ae_loss': 0.66468, 'train_ucc_loss': 0.38481, 'train_ucc_acc': 0.90625, 'loss': 0.52475}\n",
            "Step 152700: {'train_ae_loss': 0.67483, 'train_ucc_loss': 0.34394, 'train_ucc_acc': 0.96875, 'loss': 0.50938}\n",
            "Step 152720: {'train_ae_loss': 0.65477, 'train_ucc_loss': 0.37934, 'train_ucc_acc': 0.9375, 'loss': 0.51706}\n",
            "Step 152740: {'train_ae_loss': 0.65142, 'train_ucc_loss': 0.53955, 'train_ucc_acc': 0.75, 'loss': 0.59549}\n",
            "Step 152760: {'train_ae_loss': 0.66255, 'train_ucc_loss': 0.39761, 'train_ucc_acc': 0.90625, 'loss': 0.53008}\n",
            "Step 152780: {'train_ae_loss': 0.6719, 'train_ucc_loss': 0.51155, 'train_ucc_acc': 0.78125, 'loss': 0.59172}\n",
            "Step 152800: {'train_ae_loss': 0.68198, 'train_ucc_loss': 0.53413, 'train_ucc_acc': 0.75, 'loss': 0.60805}\n",
            "Step 152820: {'train_ae_loss': 0.67625, 'train_ucc_loss': 0.4759, 'train_ucc_acc': 0.78125, 'loss': 0.57607}\n",
            "Step 152840: {'train_ae_loss': 0.68483, 'train_ucc_loss': 0.48124, 'train_ucc_acc': 0.8125, 'loss': 0.58304}\n",
            "Step 152860: {'train_ae_loss': 0.68529, 'train_ucc_loss': 0.44741, 'train_ucc_acc': 0.84375, 'loss': 0.56635}\n",
            "Step 152880: {'train_ae_loss': 0.66981, 'train_ucc_loss': 0.49319, 'train_ucc_acc': 0.78125, 'loss': 0.5815}\n",
            "Step 152900: {'train_ae_loss': 0.66627, 'train_ucc_loss': 0.42008, 'train_ucc_acc': 0.90625, 'loss': 0.54317}\n",
            "Step 152920: {'train_ae_loss': 0.6654, 'train_ucc_loss': 0.49396, 'train_ucc_acc': 0.8125, 'loss': 0.57968}\n",
            "Step 152940: {'train_ae_loss': 0.66594, 'train_ucc_loss': 0.43032, 'train_ucc_acc': 0.875, 'loss': 0.54813}\n",
            "Step 152960: {'train_ae_loss': 0.67303, 'train_ucc_loss': 0.62929, 'train_ucc_acc': 0.6875, 'loss': 0.65116}\n",
            "Step 152980: {'train_ae_loss': 0.654, 'train_ucc_loss': 0.44853, 'train_ucc_acc': 0.875, 'loss': 0.55126}\n",
            "Step 153000: {'train_ae_loss': 0.63877, 'train_ucc_loss': 0.4342, 'train_ucc_acc': 0.875, 'loss': 0.53649}\n",
            "step: 153000,eval_ae_loss: 0.64678,eval_ucc_loss: 0.47827,eval_ucc_acc: 0.8291\n",
            "Step 153020: {'train_ae_loss': 0.65822, 'train_ucc_loss': 0.38501, 'train_ucc_acc': 0.90625, 'loss': 0.52161}\n",
            "Step 153040: {'train_ae_loss': 0.64464, 'train_ucc_loss': 0.40078, 'train_ucc_acc': 0.90625, 'loss': 0.52271}\n",
            "Step 153060: {'train_ae_loss': 0.64988, 'train_ucc_loss': 0.39424, 'train_ucc_acc': 0.875, 'loss': 0.52206}\n",
            "Step 153080: {'train_ae_loss': 0.67183, 'train_ucc_loss': 0.38091, 'train_ucc_acc': 0.9375, 'loss': 0.52637}\n",
            "Step 153100: {'train_ae_loss': 0.64789, 'train_ucc_loss': 0.51157, 'train_ucc_acc': 0.8125, 'loss': 0.57973}\n",
            "Step 153120: {'train_ae_loss': 0.67079, 'train_ucc_loss': 0.43695, 'train_ucc_acc': 0.875, 'loss': 0.55387}\n",
            "Step 153140: {'train_ae_loss': 0.67649, 'train_ucc_loss': 0.37314, 'train_ucc_acc': 0.90625, 'loss': 0.52482}\n",
            "Step 153160: {'train_ae_loss': 0.6678, 'train_ucc_loss': 0.48949, 'train_ucc_acc': 0.8125, 'loss': 0.57864}\n",
            "Step 153180: {'train_ae_loss': 0.662, 'train_ucc_loss': 0.47757, 'train_ucc_acc': 0.8125, 'loss': 0.56979}\n",
            "Step 153200: {'train_ae_loss': 0.66704, 'train_ucc_loss': 0.42499, 'train_ucc_acc': 0.875, 'loss': 0.54602}\n",
            "Step 153220: {'train_ae_loss': 0.64715, 'train_ucc_loss': 0.41391, 'train_ucc_acc': 0.90625, 'loss': 0.53053}\n",
            "Step 153240: {'train_ae_loss': 0.65865, 'train_ucc_loss': 0.51022, 'train_ucc_acc': 0.8125, 'loss': 0.58443}\n",
            "Step 153260: {'train_ae_loss': 0.64271, 'train_ucc_loss': 0.44331, 'train_ucc_acc': 0.875, 'loss': 0.54301}\n",
            "Step 153280: {'train_ae_loss': 0.65543, 'train_ucc_loss': 0.57018, 'train_ucc_acc': 0.75, 'loss': 0.61281}\n",
            "Step 153300: {'train_ae_loss': 0.64875, 'train_ucc_loss': 0.4756, 'train_ucc_acc': 0.84375, 'loss': 0.56217}\n",
            "Step 153320: {'train_ae_loss': 0.65545, 'train_ucc_loss': 0.56146, 'train_ucc_acc': 0.6875, 'loss': 0.60845}\n",
            "Step 153340: {'train_ae_loss': 0.65991, 'train_ucc_loss': 0.44854, 'train_ucc_acc': 0.84375, 'loss': 0.55422}\n",
            "Step 153360: {'train_ae_loss': 0.65272, 'train_ucc_loss': 0.42928, 'train_ucc_acc': 0.875, 'loss': 0.541}\n",
            "Step 153380: {'train_ae_loss': 0.6589, 'train_ucc_loss': 0.40999, 'train_ucc_acc': 0.90625, 'loss': 0.53445}\n",
            "Step 153400: {'train_ae_loss': 0.6543, 'train_ucc_loss': 0.43313, 'train_ucc_acc': 0.84375, 'loss': 0.54371}\n",
            "Step 153420: {'train_ae_loss': 0.65557, 'train_ucc_loss': 0.45004, 'train_ucc_acc': 0.84375, 'loss': 0.5528}\n",
            "Step 153440: {'train_ae_loss': 0.6721, 'train_ucc_loss': 0.45665, 'train_ucc_acc': 0.875, 'loss': 0.56437}\n",
            "Step 153460: {'train_ae_loss': 0.66335, 'train_ucc_loss': 0.61663, 'train_ucc_acc': 0.65625, 'loss': 0.63999}\n",
            "Step 153480: {'train_ae_loss': 0.66564, 'train_ucc_loss': 0.49258, 'train_ucc_acc': 0.8125, 'loss': 0.57911}\n",
            "Step 153500: {'train_ae_loss': 0.65114, 'train_ucc_loss': 0.45734, 'train_ucc_acc': 0.84375, 'loss': 0.55424}\n",
            "Step 153520: {'train_ae_loss': 0.68113, 'train_ucc_loss': 0.40024, 'train_ucc_acc': 0.90625, 'loss': 0.54068}\n",
            "Step 153540: {'train_ae_loss': 0.66551, 'train_ucc_loss': 0.45498, 'train_ucc_acc': 0.84375, 'loss': 0.56024}\n",
            "Step 153560: {'train_ae_loss': 0.64641, 'train_ucc_loss': 0.41498, 'train_ucc_acc': 0.90625, 'loss': 0.5307}\n",
            "Step 153580: {'train_ae_loss': 0.67298, 'train_ucc_loss': 0.5523, 'train_ucc_acc': 0.75, 'loss': 0.61264}\n",
            "Step 153600: {'train_ae_loss': 0.67337, 'train_ucc_loss': 0.3323, 'train_ucc_acc': 1.0, 'loss': 0.50284}\n",
            "Step 153620: {'train_ae_loss': 0.6483, 'train_ucc_loss': 0.47615, 'train_ucc_acc': 0.8125, 'loss': 0.56223}\n",
            "Step 153640: {'train_ae_loss': 0.64179, 'train_ucc_loss': 0.42995, 'train_ucc_acc': 0.90625, 'loss': 0.53587}\n",
            "Step 153660: {'train_ae_loss': 0.65119, 'train_ucc_loss': 0.50267, 'train_ucc_acc': 0.8125, 'loss': 0.57693}\n",
            "Step 153680: {'train_ae_loss': 0.67236, 'train_ucc_loss': 0.43005, 'train_ucc_acc': 0.875, 'loss': 0.5512}\n",
            "Step 153700: {'train_ae_loss': 0.6539, 'train_ucc_loss': 0.48413, 'train_ucc_acc': 0.8125, 'loss': 0.56902}\n",
            "Step 153720: {'train_ae_loss': 0.65633, 'train_ucc_loss': 0.40237, 'train_ucc_acc': 0.90625, 'loss': 0.52935}\n",
            "Step 153740: {'train_ae_loss': 0.66815, 'train_ucc_loss': 0.47137, 'train_ucc_acc': 0.84375, 'loss': 0.56976}\n",
            "Step 153760: {'train_ae_loss': 0.68991, 'train_ucc_loss': 0.44366, 'train_ucc_acc': 0.875, 'loss': 0.56679}\n",
            "Step 153780: {'train_ae_loss': 0.66178, 'train_ucc_loss': 0.45416, 'train_ucc_acc': 0.875, 'loss': 0.55797}\n",
            "Step 153800: {'train_ae_loss': 0.6687, 'train_ucc_loss': 0.37526, 'train_ucc_acc': 0.9375, 'loss': 0.52198}\n",
            "Step 153820: {'train_ae_loss': 0.65182, 'train_ucc_loss': 0.39602, 'train_ucc_acc': 0.9375, 'loss': 0.52392}\n",
            "Step 153840: {'train_ae_loss': 0.6714, 'train_ucc_loss': 0.40444, 'train_ucc_acc': 0.90625, 'loss': 0.53792}\n",
            "Step 153860: {'train_ae_loss': 0.66321, 'train_ucc_loss': 0.43964, 'train_ucc_acc': 0.875, 'loss': 0.55142}\n",
            "Step 153880: {'train_ae_loss': 0.66209, 'train_ucc_loss': 0.41534, 'train_ucc_acc': 0.90625, 'loss': 0.53871}\n",
            "Step 153900: {'train_ae_loss': 0.6642, 'train_ucc_loss': 0.42099, 'train_ucc_acc': 0.875, 'loss': 0.5426}\n",
            "Step 153920: {'train_ae_loss': 0.65762, 'train_ucc_loss': 0.38712, 'train_ucc_acc': 0.9375, 'loss': 0.52237}\n",
            "Step 153940: {'train_ae_loss': 0.65355, 'train_ucc_loss': 0.44831, 'train_ucc_acc': 0.90625, 'loss': 0.55093}\n",
            "Step 153960: {'train_ae_loss': 0.66195, 'train_ucc_loss': 0.56451, 'train_ucc_acc': 0.71875, 'loss': 0.61323}\n",
            "Step 153980: {'train_ae_loss': 0.65744, 'train_ucc_loss': 0.51395, 'train_ucc_acc': 0.75, 'loss': 0.58569}\n",
            "Step 154000: {'train_ae_loss': 0.65441, 'train_ucc_loss': 0.44373, 'train_ucc_acc': 0.875, 'loss': 0.54907}\n",
            "step: 154000,eval_ae_loss: 0.64565,eval_ucc_loss: 0.47662,eval_ucc_acc: 0.82812\n",
            "Step 154020: {'train_ae_loss': 0.6666, 'train_ucc_loss': 0.4341, 'train_ucc_acc': 0.875, 'loss': 0.55035}\n",
            "Step 154040: {'train_ae_loss': 0.67428, 'train_ucc_loss': 0.45335, 'train_ucc_acc': 0.8125, 'loss': 0.56381}\n",
            "Step 154060: {'train_ae_loss': 0.64477, 'train_ucc_loss': 0.37272, 'train_ucc_acc': 0.9375, 'loss': 0.50875}\n",
            "Step 154080: {'train_ae_loss': 0.67991, 'train_ucc_loss': 0.41885, 'train_ucc_acc': 0.90625, 'loss': 0.54938}\n",
            "Step 154100: {'train_ae_loss': 0.65762, 'train_ucc_loss': 0.40044, 'train_ucc_acc': 0.90625, 'loss': 0.52903}\n",
            "Step 154120: {'train_ae_loss': 0.6561, 'train_ucc_loss': 0.4117, 'train_ucc_acc': 0.875, 'loss': 0.5339}\n",
            "Step 154140: {'train_ae_loss': 0.66116, 'train_ucc_loss': 0.5359, 'train_ucc_acc': 0.75, 'loss': 0.59853}\n",
            "Step 154160: {'train_ae_loss': 0.65042, 'train_ucc_loss': 0.42456, 'train_ucc_acc': 0.90625, 'loss': 0.53749}\n",
            "Step 154180: {'train_ae_loss': 0.65601, 'train_ucc_loss': 0.44013, 'train_ucc_acc': 0.875, 'loss': 0.54807}\n",
            "Step 154200: {'train_ae_loss': 0.65805, 'train_ucc_loss': 0.45226, 'train_ucc_acc': 0.84375, 'loss': 0.55516}\n",
            "Step 154220: {'train_ae_loss': 0.66546, 'train_ucc_loss': 0.4535, 'train_ucc_acc': 0.875, 'loss': 0.55948}\n",
            "Step 154240: {'train_ae_loss': 0.65819, 'train_ucc_loss': 0.40872, 'train_ucc_acc': 0.90625, 'loss': 0.53345}\n",
            "Step 154260: {'train_ae_loss': 0.65388, 'train_ucc_loss': 0.44862, 'train_ucc_acc': 0.875, 'loss': 0.55125}\n",
            "Step 154280: {'train_ae_loss': 0.66907, 'train_ucc_loss': 0.42075, 'train_ucc_acc': 0.90625, 'loss': 0.54491}\n",
            "Step 154300: {'train_ae_loss': 0.64149, 'train_ucc_loss': 0.50242, 'train_ucc_acc': 0.8125, 'loss': 0.57196}\n",
            "Step 154320: {'train_ae_loss': 0.6472, 'train_ucc_loss': 0.42395, 'train_ucc_acc': 0.875, 'loss': 0.53558}\n",
            "Step 154340: {'train_ae_loss': 0.66151, 'train_ucc_loss': 0.38292, 'train_ucc_acc': 0.9375, 'loss': 0.52221}\n",
            "Step 154360: {'train_ae_loss': 0.66646, 'train_ucc_loss': 0.39081, 'train_ucc_acc': 0.9375, 'loss': 0.52863}\n",
            "Step 154380: {'train_ae_loss': 0.66428, 'train_ucc_loss': 0.38791, 'train_ucc_acc': 0.90625, 'loss': 0.5261}\n",
            "Step 154400: {'train_ae_loss': 0.659, 'train_ucc_loss': 0.35289, 'train_ucc_acc': 0.96875, 'loss': 0.50594}\n",
            "Step 154420: {'train_ae_loss': 0.68507, 'train_ucc_loss': 0.38771, 'train_ucc_acc': 0.9375, 'loss': 0.53639}\n",
            "Step 154440: {'train_ae_loss': 0.66695, 'train_ucc_loss': 0.50953, 'train_ucc_acc': 0.8125, 'loss': 0.58824}\n",
            "Step 154460: {'train_ae_loss': 0.67868, 'train_ucc_loss': 0.39404, 'train_ucc_acc': 0.90625, 'loss': 0.53636}\n",
            "Step 154480: {'train_ae_loss': 0.66958, 'train_ucc_loss': 0.4655, 'train_ucc_acc': 0.84375, 'loss': 0.56754}\n",
            "Step 154500: {'train_ae_loss': 0.66995, 'train_ucc_loss': 0.38597, 'train_ucc_acc': 0.90625, 'loss': 0.52796}\n",
            "Step 154520: {'train_ae_loss': 0.65935, 'train_ucc_loss': 0.38241, 'train_ucc_acc': 0.9375, 'loss': 0.52088}\n",
            "Step 154540: {'train_ae_loss': 0.65369, 'train_ucc_loss': 0.59005, 'train_ucc_acc': 0.6875, 'loss': 0.62187}\n",
            "Step 154560: {'train_ae_loss': 0.6508, 'train_ucc_loss': 0.4234, 'train_ucc_acc': 0.875, 'loss': 0.5371}\n",
            "Step 154580: {'train_ae_loss': 0.65488, 'train_ucc_loss': 0.43673, 'train_ucc_acc': 0.84375, 'loss': 0.5458}\n",
            "Step 154600: {'train_ae_loss': 0.66325, 'train_ucc_loss': 0.48577, 'train_ucc_acc': 0.8125, 'loss': 0.57451}\n",
            "Step 154620: {'train_ae_loss': 0.66629, 'train_ucc_loss': 0.38171, 'train_ucc_acc': 0.9375, 'loss': 0.524}\n",
            "Step 154640: {'train_ae_loss': 0.65381, 'train_ucc_loss': 0.46313, 'train_ucc_acc': 0.84375, 'loss': 0.55847}\n",
            "Step 154660: {'train_ae_loss': 0.66274, 'train_ucc_loss': 0.45242, 'train_ucc_acc': 0.875, 'loss': 0.55758}\n",
            "Step 154680: {'train_ae_loss': 0.66255, 'train_ucc_loss': 0.47437, 'train_ucc_acc': 0.8125, 'loss': 0.56846}\n",
            "Step 154700: {'train_ae_loss': 0.64652, 'train_ucc_loss': 0.5193, 'train_ucc_acc': 0.78125, 'loss': 0.58291}\n",
            "Step 154720: {'train_ae_loss': 0.65253, 'train_ucc_loss': 0.40103, 'train_ucc_acc': 0.9375, 'loss': 0.52678}\n",
            "Step 154740: {'train_ae_loss': 0.65393, 'train_ucc_loss': 0.46404, 'train_ucc_acc': 0.8125, 'loss': 0.55898}\n",
            "Step 154760: {'train_ae_loss': 0.67266, 'train_ucc_loss': 0.4486, 'train_ucc_acc': 0.875, 'loss': 0.56063}\n",
            "Step 154780: {'train_ae_loss': 0.64949, 'train_ucc_loss': 0.32309, 'train_ucc_acc': 1.0, 'loss': 0.48629}\n",
            "Step 154800: {'train_ae_loss': 0.67528, 'train_ucc_loss': 0.40381, 'train_ucc_acc': 0.90625, 'loss': 0.53954}\n",
            "Step 154820: {'train_ae_loss': 0.6595, 'train_ucc_loss': 0.38372, 'train_ucc_acc': 0.9375, 'loss': 0.52161}\n",
            "Step 154840: {'train_ae_loss': 0.66667, 'train_ucc_loss': 0.41546, 'train_ucc_acc': 0.875, 'loss': 0.54106}\n",
            "Step 154860: {'train_ae_loss': 0.65408, 'train_ucc_loss': 0.49674, 'train_ucc_acc': 0.75, 'loss': 0.57541}\n",
            "Step 154880: {'train_ae_loss': 0.64432, 'train_ucc_loss': 0.43028, 'train_ucc_acc': 0.875, 'loss': 0.5373}\n",
            "Step 154900: {'train_ae_loss': 0.65966, 'train_ucc_loss': 0.37805, 'train_ucc_acc': 0.9375, 'loss': 0.51886}\n",
            "Step 154920: {'train_ae_loss': 0.64728, 'train_ucc_loss': 0.39075, 'train_ucc_acc': 0.90625, 'loss': 0.51901}\n",
            "Step 154940: {'train_ae_loss': 0.65731, 'train_ucc_loss': 0.44547, 'train_ucc_acc': 0.84375, 'loss': 0.55139}\n",
            "Step 154960: {'train_ae_loss': 0.67609, 'train_ucc_loss': 0.35359, 'train_ucc_acc': 0.96875, 'loss': 0.51484}\n",
            "Step 154980: {'train_ae_loss': 0.67011, 'train_ucc_loss': 0.35579, 'train_ucc_acc': 0.96875, 'loss': 0.51295}\n",
            "Step 155000: {'train_ae_loss': 0.65926, 'train_ucc_loss': 0.43962, 'train_ucc_acc': 0.875, 'loss': 0.54944}\n",
            "step: 155000,eval_ae_loss: 0.64822,eval_ucc_loss: 0.46643,eval_ucc_acc: 0.83789\n",
            "Step 155020: {'train_ae_loss': 0.65774, 'train_ucc_loss': 0.33219, 'train_ucc_acc': 0.96875, 'loss': 0.49497}\n",
            "Step 155040: {'train_ae_loss': 0.64487, 'train_ucc_loss': 0.52122, 'train_ucc_acc': 0.75, 'loss': 0.58304}\n",
            "Step 155060: {'train_ae_loss': 0.65338, 'train_ucc_loss': 0.5004, 'train_ucc_acc': 0.8125, 'loss': 0.57689}\n",
            "Step 155080: {'train_ae_loss': 0.65572, 'train_ucc_loss': 0.41405, 'train_ucc_acc': 0.90625, 'loss': 0.53488}\n",
            "Step 155100: {'train_ae_loss': 0.66776, 'train_ucc_loss': 0.42192, 'train_ucc_acc': 0.90625, 'loss': 0.54484}\n",
            "Step 155120: {'train_ae_loss': 0.66772, 'train_ucc_loss': 0.47, 'train_ucc_acc': 0.8125, 'loss': 0.56886}\n",
            "Step 155140: {'train_ae_loss': 0.66395, 'train_ucc_loss': 0.45383, 'train_ucc_acc': 0.8125, 'loss': 0.55889}\n",
            "Step 155160: {'train_ae_loss': 0.65185, 'train_ucc_loss': 0.52515, 'train_ucc_acc': 0.78125, 'loss': 0.5885}\n",
            "Step 155180: {'train_ae_loss': 0.66564, 'train_ucc_loss': 0.39415, 'train_ucc_acc': 0.90625, 'loss': 0.52989}\n",
            "Step 155200: {'train_ae_loss': 0.66419, 'train_ucc_loss': 0.45134, 'train_ucc_acc': 0.84375, 'loss': 0.55776}\n",
            "Step 155220: {'train_ae_loss': 0.65912, 'train_ucc_loss': 0.55099, 'train_ucc_acc': 0.75, 'loss': 0.60506}\n",
            "Step 155240: {'train_ae_loss': 0.66125, 'train_ucc_loss': 0.46837, 'train_ucc_acc': 0.8125, 'loss': 0.56481}\n",
            "Step 155260: {'train_ae_loss': 0.65922, 'train_ucc_loss': 0.49498, 'train_ucc_acc': 0.8125, 'loss': 0.5771}\n",
            "Step 155280: {'train_ae_loss': 0.66319, 'train_ucc_loss': 0.40624, 'train_ucc_acc': 0.90625, 'loss': 0.53472}\n",
            "Step 155300: {'train_ae_loss': 0.64926, 'train_ucc_loss': 0.44738, 'train_ucc_acc': 0.875, 'loss': 0.54832}\n",
            "Step 155320: {'train_ae_loss': 0.66684, 'train_ucc_loss': 0.46159, 'train_ucc_acc': 0.84375, 'loss': 0.56421}\n",
            "Step 155340: {'train_ae_loss': 0.66369, 'train_ucc_loss': 0.38232, 'train_ucc_acc': 0.9375, 'loss': 0.523}\n",
            "Step 155360: {'train_ae_loss': 0.66001, 'train_ucc_loss': 0.46505, 'train_ucc_acc': 0.84375, 'loss': 0.56253}\n",
            "Step 155380: {'train_ae_loss': 0.66774, 'train_ucc_loss': 0.44818, 'train_ucc_acc': 0.84375, 'loss': 0.55796}\n",
            "Step 155400: {'train_ae_loss': 0.65983, 'train_ucc_loss': 0.36888, 'train_ucc_acc': 0.96875, 'loss': 0.51436}\n",
            "Step 155420: {'train_ae_loss': 0.66516, 'train_ucc_loss': 0.49777, 'train_ucc_acc': 0.78125, 'loss': 0.58147}\n",
            "Step 155440: {'train_ae_loss': 0.66638, 'train_ucc_loss': 0.35695, 'train_ucc_acc': 0.96875, 'loss': 0.51166}\n",
            "Step 155460: {'train_ae_loss': 0.66534, 'train_ucc_loss': 0.46122, 'train_ucc_acc': 0.84375, 'loss': 0.56328}\n",
            "Step 155480: {'train_ae_loss': 0.65517, 'train_ucc_loss': 0.42447, 'train_ucc_acc': 0.875, 'loss': 0.53982}\n",
            "Step 155500: {'train_ae_loss': 0.6577, 'train_ucc_loss': 0.40832, 'train_ucc_acc': 0.90625, 'loss': 0.53301}\n",
            "Step 155520: {'train_ae_loss': 0.67331, 'train_ucc_loss': 0.4377, 'train_ucc_acc': 0.84375, 'loss': 0.55551}\n",
            "Step 155540: {'train_ae_loss': 0.67127, 'train_ucc_loss': 0.49274, 'train_ucc_acc': 0.8125, 'loss': 0.58201}\n",
            "Step 155560: {'train_ae_loss': 0.65643, 'train_ucc_loss': 0.41943, 'train_ucc_acc': 0.90625, 'loss': 0.53793}\n",
            "Step 155580: {'train_ae_loss': 0.6686, 'train_ucc_loss': 0.35129, 'train_ucc_acc': 0.96875, 'loss': 0.50994}\n",
            "Step 155600: {'train_ae_loss': 0.67462, 'train_ucc_loss': 0.37626, 'train_ucc_acc': 0.9375, 'loss': 0.52544}\n",
            "Step 155620: {'train_ae_loss': 0.63943, 'train_ucc_loss': 0.45889, 'train_ucc_acc': 0.8125, 'loss': 0.54916}\n",
            "Step 155640: {'train_ae_loss': 0.66097, 'train_ucc_loss': 0.4664, 'train_ucc_acc': 0.84375, 'loss': 0.56369}\n",
            "Step 155660: {'train_ae_loss': 0.65372, 'train_ucc_loss': 0.57155, 'train_ucc_acc': 0.75, 'loss': 0.61264}\n",
            "Step 155680: {'train_ae_loss': 0.66602, 'train_ucc_loss': 0.4316, 'train_ucc_acc': 0.875, 'loss': 0.54881}\n",
            "Step 155700: {'train_ae_loss': 0.65835, 'train_ucc_loss': 0.43489, 'train_ucc_acc': 0.875, 'loss': 0.54662}\n",
            "Step 155720: {'train_ae_loss': 0.67489, 'train_ucc_loss': 0.46124, 'train_ucc_acc': 0.84375, 'loss': 0.56806}\n",
            "Step 155740: {'train_ae_loss': 0.66015, 'train_ucc_loss': 0.45688, 'train_ucc_acc': 0.84375, 'loss': 0.55851}\n",
            "Step 155760: {'train_ae_loss': 0.66521, 'train_ucc_loss': 0.36811, 'train_ucc_acc': 0.9375, 'loss': 0.51666}\n",
            "Step 155780: {'train_ae_loss': 0.65991, 'train_ucc_loss': 0.50656, 'train_ucc_acc': 0.78125, 'loss': 0.58323}\n",
            "Step 155800: {'train_ae_loss': 0.65036, 'train_ucc_loss': 0.47462, 'train_ucc_acc': 0.8125, 'loss': 0.56249}\n",
            "Step 155820: {'train_ae_loss': 0.65351, 'train_ucc_loss': 0.57356, 'train_ucc_acc': 0.71875, 'loss': 0.61353}\n",
            "Step 155840: {'train_ae_loss': 0.66749, 'train_ucc_loss': 0.5722, 'train_ucc_acc': 0.71875, 'loss': 0.61984}\n",
            "Step 155860: {'train_ae_loss': 0.64189, 'train_ucc_loss': 0.41248, 'train_ucc_acc': 0.90625, 'loss': 0.52719}\n",
            "Step 155880: {'train_ae_loss': 0.66174, 'train_ucc_loss': 0.40135, 'train_ucc_acc': 0.90625, 'loss': 0.53155}\n",
            "Step 155900: {'train_ae_loss': 0.65297, 'train_ucc_loss': 0.51237, 'train_ucc_acc': 0.78125, 'loss': 0.58267}\n",
            "Step 155920: {'train_ae_loss': 0.63412, 'train_ucc_loss': 0.46552, 'train_ucc_acc': 0.84375, 'loss': 0.54982}\n",
            "Step 155940: {'train_ae_loss': 0.66187, 'train_ucc_loss': 0.38373, 'train_ucc_acc': 0.90625, 'loss': 0.5228}\n",
            "Step 155960: {'train_ae_loss': 0.66744, 'train_ucc_loss': 0.43125, 'train_ucc_acc': 0.875, 'loss': 0.54934}\n",
            "Step 155980: {'train_ae_loss': 0.66552, 'train_ucc_loss': 0.39095, 'train_ucc_acc': 0.9375, 'loss': 0.52824}\n",
            "Step 156000: {'train_ae_loss': 0.65527, 'train_ucc_loss': 0.44674, 'train_ucc_acc': 0.875, 'loss': 0.55101}\n",
            "step: 156000,eval_ae_loss: 0.65125,eval_ucc_loss: 0.48238,eval_ucc_acc: 0.82129\n",
            "Step 156020: {'train_ae_loss': 0.65446, 'train_ucc_loss': 0.55393, 'train_ucc_acc': 0.71875, 'loss': 0.6042}\n",
            "Step 156040: {'train_ae_loss': 0.6619, 'train_ucc_loss': 0.37495, 'train_ucc_acc': 0.9375, 'loss': 0.51843}\n",
            "Step 156060: {'train_ae_loss': 0.63403, 'train_ucc_loss': 0.48344, 'train_ucc_acc': 0.84375, 'loss': 0.55873}\n",
            "Step 156080: {'train_ae_loss': 0.64886, 'train_ucc_loss': 0.46847, 'train_ucc_acc': 0.84375, 'loss': 0.55867}\n",
            "Step 156100: {'train_ae_loss': 0.65366, 'train_ucc_loss': 0.48125, 'train_ucc_acc': 0.78125, 'loss': 0.56746}\n",
            "Step 156120: {'train_ae_loss': 0.64813, 'train_ucc_loss': 0.38525, 'train_ucc_acc': 0.9375, 'loss': 0.51669}\n",
            "Step 156140: {'train_ae_loss': 0.68501, 'train_ucc_loss': 0.46768, 'train_ucc_acc': 0.84375, 'loss': 0.57634}\n",
            "Step 156160: {'train_ae_loss': 0.67262, 'train_ucc_loss': 0.42188, 'train_ucc_acc': 0.875, 'loss': 0.54725}\n",
            "Step 156180: {'train_ae_loss': 0.65991, 'train_ucc_loss': 0.43545, 'train_ucc_acc': 0.875, 'loss': 0.54768}\n",
            "Step 156200: {'train_ae_loss': 0.67004, 'train_ucc_loss': 0.40698, 'train_ucc_acc': 0.875, 'loss': 0.53851}\n",
            "Step 156220: {'train_ae_loss': 0.65553, 'train_ucc_loss': 0.43673, 'train_ucc_acc': 0.875, 'loss': 0.54613}\n",
            "Step 156240: {'train_ae_loss': 0.65834, 'train_ucc_loss': 0.38733, 'train_ucc_acc': 0.9375, 'loss': 0.52283}\n",
            "Step 156260: {'train_ae_loss': 0.65896, 'train_ucc_loss': 0.44883, 'train_ucc_acc': 0.8125, 'loss': 0.55389}\n",
            "Step 156280: {'train_ae_loss': 0.65299, 'train_ucc_loss': 0.36507, 'train_ucc_acc': 0.9375, 'loss': 0.50903}\n",
            "Step 156300: {'train_ae_loss': 0.65455, 'train_ucc_loss': 0.44217, 'train_ucc_acc': 0.875, 'loss': 0.54836}\n",
            "Step 156320: {'train_ae_loss': 0.64199, 'train_ucc_loss': 0.53295, 'train_ucc_acc': 0.78125, 'loss': 0.58747}\n",
            "Step 156340: {'train_ae_loss': 0.66661, 'train_ucc_loss': 0.40383, 'train_ucc_acc': 0.90625, 'loss': 0.53522}\n",
            "Step 156360: {'train_ae_loss': 0.65991, 'train_ucc_loss': 0.42986, 'train_ucc_acc': 0.875, 'loss': 0.54489}\n",
            "Step 156380: {'train_ae_loss': 0.65768, 'train_ucc_loss': 0.43442, 'train_ucc_acc': 0.875, 'loss': 0.54605}\n",
            "Step 156400: {'train_ae_loss': 0.67795, 'train_ucc_loss': 0.40685, 'train_ucc_acc': 0.875, 'loss': 0.5424}\n",
            "Step 156420: {'train_ae_loss': 0.66143, 'train_ucc_loss': 0.39054, 'train_ucc_acc': 0.90625, 'loss': 0.52599}\n",
            "Step 156440: {'train_ae_loss': 0.648, 'train_ucc_loss': 0.442, 'train_ucc_acc': 0.875, 'loss': 0.545}\n",
            "Step 156460: {'train_ae_loss': 0.6688, 'train_ucc_loss': 0.34539, 'train_ucc_acc': 0.96875, 'loss': 0.50709}\n",
            "Step 156480: {'train_ae_loss': 0.64703, 'train_ucc_loss': 0.35096, 'train_ucc_acc': 0.96875, 'loss': 0.49899}\n",
            "Step 156500: {'train_ae_loss': 0.64431, 'train_ucc_loss': 0.37796, 'train_ucc_acc': 0.875, 'loss': 0.51114}\n",
            "Step 156520: {'train_ae_loss': 0.65747, 'train_ucc_loss': 0.41967, 'train_ucc_acc': 0.90625, 'loss': 0.53857}\n",
            "Step 156540: {'train_ae_loss': 0.67812, 'train_ucc_loss': 0.49563, 'train_ucc_acc': 0.78125, 'loss': 0.58687}\n",
            "Step 156560: {'train_ae_loss': 0.66256, 'train_ucc_loss': 0.40715, 'train_ucc_acc': 0.90625, 'loss': 0.53486}\n",
            "Step 156580: {'train_ae_loss': 0.68512, 'train_ucc_loss': 0.41188, 'train_ucc_acc': 0.90625, 'loss': 0.5485}\n",
            "Step 156600: {'train_ae_loss': 0.66261, 'train_ucc_loss': 0.43058, 'train_ucc_acc': 0.90625, 'loss': 0.5466}\n",
            "Step 156620: {'train_ae_loss': 0.65709, 'train_ucc_loss': 0.40282, 'train_ucc_acc': 0.90625, 'loss': 0.52995}\n",
            "Step 156640: {'train_ae_loss': 0.65683, 'train_ucc_loss': 0.5179, 'train_ucc_acc': 0.78125, 'loss': 0.58736}\n",
            "Step 156660: {'train_ae_loss': 0.66777, 'train_ucc_loss': 0.46951, 'train_ucc_acc': 0.84375, 'loss': 0.56864}\n",
            "Step 156680: {'train_ae_loss': 0.6798, 'train_ucc_loss': 0.40317, 'train_ucc_acc': 0.90625, 'loss': 0.54148}\n",
            "Step 156700: {'train_ae_loss': 0.68363, 'train_ucc_loss': 0.41853, 'train_ucc_acc': 0.90625, 'loss': 0.55108}\n",
            "Step 156720: {'train_ae_loss': 0.65232, 'train_ucc_loss': 0.41506, 'train_ucc_acc': 0.90625, 'loss': 0.53369}\n",
            "Step 156740: {'train_ae_loss': 0.65631, 'train_ucc_loss': 0.52774, 'train_ucc_acc': 0.75, 'loss': 0.59203}\n",
            "Step 156760: {'train_ae_loss': 0.65995, 'train_ucc_loss': 0.45948, 'train_ucc_acc': 0.84375, 'loss': 0.55972}\n",
            "Step 156780: {'train_ae_loss': 0.65787, 'train_ucc_loss': 0.38435, 'train_ucc_acc': 0.90625, 'loss': 0.52111}\n",
            "Step 156800: {'train_ae_loss': 0.6573, 'train_ucc_loss': 0.40228, 'train_ucc_acc': 0.90625, 'loss': 0.52979}\n",
            "Step 156820: {'train_ae_loss': 0.64401, 'train_ucc_loss': 0.46465, 'train_ucc_acc': 0.875, 'loss': 0.55433}\n",
            "Step 156840: {'train_ae_loss': 0.67861, 'train_ucc_loss': 0.50908, 'train_ucc_acc': 0.78125, 'loss': 0.59384}\n",
            "Step 156860: {'train_ae_loss': 0.67877, 'train_ucc_loss': 0.37714, 'train_ucc_acc': 0.9375, 'loss': 0.52796}\n",
            "Step 156880: {'train_ae_loss': 0.64888, 'train_ucc_loss': 0.51025, 'train_ucc_acc': 0.78125, 'loss': 0.57957}\n",
            "Step 156900: {'train_ae_loss': 0.63987, 'train_ucc_loss': 0.51246, 'train_ucc_acc': 0.78125, 'loss': 0.57617}\n",
            "Step 156920: {'train_ae_loss': 0.6655, 'train_ucc_loss': 0.47738, 'train_ucc_acc': 0.84375, 'loss': 0.57144}\n",
            "Step 156940: {'train_ae_loss': 0.66164, 'train_ucc_loss': 0.44114, 'train_ucc_acc': 0.875, 'loss': 0.55139}\n",
            "Step 156960: {'train_ae_loss': 0.66094, 'train_ucc_loss': 0.39515, 'train_ucc_acc': 0.90625, 'loss': 0.52804}\n",
            "Step 156980: {'train_ae_loss': 0.6585, 'train_ucc_loss': 0.38865, 'train_ucc_acc': 0.90625, 'loss': 0.52358}\n",
            "Step 157000: {'train_ae_loss': 0.6439, 'train_ucc_loss': 0.40404, 'train_ucc_acc': 0.9375, 'loss': 0.52397}\n",
            "step: 157000,eval_ae_loss: 0.6521,eval_ucc_loss: 0.47743,eval_ucc_acc: 0.83105\n",
            "Step 157020: {'train_ae_loss': 0.65473, 'train_ucc_loss': 0.39689, 'train_ucc_acc': 0.9375, 'loss': 0.52581}\n",
            "Step 157040: {'train_ae_loss': 0.66686, 'train_ucc_loss': 0.44303, 'train_ucc_acc': 0.875, 'loss': 0.55495}\n",
            "Step 157060: {'train_ae_loss': 0.65665, 'train_ucc_loss': 0.46244, 'train_ucc_acc': 0.875, 'loss': 0.55954}\n",
            "Step 157080: {'train_ae_loss': 0.64552, 'train_ucc_loss': 0.48866, 'train_ucc_acc': 0.78125, 'loss': 0.56709}\n",
            "Step 157100: {'train_ae_loss': 0.66083, 'train_ucc_loss': 0.39678, 'train_ucc_acc': 0.9375, 'loss': 0.52881}\n",
            "Step 157120: {'train_ae_loss': 0.66209, 'train_ucc_loss': 0.44785, 'train_ucc_acc': 0.84375, 'loss': 0.55497}\n",
            "Step 157140: {'train_ae_loss': 0.66539, 'train_ucc_loss': 0.38986, 'train_ucc_acc': 0.9375, 'loss': 0.52763}\n",
            "Step 157160: {'train_ae_loss': 0.65526, 'train_ucc_loss': 0.4495, 'train_ucc_acc': 0.84375, 'loss': 0.55238}\n",
            "Step 157180: {'train_ae_loss': 0.64447, 'train_ucc_loss': 0.38754, 'train_ucc_acc': 0.9375, 'loss': 0.51601}\n",
            "Step 157200: {'train_ae_loss': 0.67691, 'train_ucc_loss': 0.39954, 'train_ucc_acc': 0.90625, 'loss': 0.53822}\n",
            "Step 157220: {'train_ae_loss': 0.67332, 'train_ucc_loss': 0.35059, 'train_ucc_acc': 0.96875, 'loss': 0.51195}\n",
            "Step 157240: {'train_ae_loss': 0.65641, 'train_ucc_loss': 0.5154, 'train_ucc_acc': 0.75, 'loss': 0.5859}\n",
            "Step 157260: {'train_ae_loss': 0.65882, 'train_ucc_loss': 0.43215, 'train_ucc_acc': 0.875, 'loss': 0.54548}\n",
            "Step 157280: {'train_ae_loss': 0.6603, 'train_ucc_loss': 0.3906, 'train_ucc_acc': 0.90625, 'loss': 0.52545}\n",
            "Step 157300: {'train_ae_loss': 0.67313, 'train_ucc_loss': 0.45068, 'train_ucc_acc': 0.84375, 'loss': 0.56191}\n",
            "Step 157320: {'train_ae_loss': 0.65537, 'train_ucc_loss': 0.41997, 'train_ucc_acc': 0.90625, 'loss': 0.53767}\n",
            "Step 157340: {'train_ae_loss': 0.68488, 'train_ucc_loss': 0.44947, 'train_ucc_acc': 0.84375, 'loss': 0.56717}\n",
            "Step 157360: {'train_ae_loss': 0.65228, 'train_ucc_loss': 0.5153, 'train_ucc_acc': 0.8125, 'loss': 0.58379}\n",
            "Step 157380: {'train_ae_loss': 0.64524, 'train_ucc_loss': 0.40335, 'train_ucc_acc': 0.90625, 'loss': 0.52429}\n",
            "Step 157400: {'train_ae_loss': 0.68199, 'train_ucc_loss': 0.37491, 'train_ucc_acc': 0.9375, 'loss': 0.52845}\n",
            "Step 157420: {'train_ae_loss': 0.64804, 'train_ucc_loss': 0.47613, 'train_ucc_acc': 0.84375, 'loss': 0.56209}\n",
            "Step 157440: {'train_ae_loss': 0.65031, 'train_ucc_loss': 0.40274, 'train_ucc_acc': 0.9375, 'loss': 0.52652}\n",
            "Step 157460: {'train_ae_loss': 0.63523, 'train_ucc_loss': 0.45007, 'train_ucc_acc': 0.875, 'loss': 0.54265}\n",
            "Step 157480: {'train_ae_loss': 0.65348, 'train_ucc_loss': 0.45999, 'train_ucc_acc': 0.84375, 'loss': 0.55673}\n",
            "Step 157500: {'train_ae_loss': 0.66613, 'train_ucc_loss': 0.51562, 'train_ucc_acc': 0.78125, 'loss': 0.59087}\n",
            "Step 157520: {'train_ae_loss': 0.65693, 'train_ucc_loss': 0.36797, 'train_ucc_acc': 0.9375, 'loss': 0.51245}\n",
            "Step 157540: {'train_ae_loss': 0.65044, 'train_ucc_loss': 0.50604, 'train_ucc_acc': 0.8125, 'loss': 0.57824}\n",
            "Step 157560: {'train_ae_loss': 0.67189, 'train_ucc_loss': 0.48158, 'train_ucc_acc': 0.8125, 'loss': 0.57673}\n",
            "Step 157580: {'train_ae_loss': 0.6604, 'train_ucc_loss': 0.44207, 'train_ucc_acc': 0.875, 'loss': 0.55123}\n",
            "Step 157600: {'train_ae_loss': 0.64818, 'train_ucc_loss': 0.44176, 'train_ucc_acc': 0.875, 'loss': 0.54497}\n",
            "Step 157620: {'train_ae_loss': 0.64892, 'train_ucc_loss': 0.44509, 'train_ucc_acc': 0.875, 'loss': 0.547}\n",
            "Step 157640: {'train_ae_loss': 0.65271, 'train_ucc_loss': 0.4885, 'train_ucc_acc': 0.8125, 'loss': 0.5706}\n",
            "Step 157660: {'train_ae_loss': 0.6508, 'train_ucc_loss': 0.3805, 'train_ucc_acc': 0.9375, 'loss': 0.51565}\n",
            "Step 157680: {'train_ae_loss': 0.65111, 'train_ucc_loss': 0.37744, 'train_ucc_acc': 0.9375, 'loss': 0.51428}\n",
            "Step 157700: {'train_ae_loss': 0.65428, 'train_ucc_loss': 0.39316, 'train_ucc_acc': 0.90625, 'loss': 0.52372}\n",
            "Step 157720: {'train_ae_loss': 0.64043, 'train_ucc_loss': 0.37105, 'train_ucc_acc': 0.9375, 'loss': 0.50574}\n",
            "Step 157740: {'train_ae_loss': 0.65566, 'train_ucc_loss': 0.39228, 'train_ucc_acc': 0.90625, 'loss': 0.52397}\n",
            "Step 157760: {'train_ae_loss': 0.66449, 'train_ucc_loss': 0.44674, 'train_ucc_acc': 0.875, 'loss': 0.55562}\n",
            "Step 157780: {'train_ae_loss': 0.65725, 'train_ucc_loss': 0.48863, 'train_ucc_acc': 0.8125, 'loss': 0.57294}\n",
            "Step 157800: {'train_ae_loss': 0.6482, 'train_ucc_loss': 0.41731, 'train_ucc_acc': 0.90625, 'loss': 0.53276}\n",
            "Step 157820: {'train_ae_loss': 0.6639, 'train_ucc_loss': 0.46121, 'train_ucc_acc': 0.84375, 'loss': 0.56255}\n",
            "Step 157840: {'train_ae_loss': 0.66462, 'train_ucc_loss': 0.32814, 'train_ucc_acc': 1.0, 'loss': 0.49638}\n",
            "Step 157860: {'train_ae_loss': 0.66326, 'train_ucc_loss': 0.34688, 'train_ucc_acc': 0.96875, 'loss': 0.50507}\n",
            "Step 157880: {'train_ae_loss': 0.67074, 'train_ucc_loss': 0.43694, 'train_ucc_acc': 0.875, 'loss': 0.55384}\n",
            "Step 157900: {'train_ae_loss': 0.65823, 'train_ucc_loss': 0.38676, 'train_ucc_acc': 0.9375, 'loss': 0.52249}\n",
            "Step 157920: {'train_ae_loss': 0.66209, 'train_ucc_loss': 0.37459, 'train_ucc_acc': 0.9375, 'loss': 0.51834}\n",
            "Step 157940: {'train_ae_loss': 0.65363, 'train_ucc_loss': 0.39603, 'train_ucc_acc': 0.90625, 'loss': 0.52483}\n",
            "Step 157960: {'train_ae_loss': 0.66451, 'train_ucc_loss': 0.37359, 'train_ucc_acc': 0.90625, 'loss': 0.51905}\n",
            "Step 157980: {'train_ae_loss': 0.65459, 'train_ucc_loss': 0.46431, 'train_ucc_acc': 0.8125, 'loss': 0.55945}\n",
            "Step 158000: {'train_ae_loss': 0.64226, 'train_ucc_loss': 0.4345, 'train_ucc_acc': 0.875, 'loss': 0.53838}\n",
            "step: 158000,eval_ae_loss: 0.65496,eval_ucc_loss: 0.57752,eval_ucc_acc: 0.72266\n",
            "Step 158020: {'train_ae_loss': 0.65463, 'train_ucc_loss': 0.41865, 'train_ucc_acc': 0.875, 'loss': 0.53664}\n",
            "Step 158040: {'train_ae_loss': 0.66557, 'train_ucc_loss': 0.43538, 'train_ucc_acc': 0.875, 'loss': 0.55047}\n",
            "Step 158060: {'train_ae_loss': 0.64973, 'train_ucc_loss': 0.46806, 'train_ucc_acc': 0.8125, 'loss': 0.5589}\n",
            "Step 158080: {'train_ae_loss': 0.66144, 'train_ucc_loss': 0.37188, 'train_ucc_acc': 0.9375, 'loss': 0.51666}\n",
            "Step 158100: {'train_ae_loss': 0.66115, 'train_ucc_loss': 0.39272, 'train_ucc_acc': 0.90625, 'loss': 0.52693}\n",
            "Step 158120: {'train_ae_loss': 0.66502, 'train_ucc_loss': 0.35699, 'train_ucc_acc': 0.96875, 'loss': 0.51101}\n",
            "Step 158140: {'train_ae_loss': 0.67266, 'train_ucc_loss': 0.40926, 'train_ucc_acc': 0.875, 'loss': 0.54096}\n",
            "Step 158160: {'train_ae_loss': 0.67305, 'train_ucc_loss': 0.47988, 'train_ucc_acc': 0.8125, 'loss': 0.57646}\n",
            "Step 158180: {'train_ae_loss': 0.6611, 'train_ucc_loss': 0.43994, 'train_ucc_acc': 0.875, 'loss': 0.55052}\n",
            "Step 158200: {'train_ae_loss': 0.64975, 'train_ucc_loss': 0.55631, 'train_ucc_acc': 0.75, 'loss': 0.60303}\n",
            "Step 158220: {'train_ae_loss': 0.66272, 'train_ucc_loss': 0.45508, 'train_ucc_acc': 0.875, 'loss': 0.5589}\n",
            "Step 158240: {'train_ae_loss': 0.68264, 'train_ucc_loss': 0.45763, 'train_ucc_acc': 0.875, 'loss': 0.57013}\n",
            "Step 158260: {'train_ae_loss': 0.66338, 'train_ucc_loss': 0.44167, 'train_ucc_acc': 0.84375, 'loss': 0.55252}\n",
            "Step 158280: {'train_ae_loss': 0.67324, 'train_ucc_loss': 0.45213, 'train_ucc_acc': 0.84375, 'loss': 0.56269}\n",
            "Step 158300: {'train_ae_loss': 0.65463, 'train_ucc_loss': 0.44449, 'train_ucc_acc': 0.875, 'loss': 0.54956}\n",
            "Step 158320: {'train_ae_loss': 0.65823, 'train_ucc_loss': 0.48261, 'train_ucc_acc': 0.78125, 'loss': 0.57042}\n",
            "Step 158340: {'train_ae_loss': 0.6623, 'train_ucc_loss': 0.47296, 'train_ucc_acc': 0.84375, 'loss': 0.56763}\n",
            "Step 158360: {'train_ae_loss': 0.64095, 'train_ucc_loss': 0.41188, 'train_ucc_acc': 0.90625, 'loss': 0.52641}\n",
            "Step 158380: {'train_ae_loss': 0.66344, 'train_ucc_loss': 0.45966, 'train_ucc_acc': 0.84375, 'loss': 0.56155}\n",
            "Step 158400: {'train_ae_loss': 0.65792, 'train_ucc_loss': 0.52177, 'train_ucc_acc': 0.78125, 'loss': 0.58985}\n",
            "Step 158420: {'train_ae_loss': 0.66637, 'train_ucc_loss': 0.47291, 'train_ucc_acc': 0.84375, 'loss': 0.56964}\n",
            "Step 158440: {'train_ae_loss': 0.64891, 'train_ucc_loss': 0.49669, 'train_ucc_acc': 0.8125, 'loss': 0.5728}\n",
            "Step 158460: {'train_ae_loss': 0.66394, 'train_ucc_loss': 0.47325, 'train_ucc_acc': 0.84375, 'loss': 0.56859}\n",
            "Step 158480: {'train_ae_loss': 0.66487, 'train_ucc_loss': 0.43228, 'train_ucc_acc': 0.875, 'loss': 0.54857}\n",
            "Step 158500: {'train_ae_loss': 0.66065, 'train_ucc_loss': 0.50193, 'train_ucc_acc': 0.8125, 'loss': 0.58129}\n",
            "Step 158520: {'train_ae_loss': 0.66338, 'train_ucc_loss': 0.48202, 'train_ucc_acc': 0.78125, 'loss': 0.5727}\n",
            "Step 158540: {'train_ae_loss': 0.64809, 'train_ucc_loss': 0.5037, 'train_ucc_acc': 0.8125, 'loss': 0.5759}\n",
            "Step 158560: {'train_ae_loss': 0.66502, 'train_ucc_loss': 0.37487, 'train_ucc_acc': 0.90625, 'loss': 0.51995}\n",
            "Step 158580: {'train_ae_loss': 0.65285, 'train_ucc_loss': 0.54599, 'train_ucc_acc': 0.71875, 'loss': 0.59942}\n",
            "Step 158600: {'train_ae_loss': 0.65411, 'train_ucc_loss': 0.45037, 'train_ucc_acc': 0.84375, 'loss': 0.55224}\n",
            "Step 158620: {'train_ae_loss': 0.66785, 'train_ucc_loss': 0.44775, 'train_ucc_acc': 0.84375, 'loss': 0.5578}\n",
            "Step 158640: {'train_ae_loss': 0.66733, 'train_ucc_loss': 0.55217, 'train_ucc_acc': 0.78125, 'loss': 0.60975}\n",
            "Step 158660: {'train_ae_loss': 0.65988, 'train_ucc_loss': 0.41912, 'train_ucc_acc': 0.875, 'loss': 0.5395}\n",
            "Step 158680: {'train_ae_loss': 0.65312, 'train_ucc_loss': 0.42822, 'train_ucc_acc': 0.875, 'loss': 0.54067}\n",
            "Step 158700: {'train_ae_loss': 0.65276, 'train_ucc_loss': 0.50688, 'train_ucc_acc': 0.78125, 'loss': 0.57982}\n",
            "Step 158720: {'train_ae_loss': 0.66312, 'train_ucc_loss': 0.41508, 'train_ucc_acc': 0.875, 'loss': 0.5391}\n",
            "Step 158740: {'train_ae_loss': 0.66054, 'train_ucc_loss': 0.42708, 'train_ucc_acc': 0.90625, 'loss': 0.54381}\n",
            "Step 158760: {'train_ae_loss': 0.6546, 'train_ucc_loss': 0.4136, 'train_ucc_acc': 0.90625, 'loss': 0.5341}\n",
            "Step 158780: {'train_ae_loss': 0.66713, 'train_ucc_loss': 0.36997, 'train_ucc_acc': 0.9375, 'loss': 0.51855}\n",
            "Step 158800: {'train_ae_loss': 0.64621, 'train_ucc_loss': 0.46587, 'train_ucc_acc': 0.8125, 'loss': 0.55604}\n",
            "Step 158820: {'train_ae_loss': 0.6556, 'train_ucc_loss': 0.44933, 'train_ucc_acc': 0.875, 'loss': 0.55247}\n",
            "Step 158840: {'train_ae_loss': 0.65643, 'train_ucc_loss': 0.47668, 'train_ucc_acc': 0.84375, 'loss': 0.56655}\n",
            "Step 158860: {'train_ae_loss': 0.68128, 'train_ucc_loss': 0.34086, 'train_ucc_acc': 0.96875, 'loss': 0.51107}\n",
            "Step 158880: {'train_ae_loss': 0.68232, 'train_ucc_loss': 0.41061, 'train_ucc_acc': 0.90625, 'loss': 0.54646}\n",
            "Step 158900: {'train_ae_loss': 0.66673, 'train_ucc_loss': 0.54116, 'train_ucc_acc': 0.78125, 'loss': 0.60395}\n",
            "Step 158920: {'train_ae_loss': 0.66349, 'train_ucc_loss': 0.46624, 'train_ucc_acc': 0.84375, 'loss': 0.56487}\n",
            "Step 158940: {'train_ae_loss': 0.66024, 'train_ucc_loss': 0.3647, 'train_ucc_acc': 0.9375, 'loss': 0.51247}\n",
            "Step 158960: {'train_ae_loss': 0.63763, 'train_ucc_loss': 0.51127, 'train_ucc_acc': 0.78125, 'loss': 0.57445}\n",
            "Step 158980: {'train_ae_loss': 0.66813, 'train_ucc_loss': 0.4305, 'train_ucc_acc': 0.875, 'loss': 0.54932}\n",
            "Step 159000: {'train_ae_loss': 0.66444, 'train_ucc_loss': 0.4319, 'train_ucc_acc': 0.875, 'loss': 0.54817}\n",
            "step: 159000,eval_ae_loss: 0.65343,eval_ucc_loss: 0.48893,eval_ucc_acc: 0.81738\n",
            "Step 159020: {'train_ae_loss': 0.67261, 'train_ucc_loss': 0.43818, 'train_ucc_acc': 0.90625, 'loss': 0.5554}\n",
            "Step 159040: {'train_ae_loss': 0.6545, 'train_ucc_loss': 0.40278, 'train_ucc_acc': 0.90625, 'loss': 0.52864}\n",
            "Step 159060: {'train_ae_loss': 0.65231, 'train_ucc_loss': 0.375, 'train_ucc_acc': 0.9375, 'loss': 0.51366}\n",
            "Step 159080: {'train_ae_loss': 0.65706, 'train_ucc_loss': 0.52206, 'train_ucc_acc': 0.78125, 'loss': 0.58956}\n",
            "Step 159100: {'train_ae_loss': 0.67539, 'train_ucc_loss': 0.43763, 'train_ucc_acc': 0.875, 'loss': 0.55651}\n",
            "Step 159120: {'train_ae_loss': 0.65033, 'train_ucc_loss': 0.59127, 'train_ucc_acc': 0.6875, 'loss': 0.6208}\n",
            "Step 159140: {'train_ae_loss': 0.65524, 'train_ucc_loss': 0.38113, 'train_ucc_acc': 0.9375, 'loss': 0.51818}\n",
            "Step 159160: {'train_ae_loss': 0.66141, 'train_ucc_loss': 0.46778, 'train_ucc_acc': 0.84375, 'loss': 0.56459}\n",
            "Step 159180: {'train_ae_loss': 0.66472, 'train_ucc_loss': 0.38899, 'train_ucc_acc': 0.9375, 'loss': 0.52685}\n",
            "Step 159200: {'train_ae_loss': 0.66393, 'train_ucc_loss': 0.35211, 'train_ucc_acc': 0.96875, 'loss': 0.50802}\n",
            "Step 159220: {'train_ae_loss': 0.64956, 'train_ucc_loss': 0.38266, 'train_ucc_acc': 0.9375, 'loss': 0.51611}\n",
            "Step 159240: {'train_ae_loss': 0.66099, 'train_ucc_loss': 0.44596, 'train_ucc_acc': 0.84375, 'loss': 0.55348}\n",
            "Step 159260: {'train_ae_loss': 0.65895, 'train_ucc_loss': 0.55977, 'train_ucc_acc': 0.75, 'loss': 0.60936}\n",
            "Step 159280: {'train_ae_loss': 0.65235, 'train_ucc_loss': 0.4513, 'train_ucc_acc': 0.875, 'loss': 0.55182}\n",
            "Step 159300: {'train_ae_loss': 0.65232, 'train_ucc_loss': 0.3514, 'train_ucc_acc': 0.96875, 'loss': 0.50186}\n",
            "Step 159320: {'train_ae_loss': 0.66635, 'train_ucc_loss': 0.41091, 'train_ucc_acc': 0.875, 'loss': 0.53863}\n",
            "Step 159340: {'train_ae_loss': 0.65991, 'train_ucc_loss': 0.35566, 'train_ucc_acc': 0.9375, 'loss': 0.50778}\n",
            "Step 159360: {'train_ae_loss': 0.66026, 'train_ucc_loss': 0.46248, 'train_ucc_acc': 0.84375, 'loss': 0.56137}\n",
            "Step 159380: {'train_ae_loss': 0.67016, 'train_ucc_loss': 0.50243, 'train_ucc_acc': 0.78125, 'loss': 0.58629}\n",
            "Step 159400: {'train_ae_loss': 0.66714, 'train_ucc_loss': 0.45611, 'train_ucc_acc': 0.84375, 'loss': 0.56162}\n",
            "Step 159420: {'train_ae_loss': 0.66989, 'train_ucc_loss': 0.44085, 'train_ucc_acc': 0.84375, 'loss': 0.55537}\n",
            "Step 159440: {'train_ae_loss': 0.68483, 'train_ucc_loss': 0.37258, 'train_ucc_acc': 0.9375, 'loss': 0.52871}\n",
            "Step 159460: {'train_ae_loss': 0.66284, 'train_ucc_loss': 0.36648, 'train_ucc_acc': 0.9375, 'loss': 0.51466}\n",
            "Step 159480: {'train_ae_loss': 0.67491, 'train_ucc_loss': 0.39758, 'train_ucc_acc': 0.90625, 'loss': 0.53625}\n",
            "Step 159500: {'train_ae_loss': 0.64492, 'train_ucc_loss': 0.45769, 'train_ucc_acc': 0.84375, 'loss': 0.55131}\n",
            "Step 159520: {'train_ae_loss': 0.67489, 'train_ucc_loss': 0.36658, 'train_ucc_acc': 0.96875, 'loss': 0.52073}\n",
            "Step 159540: {'train_ae_loss': 0.66806, 'train_ucc_loss': 0.45557, 'train_ucc_acc': 0.84375, 'loss': 0.56182}\n",
            "Step 159560: {'train_ae_loss': 0.65967, 'train_ucc_loss': 0.40376, 'train_ucc_acc': 0.90625, 'loss': 0.53172}\n",
            "Step 159580: {'train_ae_loss': 0.66579, 'train_ucc_loss': 0.50102, 'train_ucc_acc': 0.8125, 'loss': 0.5834}\n",
            "Step 159600: {'train_ae_loss': 0.66632, 'train_ucc_loss': 0.44508, 'train_ucc_acc': 0.875, 'loss': 0.5557}\n",
            "Step 159620: {'train_ae_loss': 0.67982, 'train_ucc_loss': 0.62974, 'train_ucc_acc': 0.65625, 'loss': 0.65478}\n",
            "Step 159640: {'train_ae_loss': 0.64985, 'train_ucc_loss': 0.38633, 'train_ucc_acc': 0.9375, 'loss': 0.51809}\n",
            "Step 159660: {'train_ae_loss': 0.67164, 'train_ucc_loss': 0.43274, 'train_ucc_acc': 0.84375, 'loss': 0.55219}\n",
            "Step 159680: {'train_ae_loss': 0.67309, 'train_ucc_loss': 0.42376, 'train_ucc_acc': 0.90625, 'loss': 0.54842}\n",
            "Step 159700: {'train_ae_loss': 0.66398, 'train_ucc_loss': 0.48613, 'train_ucc_acc': 0.8125, 'loss': 0.57505}\n",
            "Step 159720: {'train_ae_loss': 0.66478, 'train_ucc_loss': 0.3973, 'train_ucc_acc': 0.9375, 'loss': 0.53104}\n",
            "Step 159740: {'train_ae_loss': 0.67706, 'train_ucc_loss': 0.43243, 'train_ucc_acc': 0.875, 'loss': 0.55475}\n",
            "Step 159760: {'train_ae_loss': 0.67367, 'train_ucc_loss': 0.40592, 'train_ucc_acc': 0.90625, 'loss': 0.5398}\n",
            "Step 159780: {'train_ae_loss': 0.6664, 'train_ucc_loss': 0.47306, 'train_ucc_acc': 0.84375, 'loss': 0.56973}\n",
            "Step 159800: {'train_ae_loss': 0.64678, 'train_ucc_loss': 0.4616, 'train_ucc_acc': 0.84375, 'loss': 0.55419}\n",
            "Step 159820: {'train_ae_loss': 0.63638, 'train_ucc_loss': 0.41427, 'train_ucc_acc': 0.90625, 'loss': 0.52533}\n",
            "Step 159840: {'train_ae_loss': 0.67056, 'train_ucc_loss': 0.42447, 'train_ucc_acc': 0.875, 'loss': 0.54752}\n",
            "Step 159860: {'train_ae_loss': 0.6731, 'train_ucc_loss': 0.4229, 'train_ucc_acc': 0.875, 'loss': 0.548}\n",
            "Step 159880: {'train_ae_loss': 0.66208, 'train_ucc_loss': 0.48192, 'train_ucc_acc': 0.84375, 'loss': 0.572}\n",
            "Step 159900: {'train_ae_loss': 0.66111, 'train_ucc_loss': 0.3746, 'train_ucc_acc': 0.9375, 'loss': 0.51786}\n",
            "Step 159920: {'train_ae_loss': 0.66735, 'train_ucc_loss': 0.43379, 'train_ucc_acc': 0.84375, 'loss': 0.55057}\n",
            "Step 159940: {'train_ae_loss': 0.65583, 'train_ucc_loss': 0.4393, 'train_ucc_acc': 0.84375, 'loss': 0.54756}\n",
            "Step 159960: {'train_ae_loss': 0.65699, 'train_ucc_loss': 0.4755, 'train_ucc_acc': 0.84375, 'loss': 0.56625}\n",
            "Step 159980: {'train_ae_loss': 0.64496, 'train_ucc_loss': 0.50584, 'train_ucc_acc': 0.8125, 'loss': 0.5754}\n",
            "Step 160000: {'train_ae_loss': 0.66741, 'train_ucc_loss': 0.43089, 'train_ucc_acc': 0.875, 'loss': 0.54915}\n",
            "step: 160000,eval_ae_loss: 0.65333,eval_ucc_loss: 0.53949,eval_ucc_acc: 0.75879\n",
            "Step 160020: {'train_ae_loss': 0.66068, 'train_ucc_loss': 0.49001, 'train_ucc_acc': 0.8125, 'loss': 0.57534}\n",
            "Step 160040: {'train_ae_loss': 0.65678, 'train_ucc_loss': 0.41548, 'train_ucc_acc': 0.90625, 'loss': 0.53613}\n",
            "Step 160060: {'train_ae_loss': 0.67449, 'train_ucc_loss': 0.39103, 'train_ucc_acc': 0.9375, 'loss': 0.53276}\n",
            "Step 160080: {'train_ae_loss': 0.65223, 'train_ucc_loss': 0.40049, 'train_ucc_acc': 0.90625, 'loss': 0.52636}\n",
            "Step 160100: {'train_ae_loss': 0.658, 'train_ucc_loss': 0.46108, 'train_ucc_acc': 0.84375, 'loss': 0.55954}\n",
            "Step 160120: {'train_ae_loss': 0.64271, 'train_ucc_loss': 0.48341, 'train_ucc_acc': 0.84375, 'loss': 0.56306}\n",
            "Step 160140: {'train_ae_loss': 0.66214, 'train_ucc_loss': 0.41474, 'train_ucc_acc': 0.90625, 'loss': 0.53844}\n",
            "Step 160160: {'train_ae_loss': 0.64217, 'train_ucc_loss': 0.44168, 'train_ucc_acc': 0.875, 'loss': 0.54193}\n",
            "Step 160180: {'train_ae_loss': 0.65622, 'train_ucc_loss': 0.39617, 'train_ucc_acc': 0.90625, 'loss': 0.52619}\n",
            "Step 160200: {'train_ae_loss': 0.656, 'train_ucc_loss': 0.38687, 'train_ucc_acc': 0.9375, 'loss': 0.52143}\n",
            "Step 160220: {'train_ae_loss': 0.64412, 'train_ucc_loss': 0.48102, 'train_ucc_acc': 0.8125, 'loss': 0.56257}\n",
            "Step 160240: {'train_ae_loss': 0.64455, 'train_ucc_loss': 0.3386, 'train_ucc_acc': 0.96875, 'loss': 0.49158}\n",
            "Step 160260: {'train_ae_loss': 0.64033, 'train_ucc_loss': 0.36347, 'train_ucc_acc': 0.96875, 'loss': 0.5019}\n",
            "Step 160280: {'train_ae_loss': 0.66954, 'train_ucc_loss': 0.46559, 'train_ucc_acc': 0.8125, 'loss': 0.56757}\n",
            "Step 160300: {'train_ae_loss': 0.6576, 'train_ucc_loss': 0.42235, 'train_ucc_acc': 0.90625, 'loss': 0.53998}\n",
            "Step 160320: {'train_ae_loss': 0.65647, 'train_ucc_loss': 0.44133, 'train_ucc_acc': 0.875, 'loss': 0.5489}\n",
            "Step 160340: {'train_ae_loss': 0.65029, 'train_ucc_loss': 0.48631, 'train_ucc_acc': 0.78125, 'loss': 0.5683}\n",
            "Step 160360: {'train_ae_loss': 0.65736, 'train_ucc_loss': 0.51314, 'train_ucc_acc': 0.78125, 'loss': 0.58525}\n",
            "Step 160380: {'train_ae_loss': 0.66317, 'train_ucc_loss': 0.53458, 'train_ucc_acc': 0.75, 'loss': 0.59887}\n",
            "Step 160400: {'train_ae_loss': 0.65441, 'train_ucc_loss': 0.43548, 'train_ucc_acc': 0.84375, 'loss': 0.54494}\n",
            "Step 160420: {'train_ae_loss': 0.66203, 'train_ucc_loss': 0.37986, 'train_ucc_acc': 0.9375, 'loss': 0.52095}\n",
            "Step 160440: {'train_ae_loss': 0.66552, 'train_ucc_loss': 0.40184, 'train_ucc_acc': 0.90625, 'loss': 0.53368}\n",
            "Step 160460: {'train_ae_loss': 0.6689, 'train_ucc_loss': 0.47479, 'train_ucc_acc': 0.8125, 'loss': 0.57184}\n",
            "Step 160480: {'train_ae_loss': 0.65692, 'train_ucc_loss': 0.45952, 'train_ucc_acc': 0.875, 'loss': 0.55822}\n",
            "Step 160500: {'train_ae_loss': 0.65702, 'train_ucc_loss': 0.3833, 'train_ucc_acc': 0.90625, 'loss': 0.52016}\n",
            "Step 160520: {'train_ae_loss': 0.66077, 'train_ucc_loss': 0.326, 'train_ucc_acc': 1.0, 'loss': 0.49338}\n",
            "Step 160540: {'train_ae_loss': 0.65539, 'train_ucc_loss': 0.38764, 'train_ucc_acc': 0.90625, 'loss': 0.52152}\n",
            "Step 160560: {'train_ae_loss': 0.65513, 'train_ucc_loss': 0.44682, 'train_ucc_acc': 0.84375, 'loss': 0.55098}\n",
            "Step 160580: {'train_ae_loss': 0.66496, 'train_ucc_loss': 0.44473, 'train_ucc_acc': 0.84375, 'loss': 0.55485}\n",
            "Step 160600: {'train_ae_loss': 0.65543, 'train_ucc_loss': 0.50345, 'train_ucc_acc': 0.8125, 'loss': 0.57944}\n",
            "Step 160620: {'train_ae_loss': 0.64931, 'train_ucc_loss': 0.36798, 'train_ucc_acc': 0.9375, 'loss': 0.50865}\n",
            "Step 160640: {'train_ae_loss': 0.66304, 'train_ucc_loss': 0.35594, 'train_ucc_acc': 0.9375, 'loss': 0.50949}\n",
            "Step 160660: {'train_ae_loss': 0.68088, 'train_ucc_loss': 0.44254, 'train_ucc_acc': 0.875, 'loss': 0.56171}\n",
            "Step 160680: {'train_ae_loss': 0.6468, 'train_ucc_loss': 0.3997, 'train_ucc_acc': 0.90625, 'loss': 0.52325}\n",
            "Step 160700: {'train_ae_loss': 0.65696, 'train_ucc_loss': 0.5323, 'train_ucc_acc': 0.75, 'loss': 0.59463}\n",
            "Step 160720: {'train_ae_loss': 0.66081, 'train_ucc_loss': 0.36619, 'train_ucc_acc': 0.9375, 'loss': 0.5135}\n",
            "Step 160740: {'train_ae_loss': 0.67321, 'train_ucc_loss': 0.3871, 'train_ucc_acc': 0.9375, 'loss': 0.53015}\n",
            "Step 160760: {'train_ae_loss': 0.6614, 'train_ucc_loss': 0.55898, 'train_ucc_acc': 0.75, 'loss': 0.61019}\n",
            "Step 160780: {'train_ae_loss': 0.65989, 'train_ucc_loss': 0.46, 'train_ucc_acc': 0.84375, 'loss': 0.55995}\n",
            "Step 160800: {'train_ae_loss': 0.64934, 'train_ucc_loss': 0.44868, 'train_ucc_acc': 0.875, 'loss': 0.54901}\n",
            "Step 160820: {'train_ae_loss': 0.66031, 'train_ucc_loss': 0.39772, 'train_ucc_acc': 0.90625, 'loss': 0.52901}\n",
            "Step 160840: {'train_ae_loss': 0.67113, 'train_ucc_loss': 0.37899, 'train_ucc_acc': 0.9375, 'loss': 0.52506}\n",
            "Step 160860: {'train_ae_loss': 0.67267, 'train_ucc_loss': 0.42398, 'train_ucc_acc': 0.875, 'loss': 0.54833}\n",
            "Step 160880: {'train_ae_loss': 0.65459, 'train_ucc_loss': 0.41056, 'train_ucc_acc': 0.90625, 'loss': 0.53257}\n",
            "Step 160900: {'train_ae_loss': 0.66053, 'train_ucc_loss': 0.37553, 'train_ucc_acc': 0.9375, 'loss': 0.51803}\n",
            "Step 160920: {'train_ae_loss': 0.65717, 'train_ucc_loss': 0.39752, 'train_ucc_acc': 0.90625, 'loss': 0.52734}\n",
            "Step 160940: {'train_ae_loss': 0.66256, 'train_ucc_loss': 0.41559, 'train_ucc_acc': 0.90625, 'loss': 0.53908}\n",
            "Step 160960: {'train_ae_loss': 0.65449, 'train_ucc_loss': 0.41137, 'train_ucc_acc': 0.875, 'loss': 0.53293}\n",
            "Step 160980: {'train_ae_loss': 0.66927, 'train_ucc_loss': 0.44855, 'train_ucc_acc': 0.875, 'loss': 0.55891}\n",
            "Step 161000: {'train_ae_loss': 0.66234, 'train_ucc_loss': 0.51582, 'train_ucc_acc': 0.78125, 'loss': 0.58908}\n",
            "step: 161000,eval_ae_loss: 0.64403,eval_ucc_loss: 0.4822,eval_ucc_acc: 0.82617\n",
            "Step 161020: {'train_ae_loss': 0.66608, 'train_ucc_loss': 0.45249, 'train_ucc_acc': 0.84375, 'loss': 0.55929}\n",
            "Step 161040: {'train_ae_loss': 0.66251, 'train_ucc_loss': 0.45329, 'train_ucc_acc': 0.84375, 'loss': 0.5579}\n",
            "Step 161060: {'train_ae_loss': 0.65091, 'train_ucc_loss': 0.3184, 'train_ucc_acc': 1.0, 'loss': 0.48466}\n",
            "Step 161080: {'train_ae_loss': 0.66308, 'train_ucc_loss': 0.41324, 'train_ucc_acc': 0.875, 'loss': 0.53816}\n",
            "Step 161100: {'train_ae_loss': 0.66017, 'train_ucc_loss': 0.43842, 'train_ucc_acc': 0.875, 'loss': 0.5493}\n",
            "Step 161120: {'train_ae_loss': 0.64807, 'train_ucc_loss': 0.34948, 'train_ucc_acc': 0.96875, 'loss': 0.49878}\n",
            "Step 161140: {'train_ae_loss': 0.66421, 'train_ucc_loss': 0.41245, 'train_ucc_acc': 0.875, 'loss': 0.53833}\n",
            "Step 161160: {'train_ae_loss': 0.67036, 'train_ucc_loss': 0.40915, 'train_ucc_acc': 0.90625, 'loss': 0.53975}\n",
            "Step 161180: {'train_ae_loss': 0.66567, 'train_ucc_loss': 0.39056, 'train_ucc_acc': 0.875, 'loss': 0.52812}\n",
            "Step 161200: {'train_ae_loss': 0.65755, 'train_ucc_loss': 0.47114, 'train_ucc_acc': 0.84375, 'loss': 0.56435}\n",
            "Step 161220: {'train_ae_loss': 0.66743, 'train_ucc_loss': 0.39664, 'train_ucc_acc': 0.90625, 'loss': 0.53203}\n",
            "Step 161240: {'train_ae_loss': 0.65562, 'train_ucc_loss': 0.43998, 'train_ucc_acc': 0.875, 'loss': 0.5478}\n",
            "Step 161260: {'train_ae_loss': 0.6646, 'train_ucc_loss': 0.48225, 'train_ucc_acc': 0.78125, 'loss': 0.57342}\n",
            "Step 161280: {'train_ae_loss': 0.65994, 'train_ucc_loss': 0.39887, 'train_ucc_acc': 0.90625, 'loss': 0.5294}\n",
            "Step 161300: {'train_ae_loss': 0.67922, 'train_ucc_loss': 0.44712, 'train_ucc_acc': 0.875, 'loss': 0.56317}\n",
            "Step 161320: {'train_ae_loss': 0.64468, 'train_ucc_loss': 0.50769, 'train_ucc_acc': 0.8125, 'loss': 0.57618}\n",
            "Step 161340: {'train_ae_loss': 0.65675, 'train_ucc_loss': 0.42289, 'train_ucc_acc': 0.875, 'loss': 0.53982}\n",
            "Step 161360: {'train_ae_loss': 0.6324, 'train_ucc_loss': 0.41089, 'train_ucc_acc': 0.875, 'loss': 0.52164}\n",
            "Step 161380: {'train_ae_loss': 0.67118, 'train_ucc_loss': 0.43219, 'train_ucc_acc': 0.875, 'loss': 0.55168}\n",
            "Step 161400: {'train_ae_loss': 0.65647, 'train_ucc_loss': 0.41087, 'train_ucc_acc': 0.875, 'loss': 0.53367}\n",
            "Step 161420: {'train_ae_loss': 0.66302, 'train_ucc_loss': 0.40832, 'train_ucc_acc': 0.875, 'loss': 0.53567}\n",
            "Step 161440: {'train_ae_loss': 0.64597, 'train_ucc_loss': 0.5203, 'train_ucc_acc': 0.78125, 'loss': 0.58314}\n",
            "Step 161460: {'train_ae_loss': 0.65321, 'train_ucc_loss': 0.40896, 'train_ucc_acc': 0.90625, 'loss': 0.53109}\n",
            "Step 161480: {'train_ae_loss': 0.65107, 'train_ucc_loss': 0.35259, 'train_ucc_acc': 0.96875, 'loss': 0.50183}\n",
            "Step 161500: {'train_ae_loss': 0.66186, 'train_ucc_loss': 0.48673, 'train_ucc_acc': 0.84375, 'loss': 0.57429}\n",
            "Step 161520: {'train_ae_loss': 0.64603, 'train_ucc_loss': 0.43661, 'train_ucc_acc': 0.875, 'loss': 0.54132}\n",
            "Step 161540: {'train_ae_loss': 0.68116, 'train_ucc_loss': 0.34963, 'train_ucc_acc': 0.96875, 'loss': 0.5154}\n",
            "Step 161560: {'train_ae_loss': 0.65966, 'train_ucc_loss': 0.38693, 'train_ucc_acc': 0.9375, 'loss': 0.5233}\n",
            "Step 161580: {'train_ae_loss': 0.64318, 'train_ucc_loss': 0.39321, 'train_ucc_acc': 0.9375, 'loss': 0.51819}\n",
            "Step 161600: {'train_ae_loss': 0.63864, 'train_ucc_loss': 0.41729, 'train_ucc_acc': 0.90625, 'loss': 0.52797}\n",
            "Step 161620: {'train_ae_loss': 0.64666, 'train_ucc_loss': 0.38709, 'train_ucc_acc': 0.9375, 'loss': 0.51688}\n",
            "Step 161640: {'train_ae_loss': 0.65728, 'train_ucc_loss': 0.48888, 'train_ucc_acc': 0.8125, 'loss': 0.57308}\n",
            "Step 161660: {'train_ae_loss': 0.67162, 'train_ucc_loss': 0.43915, 'train_ucc_acc': 0.875, 'loss': 0.55539}\n",
            "Step 161680: {'train_ae_loss': 0.65415, 'train_ucc_loss': 0.43403, 'train_ucc_acc': 0.875, 'loss': 0.54409}\n",
            "Step 161700: {'train_ae_loss': 0.67267, 'train_ucc_loss': 0.37764, 'train_ucc_acc': 0.9375, 'loss': 0.52516}\n",
            "Step 161720: {'train_ae_loss': 0.66182, 'train_ucc_loss': 0.41645, 'train_ucc_acc': 0.90625, 'loss': 0.53914}\n",
            "Step 161740: {'train_ae_loss': 0.66614, 'train_ucc_loss': 0.38816, 'train_ucc_acc': 0.90625, 'loss': 0.52715}\n",
            "Step 161760: {'train_ae_loss': 0.66002, 'train_ucc_loss': 0.41723, 'train_ucc_acc': 0.90625, 'loss': 0.53863}\n",
            "Step 161780: {'train_ae_loss': 0.66185, 'train_ucc_loss': 0.43001, 'train_ucc_acc': 0.84375, 'loss': 0.54593}\n",
            "Step 161800: {'train_ae_loss': 0.66145, 'train_ucc_loss': 0.44384, 'train_ucc_acc': 0.84375, 'loss': 0.55265}\n",
            "Step 161820: {'train_ae_loss': 0.65632, 'train_ucc_loss': 0.49378, 'train_ucc_acc': 0.84375, 'loss': 0.57505}\n",
            "Step 161840: {'train_ae_loss': 0.65548, 'train_ucc_loss': 0.51945, 'train_ucc_acc': 0.78125, 'loss': 0.58747}\n",
            "Step 161860: {'train_ae_loss': 0.64656, 'train_ucc_loss': 0.36538, 'train_ucc_acc': 0.9375, 'loss': 0.50597}\n",
            "Step 161880: {'train_ae_loss': 0.64812, 'train_ucc_loss': 0.34357, 'train_ucc_acc': 0.96875, 'loss': 0.49585}\n",
            "Step 161900: {'train_ae_loss': 0.65683, 'train_ucc_loss': 0.42903, 'train_ucc_acc': 0.90625, 'loss': 0.54293}\n",
            "Step 161920: {'train_ae_loss': 0.66157, 'train_ucc_loss': 0.43363, 'train_ucc_acc': 0.84375, 'loss': 0.5476}\n",
            "Step 161940: {'train_ae_loss': 0.66444, 'train_ucc_loss': 0.46902, 'train_ucc_acc': 0.84375, 'loss': 0.56673}\n",
            "Step 161960: {'train_ae_loss': 0.66362, 'train_ucc_loss': 0.5003, 'train_ucc_acc': 0.8125, 'loss': 0.58196}\n",
            "Step 161980: {'train_ae_loss': 0.66848, 'train_ucc_loss': 0.47074, 'train_ucc_acc': 0.8125, 'loss': 0.56961}\n",
            "Step 162000: {'train_ae_loss': 0.67838, 'train_ucc_loss': 0.41754, 'train_ucc_acc': 0.90625, 'loss': 0.54796}\n",
            "step: 162000,eval_ae_loss: 0.64336,eval_ucc_loss: 0.48629,eval_ucc_acc: 0.82422\n",
            "Step 162020: {'train_ae_loss': 0.66319, 'train_ucc_loss': 0.51078, 'train_ucc_acc': 0.8125, 'loss': 0.58698}\n",
            "Step 162040: {'train_ae_loss': 0.64774, 'train_ucc_loss': 0.43842, 'train_ucc_acc': 0.84375, 'loss': 0.54308}\n",
            "Step 162060: {'train_ae_loss': 0.66073, 'train_ucc_loss': 0.38968, 'train_ucc_acc': 0.90625, 'loss': 0.52521}\n",
            "Step 162080: {'train_ae_loss': 0.65766, 'train_ucc_loss': 0.3869, 'train_ucc_acc': 0.9375, 'loss': 0.52228}\n",
            "Step 162100: {'train_ae_loss': 0.64904, 'train_ucc_loss': 0.40112, 'train_ucc_acc': 0.90625, 'loss': 0.52508}\n",
            "Step 162120: {'train_ae_loss': 0.6689, 'train_ucc_loss': 0.38132, 'train_ucc_acc': 0.9375, 'loss': 0.52511}\n",
            "Step 162140: {'train_ae_loss': 0.65127, 'train_ucc_loss': 0.4404, 'train_ucc_acc': 0.875, 'loss': 0.54583}\n",
            "Step 162160: {'train_ae_loss': 0.66852, 'train_ucc_loss': 0.4264, 'train_ucc_acc': 0.875, 'loss': 0.54746}\n",
            "Step 162180: {'train_ae_loss': 0.64504, 'train_ucc_loss': 0.40919, 'train_ucc_acc': 0.90625, 'loss': 0.52711}\n",
            "Step 162200: {'train_ae_loss': 0.65348, 'train_ucc_loss': 0.41822, 'train_ucc_acc': 0.90625, 'loss': 0.53585}\n",
            "Step 162220: {'train_ae_loss': 0.64889, 'train_ucc_loss': 0.43245, 'train_ucc_acc': 0.875, 'loss': 0.54067}\n",
            "Step 162240: {'train_ae_loss': 0.64583, 'train_ucc_loss': 0.53436, 'train_ucc_acc': 0.78125, 'loss': 0.5901}\n",
            "Step 162260: {'train_ae_loss': 0.6678, 'train_ucc_loss': 0.41794, 'train_ucc_acc': 0.90625, 'loss': 0.54287}\n",
            "Step 162280: {'train_ae_loss': 0.64712, 'train_ucc_loss': 0.44274, 'train_ucc_acc': 0.875, 'loss': 0.54493}\n",
            "Step 162300: {'train_ae_loss': 0.65132, 'train_ucc_loss': 0.36405, 'train_ucc_acc': 0.9375, 'loss': 0.50768}\n",
            "Step 162320: {'train_ae_loss': 0.65331, 'train_ucc_loss': 0.34993, 'train_ucc_acc': 0.96875, 'loss': 0.50162}\n",
            "Step 162340: {'train_ae_loss': 0.64679, 'train_ucc_loss': 0.43933, 'train_ucc_acc': 0.84375, 'loss': 0.54306}\n",
            "Step 162360: {'train_ae_loss': 0.6567, 'train_ucc_loss': 0.48928, 'train_ucc_acc': 0.8125, 'loss': 0.57299}\n",
            "Step 162380: {'train_ae_loss': 0.67727, 'train_ucc_loss': 0.38022, 'train_ucc_acc': 0.9375, 'loss': 0.52874}\n",
            "Step 162400: {'train_ae_loss': 0.67056, 'train_ucc_loss': 0.46782, 'train_ucc_acc': 0.84375, 'loss': 0.56919}\n",
            "Step 162420: {'train_ae_loss': 0.65658, 'train_ucc_loss': 0.37617, 'train_ucc_acc': 0.90625, 'loss': 0.51638}\n",
            "Step 162440: {'train_ae_loss': 0.65169, 'train_ucc_loss': 0.44396, 'train_ucc_acc': 0.875, 'loss': 0.54782}\n",
            "Step 162460: {'train_ae_loss': 0.665, 'train_ucc_loss': 0.38563, 'train_ucc_acc': 0.90625, 'loss': 0.52532}\n",
            "Step 162480: {'train_ae_loss': 0.67817, 'train_ucc_loss': 0.46731, 'train_ucc_acc': 0.84375, 'loss': 0.57274}\n",
            "Step 162500: {'train_ae_loss': 0.65688, 'train_ucc_loss': 0.44965, 'train_ucc_acc': 0.84375, 'loss': 0.55326}\n",
            "Step 162520: {'train_ae_loss': 0.666, 'train_ucc_loss': 0.55769, 'train_ucc_acc': 0.71875, 'loss': 0.61185}\n",
            "Step 162540: {'train_ae_loss': 0.6622, 'train_ucc_loss': 0.50489, 'train_ucc_acc': 0.8125, 'loss': 0.58355}\n",
            "Step 162560: {'train_ae_loss': 0.6736, 'train_ucc_loss': 0.39701, 'train_ucc_acc': 0.90625, 'loss': 0.5353}\n",
            "Step 162580: {'train_ae_loss': 0.65501, 'train_ucc_loss': 0.36487, 'train_ucc_acc': 0.9375, 'loss': 0.50994}\n",
            "Step 162600: {'train_ae_loss': 0.6466, 'train_ucc_loss': 0.45879, 'train_ucc_acc': 0.875, 'loss': 0.55269}\n",
            "Step 162620: {'train_ae_loss': 0.64833, 'train_ucc_loss': 0.39082, 'train_ucc_acc': 0.9375, 'loss': 0.51957}\n",
            "Step 162640: {'train_ae_loss': 0.65436, 'train_ucc_loss': 0.41599, 'train_ucc_acc': 0.90625, 'loss': 0.53518}\n",
            "Step 162660: {'train_ae_loss': 0.6576, 'train_ucc_loss': 0.4508, 'train_ucc_acc': 0.84375, 'loss': 0.5542}\n",
            "Step 162680: {'train_ae_loss': 0.65737, 'train_ucc_loss': 0.38435, 'train_ucc_acc': 0.96875, 'loss': 0.52086}\n",
            "Step 162700: {'train_ae_loss': 0.6749, 'train_ucc_loss': 0.48547, 'train_ucc_acc': 0.8125, 'loss': 0.58019}\n",
            "Step 162720: {'train_ae_loss': 0.65048, 'train_ucc_loss': 0.47206, 'train_ucc_acc': 0.8125, 'loss': 0.56127}\n",
            "Step 162740: {'train_ae_loss': 0.65348, 'train_ucc_loss': 0.40072, 'train_ucc_acc': 0.90625, 'loss': 0.5271}\n",
            "Step 162760: {'train_ae_loss': 0.65692, 'train_ucc_loss': 0.41297, 'train_ucc_acc': 0.875, 'loss': 0.53494}\n",
            "Step 162780: {'train_ae_loss': 0.64841, 'train_ucc_loss': 0.53107, 'train_ucc_acc': 0.71875, 'loss': 0.58974}\n",
            "Step 162800: {'train_ae_loss': 0.65194, 'train_ucc_loss': 0.42662, 'train_ucc_acc': 0.875, 'loss': 0.53928}\n",
            "Step 162820: {'train_ae_loss': 0.66655, 'train_ucc_loss': 0.53943, 'train_ucc_acc': 0.75, 'loss': 0.60299}\n",
            "Step 162840: {'train_ae_loss': 0.62896, 'train_ucc_loss': 0.38888, 'train_ucc_acc': 0.9375, 'loss': 0.50892}\n",
            "Step 162860: {'train_ae_loss': 0.66189, 'train_ucc_loss': 0.44807, 'train_ucc_acc': 0.875, 'loss': 0.55498}\n",
            "Step 162880: {'train_ae_loss': 0.63313, 'train_ucc_loss': 0.46482, 'train_ucc_acc': 0.84375, 'loss': 0.54898}\n",
            "Step 162900: {'train_ae_loss': 0.6655, 'train_ucc_loss': 0.44263, 'train_ucc_acc': 0.84375, 'loss': 0.55406}\n",
            "Step 162920: {'train_ae_loss': 0.68263, 'train_ucc_loss': 0.41989, 'train_ucc_acc': 0.875, 'loss': 0.55126}\n",
            "Step 162940: {'train_ae_loss': 0.65135, 'train_ucc_loss': 0.34521, 'train_ucc_acc': 0.96875, 'loss': 0.49828}\n",
            "Step 162960: {'train_ae_loss': 0.64933, 'train_ucc_loss': 0.41279, 'train_ucc_acc': 0.90625, 'loss': 0.53106}\n",
            "Step 162980: {'train_ae_loss': 0.64739, 'train_ucc_loss': 0.46333, 'train_ucc_acc': 0.84375, 'loss': 0.55536}\n",
            "Step 163000: {'train_ae_loss': 0.66181, 'train_ucc_loss': 0.37499, 'train_ucc_acc': 0.90625, 'loss': 0.5184}\n",
            "step: 163000,eval_ae_loss: 0.6517,eval_ucc_loss: 0.48153,eval_ucc_acc: 0.82422\n",
            "Step 163020: {'train_ae_loss': 0.6454, 'train_ucc_loss': 0.58106, 'train_ucc_acc': 0.75, 'loss': 0.61323}\n",
            "Step 163040: {'train_ae_loss': 0.65643, 'train_ucc_loss': 0.41243, 'train_ucc_acc': 0.90625, 'loss': 0.53443}\n",
            "Step 163060: {'train_ae_loss': 0.6669, 'train_ucc_loss': 0.40234, 'train_ucc_acc': 0.875, 'loss': 0.53462}\n",
            "Step 163080: {'train_ae_loss': 0.66075, 'train_ucc_loss': 0.36282, 'train_ucc_acc': 0.9375, 'loss': 0.51179}\n",
            "Step 163100: {'train_ae_loss': 0.65262, 'train_ucc_loss': 0.4872, 'train_ucc_acc': 0.8125, 'loss': 0.56991}\n",
            "Step 163120: {'train_ae_loss': 0.66268, 'train_ucc_loss': 0.47178, 'train_ucc_acc': 0.84375, 'loss': 0.56723}\n",
            "Step 163140: {'train_ae_loss': 0.65918, 'train_ucc_loss': 0.38146, 'train_ucc_acc': 0.90625, 'loss': 0.52032}\n",
            "Step 163160: {'train_ae_loss': 0.66563, 'train_ucc_loss': 0.40878, 'train_ucc_acc': 0.90625, 'loss': 0.5372}\n",
            "Step 163180: {'train_ae_loss': 0.6567, 'train_ucc_loss': 0.49751, 'train_ucc_acc': 0.8125, 'loss': 0.5771}\n",
            "Step 163200: {'train_ae_loss': 0.6584, 'train_ucc_loss': 0.50114, 'train_ucc_acc': 0.8125, 'loss': 0.57977}\n",
            "Step 163220: {'train_ae_loss': 0.64966, 'train_ucc_loss': 0.39976, 'train_ucc_acc': 0.90625, 'loss': 0.52471}\n",
            "Step 163240: {'train_ae_loss': 0.64902, 'train_ucc_loss': 0.44514, 'train_ucc_acc': 0.875, 'loss': 0.54708}\n",
            "Step 163260: {'train_ae_loss': 0.65303, 'train_ucc_loss': 0.42502, 'train_ucc_acc': 0.875, 'loss': 0.53903}\n",
            "Step 163280: {'train_ae_loss': 0.64646, 'train_ucc_loss': 0.65009, 'train_ucc_acc': 0.625, 'loss': 0.64828}\n",
            "Step 163300: {'train_ae_loss': 0.65742, 'train_ucc_loss': 0.51987, 'train_ucc_acc': 0.78125, 'loss': 0.58865}\n",
            "Step 163320: {'train_ae_loss': 0.65491, 'train_ucc_loss': 0.38951, 'train_ucc_acc': 0.90625, 'loss': 0.52221}\n",
            "Step 163340: {'train_ae_loss': 0.6612, 'train_ucc_loss': 0.4281, 'train_ucc_acc': 0.875, 'loss': 0.54465}\n",
            "Step 163360: {'train_ae_loss': 0.66203, 'train_ucc_loss': 0.53593, 'train_ucc_acc': 0.78125, 'loss': 0.59898}\n",
            "Step 163380: {'train_ae_loss': 0.68466, 'train_ucc_loss': 0.43919, 'train_ucc_acc': 0.84375, 'loss': 0.56192}\n",
            "Step 163400: {'train_ae_loss': 0.6743, 'train_ucc_loss': 0.36718, 'train_ucc_acc': 0.96875, 'loss': 0.52074}\n",
            "Step 163420: {'train_ae_loss': 0.67126, 'train_ucc_loss': 0.39791, 'train_ucc_acc': 0.90625, 'loss': 0.53459}\n",
            "Step 163440: {'train_ae_loss': 0.63681, 'train_ucc_loss': 0.38066, 'train_ucc_acc': 0.9375, 'loss': 0.50874}\n",
            "Step 163460: {'train_ae_loss': 0.65591, 'train_ucc_loss': 0.45274, 'train_ucc_acc': 0.84375, 'loss': 0.55433}\n",
            "Step 163480: {'train_ae_loss': 0.64884, 'train_ucc_loss': 0.48147, 'train_ucc_acc': 0.78125, 'loss': 0.56516}\n",
            "Step 163500: {'train_ae_loss': 0.65106, 'train_ucc_loss': 0.56488, 'train_ucc_acc': 0.75, 'loss': 0.60797}\n",
            "Step 163520: {'train_ae_loss': 0.66156, 'train_ucc_loss': 0.41343, 'train_ucc_acc': 0.90625, 'loss': 0.5375}\n",
            "Step 163540: {'train_ae_loss': 0.63524, 'train_ucc_loss': 0.49788, 'train_ucc_acc': 0.78125, 'loss': 0.56656}\n",
            "Step 163560: {'train_ae_loss': 0.66237, 'train_ucc_loss': 0.41673, 'train_ucc_acc': 0.875, 'loss': 0.53955}\n",
            "Step 163580: {'train_ae_loss': 0.64854, 'train_ucc_loss': 0.39518, 'train_ucc_acc': 0.90625, 'loss': 0.52186}\n",
            "Step 163600: {'train_ae_loss': 0.66728, 'train_ucc_loss': 0.42816, 'train_ucc_acc': 0.875, 'loss': 0.54772}\n",
            "Step 163620: {'train_ae_loss': 0.66654, 'train_ucc_loss': 0.38134, 'train_ucc_acc': 0.9375, 'loss': 0.52394}\n",
            "Step 163640: {'train_ae_loss': 0.64599, 'train_ucc_loss': 0.43442, 'train_ucc_acc': 0.875, 'loss': 0.5402}\n",
            "Step 163660: {'train_ae_loss': 0.65609, 'train_ucc_loss': 0.44751, 'train_ucc_acc': 0.84375, 'loss': 0.5518}\n",
            "Step 163680: {'train_ae_loss': 0.68141, 'train_ucc_loss': 0.40694, 'train_ucc_acc': 0.875, 'loss': 0.54418}\n",
            "Step 163700: {'train_ae_loss': 0.65008, 'train_ucc_loss': 0.37995, 'train_ucc_acc': 0.90625, 'loss': 0.51502}\n",
            "Step 163720: {'train_ae_loss': 0.66598, 'train_ucc_loss': 0.32162, 'train_ucc_acc': 1.0, 'loss': 0.4938}\n",
            "Step 163740: {'train_ae_loss': 0.66277, 'train_ucc_loss': 0.45429, 'train_ucc_acc': 0.84375, 'loss': 0.55853}\n",
            "Step 163760: {'train_ae_loss': 0.65046, 'train_ucc_loss': 0.39359, 'train_ucc_acc': 0.9375, 'loss': 0.52203}\n",
            "Step 163780: {'train_ae_loss': 0.6662, 'train_ucc_loss': 0.40994, 'train_ucc_acc': 0.90625, 'loss': 0.53807}\n",
            "Step 163800: {'train_ae_loss': 0.63144, 'train_ucc_loss': 0.43931, 'train_ucc_acc': 0.875, 'loss': 0.53537}\n",
            "Step 163820: {'train_ae_loss': 0.65538, 'train_ucc_loss': 0.37801, 'train_ucc_acc': 0.9375, 'loss': 0.51669}\n",
            "Step 163840: {'train_ae_loss': 0.67288, 'train_ucc_loss': 0.59173, 'train_ucc_acc': 0.65625, 'loss': 0.6323}\n",
            "Step 163860: {'train_ae_loss': 0.6724, 'train_ucc_loss': 0.51345, 'train_ucc_acc': 0.78125, 'loss': 0.59292}\n",
            "Step 163880: {'train_ae_loss': 0.66684, 'train_ucc_loss': 0.47691, 'train_ucc_acc': 0.8125, 'loss': 0.57188}\n",
            "Step 163900: {'train_ae_loss': 0.65475, 'train_ucc_loss': 0.39137, 'train_ucc_acc': 0.9375, 'loss': 0.52306}\n",
            "Step 163920: {'train_ae_loss': 0.65451, 'train_ucc_loss': 0.40388, 'train_ucc_acc': 0.875, 'loss': 0.52919}\n",
            "Step 163940: {'train_ae_loss': 0.66137, 'train_ucc_loss': 0.41089, 'train_ucc_acc': 0.90625, 'loss': 0.53613}\n",
            "Step 163960: {'train_ae_loss': 0.65809, 'train_ucc_loss': 0.40801, 'train_ucc_acc': 0.90625, 'loss': 0.53305}\n",
            "Step 163980: {'train_ae_loss': 0.65913, 'train_ucc_loss': 0.44993, 'train_ucc_acc': 0.8125, 'loss': 0.55453}\n",
            "Step 164000: {'train_ae_loss': 0.65923, 'train_ucc_loss': 0.42536, 'train_ucc_acc': 0.875, 'loss': 0.54229}\n",
            "step: 164000,eval_ae_loss: 0.65101,eval_ucc_loss: 0.48591,eval_ucc_acc: 0.81738\n",
            "Step 164020: {'train_ae_loss': 0.66835, 'train_ucc_loss': 0.54624, 'train_ucc_acc': 0.78125, 'loss': 0.60729}\n",
            "Step 164040: {'train_ae_loss': 0.65554, 'train_ucc_loss': 0.45519, 'train_ucc_acc': 0.8125, 'loss': 0.55537}\n",
            "Step 164060: {'train_ae_loss': 0.66089, 'train_ucc_loss': 0.46947, 'train_ucc_acc': 0.84375, 'loss': 0.56518}\n",
            "Step 164080: {'train_ae_loss': 0.65266, 'train_ucc_loss': 0.4395, 'train_ucc_acc': 0.875, 'loss': 0.54608}\n",
            "Step 164100: {'train_ae_loss': 0.64263, 'train_ucc_loss': 0.35025, 'train_ucc_acc': 0.96875, 'loss': 0.49644}\n",
            "Step 164120: {'train_ae_loss': 0.65964, 'train_ucc_loss': 0.41479, 'train_ucc_acc': 0.875, 'loss': 0.53721}\n",
            "Step 164140: {'train_ae_loss': 0.64539, 'train_ucc_loss': 0.4265, 'train_ucc_acc': 0.875, 'loss': 0.53595}\n",
            "Step 164160: {'train_ae_loss': 0.67723, 'train_ucc_loss': 0.41638, 'train_ucc_acc': 0.90625, 'loss': 0.54681}\n",
            "Step 164180: {'train_ae_loss': 0.67528, 'train_ucc_loss': 0.41031, 'train_ucc_acc': 0.90625, 'loss': 0.54279}\n",
            "Step 164200: {'train_ae_loss': 0.66252, 'train_ucc_loss': 0.43877, 'train_ucc_acc': 0.875, 'loss': 0.55065}\n",
            "Step 164220: {'train_ae_loss': 0.67347, 'train_ucc_loss': 0.4228, 'train_ucc_acc': 0.90625, 'loss': 0.54814}\n",
            "Step 164240: {'train_ae_loss': 0.65756, 'train_ucc_loss': 0.48368, 'train_ucc_acc': 0.84375, 'loss': 0.57062}\n",
            "Step 164260: {'train_ae_loss': 0.66096, 'train_ucc_loss': 0.40824, 'train_ucc_acc': 0.875, 'loss': 0.5346}\n",
            "Step 164280: {'train_ae_loss': 0.66987, 'train_ucc_loss': 0.36295, 'train_ucc_acc': 0.96875, 'loss': 0.51641}\n",
            "Step 164300: {'train_ae_loss': 0.67917, 'train_ucc_loss': 0.43118, 'train_ucc_acc': 0.875, 'loss': 0.55518}\n",
            "Step 164320: {'train_ae_loss': 0.67203, 'train_ucc_loss': 0.40952, 'train_ucc_acc': 0.90625, 'loss': 0.54078}\n",
            "Step 164340: {'train_ae_loss': 0.67428, 'train_ucc_loss': 0.51605, 'train_ucc_acc': 0.75, 'loss': 0.59516}\n",
            "Step 164360: {'train_ae_loss': 0.67067, 'train_ucc_loss': 0.42031, 'train_ucc_acc': 0.875, 'loss': 0.54549}\n",
            "Step 164380: {'train_ae_loss': 0.66972, 'train_ucc_loss': 0.44559, 'train_ucc_acc': 0.84375, 'loss': 0.55765}\n",
            "Step 164400: {'train_ae_loss': 0.66687, 'train_ucc_loss': 0.44072, 'train_ucc_acc': 0.875, 'loss': 0.5538}\n",
            "Step 164420: {'train_ae_loss': 0.67984, 'train_ucc_loss': 0.43299, 'train_ucc_acc': 0.875, 'loss': 0.55641}\n",
            "Step 164440: {'train_ae_loss': 0.66753, 'train_ucc_loss': 0.36321, 'train_ucc_acc': 0.96875, 'loss': 0.51537}\n",
            "Step 164460: {'train_ae_loss': 0.66069, 'train_ucc_loss': 0.58149, 'train_ucc_acc': 0.71875, 'loss': 0.62109}\n",
            "Step 164480: {'train_ae_loss': 0.64673, 'train_ucc_loss': 0.34497, 'train_ucc_acc': 0.9375, 'loss': 0.49585}\n",
            "Step 164500: {'train_ae_loss': 0.655, 'train_ucc_loss': 0.34545, 'train_ucc_acc': 0.96875, 'loss': 0.50023}\n",
            "Step 164520: {'train_ae_loss': 0.65643, 'train_ucc_loss': 0.44961, 'train_ucc_acc': 0.875, 'loss': 0.55302}\n",
            "Step 164540: {'train_ae_loss': 0.66349, 'train_ucc_loss': 0.47277, 'train_ucc_acc': 0.84375, 'loss': 0.56813}\n",
            "Step 164560: {'train_ae_loss': 0.64434, 'train_ucc_loss': 0.40426, 'train_ucc_acc': 0.90625, 'loss': 0.5243}\n",
            "Step 164580: {'train_ae_loss': 0.65252, 'train_ucc_loss': 0.45309, 'train_ucc_acc': 0.84375, 'loss': 0.55281}\n",
            "Step 164600: {'train_ae_loss': 0.66444, 'train_ucc_loss': 0.39333, 'train_ucc_acc': 0.9375, 'loss': 0.52889}\n",
            "Step 164620: {'train_ae_loss': 0.66475, 'train_ucc_loss': 0.38264, 'train_ucc_acc': 0.9375, 'loss': 0.5237}\n",
            "Step 164640: {'train_ae_loss': 0.6563, 'train_ucc_loss': 0.48279, 'train_ucc_acc': 0.84375, 'loss': 0.56955}\n",
            "Step 164660: {'train_ae_loss': 0.66107, 'train_ucc_loss': 0.44907, 'train_ucc_acc': 0.84375, 'loss': 0.55507}\n",
            "Step 164680: {'train_ae_loss': 0.65711, 'train_ucc_loss': 0.45449, 'train_ucc_acc': 0.84375, 'loss': 0.5558}\n",
            "Step 164700: {'train_ae_loss': 0.66532, 'train_ucc_loss': 0.40634, 'train_ucc_acc': 0.875, 'loss': 0.53583}\n",
            "Step 164720: {'train_ae_loss': 0.6513, 'train_ucc_loss': 0.32983, 'train_ucc_acc': 1.0, 'loss': 0.49057}\n",
            "Step 164740: {'train_ae_loss': 0.66925, 'train_ucc_loss': 0.50101, 'train_ucc_acc': 0.8125, 'loss': 0.58513}\n",
            "Step 164760: {'train_ae_loss': 0.65358, 'train_ucc_loss': 0.41636, 'train_ucc_acc': 0.875, 'loss': 0.53497}\n",
            "Step 164780: {'train_ae_loss': 0.66653, 'train_ucc_loss': 0.38592, 'train_ucc_acc': 0.9375, 'loss': 0.52623}\n",
            "Step 164800: {'train_ae_loss': 0.66002, 'train_ucc_loss': 0.34589, 'train_ucc_acc': 0.9375, 'loss': 0.50295}\n",
            "Step 164820: {'train_ae_loss': 0.65778, 'train_ucc_loss': 0.48418, 'train_ucc_acc': 0.84375, 'loss': 0.57098}\n",
            "Step 164840: {'train_ae_loss': 0.65309, 'train_ucc_loss': 0.49896, 'train_ucc_acc': 0.8125, 'loss': 0.57603}\n",
            "Step 164860: {'train_ae_loss': 0.65928, 'train_ucc_loss': 0.40538, 'train_ucc_acc': 0.90625, 'loss': 0.53233}\n",
            "Step 164880: {'train_ae_loss': 0.6648, 'train_ucc_loss': 0.4442, 'train_ucc_acc': 0.875, 'loss': 0.5545}\n",
            "Step 164900: {'train_ae_loss': 0.65325, 'train_ucc_loss': 0.44482, 'train_ucc_acc': 0.875, 'loss': 0.54903}\n",
            "Step 164920: {'train_ae_loss': 0.65186, 'train_ucc_loss': 0.49304, 'train_ucc_acc': 0.8125, 'loss': 0.57245}\n",
            "Step 164940: {'train_ae_loss': 0.65233, 'train_ucc_loss': 0.49287, 'train_ucc_acc': 0.78125, 'loss': 0.5726}\n",
            "Step 164960: {'train_ae_loss': 0.65516, 'train_ucc_loss': 0.44425, 'train_ucc_acc': 0.875, 'loss': 0.5497}\n",
            "Step 164980: {'train_ae_loss': 0.67045, 'train_ucc_loss': 0.43813, 'train_ucc_acc': 0.875, 'loss': 0.55429}\n",
            "Step 165000: {'train_ae_loss': 0.65978, 'train_ucc_loss': 0.45625, 'train_ucc_acc': 0.875, 'loss': 0.55802}\n",
            "step: 165000,eval_ae_loss: 0.64993,eval_ucc_loss: 0.50329,eval_ucc_acc: 0.80078\n",
            "Step 165020: {'train_ae_loss': 0.64921, 'train_ucc_loss': 0.49494, 'train_ucc_acc': 0.8125, 'loss': 0.57207}\n",
            "Step 165040: {'train_ae_loss': 0.65449, 'train_ucc_loss': 0.54636, 'train_ucc_acc': 0.78125, 'loss': 0.60043}\n",
            "Step 165060: {'train_ae_loss': 0.66307, 'train_ucc_loss': 0.46592, 'train_ucc_acc': 0.84375, 'loss': 0.5645}\n",
            "Step 165080: {'train_ae_loss': 0.68031, 'train_ucc_loss': 0.38896, 'train_ucc_acc': 0.9375, 'loss': 0.53463}\n",
            "Step 165100: {'train_ae_loss': 0.66942, 'train_ucc_loss': 0.4325, 'train_ucc_acc': 0.875, 'loss': 0.55096}\n",
            "Step 165120: {'train_ae_loss': 0.66695, 'train_ucc_loss': 0.45931, 'train_ucc_acc': 0.84375, 'loss': 0.56313}\n",
            "Step 165140: {'train_ae_loss': 0.66254, 'train_ucc_loss': 0.41343, 'train_ucc_acc': 0.90625, 'loss': 0.53799}\n",
            "Step 165160: {'train_ae_loss': 0.66058, 'train_ucc_loss': 0.44904, 'train_ucc_acc': 0.875, 'loss': 0.55481}\n",
            "Step 165180: {'train_ae_loss': 0.66619, 'train_ucc_loss': 0.41613, 'train_ucc_acc': 0.90625, 'loss': 0.54116}\n",
            "Step 165200: {'train_ae_loss': 0.67674, 'train_ucc_loss': 0.39855, 'train_ucc_acc': 0.90625, 'loss': 0.53765}\n",
            "Step 165220: {'train_ae_loss': 0.6563, 'train_ucc_loss': 0.48646, 'train_ucc_acc': 0.84375, 'loss': 0.57138}\n",
            "Step 165240: {'train_ae_loss': 0.63991, 'train_ucc_loss': 0.49894, 'train_ucc_acc': 0.84375, 'loss': 0.56943}\n",
            "Step 165260: {'train_ae_loss': 0.65762, 'train_ucc_loss': 0.38772, 'train_ucc_acc': 0.9375, 'loss': 0.52267}\n",
            "Step 165280: {'train_ae_loss': 0.66591, 'train_ucc_loss': 0.45104, 'train_ucc_acc': 0.84375, 'loss': 0.55848}\n",
            "Step 165300: {'train_ae_loss': 0.65251, 'train_ucc_loss': 0.41105, 'train_ucc_acc': 0.90625, 'loss': 0.53178}\n",
            "Step 165320: {'train_ae_loss': 0.65481, 'train_ucc_loss': 0.47035, 'train_ucc_acc': 0.84375, 'loss': 0.56258}\n",
            "Step 165340: {'train_ae_loss': 0.649, 'train_ucc_loss': 0.34787, 'train_ucc_acc': 0.96875, 'loss': 0.49843}\n",
            "Step 165360: {'train_ae_loss': 0.66009, 'train_ucc_loss': 0.46935, 'train_ucc_acc': 0.84375, 'loss': 0.56472}\n",
            "Step 165380: {'train_ae_loss': 0.64725, 'train_ucc_loss': 0.46568, 'train_ucc_acc': 0.8125, 'loss': 0.55647}\n",
            "Step 165400: {'train_ae_loss': 0.65656, 'train_ucc_loss': 0.47979, 'train_ucc_acc': 0.8125, 'loss': 0.56817}\n",
            "Step 165420: {'train_ae_loss': 0.65726, 'train_ucc_loss': 0.5136, 'train_ucc_acc': 0.78125, 'loss': 0.58543}\n",
            "Step 165440: {'train_ae_loss': 0.65669, 'train_ucc_loss': 0.45796, 'train_ucc_acc': 0.84375, 'loss': 0.55733}\n",
            "Step 165460: {'train_ae_loss': 0.6441, 'train_ucc_loss': 0.459, 'train_ucc_acc': 0.84375, 'loss': 0.55155}\n",
            "Step 165480: {'train_ae_loss': 0.65177, 'train_ucc_loss': 0.44274, 'train_ucc_acc': 0.84375, 'loss': 0.54726}\n",
            "Step 165500: {'train_ae_loss': 0.6465, 'train_ucc_loss': 0.4684, 'train_ucc_acc': 0.875, 'loss': 0.55745}\n",
            "Step 165520: {'train_ae_loss': 0.64982, 'train_ucc_loss': 0.35472, 'train_ucc_acc': 0.96875, 'loss': 0.50227}\n",
            "Step 165540: {'train_ae_loss': 0.66689, 'train_ucc_loss': 0.35783, 'train_ucc_acc': 0.9375, 'loss': 0.51236}\n",
            "Step 165560: {'train_ae_loss': 0.65555, 'train_ucc_loss': 0.40961, 'train_ucc_acc': 0.90625, 'loss': 0.53258}\n",
            "Step 165580: {'train_ae_loss': 0.67558, 'train_ucc_loss': 0.34796, 'train_ucc_acc': 0.96875, 'loss': 0.51177}\n",
            "Step 165600: {'train_ae_loss': 0.65368, 'train_ucc_loss': 0.35212, 'train_ucc_acc': 0.96875, 'loss': 0.5029}\n",
            "Step 165620: {'train_ae_loss': 0.65818, 'train_ucc_loss': 0.38874, 'train_ucc_acc': 0.90625, 'loss': 0.52346}\n",
            "Step 165640: {'train_ae_loss': 0.6619, 'train_ucc_loss': 0.43975, 'train_ucc_acc': 0.875, 'loss': 0.55083}\n",
            "Step 165660: {'train_ae_loss': 0.64304, 'train_ucc_loss': 0.4461, 'train_ucc_acc': 0.84375, 'loss': 0.54457}\n",
            "Step 165680: {'train_ae_loss': 0.67386, 'train_ucc_loss': 0.44308, 'train_ucc_acc': 0.84375, 'loss': 0.55847}\n",
            "Step 165700: {'train_ae_loss': 0.66599, 'train_ucc_loss': 0.41916, 'train_ucc_acc': 0.875, 'loss': 0.54257}\n",
            "Step 165720: {'train_ae_loss': 0.67082, 'train_ucc_loss': 0.35756, 'train_ucc_acc': 0.96875, 'loss': 0.51419}\n",
            "Step 165740: {'train_ae_loss': 0.64626, 'train_ucc_loss': 0.46245, 'train_ucc_acc': 0.84375, 'loss': 0.55435}\n",
            "Step 165760: {'train_ae_loss': 0.65872, 'train_ucc_loss': 0.42364, 'train_ucc_acc': 0.875, 'loss': 0.54118}\n",
            "Step 165780: {'train_ae_loss': 0.63909, 'train_ucc_loss': 0.43548, 'train_ucc_acc': 0.875, 'loss': 0.53728}\n",
            "Step 165800: {'train_ae_loss': 0.65732, 'train_ucc_loss': 0.46093, 'train_ucc_acc': 0.875, 'loss': 0.55913}\n",
            "Step 165820: {'train_ae_loss': 0.67415, 'train_ucc_loss': 0.42412, 'train_ucc_acc': 0.90625, 'loss': 0.54914}\n",
            "Step 165840: {'train_ae_loss': 0.6552, 'train_ucc_loss': 0.41458, 'train_ucc_acc': 0.875, 'loss': 0.53489}\n",
            "Step 165860: {'train_ae_loss': 0.66749, 'train_ucc_loss': 0.43378, 'train_ucc_acc': 0.875, 'loss': 0.55063}\n",
            "Step 165880: {'train_ae_loss': 0.65847, 'train_ucc_loss': 0.39444, 'train_ucc_acc': 0.9375, 'loss': 0.52645}\n",
            "Step 165900: {'train_ae_loss': 0.66768, 'train_ucc_loss': 0.47171, 'train_ucc_acc': 0.8125, 'loss': 0.5697}\n",
            "Step 165920: {'train_ae_loss': 0.66426, 'train_ucc_loss': 0.38704, 'train_ucc_acc': 0.9375, 'loss': 0.52565}\n",
            "Step 165940: {'train_ae_loss': 0.65749, 'train_ucc_loss': 0.44904, 'train_ucc_acc': 0.84375, 'loss': 0.55327}\n",
            "Step 165960: {'train_ae_loss': 0.67808, 'train_ucc_loss': 0.51307, 'train_ucc_acc': 0.78125, 'loss': 0.59557}\n",
            "Step 165980: {'train_ae_loss': 0.68241, 'train_ucc_loss': 0.3226, 'train_ucc_acc': 1.0, 'loss': 0.5025}\n",
            "Step 166000: {'train_ae_loss': 0.64745, 'train_ucc_loss': 0.57486, 'train_ucc_acc': 0.75, 'loss': 0.61116}\n",
            "step: 166000,eval_ae_loss: 0.65188,eval_ucc_loss: 0.4651,eval_ucc_acc: 0.83984\n",
            "Step 166020: {'train_ae_loss': 0.64578, 'train_ucc_loss': 0.42383, 'train_ucc_acc': 0.90625, 'loss': 0.53481}\n",
            "Step 166040: {'train_ae_loss': 0.64735, 'train_ucc_loss': 0.46152, 'train_ucc_acc': 0.84375, 'loss': 0.55444}\n",
            "Step 166060: {'train_ae_loss': 0.66337, 'train_ucc_loss': 0.35194, 'train_ucc_acc': 0.96875, 'loss': 0.50765}\n",
            "Step 166080: {'train_ae_loss': 0.66856, 'train_ucc_loss': 0.39338, 'train_ucc_acc': 0.9375, 'loss': 0.53097}\n",
            "Step 166100: {'train_ae_loss': 0.64176, 'train_ucc_loss': 0.52041, 'train_ucc_acc': 0.78125, 'loss': 0.58108}\n",
            "Step 166120: {'train_ae_loss': 0.6442, 'train_ucc_loss': 0.46218, 'train_ucc_acc': 0.84375, 'loss': 0.55319}\n",
            "Step 166140: {'train_ae_loss': 0.65676, 'train_ucc_loss': 0.55899, 'train_ucc_acc': 0.75, 'loss': 0.60788}\n",
            "Step 166160: {'train_ae_loss': 0.65621, 'train_ucc_loss': 0.4159, 'train_ucc_acc': 0.90625, 'loss': 0.53606}\n",
            "Step 166180: {'train_ae_loss': 0.65003, 'train_ucc_loss': 0.37429, 'train_ucc_acc': 0.9375, 'loss': 0.51216}\n",
            "Step 166200: {'train_ae_loss': 0.65475, 'train_ucc_loss': 0.45563, 'train_ucc_acc': 0.84375, 'loss': 0.55519}\n",
            "Step 166220: {'train_ae_loss': 0.668, 'train_ucc_loss': 0.35452, 'train_ucc_acc': 0.96875, 'loss': 0.51126}\n",
            "Step 166240: {'train_ae_loss': 0.66539, 'train_ucc_loss': 0.38438, 'train_ucc_acc': 0.9375, 'loss': 0.52488}\n",
            "Step 166260: {'train_ae_loss': 0.67721, 'train_ucc_loss': 0.45703, 'train_ucc_acc': 0.84375, 'loss': 0.56712}\n",
            "Step 166280: {'train_ae_loss': 0.64163, 'train_ucc_loss': 0.47176, 'train_ucc_acc': 0.84375, 'loss': 0.5567}\n",
            "Step 166300: {'train_ae_loss': 0.65512, 'train_ucc_loss': 0.56046, 'train_ucc_acc': 0.71875, 'loss': 0.60779}\n",
            "Step 166320: {'train_ae_loss': 0.66785, 'train_ucc_loss': 0.44092, 'train_ucc_acc': 0.90625, 'loss': 0.55438}\n",
            "Step 166340: {'train_ae_loss': 0.65782, 'train_ucc_loss': 0.52156, 'train_ucc_acc': 0.78125, 'loss': 0.58969}\n",
            "Step 166360: {'train_ae_loss': 0.66342, 'train_ucc_loss': 0.44835, 'train_ucc_acc': 0.875, 'loss': 0.55589}\n",
            "Step 166380: {'train_ae_loss': 0.66041, 'train_ucc_loss': 0.46026, 'train_ucc_acc': 0.84375, 'loss': 0.56034}\n",
            "Step 166400: {'train_ae_loss': 0.65284, 'train_ucc_loss': 0.47459, 'train_ucc_acc': 0.84375, 'loss': 0.56372}\n",
            "Step 166420: {'train_ae_loss': 0.65566, 'train_ucc_loss': 0.46692, 'train_ucc_acc': 0.8125, 'loss': 0.56129}\n",
            "Step 166440: {'train_ae_loss': 0.6673, 'train_ucc_loss': 0.42655, 'train_ucc_acc': 0.90625, 'loss': 0.54693}\n",
            "Step 166460: {'train_ae_loss': 0.65693, 'train_ucc_loss': 0.43963, 'train_ucc_acc': 0.875, 'loss': 0.54828}\n",
            "Step 166480: {'train_ae_loss': 0.6764, 'train_ucc_loss': 0.36284, 'train_ucc_acc': 0.9375, 'loss': 0.51962}\n",
            "Step 166500: {'train_ae_loss': 0.66353, 'train_ucc_loss': 0.36787, 'train_ucc_acc': 0.9375, 'loss': 0.5157}\n",
            "Step 166520: {'train_ae_loss': 0.65685, 'train_ucc_loss': 0.4199, 'train_ucc_acc': 0.90625, 'loss': 0.53838}\n",
            "Step 166540: {'train_ae_loss': 0.65405, 'train_ucc_loss': 0.43948, 'train_ucc_acc': 0.875, 'loss': 0.54676}\n",
            "Step 166560: {'train_ae_loss': 0.66398, 'train_ucc_loss': 0.4208, 'train_ucc_acc': 0.875, 'loss': 0.54239}\n",
            "Step 166580: {'train_ae_loss': 0.66306, 'train_ucc_loss': 0.38402, 'train_ucc_acc': 0.9375, 'loss': 0.52354}\n",
            "Step 166600: {'train_ae_loss': 0.66707, 'train_ucc_loss': 0.37213, 'train_ucc_acc': 0.9375, 'loss': 0.5196}\n",
            "Step 166620: {'train_ae_loss': 0.66118, 'train_ucc_loss': 0.41817, 'train_ucc_acc': 0.90625, 'loss': 0.53967}\n",
            "Step 166640: {'train_ae_loss': 0.66767, 'train_ucc_loss': 0.43558, 'train_ucc_acc': 0.875, 'loss': 0.55163}\n",
            "Step 166660: {'train_ae_loss': 0.65566, 'train_ucc_loss': 0.44823, 'train_ucc_acc': 0.875, 'loss': 0.55194}\n",
            "Step 166680: {'train_ae_loss': 0.64968, 'train_ucc_loss': 0.55892, 'train_ucc_acc': 0.75, 'loss': 0.6043}\n",
            "Step 166700: {'train_ae_loss': 0.6484, 'train_ucc_loss': 0.45564, 'train_ucc_acc': 0.84375, 'loss': 0.55202}\n",
            "Step 166720: {'train_ae_loss': 0.65721, 'train_ucc_loss': 0.48136, 'train_ucc_acc': 0.8125, 'loss': 0.56928}\n",
            "Step 166740: {'train_ae_loss': 0.65337, 'train_ucc_loss': 0.46943, 'train_ucc_acc': 0.84375, 'loss': 0.5614}\n",
            "Step 166760: {'train_ae_loss': 0.65609, 'train_ucc_loss': 0.4357, 'train_ucc_acc': 0.90625, 'loss': 0.5459}\n",
            "Step 166780: {'train_ae_loss': 0.65756, 'train_ucc_loss': 0.47724, 'train_ucc_acc': 0.84375, 'loss': 0.5674}\n",
            "Step 166800: {'train_ae_loss': 0.65581, 'train_ucc_loss': 0.36225, 'train_ucc_acc': 0.9375, 'loss': 0.50903}\n",
            "Step 166820: {'train_ae_loss': 0.65008, 'train_ucc_loss': 0.38407, 'train_ucc_acc': 0.90625, 'loss': 0.51708}\n",
            "Step 166840: {'train_ae_loss': 0.65826, 'train_ucc_loss': 0.57254, 'train_ucc_acc': 0.6875, 'loss': 0.6154}\n",
            "Step 166860: {'train_ae_loss': 0.66555, 'train_ucc_loss': 0.40801, 'train_ucc_acc': 0.90625, 'loss': 0.53678}\n",
            "Step 166880: {'train_ae_loss': 0.65827, 'train_ucc_loss': 0.39296, 'train_ucc_acc': 0.9375, 'loss': 0.52562}\n",
            "Step 166900: {'train_ae_loss': 0.6505, 'train_ucc_loss': 0.38023, 'train_ucc_acc': 0.90625, 'loss': 0.51537}\n",
            "Step 166920: {'train_ae_loss': 0.65281, 'train_ucc_loss': 0.43437, 'train_ucc_acc': 0.84375, 'loss': 0.54359}\n",
            "Step 166940: {'train_ae_loss': 0.65477, 'train_ucc_loss': 0.44984, 'train_ucc_acc': 0.875, 'loss': 0.55231}\n",
            "Step 166960: {'train_ae_loss': 0.65118, 'train_ucc_loss': 0.42999, 'train_ucc_acc': 0.90625, 'loss': 0.54058}\n",
            "Step 166980: {'train_ae_loss': 0.65474, 'train_ucc_loss': 0.42886, 'train_ucc_acc': 0.90625, 'loss': 0.5418}\n",
            "Step 167000: {'train_ae_loss': 0.65356, 'train_ucc_loss': 0.42313, 'train_ucc_acc': 0.90625, 'loss': 0.53835}\n",
            "step: 167000,eval_ae_loss: 0.6492,eval_ucc_loss: 0.44709,eval_ucc_acc: 0.86426\n",
            "Step 167020: {'train_ae_loss': 0.66288, 'train_ucc_loss': 0.412, 'train_ucc_acc': 0.90625, 'loss': 0.53744}\n",
            "Step 167040: {'train_ae_loss': 0.6582, 'train_ucc_loss': 0.48549, 'train_ucc_acc': 0.8125, 'loss': 0.57184}\n",
            "Step 167060: {'train_ae_loss': 0.66671, 'train_ucc_loss': 0.40581, 'train_ucc_acc': 0.90625, 'loss': 0.53626}\n",
            "Step 167080: {'train_ae_loss': 0.65307, 'train_ucc_loss': 0.52994, 'train_ucc_acc': 0.75, 'loss': 0.59151}\n",
            "Step 167100: {'train_ae_loss': 0.66981, 'train_ucc_loss': 0.45035, 'train_ucc_acc': 0.875, 'loss': 0.56008}\n",
            "Step 167120: {'train_ae_loss': 0.64823, 'train_ucc_loss': 0.46078, 'train_ucc_acc': 0.8125, 'loss': 0.5545}\n",
            "Step 167140: {'train_ae_loss': 0.66555, 'train_ucc_loss': 0.51768, 'train_ucc_acc': 0.78125, 'loss': 0.59162}\n",
            "Step 167160: {'train_ae_loss': 0.65711, 'train_ucc_loss': 0.45574, 'train_ucc_acc': 0.84375, 'loss': 0.55642}\n",
            "Step 167180: {'train_ae_loss': 0.65066, 'train_ucc_loss': 0.36451, 'train_ucc_acc': 0.96875, 'loss': 0.50758}\n",
            "Step 167200: {'train_ae_loss': 0.69867, 'train_ucc_loss': 0.35436, 'train_ucc_acc': 0.96875, 'loss': 0.52652}\n",
            "Step 167220: {'train_ae_loss': 0.66652, 'train_ucc_loss': 0.47538, 'train_ucc_acc': 0.84375, 'loss': 0.57095}\n",
            "Step 167240: {'train_ae_loss': 0.64324, 'train_ucc_loss': 0.4102, 'train_ucc_acc': 0.875, 'loss': 0.52672}\n",
            "Step 167260: {'train_ae_loss': 0.6548, 'train_ucc_loss': 0.42971, 'train_ucc_acc': 0.875, 'loss': 0.54226}\n",
            "Step 167280: {'train_ae_loss': 0.66537, 'train_ucc_loss': 0.46083, 'train_ucc_acc': 0.84375, 'loss': 0.5631}\n",
            "Step 167300: {'train_ae_loss': 0.66204, 'train_ucc_loss': 0.3241, 'train_ucc_acc': 1.0, 'loss': 0.49307}\n",
            "Step 167320: {'train_ae_loss': 0.65586, 'train_ucc_loss': 0.4219, 'train_ucc_acc': 0.875, 'loss': 0.53888}\n",
            "Step 167340: {'train_ae_loss': 0.65261, 'train_ucc_loss': 0.41763, 'train_ucc_acc': 0.90625, 'loss': 0.53512}\n",
            "Step 167360: {'train_ae_loss': 0.66128, 'train_ucc_loss': 0.38542, 'train_ucc_acc': 0.9375, 'loss': 0.52335}\n",
            "Step 167380: {'train_ae_loss': 0.66744, 'train_ucc_loss': 0.35179, 'train_ucc_acc': 0.96875, 'loss': 0.50962}\n",
            "Step 167400: {'train_ae_loss': 0.65596, 'train_ucc_loss': 0.47319, 'train_ucc_acc': 0.8125, 'loss': 0.56457}\n",
            "Step 167420: {'train_ae_loss': 0.66856, 'train_ucc_loss': 0.34949, 'train_ucc_acc': 0.96875, 'loss': 0.50902}\n",
            "Step 167440: {'train_ae_loss': 0.66391, 'train_ucc_loss': 0.37313, 'train_ucc_acc': 0.9375, 'loss': 0.51852}\n",
            "Step 167460: {'train_ae_loss': 0.65782, 'train_ucc_loss': 0.58221, 'train_ucc_acc': 0.71875, 'loss': 0.62002}\n",
            "Step 167480: {'train_ae_loss': 0.66294, 'train_ucc_loss': 0.43686, 'train_ucc_acc': 0.875, 'loss': 0.5499}\n",
            "Step 167500: {'train_ae_loss': 0.66547, 'train_ucc_loss': 0.44611, 'train_ucc_acc': 0.875, 'loss': 0.55579}\n",
            "Step 167520: {'train_ae_loss': 0.69058, 'train_ucc_loss': 0.37516, 'train_ucc_acc': 0.9375, 'loss': 0.53287}\n",
            "Step 167540: {'train_ae_loss': 0.66372, 'train_ucc_loss': 0.3475, 'train_ucc_acc': 0.96875, 'loss': 0.50561}\n",
            "Step 167560: {'train_ae_loss': 0.65557, 'train_ucc_loss': 0.40991, 'train_ucc_acc': 0.90625, 'loss': 0.53274}\n",
            "Step 167580: {'train_ae_loss': 0.66815, 'train_ucc_loss': 0.54732, 'train_ucc_acc': 0.75, 'loss': 0.60773}\n",
            "Step 167600: {'train_ae_loss': 0.67332, 'train_ucc_loss': 0.43041, 'train_ucc_acc': 0.875, 'loss': 0.55186}\n",
            "Step 167620: {'train_ae_loss': 0.64221, 'train_ucc_loss': 0.45881, 'train_ucc_acc': 0.84375, 'loss': 0.55051}\n",
            "Step 167640: {'train_ae_loss': 0.67254, 'train_ucc_loss': 0.50335, 'train_ucc_acc': 0.78125, 'loss': 0.58795}\n",
            "Step 167660: {'train_ae_loss': 0.64487, 'train_ucc_loss': 0.43617, 'train_ucc_acc': 0.875, 'loss': 0.54052}\n",
            "Step 167680: {'train_ae_loss': 0.63799, 'train_ucc_loss': 0.4315, 'train_ucc_acc': 0.84375, 'loss': 0.53475}\n",
            "Step 167700: {'train_ae_loss': 0.64745, 'train_ucc_loss': 0.4534, 'train_ucc_acc': 0.875, 'loss': 0.55042}\n",
            "Step 167720: {'train_ae_loss': 0.67285, 'train_ucc_loss': 0.41997, 'train_ucc_acc': 0.875, 'loss': 0.54641}\n",
            "Step 167740: {'train_ae_loss': 0.67844, 'train_ucc_loss': 0.43993, 'train_ucc_acc': 0.90625, 'loss': 0.55919}\n",
            "Step 167760: {'train_ae_loss': 0.65667, 'train_ucc_loss': 0.43313, 'train_ucc_acc': 0.875, 'loss': 0.5449}\n",
            "Step 167780: {'train_ae_loss': 0.66572, 'train_ucc_loss': 0.35218, 'train_ucc_acc': 0.96875, 'loss': 0.50895}\n",
            "Step 167800: {'train_ae_loss': 0.64581, 'train_ucc_loss': 0.43147, 'train_ucc_acc': 0.90625, 'loss': 0.53864}\n",
            "Step 167820: {'train_ae_loss': 0.66012, 'train_ucc_loss': 0.33834, 'train_ucc_acc': 1.0, 'loss': 0.49923}\n",
            "Step 167840: {'train_ae_loss': 0.66019, 'train_ucc_loss': 0.37944, 'train_ucc_acc': 0.9375, 'loss': 0.51982}\n",
            "Step 167860: {'train_ae_loss': 0.6463, 'train_ucc_loss': 0.33331, 'train_ucc_acc': 0.96875, 'loss': 0.4898}\n",
            "Step 167880: {'train_ae_loss': 0.65262, 'train_ucc_loss': 0.48211, 'train_ucc_acc': 0.8125, 'loss': 0.56736}\n",
            "Step 167900: {'train_ae_loss': 0.66691, 'train_ucc_loss': 0.39538, 'train_ucc_acc': 0.9375, 'loss': 0.53114}\n",
            "Step 167920: {'train_ae_loss': 0.6587, 'train_ucc_loss': 0.50703, 'train_ucc_acc': 0.8125, 'loss': 0.58287}\n",
            "Step 167940: {'train_ae_loss': 0.6629, 'train_ucc_loss': 0.49911, 'train_ucc_acc': 0.8125, 'loss': 0.58101}\n",
            "Step 167960: {'train_ae_loss': 0.64715, 'train_ucc_loss': 0.35857, 'train_ucc_acc': 0.96875, 'loss': 0.50286}\n",
            "Step 167980: {'train_ae_loss': 0.65936, 'train_ucc_loss': 0.4594, 'train_ucc_acc': 0.84375, 'loss': 0.55938}\n",
            "Step 168000: {'train_ae_loss': 0.64965, 'train_ucc_loss': 0.53897, 'train_ucc_acc': 0.75, 'loss': 0.59431}\n",
            "step: 168000,eval_ae_loss: 0.64716,eval_ucc_loss: 0.46596,eval_ucc_acc: 0.83789\n",
            "Step 168020: {'train_ae_loss': 0.66152, 'train_ucc_loss': 0.40252, 'train_ucc_acc': 0.90625, 'loss': 0.53202}\n",
            "Step 168040: {'train_ae_loss': 0.64382, 'train_ucc_loss': 0.46775, 'train_ucc_acc': 0.84375, 'loss': 0.55578}\n",
            "Step 168060: {'train_ae_loss': 0.6618, 'train_ucc_loss': 0.43106, 'train_ucc_acc': 0.84375, 'loss': 0.54643}\n",
            "Step 168080: {'train_ae_loss': 0.64582, 'train_ucc_loss': 0.48473, 'train_ucc_acc': 0.8125, 'loss': 0.56528}\n",
            "Step 168100: {'train_ae_loss': 0.65668, 'train_ucc_loss': 0.4344, 'train_ucc_acc': 0.90625, 'loss': 0.54554}\n",
            "Step 168120: {'train_ae_loss': 0.66426, 'train_ucc_loss': 0.39749, 'train_ucc_acc': 0.90625, 'loss': 0.53088}\n",
            "Step 168140: {'train_ae_loss': 0.67105, 'train_ucc_loss': 0.4998, 'train_ucc_acc': 0.8125, 'loss': 0.58542}\n",
            "Step 168160: {'train_ae_loss': 0.67579, 'train_ucc_loss': 0.45884, 'train_ucc_acc': 0.84375, 'loss': 0.56731}\n",
            "Step 168180: {'train_ae_loss': 0.65833, 'train_ucc_loss': 0.45233, 'train_ucc_acc': 0.8125, 'loss': 0.55533}\n",
            "Step 168200: {'train_ae_loss': 0.64954, 'train_ucc_loss': 0.40935, 'train_ucc_acc': 0.90625, 'loss': 0.52945}\n",
            "Step 168220: {'train_ae_loss': 0.64877, 'train_ucc_loss': 0.38587, 'train_ucc_acc': 0.9375, 'loss': 0.51732}\n",
            "Step 168240: {'train_ae_loss': 0.66594, 'train_ucc_loss': 0.43387, 'train_ucc_acc': 0.875, 'loss': 0.54991}\n",
            "Step 168260: {'train_ae_loss': 0.66205, 'train_ucc_loss': 0.48234, 'train_ucc_acc': 0.8125, 'loss': 0.5722}\n",
            "Step 168280: {'train_ae_loss': 0.65621, 'train_ucc_loss': 0.39657, 'train_ucc_acc': 0.90625, 'loss': 0.52639}\n",
            "Step 168300: {'train_ae_loss': 0.64594, 'train_ucc_loss': 0.41932, 'train_ucc_acc': 0.875, 'loss': 0.53263}\n",
            "Step 168320: {'train_ae_loss': 0.65924, 'train_ucc_loss': 0.45422, 'train_ucc_acc': 0.84375, 'loss': 0.55673}\n",
            "Step 168340: {'train_ae_loss': 0.66948, 'train_ucc_loss': 0.55028, 'train_ucc_acc': 0.78125, 'loss': 0.60988}\n",
            "Step 168360: {'train_ae_loss': 0.66188, 'train_ucc_loss': 0.34512, 'train_ucc_acc': 0.96875, 'loss': 0.5035}\n",
            "Step 168380: {'train_ae_loss': 0.66212, 'train_ucc_loss': 0.48175, 'train_ucc_acc': 0.78125, 'loss': 0.57193}\n",
            "Step 168400: {'train_ae_loss': 0.66899, 'train_ucc_loss': 0.39932, 'train_ucc_acc': 0.9375, 'loss': 0.53415}\n",
            "Step 168420: {'train_ae_loss': 0.65501, 'train_ucc_loss': 0.43407, 'train_ucc_acc': 0.875, 'loss': 0.54454}\n",
            "Step 168440: {'train_ae_loss': 0.65173, 'train_ucc_loss': 0.36165, 'train_ucc_acc': 0.96875, 'loss': 0.50669}\n",
            "Step 168460: {'train_ae_loss': 0.65509, 'train_ucc_loss': 0.49906, 'train_ucc_acc': 0.8125, 'loss': 0.57707}\n",
            "Step 168480: {'train_ae_loss': 0.64492, 'train_ucc_loss': 0.541, 'train_ucc_acc': 0.75, 'loss': 0.59296}\n",
            "Step 168500: {'train_ae_loss': 0.66932, 'train_ucc_loss': 0.44055, 'train_ucc_acc': 0.84375, 'loss': 0.55494}\n",
            "Step 168520: {'train_ae_loss': 0.64859, 'train_ucc_loss': 0.51274, 'train_ucc_acc': 0.78125, 'loss': 0.58067}\n",
            "Step 168540: {'train_ae_loss': 0.6534, 'train_ucc_loss': 0.43429, 'train_ucc_acc': 0.875, 'loss': 0.54385}\n",
            "Step 168560: {'train_ae_loss': 0.65706, 'train_ucc_loss': 0.39666, 'train_ucc_acc': 0.90625, 'loss': 0.52686}\n",
            "Step 168580: {'train_ae_loss': 0.66473, 'train_ucc_loss': 0.47181, 'train_ucc_acc': 0.84375, 'loss': 0.56827}\n",
            "Step 168600: {'train_ae_loss': 0.66716, 'train_ucc_loss': 0.3807, 'train_ucc_acc': 0.9375, 'loss': 0.52393}\n",
            "Step 168620: {'train_ae_loss': 0.66451, 'train_ucc_loss': 0.44261, 'train_ucc_acc': 0.84375, 'loss': 0.55356}\n",
            "Step 168640: {'train_ae_loss': 0.67375, 'train_ucc_loss': 0.37475, 'train_ucc_acc': 0.9375, 'loss': 0.52425}\n",
            "Step 168660: {'train_ae_loss': 0.64811, 'train_ucc_loss': 0.52119, 'train_ucc_acc': 0.78125, 'loss': 0.58465}\n",
            "Step 168680: {'train_ae_loss': 0.66013, 'train_ucc_loss': 0.34527, 'train_ucc_acc': 0.96875, 'loss': 0.5027}\n",
            "Step 168700: {'train_ae_loss': 0.66787, 'train_ucc_loss': 0.42118, 'train_ucc_acc': 0.90625, 'loss': 0.54453}\n",
            "Step 168720: {'train_ae_loss': 0.65196, 'train_ucc_loss': 0.44302, 'train_ucc_acc': 0.84375, 'loss': 0.54749}\n",
            "Step 168740: {'train_ae_loss': 0.67572, 'train_ucc_loss': 0.46166, 'train_ucc_acc': 0.84375, 'loss': 0.56869}\n",
            "Step 168760: {'train_ae_loss': 0.68489, 'train_ucc_loss': 0.41183, 'train_ucc_acc': 0.84375, 'loss': 0.54836}\n",
            "Step 168780: {'train_ae_loss': 0.6595, 'train_ucc_loss': 0.47086, 'train_ucc_acc': 0.8125, 'loss': 0.56518}\n",
            "Step 168800: {'train_ae_loss': 0.6615, 'train_ucc_loss': 0.33573, 'train_ucc_acc': 1.0, 'loss': 0.49861}\n",
            "Step 168820: {'train_ae_loss': 0.6503, 'train_ucc_loss': 0.46663, 'train_ucc_acc': 0.84375, 'loss': 0.55846}\n",
            "Step 168840: {'train_ae_loss': 0.67022, 'train_ucc_loss': 0.47833, 'train_ucc_acc': 0.8125, 'loss': 0.57428}\n",
            "Step 168860: {'train_ae_loss': 0.66081, 'train_ucc_loss': 0.43111, 'train_ucc_acc': 0.875, 'loss': 0.54596}\n",
            "Step 168880: {'train_ae_loss': 0.65405, 'train_ucc_loss': 0.47352, 'train_ucc_acc': 0.8125, 'loss': 0.56379}\n",
            "Step 168900: {'train_ae_loss': 0.66418, 'train_ucc_loss': 0.41765, 'train_ucc_acc': 0.875, 'loss': 0.54091}\n",
            "Step 168920: {'train_ae_loss': 0.63813, 'train_ucc_loss': 0.36971, 'train_ucc_acc': 0.9375, 'loss': 0.50392}\n",
            "Step 168940: {'train_ae_loss': 0.64712, 'train_ucc_loss': 0.5165, 'train_ucc_acc': 0.8125, 'loss': 0.58181}\n",
            "Step 168960: {'train_ae_loss': 0.64838, 'train_ucc_loss': 0.5356, 'train_ucc_acc': 0.75, 'loss': 0.59199}\n",
            "Step 168980: {'train_ae_loss': 0.65634, 'train_ucc_loss': 0.39866, 'train_ucc_acc': 0.9375, 'loss': 0.5275}\n",
            "Step 169000: {'train_ae_loss': 0.6384, 'train_ucc_loss': 0.5583, 'train_ucc_acc': 0.71875, 'loss': 0.59835}\n",
            "step: 169000,eval_ae_loss: 0.64812,eval_ucc_loss: 0.47686,eval_ucc_acc: 0.83008\n",
            "Step 169020: {'train_ae_loss': 0.64511, 'train_ucc_loss': 0.4873, 'train_ucc_acc': 0.8125, 'loss': 0.56621}\n",
            "Step 169040: {'train_ae_loss': 0.65986, 'train_ucc_loss': 0.36545, 'train_ucc_acc': 0.9375, 'loss': 0.51266}\n",
            "Step 169060: {'train_ae_loss': 0.65978, 'train_ucc_loss': 0.44202, 'train_ucc_acc': 0.875, 'loss': 0.5509}\n",
            "Step 169080: {'train_ae_loss': 0.64655, 'train_ucc_loss': 0.35651, 'train_ucc_acc': 0.96875, 'loss': 0.50153}\n",
            "Step 169100: {'train_ae_loss': 0.65488, 'train_ucc_loss': 0.48049, 'train_ucc_acc': 0.8125, 'loss': 0.56769}\n",
            "Step 169120: {'train_ae_loss': 0.67676, 'train_ucc_loss': 0.40341, 'train_ucc_acc': 0.90625, 'loss': 0.54008}\n",
            "Step 169140: {'train_ae_loss': 0.65806, 'train_ucc_loss': 0.36516, 'train_ucc_acc': 0.96875, 'loss': 0.51161}\n",
            "Step 169160: {'train_ae_loss': 0.6491, 'train_ucc_loss': 0.39393, 'train_ucc_acc': 0.875, 'loss': 0.52152}\n",
            "Step 169180: {'train_ae_loss': 0.6765, 'train_ucc_loss': 0.39336, 'train_ucc_acc': 0.9375, 'loss': 0.53493}\n",
            "Step 169200: {'train_ae_loss': 0.65187, 'train_ucc_loss': 0.3971, 'train_ucc_acc': 0.9375, 'loss': 0.52449}\n",
            "Step 169220: {'train_ae_loss': 0.66245, 'train_ucc_loss': 0.45217, 'train_ucc_acc': 0.84375, 'loss': 0.55731}\n",
            "Step 169240: {'train_ae_loss': 0.66665, 'train_ucc_loss': 0.43052, 'train_ucc_acc': 0.875, 'loss': 0.54859}\n",
            "Step 169260: {'train_ae_loss': 0.66535, 'train_ucc_loss': 0.45522, 'train_ucc_acc': 0.875, 'loss': 0.56029}\n",
            "Step 169280: {'train_ae_loss': 0.64371, 'train_ucc_loss': 0.45574, 'train_ucc_acc': 0.84375, 'loss': 0.54972}\n",
            "Step 169300: {'train_ae_loss': 0.64421, 'train_ucc_loss': 0.50772, 'train_ucc_acc': 0.78125, 'loss': 0.57596}\n",
            "Step 169320: {'train_ae_loss': 0.66226, 'train_ucc_loss': 0.5122, 'train_ucc_acc': 0.78125, 'loss': 0.58723}\n",
            "Step 169340: {'train_ae_loss': 0.67093, 'train_ucc_loss': 0.48015, 'train_ucc_acc': 0.8125, 'loss': 0.57554}\n",
            "Step 169360: {'train_ae_loss': 0.65498, 'train_ucc_loss': 0.41138, 'train_ucc_acc': 0.90625, 'loss': 0.53318}\n",
            "Step 169380: {'train_ae_loss': 0.65278, 'train_ucc_loss': 0.46307, 'train_ucc_acc': 0.84375, 'loss': 0.55793}\n",
            "Step 169400: {'train_ae_loss': 0.66942, 'train_ucc_loss': 0.34672, 'train_ucc_acc': 0.96875, 'loss': 0.50807}\n",
            "Step 169420: {'train_ae_loss': 0.66907, 'train_ucc_loss': 0.42337, 'train_ucc_acc': 0.875, 'loss': 0.54622}\n",
            "Step 169440: {'train_ae_loss': 0.64794, 'train_ucc_loss': 0.45801, 'train_ucc_acc': 0.84375, 'loss': 0.55298}\n",
            "Step 169460: {'train_ae_loss': 0.67708, 'train_ucc_loss': 0.35868, 'train_ucc_acc': 0.96875, 'loss': 0.51788}\n",
            "Step 169480: {'train_ae_loss': 0.64229, 'train_ucc_loss': 0.49025, 'train_ucc_acc': 0.84375, 'loss': 0.56627}\n",
            "Step 169500: {'train_ae_loss': 0.66427, 'train_ucc_loss': 0.49915, 'train_ucc_acc': 0.8125, 'loss': 0.58171}\n",
            "Step 169520: {'train_ae_loss': 0.65137, 'train_ucc_loss': 0.44883, 'train_ucc_acc': 0.84375, 'loss': 0.5501}\n",
            "Step 169540: {'train_ae_loss': 0.6614, 'train_ucc_loss': 0.41938, 'train_ucc_acc': 0.90625, 'loss': 0.54039}\n",
            "Step 169560: {'train_ae_loss': 0.66485, 'train_ucc_loss': 0.34888, 'train_ucc_acc': 0.96875, 'loss': 0.50686}\n",
            "Step 169580: {'train_ae_loss': 0.66189, 'train_ucc_loss': 0.39648, 'train_ucc_acc': 0.90625, 'loss': 0.52918}\n",
            "Step 169600: {'train_ae_loss': 0.67481, 'train_ucc_loss': 0.51108, 'train_ucc_acc': 0.8125, 'loss': 0.59294}\n",
            "Step 169620: {'train_ae_loss': 0.64554, 'train_ucc_loss': 0.39737, 'train_ucc_acc': 0.90625, 'loss': 0.52145}\n",
            "Step 169640: {'train_ae_loss': 0.66012, 'train_ucc_loss': 0.54218, 'train_ucc_acc': 0.75, 'loss': 0.60115}\n",
            "Step 169660: {'train_ae_loss': 0.65465, 'train_ucc_loss': 0.475, 'train_ucc_acc': 0.8125, 'loss': 0.56483}\n",
            "Step 169680: {'train_ae_loss': 0.65156, 'train_ucc_loss': 0.43657, 'train_ucc_acc': 0.875, 'loss': 0.54406}\n",
            "Step 169700: {'train_ae_loss': 0.65159, 'train_ucc_loss': 0.48981, 'train_ucc_acc': 0.8125, 'loss': 0.5707}\n",
            "Step 169720: {'train_ae_loss': 0.62568, 'train_ucc_loss': 0.51552, 'train_ucc_acc': 0.8125, 'loss': 0.5706}\n",
            "Step 169740: {'train_ae_loss': 0.66382, 'train_ucc_loss': 0.48475, 'train_ucc_acc': 0.8125, 'loss': 0.57428}\n",
            "Step 169760: {'train_ae_loss': 0.65135, 'train_ucc_loss': 0.50968, 'train_ucc_acc': 0.78125, 'loss': 0.58051}\n",
            "Step 169780: {'train_ae_loss': 0.656, 'train_ucc_loss': 0.38961, 'train_ucc_acc': 0.9375, 'loss': 0.52281}\n",
            "Step 169800: {'train_ae_loss': 0.65131, 'train_ucc_loss': 0.43248, 'train_ucc_acc': 0.875, 'loss': 0.54189}\n",
            "Step 169820: {'train_ae_loss': 0.65845, 'train_ucc_loss': 0.4821, 'train_ucc_acc': 0.84375, 'loss': 0.57027}\n",
            "Step 169840: {'train_ae_loss': 0.65677, 'train_ucc_loss': 0.45917, 'train_ucc_acc': 0.84375, 'loss': 0.55797}\n",
            "Step 169860: {'train_ae_loss': 0.66042, 'train_ucc_loss': 0.53417, 'train_ucc_acc': 0.78125, 'loss': 0.59729}\n",
            "Step 169880: {'train_ae_loss': 0.63065, 'train_ucc_loss': 0.37226, 'train_ucc_acc': 0.9375, 'loss': 0.50146}\n",
            "Step 169900: {'train_ae_loss': 0.67586, 'train_ucc_loss': 0.41392, 'train_ucc_acc': 0.90625, 'loss': 0.54489}\n",
            "Step 169920: {'train_ae_loss': 0.66589, 'train_ucc_loss': 0.43073, 'train_ucc_acc': 0.84375, 'loss': 0.54831}\n",
            "Step 169940: {'train_ae_loss': 0.67731, 'train_ucc_loss': 0.39174, 'train_ucc_acc': 0.9375, 'loss': 0.53453}\n",
            "Step 169960: {'train_ae_loss': 0.65764, 'train_ucc_loss': 0.39623, 'train_ucc_acc': 0.9375, 'loss': 0.52694}\n",
            "Step 169980: {'train_ae_loss': 0.66606, 'train_ucc_loss': 0.44783, 'train_ucc_acc': 0.84375, 'loss': 0.55694}\n",
            "Step 170000: {'train_ae_loss': 0.64981, 'train_ucc_loss': 0.38213, 'train_ucc_acc': 0.9375, 'loss': 0.51597}\n",
            "step: 170000,eval_ae_loss: 0.64838,eval_ucc_loss: 0.45055,eval_ucc_acc: 0.85352\n",
            "Step 170020: {'train_ae_loss': 0.65493, 'train_ucc_loss': 0.34615, 'train_ucc_acc': 0.96875, 'loss': 0.50054}\n",
            "Step 170040: {'train_ae_loss': 0.66607, 'train_ucc_loss': 0.52446, 'train_ucc_acc': 0.78125, 'loss': 0.59527}\n",
            "Step 170060: {'train_ae_loss': 0.67482, 'train_ucc_loss': 0.39175, 'train_ucc_acc': 0.9375, 'loss': 0.53329}\n",
            "Step 170080: {'train_ae_loss': 0.63776, 'train_ucc_loss': 0.4898, 'train_ucc_acc': 0.8125, 'loss': 0.56378}\n",
            "Step 170100: {'train_ae_loss': 0.65294, 'train_ucc_loss': 0.52467, 'train_ucc_acc': 0.78125, 'loss': 0.58881}\n",
            "Step 170120: {'train_ae_loss': 0.67193, 'train_ucc_loss': 0.45225, 'train_ucc_acc': 0.84375, 'loss': 0.56209}\n",
            "Step 170140: {'train_ae_loss': 0.66358, 'train_ucc_loss': 0.39954, 'train_ucc_acc': 0.90625, 'loss': 0.53156}\n",
            "Step 170160: {'train_ae_loss': 0.65581, 'train_ucc_loss': 0.40434, 'train_ucc_acc': 0.90625, 'loss': 0.53007}\n",
            "Step 170180: {'train_ae_loss': 0.6458, 'train_ucc_loss': 0.50156, 'train_ucc_acc': 0.8125, 'loss': 0.57368}\n",
            "Step 170200: {'train_ae_loss': 0.66721, 'train_ucc_loss': 0.42984, 'train_ucc_acc': 0.875, 'loss': 0.54853}\n",
            "Step 170220: {'train_ae_loss': 0.6638, 'train_ucc_loss': 0.45408, 'train_ucc_acc': 0.84375, 'loss': 0.55894}\n",
            "Step 170240: {'train_ae_loss': 0.64931, 'train_ucc_loss': 0.47711, 'train_ucc_acc': 0.8125, 'loss': 0.56321}\n",
            "Step 170260: {'train_ae_loss': 0.64542, 'train_ucc_loss': 0.41643, 'train_ucc_acc': 0.90625, 'loss': 0.53093}\n",
            "Step 170280: {'train_ae_loss': 0.66855, 'train_ucc_loss': 0.40771, 'train_ucc_acc': 0.90625, 'loss': 0.53813}\n",
            "Step 170300: {'train_ae_loss': 0.65976, 'train_ucc_loss': 0.44479, 'train_ucc_acc': 0.875, 'loss': 0.55227}\n",
            "Step 170320: {'train_ae_loss': 0.64993, 'train_ucc_loss': 0.52753, 'train_ucc_acc': 0.78125, 'loss': 0.58873}\n",
            "Step 170340: {'train_ae_loss': 0.67921, 'train_ucc_loss': 0.41194, 'train_ucc_acc': 0.90625, 'loss': 0.54557}\n",
            "Step 170360: {'train_ae_loss': 0.67097, 'train_ucc_loss': 0.46327, 'train_ucc_acc': 0.84375, 'loss': 0.56712}\n",
            "Step 170380: {'train_ae_loss': 0.66886, 'train_ucc_loss': 0.38709, 'train_ucc_acc': 0.9375, 'loss': 0.52797}\n",
            "Step 170400: {'train_ae_loss': 0.65465, 'train_ucc_loss': 0.43278, 'train_ucc_acc': 0.90625, 'loss': 0.54371}\n",
            "Step 170420: {'train_ae_loss': 0.66202, 'train_ucc_loss': 0.4175, 'train_ucc_acc': 0.90625, 'loss': 0.53976}\n",
            "Step 170440: {'train_ae_loss': 0.6569, 'train_ucc_loss': 0.44659, 'train_ucc_acc': 0.875, 'loss': 0.55174}\n",
            "Step 170460: {'train_ae_loss': 0.66854, 'train_ucc_loss': 0.46067, 'train_ucc_acc': 0.8125, 'loss': 0.56461}\n",
            "Step 170480: {'train_ae_loss': 0.65118, 'train_ucc_loss': 0.55673, 'train_ucc_acc': 0.75, 'loss': 0.60396}\n",
            "Step 170500: {'train_ae_loss': 0.67351, 'train_ucc_loss': 0.35163, 'train_ucc_acc': 0.96875, 'loss': 0.51257}\n",
            "Step 170520: {'train_ae_loss': 0.65566, 'train_ucc_loss': 0.40354, 'train_ucc_acc': 0.875, 'loss': 0.5296}\n",
            "Step 170540: {'train_ae_loss': 0.65668, 'train_ucc_loss': 0.50153, 'train_ucc_acc': 0.8125, 'loss': 0.57911}\n",
            "Step 170560: {'train_ae_loss': 0.65706, 'train_ucc_loss': 0.41272, 'train_ucc_acc': 0.90625, 'loss': 0.53489}\n",
            "Step 170580: {'train_ae_loss': 0.66343, 'train_ucc_loss': 0.42884, 'train_ucc_acc': 0.875, 'loss': 0.54613}\n",
            "Step 170600: {'train_ae_loss': 0.64868, 'train_ucc_loss': 0.37894, 'train_ucc_acc': 0.9375, 'loss': 0.51381}\n",
            "Step 170620: {'train_ae_loss': 0.66335, 'train_ucc_loss': 0.37485, 'train_ucc_acc': 0.9375, 'loss': 0.5191}\n",
            "Step 170640: {'train_ae_loss': 0.64371, 'train_ucc_loss': 0.44886, 'train_ucc_acc': 0.84375, 'loss': 0.54628}\n",
            "Step 170660: {'train_ae_loss': 0.65435, 'train_ucc_loss': 0.42592, 'train_ucc_acc': 0.875, 'loss': 0.54013}\n",
            "Step 170680: {'train_ae_loss': 0.65183, 'train_ucc_loss': 0.35559, 'train_ucc_acc': 0.96875, 'loss': 0.50371}\n",
            "Step 170700: {'train_ae_loss': 0.67146, 'train_ucc_loss': 0.5107, 'train_ucc_acc': 0.8125, 'loss': 0.59108}\n",
            "Step 170720: {'train_ae_loss': 0.6492, 'train_ucc_loss': 0.44181, 'train_ucc_acc': 0.84375, 'loss': 0.5455}\n",
            "Step 170740: {'train_ae_loss': 0.67096, 'train_ucc_loss': 0.47081, 'train_ucc_acc': 0.8125, 'loss': 0.57089}\n",
            "Step 170760: {'train_ae_loss': 0.65741, 'train_ucc_loss': 0.43996, 'train_ucc_acc': 0.84375, 'loss': 0.54869}\n",
            "Step 170780: {'train_ae_loss': 0.64892, 'train_ucc_loss': 0.45233, 'train_ucc_acc': 0.84375, 'loss': 0.55062}\n",
            "Step 170800: {'train_ae_loss': 0.66987, 'train_ucc_loss': 0.52906, 'train_ucc_acc': 0.78125, 'loss': 0.59947}\n",
            "Step 170820: {'train_ae_loss': 0.69281, 'train_ucc_loss': 0.35846, 'train_ucc_acc': 0.96875, 'loss': 0.52563}\n",
            "Step 170840: {'train_ae_loss': 0.67896, 'train_ucc_loss': 0.37593, 'train_ucc_acc': 0.9375, 'loss': 0.52745}\n",
            "Step 170860: {'train_ae_loss': 0.65871, 'train_ucc_loss': 0.48882, 'train_ucc_acc': 0.8125, 'loss': 0.57376}\n",
            "Step 170880: {'train_ae_loss': 0.65614, 'train_ucc_loss': 0.43488, 'train_ucc_acc': 0.875, 'loss': 0.54551}\n",
            "Step 170900: {'train_ae_loss': 0.6531, 'train_ucc_loss': 0.43652, 'train_ucc_acc': 0.84375, 'loss': 0.54481}\n",
            "Step 170920: {'train_ae_loss': 0.67681, 'train_ucc_loss': 0.42906, 'train_ucc_acc': 0.875, 'loss': 0.55294}\n",
            "Step 170940: {'train_ae_loss': 0.64701, 'train_ucc_loss': 0.46506, 'train_ucc_acc': 0.8125, 'loss': 0.55604}\n",
            "Step 170960: {'train_ae_loss': 0.66102, 'train_ucc_loss': 0.39819, 'train_ucc_acc': 0.90625, 'loss': 0.5296}\n",
            "Step 170980: {'train_ae_loss': 0.66913, 'train_ucc_loss': 0.46357, 'train_ucc_acc': 0.84375, 'loss': 0.56635}\n",
            "Step 171000: {'train_ae_loss': 0.65796, 'train_ucc_loss': 0.47977, 'train_ucc_acc': 0.84375, 'loss': 0.56887}\n",
            "step: 171000,eval_ae_loss: 0.65599,eval_ucc_loss: 0.49649,eval_ucc_acc: 0.81348\n",
            "Step 171020: {'train_ae_loss': 0.6372, 'train_ucc_loss': 0.41108, 'train_ucc_acc': 0.90625, 'loss': 0.52414}\n",
            "Step 171040: {'train_ae_loss': 0.67098, 'train_ucc_loss': 0.44855, 'train_ucc_acc': 0.875, 'loss': 0.55976}\n",
            "Step 171060: {'train_ae_loss': 0.6459, 'train_ucc_loss': 0.40875, 'train_ucc_acc': 0.90625, 'loss': 0.52733}\n",
            "Step 171080: {'train_ae_loss': 0.65692, 'train_ucc_loss': 0.41435, 'train_ucc_acc': 0.90625, 'loss': 0.53563}\n",
            "Step 171100: {'train_ae_loss': 0.6496, 'train_ucc_loss': 0.44828, 'train_ucc_acc': 0.84375, 'loss': 0.54894}\n",
            "Step 171120: {'train_ae_loss': 0.68266, 'train_ucc_loss': 0.34721, 'train_ucc_acc': 0.96875, 'loss': 0.51494}\n",
            "Step 171140: {'train_ae_loss': 0.67229, 'train_ucc_loss': 0.44332, 'train_ucc_acc': 0.84375, 'loss': 0.55781}\n",
            "Step 171160: {'train_ae_loss': 0.65143, 'train_ucc_loss': 0.36961, 'train_ucc_acc': 0.9375, 'loss': 0.51052}\n",
            "Step 171180: {'train_ae_loss': 0.65878, 'train_ucc_loss': 0.45936, 'train_ucc_acc': 0.84375, 'loss': 0.55907}\n",
            "Step 171200: {'train_ae_loss': 0.62506, 'train_ucc_loss': 0.40626, 'train_ucc_acc': 0.90625, 'loss': 0.51566}\n",
            "Step 171220: {'train_ae_loss': 0.64871, 'train_ucc_loss': 0.43729, 'train_ucc_acc': 0.875, 'loss': 0.543}\n",
            "Step 171240: {'train_ae_loss': 0.66882, 'train_ucc_loss': 0.43978, 'train_ucc_acc': 0.84375, 'loss': 0.5543}\n",
            "Step 171260: {'train_ae_loss': 0.66118, 'train_ucc_loss': 0.44124, 'train_ucc_acc': 0.875, 'loss': 0.55121}\n",
            "Step 171280: {'train_ae_loss': 0.67234, 'train_ucc_loss': 0.4034, 'train_ucc_acc': 0.90625, 'loss': 0.53787}\n",
            "Step 171300: {'train_ae_loss': 0.65126, 'train_ucc_loss': 0.491, 'train_ucc_acc': 0.78125, 'loss': 0.57113}\n",
            "Step 171320: {'train_ae_loss': 0.66875, 'train_ucc_loss': 0.40762, 'train_ucc_acc': 0.90625, 'loss': 0.53819}\n",
            "Step 171340: {'train_ae_loss': 0.63917, 'train_ucc_loss': 0.46593, 'train_ucc_acc': 0.84375, 'loss': 0.55255}\n",
            "Step 171360: {'train_ae_loss': 0.66033, 'train_ucc_loss': 0.48757, 'train_ucc_acc': 0.8125, 'loss': 0.57395}\n",
            "Step 171380: {'train_ae_loss': 0.65455, 'train_ucc_loss': 0.59809, 'train_ucc_acc': 0.6875, 'loss': 0.62632}\n",
            "Step 171400: {'train_ae_loss': 0.65272, 'train_ucc_loss': 0.54225, 'train_ucc_acc': 0.75, 'loss': 0.59749}\n",
            "Step 171420: {'train_ae_loss': 0.6756, 'train_ucc_loss': 0.32485, 'train_ucc_acc': 1.0, 'loss': 0.50022}\n",
            "Step 171440: {'train_ae_loss': 0.66233, 'train_ucc_loss': 0.40889, 'train_ucc_acc': 0.90625, 'loss': 0.53561}\n",
            "Step 171460: {'train_ae_loss': 0.6606, 'train_ucc_loss': 0.39846, 'train_ucc_acc': 0.90625, 'loss': 0.52953}\n",
            "Step 171480: {'train_ae_loss': 0.66214, 'train_ucc_loss': 0.44213, 'train_ucc_acc': 0.875, 'loss': 0.55213}\n",
            "Step 171500: {'train_ae_loss': 0.66356, 'train_ucc_loss': 0.47543, 'train_ucc_acc': 0.84375, 'loss': 0.5695}\n",
            "Step 171520: {'train_ae_loss': 0.67094, 'train_ucc_loss': 0.38009, 'train_ucc_acc': 0.9375, 'loss': 0.52552}\n",
            "Step 171540: {'train_ae_loss': 0.65418, 'train_ucc_loss': 0.45672, 'train_ucc_acc': 0.875, 'loss': 0.55545}\n",
            "Step 171560: {'train_ae_loss': 0.66564, 'train_ucc_loss': 0.38222, 'train_ucc_acc': 0.875, 'loss': 0.52393}\n",
            "Step 171580: {'train_ae_loss': 0.66413, 'train_ucc_loss': 0.52963, 'train_ucc_acc': 0.75, 'loss': 0.59688}\n",
            "Step 171600: {'train_ae_loss': 0.67304, 'train_ucc_loss': 0.42563, 'train_ucc_acc': 0.875, 'loss': 0.54934}\n",
            "Step 171620: {'train_ae_loss': 0.63887, 'train_ucc_loss': 0.43106, 'train_ucc_acc': 0.875, 'loss': 0.53497}\n",
            "Step 171640: {'train_ae_loss': 0.65141, 'train_ucc_loss': 0.43726, 'train_ucc_acc': 0.875, 'loss': 0.54434}\n",
            "Step 171660: {'train_ae_loss': 0.64609, 'train_ucc_loss': 0.40213, 'train_ucc_acc': 0.90625, 'loss': 0.52411}\n",
            "Step 171680: {'train_ae_loss': 0.65295, 'train_ucc_loss': 0.35256, 'train_ucc_acc': 0.96875, 'loss': 0.50276}\n",
            "Step 171700: {'train_ae_loss': 0.65164, 'train_ucc_loss': 0.44983, 'train_ucc_acc': 0.8125, 'loss': 0.55074}\n",
            "Step 171720: {'train_ae_loss': 0.63369, 'train_ucc_loss': 0.35955, 'train_ucc_acc': 0.96875, 'loss': 0.49662}\n",
            "Step 171740: {'train_ae_loss': 0.64725, 'train_ucc_loss': 0.39531, 'train_ucc_acc': 0.90625, 'loss': 0.52128}\n",
            "Step 171760: {'train_ae_loss': 0.64658, 'train_ucc_loss': 0.4185, 'train_ucc_acc': 0.90625, 'loss': 0.53254}\n",
            "Step 171780: {'train_ae_loss': 0.64232, 'train_ucc_loss': 0.44593, 'train_ucc_acc': 0.84375, 'loss': 0.54413}\n",
            "Step 171800: {'train_ae_loss': 0.65137, 'train_ucc_loss': 0.55868, 'train_ucc_acc': 0.75, 'loss': 0.60502}\n",
            "Step 171820: {'train_ae_loss': 0.66954, 'train_ucc_loss': 0.46363, 'train_ucc_acc': 0.84375, 'loss': 0.56659}\n",
            "Step 171840: {'train_ae_loss': 0.67317, 'train_ucc_loss': 0.50308, 'train_ucc_acc': 0.8125, 'loss': 0.58812}\n",
            "Step 171860: {'train_ae_loss': 0.67301, 'train_ucc_loss': 0.45736, 'train_ucc_acc': 0.84375, 'loss': 0.56519}\n",
            "Step 171880: {'train_ae_loss': 0.65139, 'train_ucc_loss': 0.42321, 'train_ucc_acc': 0.875, 'loss': 0.5373}\n",
            "Step 171900: {'train_ae_loss': 0.67078, 'train_ucc_loss': 0.48466, 'train_ucc_acc': 0.84375, 'loss': 0.57772}\n",
            "Step 171920: {'train_ae_loss': 0.64373, 'train_ucc_loss': 0.43595, 'train_ucc_acc': 0.875, 'loss': 0.53984}\n",
            "Step 171940: {'train_ae_loss': 0.66433, 'train_ucc_loss': 0.35597, 'train_ucc_acc': 0.96875, 'loss': 0.51015}\n",
            "Step 171960: {'train_ae_loss': 0.67537, 'train_ucc_loss': 0.46702, 'train_ucc_acc': 0.84375, 'loss': 0.5712}\n",
            "Step 171980: {'train_ae_loss': 0.66661, 'train_ucc_loss': 0.38174, 'train_ucc_acc': 0.90625, 'loss': 0.52417}\n",
            "Step 172000: {'train_ae_loss': 0.6724, 'train_ucc_loss': 0.40307, 'train_ucc_acc': 0.9375, 'loss': 0.53773}\n",
            "step: 172000,eval_ae_loss: 0.65576,eval_ucc_loss: 0.44816,eval_ucc_acc: 0.86426\n",
            "Step 172020: {'train_ae_loss': 0.66374, 'train_ucc_loss': 0.52566, 'train_ucc_acc': 0.78125, 'loss': 0.5947}\n",
            "Step 172040: {'train_ae_loss': 0.6572, 'train_ucc_loss': 0.37556, 'train_ucc_acc': 0.9375, 'loss': 0.51638}\n",
            "Step 172060: {'train_ae_loss': 0.67347, 'train_ucc_loss': 0.40492, 'train_ucc_acc': 0.875, 'loss': 0.5392}\n",
            "Step 172080: {'train_ae_loss': 0.66682, 'train_ucc_loss': 0.40405, 'train_ucc_acc': 0.90625, 'loss': 0.53543}\n",
            "Step 172100: {'train_ae_loss': 0.66329, 'train_ucc_loss': 0.41679, 'train_ucc_acc': 0.90625, 'loss': 0.54004}\n",
            "Step 172120: {'train_ae_loss': 0.67626, 'train_ucc_loss': 0.32828, 'train_ucc_acc': 1.0, 'loss': 0.50227}\n",
            "Step 172140: {'train_ae_loss': 0.66541, 'train_ucc_loss': 0.43855, 'train_ucc_acc': 0.84375, 'loss': 0.55198}\n",
            "Step 172160: {'train_ae_loss': 0.66698, 'train_ucc_loss': 0.4017, 'train_ucc_acc': 0.875, 'loss': 0.53434}\n",
            "Step 172180: {'train_ae_loss': 0.67862, 'train_ucc_loss': 0.3942, 'train_ucc_acc': 0.9375, 'loss': 0.53641}\n",
            "Step 172200: {'train_ae_loss': 0.67252, 'train_ucc_loss': 0.41459, 'train_ucc_acc': 0.90625, 'loss': 0.54355}\n",
            "Step 172220: {'train_ae_loss': 0.66277, 'train_ucc_loss': 0.36214, 'train_ucc_acc': 0.9375, 'loss': 0.51246}\n",
            "Step 172240: {'train_ae_loss': 0.65494, 'train_ucc_loss': 0.46851, 'train_ucc_acc': 0.8125, 'loss': 0.56173}\n",
            "Step 172260: {'train_ae_loss': 0.66952, 'train_ucc_loss': 0.42774, 'train_ucc_acc': 0.875, 'loss': 0.54863}\n",
            "Step 172280: {'train_ae_loss': 0.65375, 'train_ucc_loss': 0.46245, 'train_ucc_acc': 0.84375, 'loss': 0.5581}\n",
            "Step 172300: {'train_ae_loss': 0.66613, 'train_ucc_loss': 0.45857, 'train_ucc_acc': 0.8125, 'loss': 0.56235}\n",
            "Step 172320: {'train_ae_loss': 0.65517, 'train_ucc_loss': 0.39459, 'train_ucc_acc': 0.9375, 'loss': 0.52488}\n",
            "Step 172340: {'train_ae_loss': 0.6648, 'train_ucc_loss': 0.40994, 'train_ucc_acc': 0.875, 'loss': 0.53737}\n",
            "Step 172360: {'train_ae_loss': 0.65192, 'train_ucc_loss': 0.51411, 'train_ucc_acc': 0.78125, 'loss': 0.58302}\n",
            "Step 172380: {'train_ae_loss': 0.67111, 'train_ucc_loss': 0.47388, 'train_ucc_acc': 0.8125, 'loss': 0.57249}\n",
            "Step 172400: {'train_ae_loss': 0.6578, 'train_ucc_loss': 0.4159, 'train_ucc_acc': 0.90625, 'loss': 0.53685}\n",
            "Step 172420: {'train_ae_loss': 0.65844, 'train_ucc_loss': 0.46034, 'train_ucc_acc': 0.84375, 'loss': 0.55939}\n",
            "Step 172440: {'train_ae_loss': 0.65882, 'train_ucc_loss': 0.53357, 'train_ucc_acc': 0.75, 'loss': 0.5962}\n",
            "Step 172460: {'train_ae_loss': 0.67277, 'train_ucc_loss': 0.56221, 'train_ucc_acc': 0.75, 'loss': 0.61749}\n",
            "Step 172480: {'train_ae_loss': 0.65815, 'train_ucc_loss': 0.4578, 'train_ucc_acc': 0.875, 'loss': 0.55797}\n",
            "Step 172500: {'train_ae_loss': 0.65858, 'train_ucc_loss': 0.53326, 'train_ucc_acc': 0.75, 'loss': 0.59592}\n",
            "Step 172520: {'train_ae_loss': 0.6557, 'train_ucc_loss': 0.42859, 'train_ucc_acc': 0.875, 'loss': 0.54215}\n",
            "Step 172540: {'train_ae_loss': 0.65554, 'train_ucc_loss': 0.41191, 'train_ucc_acc': 0.90625, 'loss': 0.53372}\n",
            "Step 172560: {'train_ae_loss': 0.66271, 'train_ucc_loss': 0.44849, 'train_ucc_acc': 0.84375, 'loss': 0.5556}\n",
            "Step 172580: {'train_ae_loss': 0.67131, 'train_ucc_loss': 0.42257, 'train_ucc_acc': 0.90625, 'loss': 0.54694}\n",
            "Step 172600: {'train_ae_loss': 0.66465, 'train_ucc_loss': 0.46908, 'train_ucc_acc': 0.84375, 'loss': 0.56686}\n",
            "Step 172620: {'train_ae_loss': 0.65198, 'train_ucc_loss': 0.55046, 'train_ucc_acc': 0.71875, 'loss': 0.60122}\n",
            "Step 172640: {'train_ae_loss': 0.66482, 'train_ucc_loss': 0.39865, 'train_ucc_acc': 0.90625, 'loss': 0.53173}\n",
            "Step 172660: {'train_ae_loss': 0.65543, 'train_ucc_loss': 0.48893, 'train_ucc_acc': 0.84375, 'loss': 0.57218}\n",
            "Step 172680: {'train_ae_loss': 0.67867, 'train_ucc_loss': 0.4173, 'train_ucc_acc': 0.90625, 'loss': 0.54799}\n",
            "Step 172700: {'train_ae_loss': 0.67998, 'train_ucc_loss': 0.46757, 'train_ucc_acc': 0.84375, 'loss': 0.57378}\n",
            "Step 172720: {'train_ae_loss': 0.67643, 'train_ucc_loss': 0.45674, 'train_ucc_acc': 0.875, 'loss': 0.56659}\n",
            "Step 172740: {'train_ae_loss': 0.68031, 'train_ucc_loss': 0.49452, 'train_ucc_acc': 0.8125, 'loss': 0.58741}\n",
            "Step 172760: {'train_ae_loss': 0.684, 'train_ucc_loss': 0.48775, 'train_ucc_acc': 0.8125, 'loss': 0.58587}\n",
            "Step 172780: {'train_ae_loss': 0.65938, 'train_ucc_loss': 0.4135, 'train_ucc_acc': 0.875, 'loss': 0.53644}\n",
            "Step 172800: {'train_ae_loss': 0.67638, 'train_ucc_loss': 0.41503, 'train_ucc_acc': 0.90625, 'loss': 0.54571}\n",
            "Step 172820: {'train_ae_loss': 0.67492, 'train_ucc_loss': 0.4184, 'train_ucc_acc': 0.875, 'loss': 0.54666}\n",
            "Step 172840: {'train_ae_loss': 0.67847, 'train_ucc_loss': 0.40991, 'train_ucc_acc': 0.875, 'loss': 0.54419}\n",
            "Step 172860: {'train_ae_loss': 0.6882, 'train_ucc_loss': 0.47116, 'train_ucc_acc': 0.84375, 'loss': 0.57968}\n",
            "Step 172880: {'train_ae_loss': 0.68478, 'train_ucc_loss': 0.3648, 'train_ucc_acc': 0.96875, 'loss': 0.52479}\n",
            "Step 172900: {'train_ae_loss': 0.66825, 'train_ucc_loss': 0.46566, 'train_ucc_acc': 0.84375, 'loss': 0.56695}\n",
            "Step 172920: {'train_ae_loss': 0.68285, 'train_ucc_loss': 0.50226, 'train_ucc_acc': 0.78125, 'loss': 0.59255}\n",
            "Step 172940: {'train_ae_loss': 0.68197, 'train_ucc_loss': 0.35786, 'train_ucc_acc': 0.9375, 'loss': 0.51992}\n",
            "Step 172960: {'train_ae_loss': 0.67412, 'train_ucc_loss': 0.37451, 'train_ucc_acc': 0.9375, 'loss': 0.52431}\n",
            "Step 172980: {'train_ae_loss': 0.67327, 'train_ucc_loss': 0.47135, 'train_ucc_acc': 0.84375, 'loss': 0.57231}\n",
            "Step 173000: {'train_ae_loss': 0.67067, 'train_ucc_loss': 0.38119, 'train_ucc_acc': 0.9375, 'loss': 0.52593}\n",
            "step: 173000,eval_ae_loss: 0.66096,eval_ucc_loss: 0.46514,eval_ucc_acc: 0.83984\n",
            "Step 173020: {'train_ae_loss': 0.68115, 'train_ucc_loss': 0.54835, 'train_ucc_acc': 0.71875, 'loss': 0.61475}\n",
            "Step 173040: {'train_ae_loss': 0.66749, 'train_ucc_loss': 0.3901, 'train_ucc_acc': 0.9375, 'loss': 0.52879}\n",
            "Step 173060: {'train_ae_loss': 0.66015, 'train_ucc_loss': 0.37593, 'train_ucc_acc': 0.9375, 'loss': 0.51804}\n",
            "Step 173080: {'train_ae_loss': 0.68368, 'train_ucc_loss': 0.43079, 'train_ucc_acc': 0.90625, 'loss': 0.55724}\n",
            "Step 173100: {'train_ae_loss': 0.6719, 'train_ucc_loss': 0.5183, 'train_ucc_acc': 0.78125, 'loss': 0.5951}\n",
            "Step 173120: {'train_ae_loss': 0.66434, 'train_ucc_loss': 0.42052, 'train_ucc_acc': 0.875, 'loss': 0.54243}\n",
            "Step 173140: {'train_ae_loss': 0.65052, 'train_ucc_loss': 0.45344, 'train_ucc_acc': 0.875, 'loss': 0.55198}\n",
            "Step 173160: {'train_ae_loss': 0.67326, 'train_ucc_loss': 0.39586, 'train_ucc_acc': 0.90625, 'loss': 0.53456}\n",
            "Step 173180: {'train_ae_loss': 0.6645, 'train_ucc_loss': 0.3851, 'train_ucc_acc': 0.875, 'loss': 0.5248}\n",
            "Step 173200: {'train_ae_loss': 0.67339, 'train_ucc_loss': 0.52264, 'train_ucc_acc': 0.78125, 'loss': 0.59802}\n",
            "Step 173220: {'train_ae_loss': 0.67091, 'train_ucc_loss': 0.38119, 'train_ucc_acc': 0.9375, 'loss': 0.52605}\n",
            "Step 173240: {'train_ae_loss': 0.67534, 'train_ucc_loss': 0.39136, 'train_ucc_acc': 0.9375, 'loss': 0.53335}\n",
            "Step 173260: {'train_ae_loss': 0.69363, 'train_ucc_loss': 0.37767, 'train_ucc_acc': 0.9375, 'loss': 0.53565}\n",
            "Step 173280: {'train_ae_loss': 0.66993, 'train_ucc_loss': 0.46238, 'train_ucc_acc': 0.8125, 'loss': 0.56615}\n",
            "Step 173300: {'train_ae_loss': 0.65755, 'train_ucc_loss': 0.44074, 'train_ucc_acc': 0.875, 'loss': 0.54915}\n",
            "Step 173320: {'train_ae_loss': 0.67833, 'train_ucc_loss': 0.39691, 'train_ucc_acc': 0.90625, 'loss': 0.53762}\n",
            "Step 173340: {'train_ae_loss': 0.6522, 'train_ucc_loss': 0.41401, 'train_ucc_acc': 0.90625, 'loss': 0.53311}\n",
            "Step 173360: {'train_ae_loss': 0.68518, 'train_ucc_loss': 0.36881, 'train_ucc_acc': 0.9375, 'loss': 0.52699}\n",
            "Step 173380: {'train_ae_loss': 0.66172, 'train_ucc_loss': 0.35755, 'train_ucc_acc': 0.96875, 'loss': 0.50964}\n",
            "Step 173400: {'train_ae_loss': 0.67746, 'train_ucc_loss': 0.38951, 'train_ucc_acc': 0.9375, 'loss': 0.53348}\n",
            "Step 173420: {'train_ae_loss': 0.6614, 'train_ucc_loss': 0.48932, 'train_ucc_acc': 0.8125, 'loss': 0.57536}\n",
            "Step 173440: {'train_ae_loss': 0.67295, 'train_ucc_loss': 0.48961, 'train_ucc_acc': 0.78125, 'loss': 0.58128}\n",
            "Step 173460: {'train_ae_loss': 0.67468, 'train_ucc_loss': 0.46543, 'train_ucc_acc': 0.8125, 'loss': 0.57005}\n",
            "Step 173480: {'train_ae_loss': 0.65701, 'train_ucc_loss': 0.38131, 'train_ucc_acc': 0.9375, 'loss': 0.51916}\n",
            "Step 173500: {'train_ae_loss': 0.69067, 'train_ucc_loss': 0.32893, 'train_ucc_acc': 1.0, 'loss': 0.5098}\n",
            "Step 173520: {'train_ae_loss': 0.65886, 'train_ucc_loss': 0.42166, 'train_ucc_acc': 0.90625, 'loss': 0.54026}\n",
            "Step 173540: {'train_ae_loss': 0.68343, 'train_ucc_loss': 0.36981, 'train_ucc_acc': 0.9375, 'loss': 0.52662}\n",
            "Step 173560: {'train_ae_loss': 0.66372, 'train_ucc_loss': 0.49274, 'train_ucc_acc': 0.8125, 'loss': 0.57823}\n",
            "Step 173580: {'train_ae_loss': 0.68788, 'train_ucc_loss': 0.36136, 'train_ucc_acc': 0.9375, 'loss': 0.52462}\n",
            "Step 173600: {'train_ae_loss': 0.69348, 'train_ucc_loss': 0.38557, 'train_ucc_acc': 0.9375, 'loss': 0.53952}\n",
            "Step 173620: {'train_ae_loss': 0.68096, 'train_ucc_loss': 0.44065, 'train_ucc_acc': 0.875, 'loss': 0.5608}\n",
            "Step 173640: {'train_ae_loss': 0.69211, 'train_ucc_loss': 0.37981, 'train_ucc_acc': 0.9375, 'loss': 0.53596}\n",
            "Step 173660: {'train_ae_loss': 0.68899, 'train_ucc_loss': 0.42294, 'train_ucc_acc': 0.875, 'loss': 0.55596}\n",
            "Step 173680: {'train_ae_loss': 0.68347, 'train_ucc_loss': 0.4624, 'train_ucc_acc': 0.8125, 'loss': 0.57294}\n",
            "Step 173700: {'train_ae_loss': 0.67676, 'train_ucc_loss': 0.43493, 'train_ucc_acc': 0.875, 'loss': 0.55585}\n",
            "Step 173720: {'train_ae_loss': 0.69172, 'train_ucc_loss': 0.53286, 'train_ucc_acc': 0.78125, 'loss': 0.61229}\n",
            "Step 173740: {'train_ae_loss': 0.71321, 'train_ucc_loss': 0.43665, 'train_ucc_acc': 0.84375, 'loss': 0.57493}\n",
            "Step 173760: {'train_ae_loss': 0.69925, 'train_ucc_loss': 0.43577, 'train_ucc_acc': 0.875, 'loss': 0.56751}\n",
            "Step 173780: {'train_ae_loss': 0.68655, 'train_ucc_loss': 0.41528, 'train_ucc_acc': 0.90625, 'loss': 0.55091}\n",
            "Step 173800: {'train_ae_loss': 0.69374, 'train_ucc_loss': 0.53754, 'train_ucc_acc': 0.78125, 'loss': 0.61564}\n",
            "Step 173820: {'train_ae_loss': 0.69051, 'train_ucc_loss': 0.41506, 'train_ucc_acc': 0.90625, 'loss': 0.55278}\n",
            "Step 173840: {'train_ae_loss': 0.67498, 'train_ucc_loss': 0.49213, 'train_ucc_acc': 0.8125, 'loss': 0.58356}\n",
            "Step 173860: {'train_ae_loss': 0.66392, 'train_ucc_loss': 0.48475, 'train_ucc_acc': 0.8125, 'loss': 0.57433}\n",
            "Step 173880: {'train_ae_loss': 0.67209, 'train_ucc_loss': 0.47085, 'train_ucc_acc': 0.84375, 'loss': 0.57147}\n",
            "Step 173900: {'train_ae_loss': 0.67967, 'train_ucc_loss': 0.39886, 'train_ucc_acc': 0.90625, 'loss': 0.53927}\n",
            "Step 173920: {'train_ae_loss': 0.66328, 'train_ucc_loss': 0.40158, 'train_ucc_acc': 0.9375, 'loss': 0.53243}\n",
            "Step 173940: {'train_ae_loss': 0.69126, 'train_ucc_loss': 0.43748, 'train_ucc_acc': 0.84375, 'loss': 0.56437}\n",
            "Step 173960: {'train_ae_loss': 0.67741, 'train_ucc_loss': 0.51847, 'train_ucc_acc': 0.78125, 'loss': 0.59794}\n",
            "Step 173980: {'train_ae_loss': 0.6741, 'train_ucc_loss': 0.52313, 'train_ucc_acc': 0.78125, 'loss': 0.59861}\n",
            "Step 174000: {'train_ae_loss': 0.68593, 'train_ucc_loss': 0.41001, 'train_ucc_acc': 0.90625, 'loss': 0.54797}\n",
            "step: 174000,eval_ae_loss: 0.67695,eval_ucc_loss: 0.46118,eval_ucc_acc: 0.83789\n",
            "Step 174020: {'train_ae_loss': 0.68571, 'train_ucc_loss': 0.45209, 'train_ucc_acc': 0.875, 'loss': 0.5689}\n",
            "Step 174040: {'train_ae_loss': 0.6706, 'train_ucc_loss': 0.39594, 'train_ucc_acc': 0.9375, 'loss': 0.53327}\n",
            "Step 174060: {'train_ae_loss': 0.67302, 'train_ucc_loss': 0.33137, 'train_ucc_acc': 1.0, 'loss': 0.5022}\n",
            "Step 174080: {'train_ae_loss': 0.6704, 'train_ucc_loss': 0.34448, 'train_ucc_acc': 0.96875, 'loss': 0.50744}\n",
            "Step 174100: {'train_ae_loss': 0.69977, 'train_ucc_loss': 0.4304, 'train_ucc_acc': 0.84375, 'loss': 0.56509}\n",
            "Step 174120: {'train_ae_loss': 0.69237, 'train_ucc_loss': 0.42924, 'train_ucc_acc': 0.90625, 'loss': 0.56081}\n",
            "Step 174140: {'train_ae_loss': 0.66006, 'train_ucc_loss': 0.50353, 'train_ucc_acc': 0.78125, 'loss': 0.58179}\n",
            "Step 174160: {'train_ae_loss': 0.68314, 'train_ucc_loss': 0.37085, 'train_ucc_acc': 0.96875, 'loss': 0.527}\n",
            "Step 174180: {'train_ae_loss': 0.65793, 'train_ucc_loss': 0.47738, 'train_ucc_acc': 0.8125, 'loss': 0.56766}\n",
            "Step 174200: {'train_ae_loss': 0.67947, 'train_ucc_loss': 0.44011, 'train_ucc_acc': 0.875, 'loss': 0.55979}\n",
            "Step 174220: {'train_ae_loss': 0.67359, 'train_ucc_loss': 0.45918, 'train_ucc_acc': 0.875, 'loss': 0.56639}\n",
            "Step 174240: {'train_ae_loss': 0.67641, 'train_ucc_loss': 0.39885, 'train_ucc_acc': 0.9375, 'loss': 0.53763}\n",
            "Step 174260: {'train_ae_loss': 0.67258, 'train_ucc_loss': 0.36038, 'train_ucc_acc': 0.96875, 'loss': 0.51648}\n",
            "Step 174280: {'train_ae_loss': 0.67227, 'train_ucc_loss': 0.45241, 'train_ucc_acc': 0.84375, 'loss': 0.56234}\n",
            "Step 174300: {'train_ae_loss': 0.67369, 'train_ucc_loss': 0.4549, 'train_ucc_acc': 0.875, 'loss': 0.56429}\n",
            "Step 174320: {'train_ae_loss': 0.66853, 'train_ucc_loss': 0.38671, 'train_ucc_acc': 0.9375, 'loss': 0.52762}\n",
            "Step 174340: {'train_ae_loss': 0.65812, 'train_ucc_loss': 0.35649, 'train_ucc_acc': 0.96875, 'loss': 0.5073}\n",
            "Step 174360: {'train_ae_loss': 0.66468, 'train_ucc_loss': 0.48827, 'train_ucc_acc': 0.8125, 'loss': 0.57648}\n",
            "Step 174380: {'train_ae_loss': 0.67338, 'train_ucc_loss': 0.4174, 'train_ucc_acc': 0.875, 'loss': 0.54539}\n",
            "Step 174400: {'train_ae_loss': 0.66092, 'train_ucc_loss': 0.51946, 'train_ucc_acc': 0.8125, 'loss': 0.59019}\n",
            "Step 174420: {'train_ae_loss': 0.6748, 'train_ucc_loss': 0.48393, 'train_ucc_acc': 0.84375, 'loss': 0.57936}\n",
            "Step 174440: {'train_ae_loss': 0.6719, 'train_ucc_loss': 0.43984, 'train_ucc_acc': 0.8125, 'loss': 0.55587}\n",
            "Step 174460: {'train_ae_loss': 0.65721, 'train_ucc_loss': 0.38302, 'train_ucc_acc': 0.9375, 'loss': 0.52012}\n",
            "Step 174480: {'train_ae_loss': 0.68501, 'train_ucc_loss': 0.43414, 'train_ucc_acc': 0.875, 'loss': 0.55958}\n",
            "Step 174500: {'train_ae_loss': 0.67509, 'train_ucc_loss': 0.44202, 'train_ucc_acc': 0.875, 'loss': 0.55855}\n",
            "Step 174520: {'train_ae_loss': 0.66986, 'train_ucc_loss': 0.44306, 'train_ucc_acc': 0.875, 'loss': 0.55646}\n",
            "Step 174540: {'train_ae_loss': 0.67168, 'train_ucc_loss': 0.54652, 'train_ucc_acc': 0.78125, 'loss': 0.6091}\n",
            "Step 174560: {'train_ae_loss': 0.6638, 'train_ucc_loss': 0.3737, 'train_ucc_acc': 0.9375, 'loss': 0.51875}\n",
            "Step 174580: {'train_ae_loss': 0.66306, 'train_ucc_loss': 0.50064, 'train_ucc_acc': 0.8125, 'loss': 0.58185}\n",
            "Step 174600: {'train_ae_loss': 0.64815, 'train_ucc_loss': 0.53914, 'train_ucc_acc': 0.75, 'loss': 0.59364}\n",
            "Step 174620: {'train_ae_loss': 0.64884, 'train_ucc_loss': 0.49444, 'train_ucc_acc': 0.84375, 'loss': 0.57164}\n",
            "Step 174640: {'train_ae_loss': 0.66076, 'train_ucc_loss': 0.41896, 'train_ucc_acc': 0.90625, 'loss': 0.53986}\n",
            "Step 174660: {'train_ae_loss': 0.66876, 'train_ucc_loss': 0.48849, 'train_ucc_acc': 0.8125, 'loss': 0.57862}\n",
            "Step 174680: {'train_ae_loss': 0.67029, 'train_ucc_loss': 0.40856, 'train_ucc_acc': 0.9375, 'loss': 0.53943}\n",
            "Step 174700: {'train_ae_loss': 0.6702, 'train_ucc_loss': 0.38743, 'train_ucc_acc': 0.9375, 'loss': 0.52882}\n",
            "Step 174720: {'train_ae_loss': 0.66425, 'train_ucc_loss': 0.43037, 'train_ucc_acc': 0.90625, 'loss': 0.54731}\n",
            "Step 174740: {'train_ae_loss': 0.66447, 'train_ucc_loss': 0.40498, 'train_ucc_acc': 0.90625, 'loss': 0.53473}\n",
            "Step 174760: {'train_ae_loss': 0.65675, 'train_ucc_loss': 0.43439, 'train_ucc_acc': 0.90625, 'loss': 0.54557}\n",
            "Step 174780: {'train_ae_loss': 0.66169, 'train_ucc_loss': 0.40159, 'train_ucc_acc': 0.90625, 'loss': 0.53164}\n",
            "Step 174800: {'train_ae_loss': 0.67926, 'train_ucc_loss': 0.43411, 'train_ucc_acc': 0.875, 'loss': 0.55668}\n",
            "Step 174820: {'train_ae_loss': 0.66763, 'train_ucc_loss': 0.5544, 'train_ucc_acc': 0.71875, 'loss': 0.61102}\n",
            "Step 174840: {'train_ae_loss': 0.6556, 'train_ucc_loss': 0.46121, 'train_ucc_acc': 0.84375, 'loss': 0.55841}\n",
            "Step 174860: {'train_ae_loss': 0.67181, 'train_ucc_loss': 0.51828, 'train_ucc_acc': 0.78125, 'loss': 0.59504}\n",
            "Step 174880: {'train_ae_loss': 0.65919, 'train_ucc_loss': 0.3415, 'train_ucc_acc': 1.0, 'loss': 0.50034}\n",
            "Step 174900: {'train_ae_loss': 0.65968, 'train_ucc_loss': 0.38074, 'train_ucc_acc': 0.9375, 'loss': 0.52021}\n",
            "Step 174920: {'train_ae_loss': 0.66973, 'train_ucc_loss': 0.46372, 'train_ucc_acc': 0.875, 'loss': 0.56673}\n",
            "Step 174940: {'train_ae_loss': 0.66485, 'train_ucc_loss': 0.37103, 'train_ucc_acc': 0.90625, 'loss': 0.51794}\n",
            "Step 174960: {'train_ae_loss': 0.66017, 'train_ucc_loss': 0.44604, 'train_ucc_acc': 0.875, 'loss': 0.55311}\n",
            "Step 174980: {'train_ae_loss': 0.65517, 'train_ucc_loss': 0.43589, 'train_ucc_acc': 0.875, 'loss': 0.54553}\n",
            "Step 175000: {'train_ae_loss': 0.65798, 'train_ucc_loss': 0.42857, 'train_ucc_acc': 0.875, 'loss': 0.54327}\n",
            "step: 175000,eval_ae_loss: 0.65214,eval_ucc_loss: 0.49192,eval_ucc_acc: 0.81348\n",
            "Step 175020: {'train_ae_loss': 0.6526, 'train_ucc_loss': 0.4206, 'train_ucc_acc': 0.875, 'loss': 0.5366}\n",
            "Step 175040: {'train_ae_loss': 0.65953, 'train_ucc_loss': 0.46968, 'train_ucc_acc': 0.84375, 'loss': 0.5646}\n",
            "Step 175060: {'train_ae_loss': 0.66133, 'train_ucc_loss': 0.54418, 'train_ucc_acc': 0.78125, 'loss': 0.60275}\n",
            "Step 175080: {'train_ae_loss': 0.65096, 'train_ucc_loss': 0.38793, 'train_ucc_acc': 0.9375, 'loss': 0.51945}\n",
            "Step 175100: {'train_ae_loss': 0.66607, 'train_ucc_loss': 0.43549, 'train_ucc_acc': 0.875, 'loss': 0.55078}\n",
            "Step 175120: {'train_ae_loss': 0.66493, 'train_ucc_loss': 0.45652, 'train_ucc_acc': 0.84375, 'loss': 0.56072}\n",
            "Step 175140: {'train_ae_loss': 0.67045, 'train_ucc_loss': 0.46627, 'train_ucc_acc': 0.84375, 'loss': 0.56836}\n",
            "Step 175160: {'train_ae_loss': 0.669, 'train_ucc_loss': 0.40443, 'train_ucc_acc': 0.90625, 'loss': 0.53672}\n",
            "Step 175180: {'train_ae_loss': 0.67111, 'train_ucc_loss': 0.46029, 'train_ucc_acc': 0.84375, 'loss': 0.5657}\n",
            "Step 175200: {'train_ae_loss': 0.66038, 'train_ucc_loss': 0.4119, 'train_ucc_acc': 0.90625, 'loss': 0.53614}\n",
            "Step 175220: {'train_ae_loss': 0.66944, 'train_ucc_loss': 0.47149, 'train_ucc_acc': 0.8125, 'loss': 0.57046}\n",
            "Step 175240: {'train_ae_loss': 0.66532, 'train_ucc_loss': 0.40124, 'train_ucc_acc': 0.90625, 'loss': 0.53328}\n",
            "Step 175260: {'train_ae_loss': 0.67759, 'train_ucc_loss': 0.40704, 'train_ucc_acc': 0.90625, 'loss': 0.54231}\n",
            "Step 175280: {'train_ae_loss': 0.68862, 'train_ucc_loss': 0.43057, 'train_ucc_acc': 0.90625, 'loss': 0.5596}\n",
            "Step 175300: {'train_ae_loss': 0.66533, 'train_ucc_loss': 0.44712, 'train_ucc_acc': 0.875, 'loss': 0.55623}\n",
            "Step 175320: {'train_ae_loss': 0.68771, 'train_ucc_loss': 0.35526, 'train_ucc_acc': 0.96875, 'loss': 0.52148}\n",
            "Step 175340: {'train_ae_loss': 0.6557, 'train_ucc_loss': 0.43152, 'train_ucc_acc': 0.875, 'loss': 0.54361}\n",
            "Step 175360: {'train_ae_loss': 0.65634, 'train_ucc_loss': 0.39997, 'train_ucc_acc': 0.90625, 'loss': 0.52816}\n",
            "Step 175380: {'train_ae_loss': 0.65597, 'train_ucc_loss': 0.35108, 'train_ucc_acc': 0.96875, 'loss': 0.50353}\n",
            "Step 175400: {'train_ae_loss': 0.66387, 'train_ucc_loss': 0.40803, 'train_ucc_acc': 0.90625, 'loss': 0.53595}\n",
            "Step 175420: {'train_ae_loss': 0.6603, 'train_ucc_loss': 0.4541, 'train_ucc_acc': 0.84375, 'loss': 0.5572}\n",
            "Step 175440: {'train_ae_loss': 0.64551, 'train_ucc_loss': 0.47064, 'train_ucc_acc': 0.84375, 'loss': 0.55807}\n",
            "Step 175460: {'train_ae_loss': 0.67572, 'train_ucc_loss': 0.37537, 'train_ucc_acc': 0.9375, 'loss': 0.52554}\n",
            "Step 175480: {'train_ae_loss': 0.65929, 'train_ucc_loss': 0.38946, 'train_ucc_acc': 0.9375, 'loss': 0.52437}\n",
            "Step 175500: {'train_ae_loss': 0.65626, 'train_ucc_loss': 0.42389, 'train_ucc_acc': 0.90625, 'loss': 0.54008}\n",
            "Step 175520: {'train_ae_loss': 0.66233, 'train_ucc_loss': 0.46393, 'train_ucc_acc': 0.875, 'loss': 0.56313}\n",
            "Step 175540: {'train_ae_loss': 0.65147, 'train_ucc_loss': 0.45546, 'train_ucc_acc': 0.84375, 'loss': 0.55346}\n",
            "Step 175560: {'train_ae_loss': 0.66223, 'train_ucc_loss': 0.36096, 'train_ucc_acc': 0.90625, 'loss': 0.5116}\n",
            "Step 175580: {'train_ae_loss': 0.64405, 'train_ucc_loss': 0.41246, 'train_ucc_acc': 0.90625, 'loss': 0.52826}\n",
            "Step 175600: {'train_ae_loss': 0.64668, 'train_ucc_loss': 0.4277, 'train_ucc_acc': 0.90625, 'loss': 0.53719}\n",
            "Step 175620: {'train_ae_loss': 0.66016, 'train_ucc_loss': 0.42218, 'train_ucc_acc': 0.90625, 'loss': 0.54117}\n",
            "Step 175640: {'train_ae_loss': 0.65851, 'train_ucc_loss': 0.48527, 'train_ucc_acc': 0.8125, 'loss': 0.57189}\n",
            "Step 175660: {'train_ae_loss': 0.65515, 'train_ucc_loss': 0.51081, 'train_ucc_acc': 0.75, 'loss': 0.58298}\n",
            "Step 175680: {'train_ae_loss': 0.64972, 'train_ucc_loss': 0.35172, 'train_ucc_acc': 0.96875, 'loss': 0.50072}\n",
            "Step 175700: {'train_ae_loss': 0.65948, 'train_ucc_loss': 0.42387, 'train_ucc_acc': 0.875, 'loss': 0.54167}\n",
            "Step 175720: {'train_ae_loss': 0.67433, 'train_ucc_loss': 0.44171, 'train_ucc_acc': 0.875, 'loss': 0.55802}\n",
            "Step 175740: {'train_ae_loss': 0.68435, 'train_ucc_loss': 0.44214, 'train_ucc_acc': 0.875, 'loss': 0.56325}\n",
            "Step 175760: {'train_ae_loss': 0.65506, 'train_ucc_loss': 0.40639, 'train_ucc_acc': 0.90625, 'loss': 0.53073}\n",
            "Step 175780: {'train_ae_loss': 0.66164, 'train_ucc_loss': 0.5273, 'train_ucc_acc': 0.78125, 'loss': 0.59447}\n",
            "Step 175800: {'train_ae_loss': 0.64653, 'train_ucc_loss': 0.53482, 'train_ucc_acc': 0.78125, 'loss': 0.59067}\n",
            "Step 175820: {'train_ae_loss': 0.66054, 'train_ucc_loss': 0.53465, 'train_ucc_acc': 0.78125, 'loss': 0.5976}\n",
            "Step 175840: {'train_ae_loss': 0.67519, 'train_ucc_loss': 0.43403, 'train_ucc_acc': 0.875, 'loss': 0.55461}\n",
            "Step 175860: {'train_ae_loss': 0.67541, 'train_ucc_loss': 0.44139, 'train_ucc_acc': 0.84375, 'loss': 0.5584}\n",
            "Step 175880: {'train_ae_loss': 0.66426, 'train_ucc_loss': 0.47786, 'train_ucc_acc': 0.8125, 'loss': 0.57106}\n",
            "Step 175900: {'train_ae_loss': 0.66382, 'train_ucc_loss': 0.50664, 'train_ucc_acc': 0.8125, 'loss': 0.58523}\n",
            "Step 175920: {'train_ae_loss': 0.64223, 'train_ucc_loss': 0.46732, 'train_ucc_acc': 0.8125, 'loss': 0.55478}\n",
            "Step 175940: {'train_ae_loss': 0.67121, 'train_ucc_loss': 0.42531, 'train_ucc_acc': 0.90625, 'loss': 0.54826}\n",
            "Step 175960: {'train_ae_loss': 0.65387, 'train_ucc_loss': 0.48105, 'train_ucc_acc': 0.8125, 'loss': 0.56746}\n",
            "Step 175980: {'train_ae_loss': 0.66213, 'train_ucc_loss': 0.41987, 'train_ucc_acc': 0.875, 'loss': 0.541}\n",
            "Step 176000: {'train_ae_loss': 0.65057, 'train_ucc_loss': 0.42507, 'train_ucc_acc': 0.875, 'loss': 0.53782}\n",
            "step: 176000,eval_ae_loss: 0.65267,eval_ucc_loss: 0.47167,eval_ucc_acc: 0.83594\n",
            "Step 176020: {'train_ae_loss': 0.65181, 'train_ucc_loss': 0.51335, 'train_ucc_acc': 0.75, 'loss': 0.58258}\n",
            "Step 176040: {'train_ae_loss': 0.65566, 'train_ucc_loss': 0.4045, 'train_ucc_acc': 0.90625, 'loss': 0.53008}\n",
            "Step 176060: {'train_ae_loss': 0.64573, 'train_ucc_loss': 0.48846, 'train_ucc_acc': 0.84375, 'loss': 0.5671}\n",
            "Step 176080: {'train_ae_loss': 0.66184, 'train_ucc_loss': 0.4351, 'train_ucc_acc': 0.875, 'loss': 0.54847}\n",
            "Step 176100: {'train_ae_loss': 0.667, 'train_ucc_loss': 0.35478, 'train_ucc_acc': 0.96875, 'loss': 0.51089}\n",
            "Step 176120: {'train_ae_loss': 0.65143, 'train_ucc_loss': 0.47323, 'train_ucc_acc': 0.84375, 'loss': 0.56233}\n",
            "Step 176140: {'train_ae_loss': 0.67069, 'train_ucc_loss': 0.51762, 'train_ucc_acc': 0.78125, 'loss': 0.59415}\n",
            "Step 176160: {'train_ae_loss': 0.65573, 'train_ucc_loss': 0.40438, 'train_ucc_acc': 0.90625, 'loss': 0.53006}\n",
            "Step 176180: {'train_ae_loss': 0.66849, 'train_ucc_loss': 0.52306, 'train_ucc_acc': 0.78125, 'loss': 0.59578}\n",
            "Step 176200: {'train_ae_loss': 0.65702, 'train_ucc_loss': 0.41791, 'train_ucc_acc': 0.90625, 'loss': 0.53746}\n",
            "Step 176220: {'train_ae_loss': 0.66541, 'train_ucc_loss': 0.47155, 'train_ucc_acc': 0.84375, 'loss': 0.56848}\n",
            "Step 176240: {'train_ae_loss': 0.65753, 'train_ucc_loss': 0.51242, 'train_ucc_acc': 0.78125, 'loss': 0.58498}\n",
            "Step 176260: {'train_ae_loss': 0.65004, 'train_ucc_loss': 0.36928, 'train_ucc_acc': 0.9375, 'loss': 0.50966}\n",
            "Step 176280: {'train_ae_loss': 0.64745, 'train_ucc_loss': 0.4347, 'train_ucc_acc': 0.875, 'loss': 0.54107}\n",
            "Step 176300: {'train_ae_loss': 0.66601, 'train_ucc_loss': 0.38591, 'train_ucc_acc': 0.9375, 'loss': 0.52596}\n",
            "Step 176320: {'train_ae_loss': 0.67495, 'train_ucc_loss': 0.40789, 'train_ucc_acc': 0.90625, 'loss': 0.54142}\n",
            "Step 176340: {'train_ae_loss': 0.65747, 'train_ucc_loss': 0.51529, 'train_ucc_acc': 0.75, 'loss': 0.58638}\n",
            "Step 176360: {'train_ae_loss': 0.65709, 'train_ucc_loss': 0.41908, 'train_ucc_acc': 0.90625, 'loss': 0.53808}\n",
            "Step 176380: {'train_ae_loss': 0.6516, 'train_ucc_loss': 0.57143, 'train_ucc_acc': 0.75, 'loss': 0.61151}\n",
            "Step 176400: {'train_ae_loss': 0.66086, 'train_ucc_loss': 0.43001, 'train_ucc_acc': 0.84375, 'loss': 0.54544}\n",
            "Step 176420: {'train_ae_loss': 0.67239, 'train_ucc_loss': 0.40328, 'train_ucc_acc': 0.90625, 'loss': 0.53783}\n",
            "Step 176440: {'train_ae_loss': 0.67006, 'train_ucc_loss': 0.41738, 'train_ucc_acc': 0.875, 'loss': 0.54372}\n",
            "Step 176460: {'train_ae_loss': 0.66607, 'train_ucc_loss': 0.46326, 'train_ucc_acc': 0.8125, 'loss': 0.56466}\n",
            "Step 176480: {'train_ae_loss': 0.67507, 'train_ucc_loss': 0.3839, 'train_ucc_acc': 0.9375, 'loss': 0.52948}\n",
            "Step 176500: {'train_ae_loss': 0.65493, 'train_ucc_loss': 0.41708, 'train_ucc_acc': 0.875, 'loss': 0.536}\n",
            "Step 176520: {'train_ae_loss': 0.67405, 'train_ucc_loss': 0.37023, 'train_ucc_acc': 0.9375, 'loss': 0.52214}\n",
            "Step 176540: {'train_ae_loss': 0.64859, 'train_ucc_loss': 0.43993, 'train_ucc_acc': 0.875, 'loss': 0.54426}\n",
            "Step 176560: {'train_ae_loss': 0.68099, 'train_ucc_loss': 0.42552, 'train_ucc_acc': 0.875, 'loss': 0.55325}\n",
            "Step 176580: {'train_ae_loss': 0.66591, 'train_ucc_loss': 0.38459, 'train_ucc_acc': 0.9375, 'loss': 0.52525}\n",
            "Step 176600: {'train_ae_loss': 0.66562, 'train_ucc_loss': 0.48816, 'train_ucc_acc': 0.8125, 'loss': 0.57689}\n",
            "Step 176620: {'train_ae_loss': 0.66231, 'train_ucc_loss': 0.42513, 'train_ucc_acc': 0.90625, 'loss': 0.54372}\n",
            "Step 176640: {'train_ae_loss': 0.67882, 'train_ucc_loss': 0.40995, 'train_ucc_acc': 0.875, 'loss': 0.54439}\n",
            "Step 176660: {'train_ae_loss': 0.66411, 'train_ucc_loss': 0.51869, 'train_ucc_acc': 0.75, 'loss': 0.5914}\n",
            "Step 176680: {'train_ae_loss': 0.66742, 'train_ucc_loss': 0.41322, 'train_ucc_acc': 0.90625, 'loss': 0.54032}\n",
            "Step 176700: {'train_ae_loss': 0.66662, 'train_ucc_loss': 0.40153, 'train_ucc_acc': 0.90625, 'loss': 0.53407}\n",
            "Step 176720: {'train_ae_loss': 0.65336, 'train_ucc_loss': 0.5221, 'train_ucc_acc': 0.78125, 'loss': 0.58773}\n",
            "Step 176740: {'train_ae_loss': 0.67286, 'train_ucc_loss': 0.47575, 'train_ucc_acc': 0.84375, 'loss': 0.57431}\n",
            "Step 176760: {'train_ae_loss': 0.64557, 'train_ucc_loss': 0.45471, 'train_ucc_acc': 0.875, 'loss': 0.55014}\n",
            "Step 176780: {'train_ae_loss': 0.65095, 'train_ucc_loss': 0.47282, 'train_ucc_acc': 0.8125, 'loss': 0.56189}\n",
            "Step 176800: {'train_ae_loss': 0.62517, 'train_ucc_loss': 0.36165, 'train_ucc_acc': 0.96875, 'loss': 0.49341}\n",
            "Step 176820: {'train_ae_loss': 0.66279, 'train_ucc_loss': 0.44315, 'train_ucc_acc': 0.875, 'loss': 0.55297}\n",
            "Step 176840: {'train_ae_loss': 0.66329, 'train_ucc_loss': 0.33225, 'train_ucc_acc': 1.0, 'loss': 0.49777}\n",
            "Step 176860: {'train_ae_loss': 0.66105, 'train_ucc_loss': 0.3584, 'train_ucc_acc': 0.9375, 'loss': 0.50972}\n",
            "Step 176880: {'train_ae_loss': 0.64981, 'train_ucc_loss': 0.38723, 'train_ucc_acc': 0.90625, 'loss': 0.51852}\n",
            "Step 176900: {'train_ae_loss': 0.63901, 'train_ucc_loss': 0.36124, 'train_ucc_acc': 0.9375, 'loss': 0.50013}\n",
            "Step 176920: {'train_ae_loss': 0.66115, 'train_ucc_loss': 0.3988, 'train_ucc_acc': 0.90625, 'loss': 0.52997}\n",
            "Step 176940: {'train_ae_loss': 0.6639, 'train_ucc_loss': 0.39679, 'train_ucc_acc': 0.9375, 'loss': 0.53034}\n",
            "Step 176960: {'train_ae_loss': 0.64622, 'train_ucc_loss': 0.43338, 'train_ucc_acc': 0.875, 'loss': 0.5398}\n",
            "Step 176980: {'train_ae_loss': 0.66989, 'train_ucc_loss': 0.4083, 'train_ucc_acc': 0.90625, 'loss': 0.53909}\n",
            "Step 177000: {'train_ae_loss': 0.66055, 'train_ucc_loss': 0.40562, 'train_ucc_acc': 0.90625, 'loss': 0.53308}\n",
            "step: 177000,eval_ae_loss: 0.64531,eval_ucc_loss: 0.46723,eval_ucc_acc: 0.83691\n",
            "Step 177020: {'train_ae_loss': 0.6637, 'train_ucc_loss': 0.42155, 'train_ucc_acc': 0.84375, 'loss': 0.54263}\n",
            "Step 177040: {'train_ae_loss': 0.6405, 'train_ucc_loss': 0.35891, 'train_ucc_acc': 0.96875, 'loss': 0.4997}\n",
            "Step 177060: {'train_ae_loss': 0.64486, 'train_ucc_loss': 0.45865, 'train_ucc_acc': 0.84375, 'loss': 0.55175}\n",
            "Step 177080: {'train_ae_loss': 0.67674, 'train_ucc_loss': 0.34417, 'train_ucc_acc': 0.96875, 'loss': 0.51045}\n",
            "Step 177100: {'train_ae_loss': 0.65242, 'train_ucc_loss': 0.42432, 'train_ucc_acc': 0.875, 'loss': 0.53837}\n",
            "Step 177120: {'train_ae_loss': 0.67286, 'train_ucc_loss': 0.50212, 'train_ucc_acc': 0.75, 'loss': 0.58749}\n",
            "Step 177140: {'train_ae_loss': 0.66132, 'train_ucc_loss': 0.45237, 'train_ucc_acc': 0.875, 'loss': 0.55685}\n",
            "Step 177160: {'train_ae_loss': 0.64683, 'train_ucc_loss': 0.41648, 'train_ucc_acc': 0.90625, 'loss': 0.53166}\n",
            "Step 177180: {'train_ae_loss': 0.65708, 'train_ucc_loss': 0.48503, 'train_ucc_acc': 0.84375, 'loss': 0.57106}\n",
            "Step 177200: {'train_ae_loss': 0.67114, 'train_ucc_loss': 0.37801, 'train_ucc_acc': 0.9375, 'loss': 0.52457}\n",
            "Step 177220: {'train_ae_loss': 0.65696, 'train_ucc_loss': 0.46411, 'train_ucc_acc': 0.84375, 'loss': 0.56054}\n",
            "Step 177240: {'train_ae_loss': 0.6602, 'train_ucc_loss': 0.54168, 'train_ucc_acc': 0.78125, 'loss': 0.60094}\n",
            "Step 177260: {'train_ae_loss': 0.6582, 'train_ucc_loss': 0.38993, 'train_ucc_acc': 0.90625, 'loss': 0.52406}\n",
            "Step 177280: {'train_ae_loss': 0.65853, 'train_ucc_loss': 0.46663, 'train_ucc_acc': 0.84375, 'loss': 0.56258}\n",
            "Step 177300: {'train_ae_loss': 0.64288, 'train_ucc_loss': 0.51607, 'train_ucc_acc': 0.78125, 'loss': 0.57948}\n",
            "Step 177320: {'train_ae_loss': 0.65324, 'train_ucc_loss': 0.47728, 'train_ucc_acc': 0.8125, 'loss': 0.56526}\n",
            "Step 177340: {'train_ae_loss': 0.67138, 'train_ucc_loss': 0.5095, 'train_ucc_acc': 0.78125, 'loss': 0.59044}\n",
            "Step 177360: {'train_ae_loss': 0.66112, 'train_ucc_loss': 0.3316, 'train_ucc_acc': 0.96875, 'loss': 0.49636}\n",
            "Step 177380: {'train_ae_loss': 0.6539, 'train_ucc_loss': 0.45421, 'train_ucc_acc': 0.84375, 'loss': 0.55406}\n",
            "Step 177400: {'train_ae_loss': 0.66649, 'train_ucc_loss': 0.49036, 'train_ucc_acc': 0.84375, 'loss': 0.57843}\n",
            "Step 177420: {'train_ae_loss': 0.66486, 'train_ucc_loss': 0.48193, 'train_ucc_acc': 0.84375, 'loss': 0.5734}\n",
            "Step 177440: {'train_ae_loss': 0.66955, 'train_ucc_loss': 0.49337, 'train_ucc_acc': 0.8125, 'loss': 0.58146}\n",
            "Step 177460: {'train_ae_loss': 0.65659, 'train_ucc_loss': 0.36037, 'train_ucc_acc': 0.96875, 'loss': 0.50848}\n",
            "Step 177480: {'train_ae_loss': 0.65074, 'train_ucc_loss': 0.50743, 'train_ucc_acc': 0.8125, 'loss': 0.57908}\n",
            "Step 177500: {'train_ae_loss': 0.65778, 'train_ucc_loss': 0.42255, 'train_ucc_acc': 0.875, 'loss': 0.54017}\n",
            "Step 177520: {'train_ae_loss': 0.66066, 'train_ucc_loss': 0.50799, 'train_ucc_acc': 0.78125, 'loss': 0.58432}\n",
            "Step 177540: {'train_ae_loss': 0.65824, 'train_ucc_loss': 0.47415, 'train_ucc_acc': 0.84375, 'loss': 0.5662}\n",
            "Step 177560: {'train_ae_loss': 0.64334, 'train_ucc_loss': 0.47158, 'train_ucc_acc': 0.84375, 'loss': 0.55746}\n",
            "Step 177580: {'train_ae_loss': 0.65927, 'train_ucc_loss': 0.51677, 'train_ucc_acc': 0.78125, 'loss': 0.58802}\n",
            "Step 177600: {'train_ae_loss': 0.66112, 'train_ucc_loss': 0.38242, 'train_ucc_acc': 0.90625, 'loss': 0.52177}\n",
            "Step 177620: {'train_ae_loss': 0.6667, 'train_ucc_loss': 0.3711, 'train_ucc_acc': 0.9375, 'loss': 0.5189}\n",
            "Step 177640: {'train_ae_loss': 0.66082, 'train_ucc_loss': 0.39122, 'train_ucc_acc': 0.9375, 'loss': 0.52602}\n",
            "Step 177660: {'train_ae_loss': 0.65272, 'train_ucc_loss': 0.3715, 'train_ucc_acc': 0.9375, 'loss': 0.51211}\n",
            "Step 177680: {'train_ae_loss': 0.65664, 'train_ucc_loss': 0.41078, 'train_ucc_acc': 0.90625, 'loss': 0.53371}\n",
            "Step 177700: {'train_ae_loss': 0.64064, 'train_ucc_loss': 0.49794, 'train_ucc_acc': 0.8125, 'loss': 0.56929}\n",
            "Step 177720: {'train_ae_loss': 0.65367, 'train_ucc_loss': 0.41694, 'train_ucc_acc': 0.90625, 'loss': 0.5353}\n",
            "Step 177740: {'train_ae_loss': 0.65365, 'train_ucc_loss': 0.46825, 'train_ucc_acc': 0.875, 'loss': 0.56095}\n",
            "Step 177760: {'train_ae_loss': 0.65615, 'train_ucc_loss': 0.39439, 'train_ucc_acc': 0.90625, 'loss': 0.52527}\n",
            "Step 177780: {'train_ae_loss': 0.65612, 'train_ucc_loss': 0.38038, 'train_ucc_acc': 0.9375, 'loss': 0.51825}\n",
            "Step 177800: {'train_ae_loss': 0.68398, 'train_ucc_loss': 0.47511, 'train_ucc_acc': 0.84375, 'loss': 0.57955}\n",
            "Step 177820: {'train_ae_loss': 0.65149, 'train_ucc_loss': 0.40571, 'train_ucc_acc': 0.875, 'loss': 0.5286}\n",
            "Step 177840: {'train_ae_loss': 0.65274, 'train_ucc_loss': 0.46211, 'train_ucc_acc': 0.8125, 'loss': 0.55742}\n",
            "Step 177860: {'train_ae_loss': 0.66505, 'train_ucc_loss': 0.32072, 'train_ucc_acc': 1.0, 'loss': 0.49288}\n",
            "Step 177880: {'train_ae_loss': 0.64709, 'train_ucc_loss': 0.44113, 'train_ucc_acc': 0.875, 'loss': 0.54411}\n",
            "Step 177900: {'train_ae_loss': 0.66227, 'train_ucc_loss': 0.403, 'train_ucc_acc': 0.90625, 'loss': 0.53264}\n",
            "Step 177920: {'train_ae_loss': 0.66146, 'train_ucc_loss': 0.51576, 'train_ucc_acc': 0.75, 'loss': 0.58861}\n",
            "Step 177940: {'train_ae_loss': 0.64893, 'train_ucc_loss': 0.5056, 'train_ucc_acc': 0.8125, 'loss': 0.57727}\n",
            "Step 177960: {'train_ae_loss': 0.65113, 'train_ucc_loss': 0.4057, 'train_ucc_acc': 0.90625, 'loss': 0.52842}\n",
            "Step 177980: {'train_ae_loss': 0.64859, 'train_ucc_loss': 0.49199, 'train_ucc_acc': 0.8125, 'loss': 0.57029}\n",
            "Step 178000: {'train_ae_loss': 0.65915, 'train_ucc_loss': 0.47608, 'train_ucc_acc': 0.8125, 'loss': 0.56761}\n",
            "step: 178000,eval_ae_loss: 0.64729,eval_ucc_loss: 0.45111,eval_ucc_acc: 0.85352\n",
            "Step 178020: {'train_ae_loss': 0.65438, 'train_ucc_loss': 0.44069, 'train_ucc_acc': 0.875, 'loss': 0.54753}\n",
            "Step 178040: {'train_ae_loss': 0.64842, 'train_ucc_loss': 0.52947, 'train_ucc_acc': 0.78125, 'loss': 0.58895}\n",
            "Step 178060: {'train_ae_loss': 0.67117, 'train_ucc_loss': 0.39643, 'train_ucc_acc': 0.90625, 'loss': 0.5338}\n",
            "Step 178080: {'train_ae_loss': 0.66909, 'train_ucc_loss': 0.40892, 'train_ucc_acc': 0.90625, 'loss': 0.539}\n",
            "Step 178100: {'train_ae_loss': 0.66092, 'train_ucc_loss': 0.47471, 'train_ucc_acc': 0.84375, 'loss': 0.56782}\n",
            "Step 178120: {'train_ae_loss': 0.66076, 'train_ucc_loss': 0.41535, 'train_ucc_acc': 0.875, 'loss': 0.53805}\n",
            "Step 178140: {'train_ae_loss': 0.65415, 'train_ucc_loss': 0.44303, 'train_ucc_acc': 0.875, 'loss': 0.54859}\n",
            "Step 178160: {'train_ae_loss': 0.65635, 'train_ucc_loss': 0.38768, 'train_ucc_acc': 0.9375, 'loss': 0.52202}\n",
            "Step 178180: {'train_ae_loss': 0.65939, 'train_ucc_loss': 0.39055, 'train_ucc_acc': 0.96875, 'loss': 0.52497}\n",
            "Step 178200: {'train_ae_loss': 0.66174, 'train_ucc_loss': 0.49341, 'train_ucc_acc': 0.84375, 'loss': 0.57758}\n",
            "Step 178220: {'train_ae_loss': 0.66609, 'train_ucc_loss': 0.51592, 'train_ucc_acc': 0.78125, 'loss': 0.591}\n",
            "Step 178240: {'train_ae_loss': 0.65919, 'train_ucc_loss': 0.42289, 'train_ucc_acc': 0.90625, 'loss': 0.54104}\n",
            "Step 178260: {'train_ae_loss': 0.65205, 'train_ucc_loss': 0.47175, 'train_ucc_acc': 0.8125, 'loss': 0.5619}\n",
            "Step 178280: {'train_ae_loss': 0.65589, 'train_ucc_loss': 0.48789, 'train_ucc_acc': 0.8125, 'loss': 0.57189}\n",
            "Step 178300: {'train_ae_loss': 0.65561, 'train_ucc_loss': 0.45432, 'train_ucc_acc': 0.8125, 'loss': 0.55496}\n",
            "Step 178320: {'train_ae_loss': 0.66134, 'train_ucc_loss': 0.41244, 'train_ucc_acc': 0.90625, 'loss': 0.53689}\n",
            "Step 178340: {'train_ae_loss': 0.66895, 'train_ucc_loss': 0.37817, 'train_ucc_acc': 0.9375, 'loss': 0.52356}\n",
            "Step 178360: {'train_ae_loss': 0.66798, 'train_ucc_loss': 0.39507, 'train_ucc_acc': 0.90625, 'loss': 0.53152}\n",
            "Step 178380: {'train_ae_loss': 0.64433, 'train_ucc_loss': 0.48927, 'train_ucc_acc': 0.8125, 'loss': 0.5668}\n",
            "Step 178400: {'train_ae_loss': 0.65005, 'train_ucc_loss': 0.39414, 'train_ucc_acc': 0.90625, 'loss': 0.52209}\n",
            "Step 178420: {'train_ae_loss': 0.67429, 'train_ucc_loss': 0.36887, 'train_ucc_acc': 0.96875, 'loss': 0.52158}\n",
            "Step 178440: {'train_ae_loss': 0.67459, 'train_ucc_loss': 0.49668, 'train_ucc_acc': 0.8125, 'loss': 0.58564}\n",
            "Step 178460: {'train_ae_loss': 0.65728, 'train_ucc_loss': 0.47605, 'train_ucc_acc': 0.84375, 'loss': 0.56666}\n",
            "Step 178480: {'train_ae_loss': 0.66125, 'train_ucc_loss': 0.44822, 'train_ucc_acc': 0.875, 'loss': 0.55473}\n",
            "Step 178500: {'train_ae_loss': 0.65536, 'train_ucc_loss': 0.39439, 'train_ucc_acc': 0.9375, 'loss': 0.52487}\n",
            "Step 178520: {'train_ae_loss': 0.65894, 'train_ucc_loss': 0.50758, 'train_ucc_acc': 0.8125, 'loss': 0.58326}\n",
            "Step 178540: {'train_ae_loss': 0.67594, 'train_ucc_loss': 0.67711, 'train_ucc_acc': 0.53125, 'loss': 0.67652}\n",
            "Step 178560: {'train_ae_loss': 0.64755, 'train_ucc_loss': 0.54863, 'train_ucc_acc': 0.75, 'loss': 0.59809}\n",
            "Step 178580: {'train_ae_loss': 0.65847, 'train_ucc_loss': 0.38527, 'train_ucc_acc': 0.9375, 'loss': 0.52187}\n",
            "Step 178600: {'train_ae_loss': 0.64938, 'train_ucc_loss': 0.43517, 'train_ucc_acc': 0.875, 'loss': 0.54228}\n",
            "Step 178620: {'train_ae_loss': 0.66337, 'train_ucc_loss': 0.39263, 'train_ucc_acc': 0.90625, 'loss': 0.528}\n",
            "Step 178640: {'train_ae_loss': 0.66974, 'train_ucc_loss': 0.5467, 'train_ucc_acc': 0.75, 'loss': 0.60822}\n",
            "Step 178660: {'train_ae_loss': 0.67804, 'train_ucc_loss': 0.45436, 'train_ucc_acc': 0.875, 'loss': 0.5662}\n",
            "Step 178680: {'train_ae_loss': 0.69066, 'train_ucc_loss': 0.47141, 'train_ucc_acc': 0.84375, 'loss': 0.58103}\n",
            "Step 178700: {'train_ae_loss': 0.64458, 'train_ucc_loss': 0.50901, 'train_ucc_acc': 0.78125, 'loss': 0.57679}\n",
            "Step 178720: {'train_ae_loss': 0.64949, 'train_ucc_loss': 0.56354, 'train_ucc_acc': 0.71875, 'loss': 0.60652}\n",
            "Step 178740: {'train_ae_loss': 0.64916, 'train_ucc_loss': 0.43658, 'train_ucc_acc': 0.84375, 'loss': 0.54287}\n",
            "Step 178760: {'train_ae_loss': 0.65163, 'train_ucc_loss': 0.42356, 'train_ucc_acc': 0.84375, 'loss': 0.5376}\n",
            "Step 178780: {'train_ae_loss': 0.65278, 'train_ucc_loss': 0.43315, 'train_ucc_acc': 0.875, 'loss': 0.54297}\n",
            "Step 178800: {'train_ae_loss': 0.68406, 'train_ucc_loss': 0.47251, 'train_ucc_acc': 0.84375, 'loss': 0.57829}\n",
            "Step 178820: {'train_ae_loss': 0.66123, 'train_ucc_loss': 0.42651, 'train_ucc_acc': 0.875, 'loss': 0.54387}\n",
            "Step 178840: {'train_ae_loss': 0.66279, 'train_ucc_loss': 0.35487, 'train_ucc_acc': 0.96875, 'loss': 0.50883}\n",
            "Step 178860: {'train_ae_loss': 0.65886, 'train_ucc_loss': 0.39871, 'train_ucc_acc': 0.90625, 'loss': 0.52878}\n",
            "Step 178880: {'train_ae_loss': 0.67211, 'train_ucc_loss': 0.44793, 'train_ucc_acc': 0.84375, 'loss': 0.56002}\n",
            "Step 178900: {'train_ae_loss': 0.66081, 'train_ucc_loss': 0.5806, 'train_ucc_acc': 0.71875, 'loss': 0.6207}\n",
            "Step 178920: {'train_ae_loss': 0.67867, 'train_ucc_loss': 0.38668, 'train_ucc_acc': 0.90625, 'loss': 0.53267}\n",
            "Step 178940: {'train_ae_loss': 0.67632, 'train_ucc_loss': 0.39874, 'train_ucc_acc': 0.90625, 'loss': 0.53753}\n",
            "Step 178960: {'train_ae_loss': 0.67519, 'train_ucc_loss': 0.38668, 'train_ucc_acc': 0.9375, 'loss': 0.53093}\n",
            "Step 178980: {'train_ae_loss': 0.67389, 'train_ucc_loss': 0.44026, 'train_ucc_acc': 0.875, 'loss': 0.55708}\n",
            "Step 179000: {'train_ae_loss': 0.66743, 'train_ucc_loss': 0.40326, 'train_ucc_acc': 0.875, 'loss': 0.53535}\n",
            "step: 179000,eval_ae_loss: 0.65826,eval_ucc_loss: 0.49705,eval_ucc_acc: 0.80273\n",
            "Step 179020: {'train_ae_loss': 0.66283, 'train_ucc_loss': 0.46793, 'train_ucc_acc': 0.84375, 'loss': 0.56538}\n",
            "Step 179040: {'train_ae_loss': 0.68165, 'train_ucc_loss': 0.47583, 'train_ucc_acc': 0.8125, 'loss': 0.57874}\n",
            "Step 179060: {'train_ae_loss': 0.68019, 'train_ucc_loss': 0.5412, 'train_ucc_acc': 0.78125, 'loss': 0.61069}\n",
            "Step 179080: {'train_ae_loss': 0.67669, 'train_ucc_loss': 0.43186, 'train_ucc_acc': 0.90625, 'loss': 0.55427}\n",
            "Step 179100: {'train_ae_loss': 0.65174, 'train_ucc_loss': 0.3625, 'train_ucc_acc': 0.96875, 'loss': 0.50712}\n",
            "Step 179120: {'train_ae_loss': 0.66212, 'train_ucc_loss': 0.4537, 'train_ucc_acc': 0.875, 'loss': 0.55791}\n",
            "Step 179140: {'train_ae_loss': 0.64333, 'train_ucc_loss': 0.52576, 'train_ucc_acc': 0.78125, 'loss': 0.58454}\n",
            "Step 179160: {'train_ae_loss': 0.66359, 'train_ucc_loss': 0.4386, 'train_ucc_acc': 0.875, 'loss': 0.55109}\n",
            "Step 179180: {'train_ae_loss': 0.67284, 'train_ucc_loss': 0.46751, 'train_ucc_acc': 0.84375, 'loss': 0.57018}\n",
            "Step 179200: {'train_ae_loss': 0.66002, 'train_ucc_loss': 0.45521, 'train_ucc_acc': 0.84375, 'loss': 0.55762}\n",
            "Step 179220: {'train_ae_loss': 0.6787, 'train_ucc_loss': 0.4718, 'train_ucc_acc': 0.84375, 'loss': 0.57525}\n",
            "Step 179240: {'train_ae_loss': 0.65515, 'train_ucc_loss': 0.44667, 'train_ucc_acc': 0.875, 'loss': 0.55091}\n",
            "Step 179260: {'train_ae_loss': 0.65299, 'train_ucc_loss': 0.46814, 'train_ucc_acc': 0.84375, 'loss': 0.56056}\n",
            "Step 179280: {'train_ae_loss': 0.66689, 'train_ucc_loss': 0.41918, 'train_ucc_acc': 0.875, 'loss': 0.54303}\n",
            "Step 179300: {'train_ae_loss': 0.65993, 'train_ucc_loss': 0.49927, 'train_ucc_acc': 0.8125, 'loss': 0.5796}\n",
            "Step 179320: {'train_ae_loss': 0.68454, 'train_ucc_loss': 0.37057, 'train_ucc_acc': 0.9375, 'loss': 0.52756}\n",
            "Step 179340: {'train_ae_loss': 0.65195, 'train_ucc_loss': 0.4244, 'train_ucc_acc': 0.90625, 'loss': 0.53817}\n",
            "Step 179360: {'train_ae_loss': 0.67732, 'train_ucc_loss': 0.41234, 'train_ucc_acc': 0.875, 'loss': 0.54483}\n",
            "Step 179380: {'train_ae_loss': 0.65139, 'train_ucc_loss': 0.3858, 'train_ucc_acc': 0.9375, 'loss': 0.5186}\n",
            "Step 179400: {'train_ae_loss': 0.6563, 'train_ucc_loss': 0.43182, 'train_ucc_acc': 0.875, 'loss': 0.54406}\n",
            "Step 179420: {'train_ae_loss': 0.63342, 'train_ucc_loss': 0.46182, 'train_ucc_acc': 0.84375, 'loss': 0.54762}\n",
            "Step 179440: {'train_ae_loss': 0.6542, 'train_ucc_loss': 0.44549, 'train_ucc_acc': 0.875, 'loss': 0.54985}\n",
            "Step 179460: {'train_ae_loss': 0.66702, 'train_ucc_loss': 0.39765, 'train_ucc_acc': 0.9375, 'loss': 0.53233}\n",
            "Step 179480: {'train_ae_loss': 0.66157, 'train_ucc_loss': 0.38691, 'train_ucc_acc': 0.9375, 'loss': 0.52424}\n",
            "Step 179500: {'train_ae_loss': 0.66087, 'train_ucc_loss': 0.45602, 'train_ucc_acc': 0.84375, 'loss': 0.55845}\n",
            "Step 179520: {'train_ae_loss': 0.64745, 'train_ucc_loss': 0.46499, 'train_ucc_acc': 0.84375, 'loss': 0.55622}\n",
            "Step 179540: {'train_ae_loss': 0.66261, 'train_ucc_loss': 0.42798, 'train_ucc_acc': 0.875, 'loss': 0.5453}\n",
            "Step 179560: {'train_ae_loss': 0.65496, 'train_ucc_loss': 0.44636, 'train_ucc_acc': 0.84375, 'loss': 0.55066}\n",
            "Step 179580: {'train_ae_loss': 0.66324, 'train_ucc_loss': 0.42906, 'train_ucc_acc': 0.875, 'loss': 0.54615}\n",
            "Step 179600: {'train_ae_loss': 0.6505, 'train_ucc_loss': 0.40932, 'train_ucc_acc': 0.90625, 'loss': 0.52991}\n",
            "Step 179620: {'train_ae_loss': 0.67526, 'train_ucc_loss': 0.42874, 'train_ucc_acc': 0.875, 'loss': 0.552}\n",
            "Step 179640: {'train_ae_loss': 0.66495, 'train_ucc_loss': 0.39441, 'train_ucc_acc': 0.9375, 'loss': 0.52968}\n",
            "Step 179660: {'train_ae_loss': 0.63548, 'train_ucc_loss': 0.54197, 'train_ucc_acc': 0.71875, 'loss': 0.58872}\n",
            "Step 179680: {'train_ae_loss': 0.64014, 'train_ucc_loss': 0.47879, 'train_ucc_acc': 0.8125, 'loss': 0.55947}\n",
            "Step 179700: {'train_ae_loss': 0.6772, 'train_ucc_loss': 0.40108, 'train_ucc_acc': 0.90625, 'loss': 0.53914}\n",
            "Step 179720: {'train_ae_loss': 0.66419, 'train_ucc_loss': 0.46262, 'train_ucc_acc': 0.875, 'loss': 0.5634}\n",
            "Step 179740: {'train_ae_loss': 0.66268, 'train_ucc_loss': 0.3711, 'train_ucc_acc': 0.9375, 'loss': 0.51689}\n",
            "Step 179760: {'train_ae_loss': 0.65357, 'train_ucc_loss': 0.53645, 'train_ucc_acc': 0.78125, 'loss': 0.59501}\n",
            "Step 179780: {'train_ae_loss': 0.65897, 'train_ucc_loss': 0.39742, 'train_ucc_acc': 0.9375, 'loss': 0.52819}\n",
            "Step 179800: {'train_ae_loss': 0.66353, 'train_ucc_loss': 0.41135, 'train_ucc_acc': 0.9375, 'loss': 0.53744}\n",
            "Step 179820: {'train_ae_loss': 0.66653, 'train_ucc_loss': 0.48132, 'train_ucc_acc': 0.8125, 'loss': 0.57392}\n",
            "Step 179840: {'train_ae_loss': 0.6669, 'train_ucc_loss': 0.3832, 'train_ucc_acc': 0.9375, 'loss': 0.52505}\n",
            "Step 179860: {'train_ae_loss': 0.66936, 'train_ucc_loss': 0.3629, 'train_ucc_acc': 0.9375, 'loss': 0.51613}\n",
            "Step 179880: {'train_ae_loss': 0.66341, 'train_ucc_loss': 0.43496, 'train_ucc_acc': 0.84375, 'loss': 0.54918}\n",
            "Step 179900: {'train_ae_loss': 0.65601, 'train_ucc_loss': 0.53124, 'train_ucc_acc': 0.78125, 'loss': 0.59362}\n",
            "Step 179920: {'train_ae_loss': 0.6615, 'train_ucc_loss': 0.51603, 'train_ucc_acc': 0.78125, 'loss': 0.58877}\n",
            "Step 179940: {'train_ae_loss': 0.64372, 'train_ucc_loss': 0.4766, 'train_ucc_acc': 0.8125, 'loss': 0.56016}\n",
            "Step 179960: {'train_ae_loss': 0.67175, 'train_ucc_loss': 0.48079, 'train_ucc_acc': 0.8125, 'loss': 0.57627}\n",
            "Step 179980: {'train_ae_loss': 0.64691, 'train_ucc_loss': 0.40096, 'train_ucc_acc': 0.875, 'loss': 0.52394}\n",
            "Step 180000: {'train_ae_loss': 0.66601, 'train_ucc_loss': 0.38439, 'train_ucc_acc': 0.9375, 'loss': 0.5252}\n",
            "step: 180000,eval_ae_loss: 0.65344,eval_ucc_loss: 0.46575,eval_ucc_acc: 0.84766\n",
            "Step 180020: {'train_ae_loss': 0.641, 'train_ucc_loss': 0.35262, 'train_ucc_acc': 0.96875, 'loss': 0.49681}\n",
            "Step 180040: {'train_ae_loss': 0.64172, 'train_ucc_loss': 0.41794, 'train_ucc_acc': 0.90625, 'loss': 0.52983}\n",
            "Step 180060: {'train_ae_loss': 0.6657, 'train_ucc_loss': 0.50428, 'train_ucc_acc': 0.78125, 'loss': 0.58499}\n",
            "Step 180080: {'train_ae_loss': 0.66929, 'train_ucc_loss': 0.48559, 'train_ucc_acc': 0.8125, 'loss': 0.57744}\n",
            "Step 180100: {'train_ae_loss': 0.64144, 'train_ucc_loss': 0.40181, 'train_ucc_acc': 0.9375, 'loss': 0.52162}\n",
            "Step 180120: {'train_ae_loss': 0.66738, 'train_ucc_loss': 0.38954, 'train_ucc_acc': 0.90625, 'loss': 0.52846}\n",
            "Step 180140: {'train_ae_loss': 0.64983, 'train_ucc_loss': 0.47384, 'train_ucc_acc': 0.8125, 'loss': 0.56184}\n",
            "Step 180160: {'train_ae_loss': 0.6593, 'train_ucc_loss': 0.45847, 'train_ucc_acc': 0.84375, 'loss': 0.55888}\n",
            "Step 180180: {'train_ae_loss': 0.67242, 'train_ucc_loss': 0.31407, 'train_ucc_acc': 1.0, 'loss': 0.49324}\n",
            "Step 180200: {'train_ae_loss': 0.65613, 'train_ucc_loss': 0.52698, 'train_ucc_acc': 0.71875, 'loss': 0.59155}\n",
            "Step 180220: {'train_ae_loss': 0.67599, 'train_ucc_loss': 0.44013, 'train_ucc_acc': 0.8125, 'loss': 0.55806}\n",
            "Step 180240: {'train_ae_loss': 0.66732, 'train_ucc_loss': 0.51755, 'train_ucc_acc': 0.78125, 'loss': 0.59244}\n",
            "Step 180260: {'train_ae_loss': 0.6532, 'train_ucc_loss': 0.36912, 'train_ucc_acc': 0.96875, 'loss': 0.51116}\n",
            "Step 180280: {'train_ae_loss': 0.66382, 'train_ucc_loss': 0.4194, 'train_ucc_acc': 0.875, 'loss': 0.54161}\n",
            "Step 180300: {'train_ae_loss': 0.6556, 'train_ucc_loss': 0.35211, 'train_ucc_acc': 0.96875, 'loss': 0.50385}\n",
            "Step 180320: {'train_ae_loss': 0.65567, 'train_ucc_loss': 0.50988, 'train_ucc_acc': 0.8125, 'loss': 0.58277}\n",
            "Step 180340: {'train_ae_loss': 0.64958, 'train_ucc_loss': 0.50065, 'train_ucc_acc': 0.8125, 'loss': 0.57512}\n",
            "Step 180360: {'train_ae_loss': 0.66308, 'train_ucc_loss': 0.35524, 'train_ucc_acc': 0.96875, 'loss': 0.50916}\n",
            "Step 180380: {'train_ae_loss': 0.66475, 'train_ucc_loss': 0.35321, 'train_ucc_acc': 0.96875, 'loss': 0.50898}\n",
            "Step 180400: {'train_ae_loss': 0.64735, 'train_ucc_loss': 0.4908, 'train_ucc_acc': 0.8125, 'loss': 0.56907}\n",
            "Step 180420: {'train_ae_loss': 0.67014, 'train_ucc_loss': 0.47699, 'train_ucc_acc': 0.8125, 'loss': 0.57357}\n",
            "Step 180440: {'train_ae_loss': 0.64474, 'train_ucc_loss': 0.39522, 'train_ucc_acc': 0.9375, 'loss': 0.51998}\n",
            "Step 180460: {'train_ae_loss': 0.66239, 'train_ucc_loss': 0.39473, 'train_ucc_acc': 0.90625, 'loss': 0.52856}\n",
            "Step 180480: {'train_ae_loss': 0.6554, 'train_ucc_loss': 0.44365, 'train_ucc_acc': 0.875, 'loss': 0.54953}\n",
            "Step 180500: {'train_ae_loss': 0.66641, 'train_ucc_loss': 0.49729, 'train_ucc_acc': 0.8125, 'loss': 0.58185}\n",
            "Step 180520: {'train_ae_loss': 0.65672, 'train_ucc_loss': 0.38282, 'train_ucc_acc': 0.9375, 'loss': 0.51977}\n",
            "Step 180540: {'train_ae_loss': 0.6593, 'train_ucc_loss': 0.42728, 'train_ucc_acc': 0.875, 'loss': 0.54329}\n",
            "Step 180560: {'train_ae_loss': 0.65902, 'train_ucc_loss': 0.45483, 'train_ucc_acc': 0.84375, 'loss': 0.55693}\n",
            "Step 180580: {'train_ae_loss': 0.65718, 'train_ucc_loss': 0.35993, 'train_ucc_acc': 0.96875, 'loss': 0.50855}\n",
            "Step 180600: {'train_ae_loss': 0.67436, 'train_ucc_loss': 0.48632, 'train_ucc_acc': 0.84375, 'loss': 0.58034}\n",
            "Step 180620: {'train_ae_loss': 0.64444, 'train_ucc_loss': 0.45992, 'train_ucc_acc': 0.875, 'loss': 0.55218}\n",
            "Step 180640: {'train_ae_loss': 0.65672, 'train_ucc_loss': 0.42354, 'train_ucc_acc': 0.875, 'loss': 0.54013}\n",
            "Step 180660: {'train_ae_loss': 0.64729, 'train_ucc_loss': 0.40517, 'train_ucc_acc': 0.90625, 'loss': 0.52623}\n",
            "Step 180680: {'train_ae_loss': 0.63825, 'train_ucc_loss': 0.35944, 'train_ucc_acc': 0.9375, 'loss': 0.49884}\n",
            "Step 180700: {'train_ae_loss': 0.65739, 'train_ucc_loss': 0.42029, 'train_ucc_acc': 0.90625, 'loss': 0.53884}\n",
            "Step 180720: {'train_ae_loss': 0.66242, 'train_ucc_loss': 0.53386, 'train_ucc_acc': 0.75, 'loss': 0.59814}\n",
            "Step 180740: {'train_ae_loss': 0.66274, 'train_ucc_loss': 0.41717, 'train_ucc_acc': 0.875, 'loss': 0.53995}\n",
            "Step 180760: {'train_ae_loss': 0.65395, 'train_ucc_loss': 0.48321, 'train_ucc_acc': 0.8125, 'loss': 0.56858}\n",
            "Step 180780: {'train_ae_loss': 0.65925, 'train_ucc_loss': 0.43758, 'train_ucc_acc': 0.875, 'loss': 0.54841}\n",
            "Step 180800: {'train_ae_loss': 0.65881, 'train_ucc_loss': 0.42376, 'train_ucc_acc': 0.875, 'loss': 0.54128}\n",
            "Step 180820: {'train_ae_loss': 0.67185, 'train_ucc_loss': 0.36537, 'train_ucc_acc': 0.96875, 'loss': 0.51861}\n",
            "Step 180840: {'train_ae_loss': 0.6601, 'train_ucc_loss': 0.43527, 'train_ucc_acc': 0.875, 'loss': 0.54768}\n",
            "Step 180860: {'train_ae_loss': 0.67478, 'train_ucc_loss': 0.40814, 'train_ucc_acc': 0.90625, 'loss': 0.54146}\n",
            "Step 180880: {'train_ae_loss': 0.66316, 'train_ucc_loss': 0.41091, 'train_ucc_acc': 0.90625, 'loss': 0.53703}\n",
            "Step 180900: {'train_ae_loss': 0.65441, 'train_ucc_loss': 0.63595, 'train_ucc_acc': 0.65625, 'loss': 0.64518}\n",
            "Step 180920: {'train_ae_loss': 0.65802, 'train_ucc_loss': 0.44193, 'train_ucc_acc': 0.90625, 'loss': 0.54998}\n",
            "Step 180940: {'train_ae_loss': 0.66453, 'train_ucc_loss': 0.47805, 'train_ucc_acc': 0.84375, 'loss': 0.57129}\n",
            "Step 180960: {'train_ae_loss': 0.63589, 'train_ucc_loss': 0.50892, 'train_ucc_acc': 0.8125, 'loss': 0.5724}\n",
            "Step 180980: {'train_ae_loss': 0.66696, 'train_ucc_loss': 0.40189, 'train_ucc_acc': 0.875, 'loss': 0.53443}\n",
            "Step 181000: {'train_ae_loss': 0.6619, 'train_ucc_loss': 0.43295, 'train_ucc_acc': 0.875, 'loss': 0.54742}\n",
            "step: 181000,eval_ae_loss: 0.65209,eval_ucc_loss: 0.4753,eval_ucc_acc: 0.83203\n",
            "Step 181020: {'train_ae_loss': 0.637, 'train_ucc_loss': 0.3734, 'train_ucc_acc': 0.9375, 'loss': 0.5052}\n",
            "Step 181040: {'train_ae_loss': 0.63991, 'train_ucc_loss': 0.38726, 'train_ucc_acc': 0.90625, 'loss': 0.51359}\n",
            "Step 181060: {'train_ae_loss': 0.64239, 'train_ucc_loss': 0.45732, 'train_ucc_acc': 0.84375, 'loss': 0.54985}\n",
            "Step 181080: {'train_ae_loss': 0.65806, 'train_ucc_loss': 0.41339, 'train_ucc_acc': 0.90625, 'loss': 0.53572}\n",
            "Step 181100: {'train_ae_loss': 0.65563, 'train_ucc_loss': 0.44543, 'train_ucc_acc': 0.84375, 'loss': 0.55053}\n",
            "Step 181120: {'train_ae_loss': 0.66918, 'train_ucc_loss': 0.40823, 'train_ucc_acc': 0.90625, 'loss': 0.53871}\n",
            "Step 181140: {'train_ae_loss': 0.66469, 'train_ucc_loss': 0.45442, 'train_ucc_acc': 0.8125, 'loss': 0.55955}\n",
            "Step 181160: {'train_ae_loss': 0.66762, 'train_ucc_loss': 0.42953, 'train_ucc_acc': 0.90625, 'loss': 0.54858}\n",
            "Step 181180: {'train_ae_loss': 0.64358, 'train_ucc_loss': 0.41278, 'train_ucc_acc': 0.90625, 'loss': 0.52818}\n",
            "Step 181200: {'train_ae_loss': 0.63303, 'train_ucc_loss': 0.46616, 'train_ucc_acc': 0.84375, 'loss': 0.5496}\n",
            "Step 181220: {'train_ae_loss': 0.66769, 'train_ucc_loss': 0.39779, 'train_ucc_acc': 0.90625, 'loss': 0.53274}\n",
            "Step 181240: {'train_ae_loss': 0.64008, 'train_ucc_loss': 0.57932, 'train_ucc_acc': 0.71875, 'loss': 0.6097}\n",
            "Step 181260: {'train_ae_loss': 0.64886, 'train_ucc_loss': 0.5058, 'train_ucc_acc': 0.8125, 'loss': 0.57733}\n",
            "Step 181280: {'train_ae_loss': 0.65909, 'train_ucc_loss': 0.39116, 'train_ucc_acc': 0.90625, 'loss': 0.52513}\n",
            "Step 181300: {'train_ae_loss': 0.67063, 'train_ucc_loss': 0.32643, 'train_ucc_acc': 1.0, 'loss': 0.49853}\n",
            "Step 181320: {'train_ae_loss': 0.66062, 'train_ucc_loss': 0.4033, 'train_ucc_acc': 0.90625, 'loss': 0.53196}\n",
            "Step 181340: {'train_ae_loss': 0.66706, 'train_ucc_loss': 0.43461, 'train_ucc_acc': 0.875, 'loss': 0.55084}\n",
            "Step 181360: {'train_ae_loss': 0.63027, 'train_ucc_loss': 0.41039, 'train_ucc_acc': 0.90625, 'loss': 0.52033}\n",
            "Step 181380: {'train_ae_loss': 0.65002, 'train_ucc_loss': 0.39387, 'train_ucc_acc': 0.9375, 'loss': 0.52195}\n",
            "Step 181400: {'train_ae_loss': 0.66358, 'train_ucc_loss': 0.44674, 'train_ucc_acc': 0.84375, 'loss': 0.55516}\n",
            "Step 181420: {'train_ae_loss': 0.6607, 'train_ucc_loss': 0.52717, 'train_ucc_acc': 0.75, 'loss': 0.59393}\n",
            "Step 181440: {'train_ae_loss': 0.66129, 'train_ucc_loss': 0.3899, 'train_ucc_acc': 0.9375, 'loss': 0.5256}\n",
            "Step 181460: {'train_ae_loss': 0.66712, 'train_ucc_loss': 0.43194, 'train_ucc_acc': 0.875, 'loss': 0.54953}\n",
            "Step 181480: {'train_ae_loss': 0.64487, 'train_ucc_loss': 0.45242, 'train_ucc_acc': 0.875, 'loss': 0.54865}\n",
            "Step 181500: {'train_ae_loss': 0.65386, 'train_ucc_loss': 0.47902, 'train_ucc_acc': 0.84375, 'loss': 0.56644}\n",
            "Step 181520: {'train_ae_loss': 0.65212, 'train_ucc_loss': 0.45577, 'train_ucc_acc': 0.875, 'loss': 0.55395}\n",
            "Step 181540: {'train_ae_loss': 0.65135, 'train_ucc_loss': 0.46519, 'train_ucc_acc': 0.875, 'loss': 0.55827}\n",
            "Step 181560: {'train_ae_loss': 0.66342, 'train_ucc_loss': 0.38621, 'train_ucc_acc': 0.90625, 'loss': 0.52481}\n",
            "Step 181580: {'train_ae_loss': 0.64853, 'train_ucc_loss': 0.38297, 'train_ucc_acc': 0.90625, 'loss': 0.51575}\n",
            "Step 181600: {'train_ae_loss': 0.65704, 'train_ucc_loss': 0.4558, 'train_ucc_acc': 0.8125, 'loss': 0.55642}\n",
            "Step 181620: {'train_ae_loss': 0.65895, 'train_ucc_loss': 0.47968, 'train_ucc_acc': 0.84375, 'loss': 0.56931}\n",
            "Step 181640: {'train_ae_loss': 0.64937, 'train_ucc_loss': 0.50622, 'train_ucc_acc': 0.78125, 'loss': 0.5778}\n",
            "Step 181660: {'train_ae_loss': 0.66115, 'train_ucc_loss': 0.51311, 'train_ucc_acc': 0.8125, 'loss': 0.58713}\n",
            "Step 181680: {'train_ae_loss': 0.6427, 'train_ucc_loss': 0.39679, 'train_ucc_acc': 0.90625, 'loss': 0.51974}\n",
            "Step 181700: {'train_ae_loss': 0.66733, 'train_ucc_loss': 0.38583, 'train_ucc_acc': 0.9375, 'loss': 0.52658}\n",
            "Step 181720: {'train_ae_loss': 0.67232, 'train_ucc_loss': 0.46982, 'train_ucc_acc': 0.875, 'loss': 0.57107}\n",
            "Step 181740: {'train_ae_loss': 0.63936, 'train_ucc_loss': 0.47971, 'train_ucc_acc': 0.84375, 'loss': 0.55953}\n",
            "Step 181760: {'train_ae_loss': 0.65197, 'train_ucc_loss': 0.35937, 'train_ucc_acc': 0.9375, 'loss': 0.50567}\n",
            "Step 181780: {'train_ae_loss': 0.6609, 'train_ucc_loss': 0.40687, 'train_ucc_acc': 0.875, 'loss': 0.53389}\n",
            "Step 181800: {'train_ae_loss': 0.6503, 'train_ucc_loss': 0.41714, 'train_ucc_acc': 0.875, 'loss': 0.53372}\n",
            "Step 181820: {'train_ae_loss': 0.66245, 'train_ucc_loss': 0.40055, 'train_ucc_acc': 0.90625, 'loss': 0.5315}\n",
            "Step 181840: {'train_ae_loss': 0.67001, 'train_ucc_loss': 0.41112, 'train_ucc_acc': 0.875, 'loss': 0.54056}\n",
            "Step 181860: {'train_ae_loss': 0.67493, 'train_ucc_loss': 0.41939, 'train_ucc_acc': 0.875, 'loss': 0.54716}\n",
            "Step 181880: {'train_ae_loss': 0.64876, 'train_ucc_loss': 0.37694, 'train_ucc_acc': 0.9375, 'loss': 0.51285}\n",
            "Step 181900: {'train_ae_loss': 0.6549, 'train_ucc_loss': 0.48779, 'train_ucc_acc': 0.78125, 'loss': 0.57135}\n",
            "Step 181920: {'train_ae_loss': 0.65228, 'train_ucc_loss': 0.48154, 'train_ucc_acc': 0.84375, 'loss': 0.56691}\n",
            "Step 181940: {'train_ae_loss': 0.65609, 'train_ucc_loss': 0.36319, 'train_ucc_acc': 0.9375, 'loss': 0.50964}\n",
            "Step 181960: {'train_ae_loss': 0.66544, 'train_ucc_loss': 0.54905, 'train_ucc_acc': 0.6875, 'loss': 0.60724}\n",
            "Step 181980: {'train_ae_loss': 0.65269, 'train_ucc_loss': 0.4273, 'train_ucc_acc': 0.875, 'loss': 0.53999}\n",
            "Step 182000: {'train_ae_loss': 0.66067, 'train_ucc_loss': 0.46871, 'train_ucc_acc': 0.8125, 'loss': 0.56469}\n",
            "step: 182000,eval_ae_loss: 0.6482,eval_ucc_loss: 0.47724,eval_ucc_acc: 0.82617\n",
            "Step 182020: {'train_ae_loss': 0.6703, 'train_ucc_loss': 0.51593, 'train_ucc_acc': 0.8125, 'loss': 0.59312}\n",
            "Step 182040: {'train_ae_loss': 0.65643, 'train_ucc_loss': 0.48355, 'train_ucc_acc': 0.8125, 'loss': 0.56999}\n",
            "Step 182060: {'train_ae_loss': 0.64862, 'train_ucc_loss': 0.39178, 'train_ucc_acc': 0.90625, 'loss': 0.5202}\n",
            "Step 182080: {'train_ae_loss': 0.63433, 'train_ucc_loss': 0.40529, 'train_ucc_acc': 0.90625, 'loss': 0.51981}\n",
            "Step 182100: {'train_ae_loss': 0.65909, 'train_ucc_loss': 0.49317, 'train_ucc_acc': 0.8125, 'loss': 0.57613}\n",
            "Step 182120: {'train_ae_loss': 0.66813, 'train_ucc_loss': 0.39162, 'train_ucc_acc': 0.90625, 'loss': 0.52988}\n",
            "Step 182140: {'train_ae_loss': 0.66142, 'train_ucc_loss': 0.43831, 'train_ucc_acc': 0.875, 'loss': 0.54987}\n",
            "Step 182160: {'train_ae_loss': 0.64044, 'train_ucc_loss': 0.42269, 'train_ucc_acc': 0.875, 'loss': 0.53157}\n",
            "Step 182180: {'train_ae_loss': 0.65601, 'train_ucc_loss': 0.38265, 'train_ucc_acc': 0.90625, 'loss': 0.51933}\n",
            "Step 182200: {'train_ae_loss': 0.64687, 'train_ucc_loss': 0.44269, 'train_ucc_acc': 0.875, 'loss': 0.54478}\n",
            "Step 182220: {'train_ae_loss': 0.65306, 'train_ucc_loss': 0.40183, 'train_ucc_acc': 0.90625, 'loss': 0.52744}\n",
            "Step 182240: {'train_ae_loss': 0.6553, 'train_ucc_loss': 0.43232, 'train_ucc_acc': 0.90625, 'loss': 0.54381}\n",
            "Step 182260: {'train_ae_loss': 0.65416, 'train_ucc_loss': 0.34137, 'train_ucc_acc': 0.96875, 'loss': 0.49777}\n",
            "Step 182280: {'train_ae_loss': 0.65757, 'train_ucc_loss': 0.43462, 'train_ucc_acc': 0.875, 'loss': 0.54609}\n",
            "Step 182300: {'train_ae_loss': 0.66313, 'train_ucc_loss': 0.45559, 'train_ucc_acc': 0.84375, 'loss': 0.55936}\n",
            "Step 182320: {'train_ae_loss': 0.6678, 'train_ucc_loss': 0.42898, 'train_ucc_acc': 0.875, 'loss': 0.54839}\n",
            "Step 182340: {'train_ae_loss': 0.66968, 'train_ucc_loss': 0.44775, 'train_ucc_acc': 0.875, 'loss': 0.55871}\n",
            "Step 182360: {'train_ae_loss': 0.65638, 'train_ucc_loss': 0.44201, 'train_ucc_acc': 0.84375, 'loss': 0.5492}\n",
            "Step 182380: {'train_ae_loss': 0.67092, 'train_ucc_loss': 0.54392, 'train_ucc_acc': 0.75, 'loss': 0.60742}\n",
            "Step 182400: {'train_ae_loss': 0.68035, 'train_ucc_loss': 0.3546, 'train_ucc_acc': 0.96875, 'loss': 0.51748}\n",
            "Step 182420: {'train_ae_loss': 0.67567, 'train_ucc_loss': 0.4394, 'train_ucc_acc': 0.875, 'loss': 0.55753}\n",
            "Step 182440: {'train_ae_loss': 0.67425, 'train_ucc_loss': 0.43987, 'train_ucc_acc': 0.875, 'loss': 0.55706}\n",
            "Step 182460: {'train_ae_loss': 0.65494, 'train_ucc_loss': 0.37972, 'train_ucc_acc': 0.9375, 'loss': 0.51733}\n",
            "Step 182480: {'train_ae_loss': 0.64149, 'train_ucc_loss': 0.44997, 'train_ucc_acc': 0.875, 'loss': 0.54573}\n",
            "Step 182500: {'train_ae_loss': 0.67746, 'train_ucc_loss': 0.48348, 'train_ucc_acc': 0.84375, 'loss': 0.58047}\n",
            "Step 182520: {'train_ae_loss': 0.6517, 'train_ucc_loss': 0.35971, 'train_ucc_acc': 0.9375, 'loss': 0.5057}\n",
            "Step 182540: {'train_ae_loss': 0.66826, 'train_ucc_loss': 0.35062, 'train_ucc_acc': 0.9375, 'loss': 0.50944}\n",
            "Step 182560: {'train_ae_loss': 0.65895, 'train_ucc_loss': 0.44018, 'train_ucc_acc': 0.875, 'loss': 0.54957}\n",
            "Step 182580: {'train_ae_loss': 0.65727, 'train_ucc_loss': 0.4628, 'train_ucc_acc': 0.84375, 'loss': 0.56003}\n",
            "Step 182600: {'train_ae_loss': 0.66814, 'train_ucc_loss': 0.42465, 'train_ucc_acc': 0.875, 'loss': 0.54639}\n",
            "Step 182620: {'train_ae_loss': 0.65416, 'train_ucc_loss': 0.42846, 'train_ucc_acc': 0.90625, 'loss': 0.54131}\n",
            "Step 182640: {'train_ae_loss': 0.66424, 'train_ucc_loss': 0.43131, 'train_ucc_acc': 0.84375, 'loss': 0.54777}\n",
            "Step 182660: {'train_ae_loss': 0.65226, 'train_ucc_loss': 0.37633, 'train_ucc_acc': 0.9375, 'loss': 0.51429}\n",
            "Step 182680: {'train_ae_loss': 0.65563, 'train_ucc_loss': 0.38297, 'train_ucc_acc': 0.90625, 'loss': 0.5193}\n",
            "Step 182700: {'train_ae_loss': 0.66226, 'train_ucc_loss': 0.43733, 'train_ucc_acc': 0.875, 'loss': 0.54979}\n",
            "Step 182720: {'train_ae_loss': 0.66447, 'train_ucc_loss': 0.35998, 'train_ucc_acc': 0.96875, 'loss': 0.51223}\n",
            "Step 182740: {'train_ae_loss': 0.64865, 'train_ucc_loss': 0.38302, 'train_ucc_acc': 0.9375, 'loss': 0.51583}\n",
            "Step 182760: {'train_ae_loss': 0.65769, 'train_ucc_loss': 0.41338, 'train_ucc_acc': 0.90625, 'loss': 0.53553}\n",
            "Step 182780: {'train_ae_loss': 0.6468, 'train_ucc_loss': 0.39213, 'train_ucc_acc': 0.9375, 'loss': 0.51946}\n",
            "Step 182800: {'train_ae_loss': 0.6493, 'train_ucc_loss': 0.49861, 'train_ucc_acc': 0.78125, 'loss': 0.57396}\n",
            "Step 182820: {'train_ae_loss': 0.66472, 'train_ucc_loss': 0.40416, 'train_ucc_acc': 0.90625, 'loss': 0.53444}\n",
            "Step 182840: {'train_ae_loss': 0.65149, 'train_ucc_loss': 0.40754, 'train_ucc_acc': 0.90625, 'loss': 0.52952}\n",
            "Step 182860: {'train_ae_loss': 0.65004, 'train_ucc_loss': 0.4037, 'train_ucc_acc': 0.90625, 'loss': 0.52687}\n",
            "Step 182880: {'train_ae_loss': 0.65672, 'train_ucc_loss': 0.41812, 'train_ucc_acc': 0.90625, 'loss': 0.53742}\n",
            "Step 182900: {'train_ae_loss': 0.65797, 'train_ucc_loss': 0.58248, 'train_ucc_acc': 0.71875, 'loss': 0.62023}\n",
            "Step 182920: {'train_ae_loss': 0.65545, 'train_ucc_loss': 0.45283, 'train_ucc_acc': 0.84375, 'loss': 0.55414}\n",
            "Step 182940: {'train_ae_loss': 0.64895, 'train_ucc_loss': 0.44116, 'train_ucc_acc': 0.875, 'loss': 0.54505}\n",
            "Step 182960: {'train_ae_loss': 0.65089, 'train_ucc_loss': 0.52839, 'train_ucc_acc': 0.78125, 'loss': 0.58964}\n",
            "Step 182980: {'train_ae_loss': 0.64828, 'train_ucc_loss': 0.41762, 'train_ucc_acc': 0.90625, 'loss': 0.53295}\n",
            "Step 183000: {'train_ae_loss': 0.65022, 'train_ucc_loss': 0.46498, 'train_ucc_acc': 0.84375, 'loss': 0.5576}\n",
            "step: 183000,eval_ae_loss: 0.64311,eval_ucc_loss: 0.46927,eval_ucc_acc: 0.83594\n",
            "Step 183020: {'train_ae_loss': 0.67282, 'train_ucc_loss': 0.40047, 'train_ucc_acc': 0.90625, 'loss': 0.53665}\n",
            "Step 183040: {'train_ae_loss': 0.6602, 'train_ucc_loss': 0.39031, 'train_ucc_acc': 0.9375, 'loss': 0.52526}\n",
            "Step 183060: {'train_ae_loss': 0.65426, 'train_ucc_loss': 0.36781, 'train_ucc_acc': 0.9375, 'loss': 0.51103}\n",
            "Step 183080: {'train_ae_loss': 0.66915, 'train_ucc_loss': 0.41262, 'train_ucc_acc': 0.90625, 'loss': 0.54089}\n",
            "Step 183100: {'train_ae_loss': 0.66092, 'train_ucc_loss': 0.42705, 'train_ucc_acc': 0.875, 'loss': 0.54399}\n",
            "Step 183120: {'train_ae_loss': 0.65821, 'train_ucc_loss': 0.41835, 'train_ucc_acc': 0.90625, 'loss': 0.53828}\n",
            "Step 183140: {'train_ae_loss': 0.66013, 'train_ucc_loss': 0.50652, 'train_ucc_acc': 0.75, 'loss': 0.58333}\n",
            "Step 183160: {'train_ae_loss': 0.66223, 'train_ucc_loss': 0.46129, 'train_ucc_acc': 0.8125, 'loss': 0.56176}\n",
            "Step 183180: {'train_ae_loss': 0.64398, 'train_ucc_loss': 0.43374, 'train_ucc_acc': 0.875, 'loss': 0.53886}\n",
            "Step 183200: {'train_ae_loss': 0.6601, 'train_ucc_loss': 0.40426, 'train_ucc_acc': 0.90625, 'loss': 0.53218}\n",
            "Step 183220: {'train_ae_loss': 0.65551, 'train_ucc_loss': 0.35794, 'train_ucc_acc': 0.96875, 'loss': 0.50673}\n",
            "Step 183240: {'train_ae_loss': 0.65105, 'train_ucc_loss': 0.33928, 'train_ucc_acc': 0.96875, 'loss': 0.49516}\n",
            "Step 183260: {'train_ae_loss': 0.64592, 'train_ucc_loss': 0.44558, 'train_ucc_acc': 0.8125, 'loss': 0.54575}\n",
            "Step 183280: {'train_ae_loss': 0.65391, 'train_ucc_loss': 0.39157, 'train_ucc_acc': 0.90625, 'loss': 0.52274}\n",
            "Step 183300: {'train_ae_loss': 0.66063, 'train_ucc_loss': 0.3852, 'train_ucc_acc': 0.9375, 'loss': 0.52291}\n",
            "Step 183320: {'train_ae_loss': 0.65239, 'train_ucc_loss': 0.48825, 'train_ucc_acc': 0.84375, 'loss': 0.57032}\n",
            "Step 183340: {'train_ae_loss': 0.66569, 'train_ucc_loss': 0.47395, 'train_ucc_acc': 0.84375, 'loss': 0.56982}\n",
            "Step 183360: {'train_ae_loss': 0.64364, 'train_ucc_loss': 0.45545, 'train_ucc_acc': 0.8125, 'loss': 0.54954}\n",
            "Step 183380: {'train_ae_loss': 0.65328, 'train_ucc_loss': 0.43988, 'train_ucc_acc': 0.84375, 'loss': 0.54658}\n",
            "Step 183400: {'train_ae_loss': 0.66064, 'train_ucc_loss': 0.41413, 'train_ucc_acc': 0.875, 'loss': 0.53739}\n",
            "Step 183420: {'train_ae_loss': 0.66964, 'train_ucc_loss': 0.46864, 'train_ucc_acc': 0.84375, 'loss': 0.56914}\n",
            "Step 183440: {'train_ae_loss': 0.64185, 'train_ucc_loss': 0.44488, 'train_ucc_acc': 0.84375, 'loss': 0.54337}\n",
            "Step 183460: {'train_ae_loss': 0.65162, 'train_ucc_loss': 0.47685, 'train_ucc_acc': 0.84375, 'loss': 0.56423}\n",
            "Step 183480: {'train_ae_loss': 0.62673, 'train_ucc_loss': 0.44329, 'train_ucc_acc': 0.84375, 'loss': 0.53501}\n",
            "Step 183500: {'train_ae_loss': 0.64267, 'train_ucc_loss': 0.50399, 'train_ucc_acc': 0.78125, 'loss': 0.57333}\n",
            "Step 183520: {'train_ae_loss': 0.65353, 'train_ucc_loss': 0.46109, 'train_ucc_acc': 0.84375, 'loss': 0.55731}\n",
            "Step 183540: {'train_ae_loss': 0.65541, 'train_ucc_loss': 0.47813, 'train_ucc_acc': 0.84375, 'loss': 0.56677}\n",
            "Step 183560: {'train_ae_loss': 0.66326, 'train_ucc_loss': 0.47521, 'train_ucc_acc': 0.84375, 'loss': 0.56924}\n",
            "Step 183580: {'train_ae_loss': 0.68329, 'train_ucc_loss': 0.40327, 'train_ucc_acc': 0.90625, 'loss': 0.54328}\n",
            "Step 183600: {'train_ae_loss': 0.65941, 'train_ucc_loss': 0.49736, 'train_ucc_acc': 0.8125, 'loss': 0.57839}\n",
            "Step 183620: {'train_ae_loss': 0.65288, 'train_ucc_loss': 0.3929, 'train_ucc_acc': 0.9375, 'loss': 0.52289}\n",
            "Step 183640: {'train_ae_loss': 0.65558, 'train_ucc_loss': 0.36368, 'train_ucc_acc': 0.9375, 'loss': 0.50963}\n",
            "Step 183660: {'train_ae_loss': 0.65981, 'train_ucc_loss': 0.4839, 'train_ucc_acc': 0.84375, 'loss': 0.57186}\n",
            "Step 183680: {'train_ae_loss': 0.65782, 'train_ucc_loss': 0.44872, 'train_ucc_acc': 0.875, 'loss': 0.55327}\n",
            "Step 183700: {'train_ae_loss': 0.66561, 'train_ucc_loss': 0.51164, 'train_ucc_acc': 0.78125, 'loss': 0.58862}\n",
            "Step 183720: {'train_ae_loss': 0.65303, 'train_ucc_loss': 0.41237, 'train_ucc_acc': 0.90625, 'loss': 0.5327}\n",
            "Step 183740: {'train_ae_loss': 0.6511, 'train_ucc_loss': 0.46334, 'train_ucc_acc': 0.8125, 'loss': 0.55722}\n",
            "Step 183760: {'train_ae_loss': 0.67351, 'train_ucc_loss': 0.39098, 'train_ucc_acc': 0.90625, 'loss': 0.53225}\n",
            "Step 183780: {'train_ae_loss': 0.63648, 'train_ucc_loss': 0.46812, 'train_ucc_acc': 0.84375, 'loss': 0.5523}\n",
            "Step 183800: {'train_ae_loss': 0.65495, 'train_ucc_loss': 0.49403, 'train_ucc_acc': 0.8125, 'loss': 0.57449}\n",
            "Step 183820: {'train_ae_loss': 0.66566, 'train_ucc_loss': 0.40955, 'train_ucc_acc': 0.90625, 'loss': 0.5376}\n",
            "Step 183840: {'train_ae_loss': 0.66541, 'train_ucc_loss': 0.53798, 'train_ucc_acc': 0.75, 'loss': 0.6017}\n",
            "Step 183860: {'train_ae_loss': 0.65728, 'train_ucc_loss': 0.36465, 'train_ucc_acc': 0.96875, 'loss': 0.51097}\n",
            "Step 183880: {'train_ae_loss': 0.65087, 'train_ucc_loss': 0.3477, 'train_ucc_acc': 0.96875, 'loss': 0.49929}\n",
            "Step 183900: {'train_ae_loss': 0.67161, 'train_ucc_loss': 0.34397, 'train_ucc_acc': 0.96875, 'loss': 0.50779}\n",
            "Step 183920: {'train_ae_loss': 0.65671, 'train_ucc_loss': 0.53829, 'train_ucc_acc': 0.75, 'loss': 0.5975}\n",
            "Step 183940: {'train_ae_loss': 0.65882, 'train_ucc_loss': 0.46021, 'train_ucc_acc': 0.84375, 'loss': 0.55952}\n",
            "Step 183960: {'train_ae_loss': 0.66532, 'train_ucc_loss': 0.44295, 'train_ucc_acc': 0.875, 'loss': 0.55413}\n",
            "Step 183980: {'train_ae_loss': 0.6614, 'train_ucc_loss': 0.55784, 'train_ucc_acc': 0.75, 'loss': 0.60962}\n",
            "Step 184000: {'train_ae_loss': 0.66898, 'train_ucc_loss': 0.40844, 'train_ucc_acc': 0.90625, 'loss': 0.53871}\n",
            "step: 184000,eval_ae_loss: 0.65499,eval_ucc_loss: 0.51571,eval_ucc_acc: 0.79199\n",
            "Step 184020: {'train_ae_loss': 0.65897, 'train_ucc_loss': 0.41798, 'train_ucc_acc': 0.90625, 'loss': 0.53848}\n",
            "Step 184040: {'train_ae_loss': 0.6542, 'train_ucc_loss': 0.45995, 'train_ucc_acc': 0.875, 'loss': 0.55708}\n",
            "Step 184060: {'train_ae_loss': 0.63661, 'train_ucc_loss': 0.40566, 'train_ucc_acc': 0.90625, 'loss': 0.52113}\n",
            "Step 184080: {'train_ae_loss': 0.64509, 'train_ucc_loss': 0.41206, 'train_ucc_acc': 0.90625, 'loss': 0.52858}\n",
            "Step 184100: {'train_ae_loss': 0.66265, 'train_ucc_loss': 0.4326, 'train_ucc_acc': 0.875, 'loss': 0.54762}\n",
            "Step 184120: {'train_ae_loss': 0.66325, 'train_ucc_loss': 0.43688, 'train_ucc_acc': 0.875, 'loss': 0.55007}\n",
            "Step 184140: {'train_ae_loss': 0.66283, 'train_ucc_loss': 0.62251, 'train_ucc_acc': 0.65625, 'loss': 0.64267}\n",
            "Step 184160: {'train_ae_loss': 0.64955, 'train_ucc_loss': 0.54494, 'train_ucc_acc': 0.71875, 'loss': 0.59724}\n",
            "Step 184180: {'train_ae_loss': 0.6539, 'train_ucc_loss': 0.46955, 'train_ucc_acc': 0.84375, 'loss': 0.56173}\n",
            "Step 184200: {'train_ae_loss': 0.6694, 'train_ucc_loss': 0.34206, 'train_ucc_acc': 0.96875, 'loss': 0.50573}\n",
            "Step 184220: {'train_ae_loss': 0.6623, 'train_ucc_loss': 0.4535, 'train_ucc_acc': 0.875, 'loss': 0.5579}\n",
            "Step 184240: {'train_ae_loss': 0.65331, 'train_ucc_loss': 0.39909, 'train_ucc_acc': 0.90625, 'loss': 0.5262}\n",
            "Step 184260: {'train_ae_loss': 0.65891, 'train_ucc_loss': 0.45965, 'train_ucc_acc': 0.84375, 'loss': 0.55928}\n",
            "Step 184280: {'train_ae_loss': 0.65492, 'train_ucc_loss': 0.42838, 'train_ucc_acc': 0.90625, 'loss': 0.54165}\n",
            "Step 184300: {'train_ae_loss': 0.66462, 'train_ucc_loss': 0.45931, 'train_ucc_acc': 0.8125, 'loss': 0.56196}\n",
            "Step 184320: {'train_ae_loss': 0.64747, 'train_ucc_loss': 0.43091, 'train_ucc_acc': 0.90625, 'loss': 0.53919}\n",
            "Step 184340: {'train_ae_loss': 0.6602, 'train_ucc_loss': 0.41457, 'train_ucc_acc': 0.875, 'loss': 0.53738}\n",
            "Step 184360: {'train_ae_loss': 0.64897, 'train_ucc_loss': 0.48121, 'train_ucc_acc': 0.84375, 'loss': 0.56509}\n",
            "Step 184380: {'train_ae_loss': 0.65321, 'train_ucc_loss': 0.40067, 'train_ucc_acc': 0.90625, 'loss': 0.52694}\n",
            "Step 184400: {'train_ae_loss': 0.6441, 'train_ucc_loss': 0.34677, 'train_ucc_acc': 0.96875, 'loss': 0.49544}\n",
            "Step 184420: {'train_ae_loss': 0.6669, 'train_ucc_loss': 0.44277, 'train_ucc_acc': 0.875, 'loss': 0.55484}\n",
            "Step 184440: {'train_ae_loss': 0.66959, 'train_ucc_loss': 0.45602, 'train_ucc_acc': 0.8125, 'loss': 0.5628}\n",
            "Step 184460: {'train_ae_loss': 0.66719, 'train_ucc_loss': 0.43059, 'train_ucc_acc': 0.875, 'loss': 0.54889}\n",
            "Step 184480: {'train_ae_loss': 0.64934, 'train_ucc_loss': 0.37251, 'train_ucc_acc': 0.96875, 'loss': 0.51093}\n",
            "Step 184500: {'train_ae_loss': 0.66742, 'train_ucc_loss': 0.3762, 'train_ucc_acc': 0.9375, 'loss': 0.52181}\n",
            "Step 184520: {'train_ae_loss': 0.65785, 'train_ucc_loss': 0.47909, 'train_ucc_acc': 0.8125, 'loss': 0.56847}\n",
            "Step 184540: {'train_ae_loss': 0.65703, 'train_ucc_loss': 0.41322, 'train_ucc_acc': 0.90625, 'loss': 0.53513}\n",
            "Step 184560: {'train_ae_loss': 0.64344, 'train_ucc_loss': 0.43427, 'train_ucc_acc': 0.875, 'loss': 0.53886}\n",
            "Step 184580: {'train_ae_loss': 0.66227, 'train_ucc_loss': 0.46663, 'train_ucc_acc': 0.84375, 'loss': 0.56445}\n",
            "Step 184600: {'train_ae_loss': 0.65087, 'train_ucc_loss': 0.40607, 'train_ucc_acc': 0.90625, 'loss': 0.52847}\n",
            "Step 184620: {'train_ae_loss': 0.63774, 'train_ucc_loss': 0.31749, 'train_ucc_acc': 1.0, 'loss': 0.47761}\n",
            "Step 184640: {'train_ae_loss': 0.646, 'train_ucc_loss': 0.49823, 'train_ucc_acc': 0.8125, 'loss': 0.57212}\n",
            "Step 184660: {'train_ae_loss': 0.63692, 'train_ucc_loss': 0.56103, 'train_ucc_acc': 0.6875, 'loss': 0.59898}\n",
            "Step 184680: {'train_ae_loss': 0.68047, 'train_ucc_loss': 0.46321, 'train_ucc_acc': 0.875, 'loss': 0.57184}\n",
            "Step 184700: {'train_ae_loss': 0.67468, 'train_ucc_loss': 0.33474, 'train_ucc_acc': 0.96875, 'loss': 0.50471}\n",
            "Step 184720: {'train_ae_loss': 0.66056, 'train_ucc_loss': 0.5178, 'train_ucc_acc': 0.78125, 'loss': 0.58918}\n",
            "Step 184740: {'train_ae_loss': 0.65977, 'train_ucc_loss': 0.42817, 'train_ucc_acc': 0.84375, 'loss': 0.54397}\n",
            "Step 184760: {'train_ae_loss': 0.65073, 'train_ucc_loss': 0.39056, 'train_ucc_acc': 0.9375, 'loss': 0.52065}\n",
            "Step 184780: {'train_ae_loss': 0.66606, 'train_ucc_loss': 0.39693, 'train_ucc_acc': 0.9375, 'loss': 0.53149}\n",
            "Step 184800: {'train_ae_loss': 0.67712, 'train_ucc_loss': 0.38706, 'train_ucc_acc': 0.9375, 'loss': 0.53209}\n",
            "Step 184820: {'train_ae_loss': 0.66279, 'train_ucc_loss': 0.43838, 'train_ucc_acc': 0.875, 'loss': 0.55059}\n",
            "Step 184840: {'train_ae_loss': 0.65053, 'train_ucc_loss': 0.46393, 'train_ucc_acc': 0.84375, 'loss': 0.55723}\n",
            "Step 184860: {'train_ae_loss': 0.66663, 'train_ucc_loss': 0.4388, 'train_ucc_acc': 0.875, 'loss': 0.55271}\n",
            "Step 184880: {'train_ae_loss': 0.64627, 'train_ucc_loss': 0.44921, 'train_ucc_acc': 0.875, 'loss': 0.54774}\n",
            "Step 184900: {'train_ae_loss': 0.68002, 'train_ucc_loss': 0.46322, 'train_ucc_acc': 0.84375, 'loss': 0.57162}\n",
            "Step 184920: {'train_ae_loss': 0.66536, 'train_ucc_loss': 0.46517, 'train_ucc_acc': 0.8125, 'loss': 0.56527}\n",
            "Step 184940: {'train_ae_loss': 0.66124, 'train_ucc_loss': 0.35878, 'train_ucc_acc': 0.9375, 'loss': 0.51001}\n",
            "Step 184960: {'train_ae_loss': 0.65276, 'train_ucc_loss': 0.39796, 'train_ucc_acc': 0.90625, 'loss': 0.52536}\n",
            "Step 184980: {'train_ae_loss': 0.65096, 'train_ucc_loss': 0.46404, 'train_ucc_acc': 0.84375, 'loss': 0.5575}\n",
            "Step 185000: {'train_ae_loss': 0.65958, 'train_ucc_loss': 0.36333, 'train_ucc_acc': 0.9375, 'loss': 0.51145}\n",
            "step: 185000,eval_ae_loss: 0.65176,eval_ucc_loss: 0.45948,eval_ucc_acc: 0.85254\n",
            "Step 185020: {'train_ae_loss': 0.67433, 'train_ucc_loss': 0.43848, 'train_ucc_acc': 0.875, 'loss': 0.5564}\n",
            "Step 185040: {'train_ae_loss': 0.68122, 'train_ucc_loss': 0.5616, 'train_ucc_acc': 0.75, 'loss': 0.62141}\n",
            "Step 185060: {'train_ae_loss': 0.64803, 'train_ucc_loss': 0.45296, 'train_ucc_acc': 0.84375, 'loss': 0.55049}\n",
            "Step 185080: {'train_ae_loss': 0.65079, 'train_ucc_loss': 0.49705, 'train_ucc_acc': 0.84375, 'loss': 0.57392}\n",
            "Step 185100: {'train_ae_loss': 0.65528, 'train_ucc_loss': 0.32643, 'train_ucc_acc': 1.0, 'loss': 0.49085}\n",
            "Step 185120: {'train_ae_loss': 0.65286, 'train_ucc_loss': 0.36816, 'train_ucc_acc': 0.9375, 'loss': 0.51051}\n",
            "Step 185140: {'train_ae_loss': 0.65734, 'train_ucc_loss': 0.53173, 'train_ucc_acc': 0.75, 'loss': 0.59454}\n",
            "Step 185160: {'train_ae_loss': 0.63931, 'train_ucc_loss': 0.43815, 'train_ucc_acc': 0.875, 'loss': 0.53873}\n",
            "Step 185180: {'train_ae_loss': 0.65407, 'train_ucc_loss': 0.41847, 'train_ucc_acc': 0.90625, 'loss': 0.53627}\n",
            "Step 185200: {'train_ae_loss': 0.64681, 'train_ucc_loss': 0.38769, 'train_ucc_acc': 0.9375, 'loss': 0.51725}\n",
            "Step 185220: {'train_ae_loss': 0.66835, 'train_ucc_loss': 0.50768, 'train_ucc_acc': 0.78125, 'loss': 0.58801}\n",
            "Step 185240: {'train_ae_loss': 0.65197, 'train_ucc_loss': 0.43538, 'train_ucc_acc': 0.875, 'loss': 0.54368}\n",
            "Step 185260: {'train_ae_loss': 0.65903, 'train_ucc_loss': 0.43905, 'train_ucc_acc': 0.84375, 'loss': 0.54904}\n",
            "Step 185280: {'train_ae_loss': 0.64496, 'train_ucc_loss': 0.45002, 'train_ucc_acc': 0.84375, 'loss': 0.54749}\n",
            "Step 185300: {'train_ae_loss': 0.6543, 'train_ucc_loss': 0.41417, 'train_ucc_acc': 0.90625, 'loss': 0.53423}\n",
            "Step 185320: {'train_ae_loss': 0.68737, 'train_ucc_loss': 0.56756, 'train_ucc_acc': 0.71875, 'loss': 0.62747}\n",
            "Step 185340: {'train_ae_loss': 0.66588, 'train_ucc_loss': 0.36462, 'train_ucc_acc': 0.96875, 'loss': 0.51525}\n",
            "Step 185360: {'train_ae_loss': 0.65771, 'train_ucc_loss': 0.58694, 'train_ucc_acc': 0.6875, 'loss': 0.62232}\n",
            "Step 185380: {'train_ae_loss': 0.65702, 'train_ucc_loss': 0.51531, 'train_ucc_acc': 0.8125, 'loss': 0.58617}\n",
            "Step 185400: {'train_ae_loss': 0.66838, 'train_ucc_loss': 0.3573, 'train_ucc_acc': 0.9375, 'loss': 0.51284}\n",
            "Step 185420: {'train_ae_loss': 0.66568, 'train_ucc_loss': 0.53923, 'train_ucc_acc': 0.75, 'loss': 0.60246}\n",
            "Step 185440: {'train_ae_loss': 0.66958, 'train_ucc_loss': 0.34436, 'train_ucc_acc': 0.96875, 'loss': 0.50697}\n",
            "Step 185460: {'train_ae_loss': 0.64561, 'train_ucc_loss': 0.37003, 'train_ucc_acc': 0.9375, 'loss': 0.50782}\n",
            "Step 185480: {'train_ae_loss': 0.66088, 'train_ucc_loss': 0.47208, 'train_ucc_acc': 0.8125, 'loss': 0.56648}\n",
            "Step 185500: {'train_ae_loss': 0.65575, 'train_ucc_loss': 0.3947, 'train_ucc_acc': 0.90625, 'loss': 0.52522}\n",
            "Step 185520: {'train_ae_loss': 0.66357, 'train_ucc_loss': 0.40286, 'train_ucc_acc': 0.90625, 'loss': 0.53321}\n",
            "Step 185540: {'train_ae_loss': 0.67921, 'train_ucc_loss': 0.39931, 'train_ucc_acc': 0.9375, 'loss': 0.53926}\n",
            "Step 185560: {'train_ae_loss': 0.66038, 'train_ucc_loss': 0.43771, 'train_ucc_acc': 0.875, 'loss': 0.54904}\n",
            "Step 185580: {'train_ae_loss': 0.67243, 'train_ucc_loss': 0.36623, 'train_ucc_acc': 0.96875, 'loss': 0.51933}\n",
            "Step 185600: {'train_ae_loss': 0.66583, 'train_ucc_loss': 0.3475, 'train_ucc_acc': 0.96875, 'loss': 0.50667}\n",
            "Step 185620: {'train_ae_loss': 0.6625, 'train_ucc_loss': 0.3768, 'train_ucc_acc': 0.9375, 'loss': 0.51965}\n",
            "Step 185640: {'train_ae_loss': 0.65826, 'train_ucc_loss': 0.43574, 'train_ucc_acc': 0.875, 'loss': 0.547}\n",
            "Step 185660: {'train_ae_loss': 0.66812, 'train_ucc_loss': 0.37828, 'train_ucc_acc': 0.9375, 'loss': 0.5232}\n",
            "Step 185680: {'train_ae_loss': 0.6544, 'train_ucc_loss': 0.42975, 'train_ucc_acc': 0.875, 'loss': 0.54208}\n",
            "Step 185700: {'train_ae_loss': 0.65808, 'train_ucc_loss': 0.3545, 'train_ucc_acc': 0.96875, 'loss': 0.50629}\n",
            "Step 185720: {'train_ae_loss': 0.68128, 'train_ucc_loss': 0.39168, 'train_ucc_acc': 0.90625, 'loss': 0.53648}\n",
            "Step 185740: {'train_ae_loss': 0.65493, 'train_ucc_loss': 0.42777, 'train_ucc_acc': 0.875, 'loss': 0.54135}\n",
            "Step 185760: {'train_ae_loss': 0.65431, 'train_ucc_loss': 0.44224, 'train_ucc_acc': 0.875, 'loss': 0.54827}\n",
            "Step 185780: {'train_ae_loss': 0.66768, 'train_ucc_loss': 0.41141, 'train_ucc_acc': 0.875, 'loss': 0.53955}\n",
            "Step 185800: {'train_ae_loss': 0.65098, 'train_ucc_loss': 0.44112, 'train_ucc_acc': 0.84375, 'loss': 0.54605}\n",
            "Step 185820: {'train_ae_loss': 0.65589, 'train_ucc_loss': 0.44267, 'train_ucc_acc': 0.84375, 'loss': 0.54928}\n",
            "Step 185840: {'train_ae_loss': 0.65523, 'train_ucc_loss': 0.43014, 'train_ucc_acc': 0.84375, 'loss': 0.54268}\n",
            "Step 185860: {'train_ae_loss': 0.65507, 'train_ucc_loss': 0.4113, 'train_ucc_acc': 0.90625, 'loss': 0.53318}\n",
            "Step 185880: {'train_ae_loss': 0.65702, 'train_ucc_loss': 0.40203, 'train_ucc_acc': 0.90625, 'loss': 0.52952}\n",
            "Step 185900: {'train_ae_loss': 0.64314, 'train_ucc_loss': 0.38058, 'train_ucc_acc': 0.90625, 'loss': 0.51186}\n",
            "Step 185920: {'train_ae_loss': 0.66068, 'train_ucc_loss': 0.43575, 'train_ucc_acc': 0.875, 'loss': 0.54822}\n",
            "Step 185940: {'train_ae_loss': 0.67577, 'train_ucc_loss': 0.39908, 'train_ucc_acc': 0.90625, 'loss': 0.53742}\n",
            "Step 185960: {'train_ae_loss': 0.64608, 'train_ucc_loss': 0.4487, 'train_ucc_acc': 0.84375, 'loss': 0.54739}\n",
            "Step 185980: {'train_ae_loss': 0.64891, 'train_ucc_loss': 0.37309, 'train_ucc_acc': 0.96875, 'loss': 0.511}\n",
            "Step 186000: {'train_ae_loss': 0.65084, 'train_ucc_loss': 0.40308, 'train_ucc_acc': 0.90625, 'loss': 0.52696}\n",
            "step: 186000,eval_ae_loss: 0.64866,eval_ucc_loss: 0.4811,eval_ucc_acc: 0.8291\n",
            "Step 186020: {'train_ae_loss': 0.66873, 'train_ucc_loss': 0.37844, 'train_ucc_acc': 0.90625, 'loss': 0.52359}\n",
            "Step 186040: {'train_ae_loss': 0.66551, 'train_ucc_loss': 0.43765, 'train_ucc_acc': 0.875, 'loss': 0.55158}\n",
            "Step 186060: {'train_ae_loss': 0.66298, 'train_ucc_loss': 0.44018, 'train_ucc_acc': 0.875, 'loss': 0.55158}\n",
            "Step 186080: {'train_ae_loss': 0.65945, 'train_ucc_loss': 0.49463, 'train_ucc_acc': 0.84375, 'loss': 0.57704}\n",
            "Step 186100: {'train_ae_loss': 0.66681, 'train_ucc_loss': 0.37846, 'train_ucc_acc': 0.9375, 'loss': 0.52264}\n",
            "Step 186120: {'train_ae_loss': 0.67458, 'train_ucc_loss': 0.50329, 'train_ucc_acc': 0.78125, 'loss': 0.58893}\n",
            "Step 186140: {'train_ae_loss': 0.66762, 'train_ucc_loss': 0.49811, 'train_ucc_acc': 0.8125, 'loss': 0.58287}\n",
            "Step 186160: {'train_ae_loss': 0.6646, 'train_ucc_loss': 0.40689, 'train_ucc_acc': 0.90625, 'loss': 0.53574}\n",
            "Step 186180: {'train_ae_loss': 0.65959, 'train_ucc_loss': 0.45036, 'train_ucc_acc': 0.84375, 'loss': 0.55497}\n",
            "Step 186200: {'train_ae_loss': 0.66746, 'train_ucc_loss': 0.46784, 'train_ucc_acc': 0.8125, 'loss': 0.56765}\n",
            "Step 186220: {'train_ae_loss': 0.65403, 'train_ucc_loss': 0.40055, 'train_ucc_acc': 0.875, 'loss': 0.52729}\n",
            "Step 186240: {'train_ae_loss': 0.65048, 'train_ucc_loss': 0.46795, 'train_ucc_acc': 0.8125, 'loss': 0.55922}\n",
            "Step 186260: {'train_ae_loss': 0.65662, 'train_ucc_loss': 0.49601, 'train_ucc_acc': 0.8125, 'loss': 0.57632}\n",
            "Step 186280: {'train_ae_loss': 0.65681, 'train_ucc_loss': 0.49789, 'train_ucc_acc': 0.78125, 'loss': 0.57735}\n",
            "Step 186300: {'train_ae_loss': 0.65521, 'train_ucc_loss': 0.4846, 'train_ucc_acc': 0.8125, 'loss': 0.5699}\n",
            "Step 186320: {'train_ae_loss': 0.65362, 'train_ucc_loss': 0.36754, 'train_ucc_acc': 0.96875, 'loss': 0.51058}\n",
            "Step 186340: {'train_ae_loss': 0.67231, 'train_ucc_loss': 0.32863, 'train_ucc_acc': 1.0, 'loss': 0.50047}\n",
            "Step 186360: {'train_ae_loss': 0.64839, 'train_ucc_loss': 0.47238, 'train_ucc_acc': 0.84375, 'loss': 0.56038}\n",
            "Step 186380: {'train_ae_loss': 0.66714, 'train_ucc_loss': 0.4205, 'train_ucc_acc': 0.875, 'loss': 0.54382}\n",
            "Step 186400: {'train_ae_loss': 0.67873, 'train_ucc_loss': 0.46877, 'train_ucc_acc': 0.8125, 'loss': 0.57375}\n",
            "Step 186420: {'train_ae_loss': 0.64905, 'train_ucc_loss': 0.41695, 'train_ucc_acc': 0.90625, 'loss': 0.533}\n",
            "Step 186440: {'train_ae_loss': 0.6799, 'train_ucc_loss': 0.43975, 'train_ucc_acc': 0.90625, 'loss': 0.55982}\n",
            "Step 186460: {'train_ae_loss': 0.66078, 'train_ucc_loss': 0.37896, 'train_ucc_acc': 0.96875, 'loss': 0.51987}\n",
            "Step 186480: {'train_ae_loss': 0.6704, 'train_ucc_loss': 0.44321, 'train_ucc_acc': 0.875, 'loss': 0.55681}\n",
            "Step 186500: {'train_ae_loss': 0.66957, 'train_ucc_loss': 0.35646, 'train_ucc_acc': 0.96875, 'loss': 0.51302}\n",
            "Step 186520: {'train_ae_loss': 0.67221, 'train_ucc_loss': 0.43548, 'train_ucc_acc': 0.90625, 'loss': 0.55385}\n",
            "Step 186540: {'train_ae_loss': 0.66857, 'train_ucc_loss': 0.47653, 'train_ucc_acc': 0.8125, 'loss': 0.57255}\n",
            "Step 186560: {'train_ae_loss': 0.65681, 'train_ucc_loss': 0.36423, 'train_ucc_acc': 0.9375, 'loss': 0.51052}\n",
            "Step 186580: {'train_ae_loss': 0.67462, 'train_ucc_loss': 0.37151, 'train_ucc_acc': 0.90625, 'loss': 0.52306}\n",
            "Step 186600: {'train_ae_loss': 0.65936, 'train_ucc_loss': 0.43447, 'train_ucc_acc': 0.84375, 'loss': 0.54691}\n",
            "Step 186620: {'train_ae_loss': 0.66862, 'train_ucc_loss': 0.3954, 'train_ucc_acc': 0.90625, 'loss': 0.53201}\n",
            "Step 186640: {'train_ae_loss': 0.64602, 'train_ucc_loss': 0.44461, 'train_ucc_acc': 0.875, 'loss': 0.54531}\n",
            "Step 186660: {'train_ae_loss': 0.66677, 'train_ucc_loss': 0.38724, 'train_ucc_acc': 0.9375, 'loss': 0.527}\n",
            "Step 186680: {'train_ae_loss': 0.65714, 'train_ucc_loss': 0.4368, 'train_ucc_acc': 0.875, 'loss': 0.54697}\n",
            "Step 186700: {'train_ae_loss': 0.65126, 'train_ucc_loss': 0.47737, 'train_ucc_acc': 0.84375, 'loss': 0.56431}\n",
            "Step 186720: {'train_ae_loss': 0.66193, 'train_ucc_loss': 0.41907, 'train_ucc_acc': 0.90625, 'loss': 0.5405}\n",
            "Step 186740: {'train_ae_loss': 0.65865, 'train_ucc_loss': 0.44485, 'train_ucc_acc': 0.875, 'loss': 0.55175}\n",
            "Step 186760: {'train_ae_loss': 0.64463, 'train_ucc_loss': 0.48413, 'train_ucc_acc': 0.8125, 'loss': 0.56438}\n",
            "Step 186780: {'train_ae_loss': 0.64547, 'train_ucc_loss': 0.41101, 'train_ucc_acc': 0.875, 'loss': 0.52824}\n",
            "Step 186800: {'train_ae_loss': 0.6601, 'train_ucc_loss': 0.46088, 'train_ucc_acc': 0.875, 'loss': 0.56049}\n",
            "Step 186820: {'train_ae_loss': 0.63808, 'train_ucc_loss': 0.4166, 'train_ucc_acc': 0.90625, 'loss': 0.52734}\n",
            "Step 186840: {'train_ae_loss': 0.6638, 'train_ucc_loss': 0.3823, 'train_ucc_acc': 0.9375, 'loss': 0.52305}\n",
            "Step 186860: {'train_ae_loss': 0.64917, 'train_ucc_loss': 0.43249, 'train_ucc_acc': 0.875, 'loss': 0.54083}\n",
            "Step 186880: {'train_ae_loss': 0.66739, 'train_ucc_loss': 0.35247, 'train_ucc_acc': 0.96875, 'loss': 0.50993}\n",
            "Step 186900: {'train_ae_loss': 0.65006, 'train_ucc_loss': 0.50794, 'train_ucc_acc': 0.75, 'loss': 0.579}\n",
            "Step 186920: {'train_ae_loss': 0.66546, 'train_ucc_loss': 0.40507, 'train_ucc_acc': 0.90625, 'loss': 0.53526}\n",
            "Step 186940: {'train_ae_loss': 0.64199, 'train_ucc_loss': 0.4496, 'train_ucc_acc': 0.875, 'loss': 0.5458}\n",
            "Step 186960: {'train_ae_loss': 0.67796, 'train_ucc_loss': 0.40827, 'train_ucc_acc': 0.875, 'loss': 0.54311}\n",
            "Step 186980: {'train_ae_loss': 0.65508, 'train_ucc_loss': 0.49621, 'train_ucc_acc': 0.8125, 'loss': 0.57564}\n",
            "Step 187000: {'train_ae_loss': 0.65777, 'train_ucc_loss': 0.4326, 'train_ucc_acc': 0.84375, 'loss': 0.54518}\n",
            "step: 187000,eval_ae_loss: 0.64836,eval_ucc_loss: 0.46155,eval_ucc_acc: 0.84277\n",
            "Step 187020: {'train_ae_loss': 0.65185, 'train_ucc_loss': 0.4227, 'train_ucc_acc': 0.875, 'loss': 0.53727}\n",
            "Step 187040: {'train_ae_loss': 0.66112, 'train_ucc_loss': 0.42527, 'train_ucc_acc': 0.84375, 'loss': 0.54319}\n",
            "Step 187060: {'train_ae_loss': 0.66027, 'train_ucc_loss': 0.41861, 'train_ucc_acc': 0.90625, 'loss': 0.53944}\n",
            "Step 187080: {'train_ae_loss': 0.65035, 'train_ucc_loss': 0.4234, 'train_ucc_acc': 0.90625, 'loss': 0.53688}\n",
            "Step 187100: {'train_ae_loss': 0.66397, 'train_ucc_loss': 0.46107, 'train_ucc_acc': 0.8125, 'loss': 0.56252}\n",
            "Step 187120: {'train_ae_loss': 0.66804, 'train_ucc_loss': 0.40856, 'train_ucc_acc': 0.90625, 'loss': 0.5383}\n",
            "Step 187140: {'train_ae_loss': 0.65186, 'train_ucc_loss': 0.38299, 'train_ucc_acc': 0.90625, 'loss': 0.51742}\n",
            "Step 187160: {'train_ae_loss': 0.65677, 'train_ucc_loss': 0.45977, 'train_ucc_acc': 0.84375, 'loss': 0.55827}\n",
            "Step 187180: {'train_ae_loss': 0.65005, 'train_ucc_loss': 0.38138, 'train_ucc_acc': 0.9375, 'loss': 0.51572}\n",
            "Step 187200: {'train_ae_loss': 0.66804, 'train_ucc_loss': 0.48728, 'train_ucc_acc': 0.8125, 'loss': 0.57766}\n",
            "Step 187220: {'train_ae_loss': 0.65107, 'train_ucc_loss': 0.43044, 'train_ucc_acc': 0.875, 'loss': 0.54076}\n",
            "Step 187240: {'train_ae_loss': 0.66304, 'train_ucc_loss': 0.45041, 'train_ucc_acc': 0.875, 'loss': 0.55673}\n",
            "Step 187260: {'train_ae_loss': 0.6539, 'train_ucc_loss': 0.48755, 'train_ucc_acc': 0.84375, 'loss': 0.57073}\n",
            "Step 187280: {'train_ae_loss': 0.67147, 'train_ucc_loss': 0.51949, 'train_ucc_acc': 0.78125, 'loss': 0.59548}\n",
            "Step 187300: {'train_ae_loss': 0.64781, 'train_ucc_loss': 0.47236, 'train_ucc_acc': 0.84375, 'loss': 0.56008}\n",
            "Step 187320: {'train_ae_loss': 0.67321, 'train_ucc_loss': 0.43508, 'train_ucc_acc': 0.875, 'loss': 0.55415}\n",
            "Step 187340: {'train_ae_loss': 0.654, 'train_ucc_loss': 0.40796, 'train_ucc_acc': 0.90625, 'loss': 0.53098}\n",
            "Step 187360: {'train_ae_loss': 0.65133, 'train_ucc_loss': 0.43663, 'train_ucc_acc': 0.84375, 'loss': 0.54398}\n",
            "Step 187380: {'train_ae_loss': 0.65329, 'train_ucc_loss': 0.41499, 'train_ucc_acc': 0.875, 'loss': 0.53414}\n",
            "Step 187400: {'train_ae_loss': 0.65344, 'train_ucc_loss': 0.43347, 'train_ucc_acc': 0.875, 'loss': 0.54346}\n",
            "Step 187420: {'train_ae_loss': 0.64258, 'train_ucc_loss': 0.46439, 'train_ucc_acc': 0.84375, 'loss': 0.55349}\n",
            "Step 187440: {'train_ae_loss': 0.6569, 'train_ucc_loss': 0.36299, 'train_ucc_acc': 0.96875, 'loss': 0.50994}\n",
            "Step 187460: {'train_ae_loss': 0.67099, 'train_ucc_loss': 0.47501, 'train_ucc_acc': 0.8125, 'loss': 0.573}\n",
            "Step 187480: {'train_ae_loss': 0.65876, 'train_ucc_loss': 0.37943, 'train_ucc_acc': 0.96875, 'loss': 0.5191}\n",
            "Step 187500: {'train_ae_loss': 0.65412, 'train_ucc_loss': 0.48815, 'train_ucc_acc': 0.84375, 'loss': 0.57114}\n",
            "Step 187520: {'train_ae_loss': 0.65807, 'train_ucc_loss': 0.41312, 'train_ucc_acc': 0.90625, 'loss': 0.53559}\n",
            "Step 187540: {'train_ae_loss': 0.65521, 'train_ucc_loss': 0.49617, 'train_ucc_acc': 0.78125, 'loss': 0.57569}\n",
            "Step 187560: {'train_ae_loss': 0.65335, 'train_ucc_loss': 0.4529, 'train_ucc_acc': 0.84375, 'loss': 0.55312}\n",
            "Step 187580: {'train_ae_loss': 0.6468, 'train_ucc_loss': 0.52065, 'train_ucc_acc': 0.78125, 'loss': 0.58373}\n",
            "Step 187600: {'train_ae_loss': 0.65511, 'train_ucc_loss': 0.40409, 'train_ucc_acc': 0.90625, 'loss': 0.5296}\n",
            "Step 187620: {'train_ae_loss': 0.65926, 'train_ucc_loss': 0.39468, 'train_ucc_acc': 0.90625, 'loss': 0.52697}\n",
            "Step 187640: {'train_ae_loss': 0.65067, 'train_ucc_loss': 0.38173, 'train_ucc_acc': 0.9375, 'loss': 0.5162}\n",
            "Step 187660: {'train_ae_loss': 0.65191, 'train_ucc_loss': 0.41894, 'train_ucc_acc': 0.84375, 'loss': 0.53543}\n",
            "Step 187680: {'train_ae_loss': 0.64393, 'train_ucc_loss': 0.43638, 'train_ucc_acc': 0.875, 'loss': 0.54015}\n",
            "Step 187700: {'train_ae_loss': 0.65852, 'train_ucc_loss': 0.49584, 'train_ucc_acc': 0.8125, 'loss': 0.57718}\n",
            "Step 187720: {'train_ae_loss': 0.65965, 'train_ucc_loss': 0.48378, 'train_ucc_acc': 0.84375, 'loss': 0.57171}\n",
            "Step 187740: {'train_ae_loss': 0.65058, 'train_ucc_loss': 0.37786, 'train_ucc_acc': 0.9375, 'loss': 0.51422}\n",
            "Step 187760: {'train_ae_loss': 0.66715, 'train_ucc_loss': 0.42983, 'train_ucc_acc': 0.875, 'loss': 0.54849}\n",
            "Step 187780: {'train_ae_loss': 0.66287, 'train_ucc_loss': 0.41659, 'train_ucc_acc': 0.90625, 'loss': 0.53973}\n",
            "Step 187800: {'train_ae_loss': 0.66835, 'train_ucc_loss': 0.4288, 'train_ucc_acc': 0.875, 'loss': 0.54858}\n",
            "Step 187820: {'train_ae_loss': 0.66243, 'train_ucc_loss': 0.44702, 'train_ucc_acc': 0.875, 'loss': 0.55472}\n",
            "Step 187840: {'train_ae_loss': 0.65438, 'train_ucc_loss': 0.4563, 'train_ucc_acc': 0.84375, 'loss': 0.55534}\n",
            "Step 187860: {'train_ae_loss': 0.66309, 'train_ucc_loss': 0.48087, 'train_ucc_acc': 0.8125, 'loss': 0.57198}\n",
            "Step 187880: {'train_ae_loss': 0.65676, 'train_ucc_loss': 0.41835, 'train_ucc_acc': 0.875, 'loss': 0.53755}\n",
            "Step 187900: {'train_ae_loss': 0.6712, 'train_ucc_loss': 0.41149, 'train_ucc_acc': 0.875, 'loss': 0.54135}\n",
            "Step 187920: {'train_ae_loss': 0.6628, 'train_ucc_loss': 0.47693, 'train_ucc_acc': 0.84375, 'loss': 0.56987}\n",
            "Step 187940: {'train_ae_loss': 0.65621, 'train_ucc_loss': 0.48716, 'train_ucc_acc': 0.84375, 'loss': 0.57169}\n",
            "Step 187960: {'train_ae_loss': 0.66518, 'train_ucc_loss': 0.42002, 'train_ucc_acc': 0.90625, 'loss': 0.5426}\n",
            "Step 187980: {'train_ae_loss': 0.67198, 'train_ucc_loss': 0.41842, 'train_ucc_acc': 0.875, 'loss': 0.5452}\n",
            "Step 188000: {'train_ae_loss': 0.65286, 'train_ucc_loss': 0.43417, 'train_ucc_acc': 0.84375, 'loss': 0.54352}\n",
            "step: 188000,eval_ae_loss: 0.64883,eval_ucc_loss: 0.46493,eval_ucc_acc: 0.84668\n",
            "Step 188020: {'train_ae_loss': 0.65253, 'train_ucc_loss': 0.46373, 'train_ucc_acc': 0.84375, 'loss': 0.55813}\n",
            "Step 188040: {'train_ae_loss': 0.64875, 'train_ucc_loss': 0.40253, 'train_ucc_acc': 0.90625, 'loss': 0.52564}\n",
            "Step 188060: {'train_ae_loss': 0.64967, 'train_ucc_loss': 0.3323, 'train_ucc_acc': 1.0, 'loss': 0.49099}\n",
            "Step 188080: {'train_ae_loss': 0.67673, 'train_ucc_loss': 0.50739, 'train_ucc_acc': 0.8125, 'loss': 0.59206}\n",
            "Step 188100: {'train_ae_loss': 0.64222, 'train_ucc_loss': 0.54566, 'train_ucc_acc': 0.78125, 'loss': 0.59394}\n",
            "Step 188120: {'train_ae_loss': 0.64823, 'train_ucc_loss': 0.4247, 'train_ucc_acc': 0.90625, 'loss': 0.53646}\n",
            "Step 188140: {'train_ae_loss': 0.67784, 'train_ucc_loss': 0.37226, 'train_ucc_acc': 0.9375, 'loss': 0.52505}\n",
            "Step 188160: {'train_ae_loss': 0.67872, 'train_ucc_loss': 0.4849, 'train_ucc_acc': 0.8125, 'loss': 0.58181}\n",
            "Step 188180: {'train_ae_loss': 0.66056, 'train_ucc_loss': 0.43031, 'train_ucc_acc': 0.875, 'loss': 0.54543}\n",
            "Step 188200: {'train_ae_loss': 0.66258, 'train_ucc_loss': 0.38535, 'train_ucc_acc': 0.9375, 'loss': 0.52396}\n",
            "Step 188220: {'train_ae_loss': 0.6667, 'train_ucc_loss': 0.41245, 'train_ucc_acc': 0.84375, 'loss': 0.53958}\n",
            "Step 188240: {'train_ae_loss': 0.65398, 'train_ucc_loss': 0.40512, 'train_ucc_acc': 0.875, 'loss': 0.52955}\n",
            "Step 188260: {'train_ae_loss': 0.66242, 'train_ucc_loss': 0.4642, 'train_ucc_acc': 0.84375, 'loss': 0.56331}\n",
            "Step 188280: {'train_ae_loss': 0.64501, 'train_ucc_loss': 0.42088, 'train_ucc_acc': 0.875, 'loss': 0.53295}\n",
            "Step 188300: {'train_ae_loss': 0.66271, 'train_ucc_loss': 0.41007, 'train_ucc_acc': 0.90625, 'loss': 0.53639}\n",
            "Step 188320: {'train_ae_loss': 0.646, 'train_ucc_loss': 0.39932, 'train_ucc_acc': 0.90625, 'loss': 0.52266}\n",
            "Step 188340: {'train_ae_loss': 0.66425, 'train_ucc_loss': 0.41933, 'train_ucc_acc': 0.84375, 'loss': 0.54179}\n",
            "Step 188360: {'train_ae_loss': 0.63927, 'train_ucc_loss': 0.49402, 'train_ucc_acc': 0.8125, 'loss': 0.56664}\n",
            "Step 188380: {'train_ae_loss': 0.65776, 'train_ucc_loss': 0.51658, 'train_ucc_acc': 0.78125, 'loss': 0.58717}\n",
            "Step 188400: {'train_ae_loss': 0.65693, 'train_ucc_loss': 0.49196, 'train_ucc_acc': 0.8125, 'loss': 0.57445}\n",
            "Step 188420: {'train_ae_loss': 0.66415, 'train_ucc_loss': 0.47499, 'train_ucc_acc': 0.8125, 'loss': 0.56957}\n",
            "Step 188440: {'train_ae_loss': 0.6462, 'train_ucc_loss': 0.48418, 'train_ucc_acc': 0.8125, 'loss': 0.56519}\n",
            "Step 188460: {'train_ae_loss': 0.66854, 'train_ucc_loss': 0.46032, 'train_ucc_acc': 0.84375, 'loss': 0.56443}\n",
            "Step 188480: {'train_ae_loss': 0.65058, 'train_ucc_loss': 0.51437, 'train_ucc_acc': 0.8125, 'loss': 0.58247}\n",
            "Step 188500: {'train_ae_loss': 0.64723, 'train_ucc_loss': 0.48999, 'train_ucc_acc': 0.8125, 'loss': 0.56861}\n",
            "Step 188520: {'train_ae_loss': 0.66276, 'train_ucc_loss': 0.52234, 'train_ucc_acc': 0.78125, 'loss': 0.59255}\n",
            "Step 188540: {'train_ae_loss': 0.6632, 'train_ucc_loss': 0.41957, 'train_ucc_acc': 0.875, 'loss': 0.54138}\n",
            "Step 188560: {'train_ae_loss': 0.67232, 'train_ucc_loss': 0.39992, 'train_ucc_acc': 0.875, 'loss': 0.53612}\n",
            "Step 188580: {'train_ae_loss': 0.66141, 'train_ucc_loss': 0.37816, 'train_ucc_acc': 0.9375, 'loss': 0.51978}\n",
            "Step 188600: {'train_ae_loss': 0.66508, 'train_ucc_loss': 0.49351, 'train_ucc_acc': 0.8125, 'loss': 0.5793}\n",
            "Step 188620: {'train_ae_loss': 0.6595, 'train_ucc_loss': 0.34936, 'train_ucc_acc': 0.96875, 'loss': 0.50443}\n",
            "Step 188640: {'train_ae_loss': 0.65284, 'train_ucc_loss': 0.43884, 'train_ucc_acc': 0.875, 'loss': 0.54584}\n",
            "Step 188660: {'train_ae_loss': 0.65231, 'train_ucc_loss': 0.41367, 'train_ucc_acc': 0.875, 'loss': 0.53299}\n",
            "Step 188680: {'train_ae_loss': 0.66483, 'train_ucc_loss': 0.37766, 'train_ucc_acc': 0.96875, 'loss': 0.52124}\n",
            "Step 188700: {'train_ae_loss': 0.65623, 'train_ucc_loss': 0.48412, 'train_ucc_acc': 0.84375, 'loss': 0.57017}\n",
            "Step 188720: {'train_ae_loss': 0.66102, 'train_ucc_loss': 0.48082, 'train_ucc_acc': 0.84375, 'loss': 0.57092}\n",
            "Step 188740: {'train_ae_loss': 0.65516, 'train_ucc_loss': 0.41104, 'train_ucc_acc': 0.875, 'loss': 0.5331}\n",
            "Step 188760: {'train_ae_loss': 0.64169, 'train_ucc_loss': 0.41309, 'train_ucc_acc': 0.875, 'loss': 0.52739}\n",
            "Step 188780: {'train_ae_loss': 0.65094, 'train_ucc_loss': 0.62587, 'train_ucc_acc': 0.65625, 'loss': 0.6384}\n",
            "Step 188800: {'train_ae_loss': 0.65845, 'train_ucc_loss': 0.43742, 'train_ucc_acc': 0.875, 'loss': 0.54794}\n",
            "Step 188820: {'train_ae_loss': 0.65766, 'train_ucc_loss': 0.36646, 'train_ucc_acc': 0.9375, 'loss': 0.51206}\n",
            "Step 188840: {'train_ae_loss': 0.64965, 'train_ucc_loss': 0.35979, 'train_ucc_acc': 0.96875, 'loss': 0.50472}\n",
            "Step 188860: {'train_ae_loss': 0.6561, 'train_ucc_loss': 0.44047, 'train_ucc_acc': 0.84375, 'loss': 0.54828}\n",
            "Step 188880: {'train_ae_loss': 0.65505, 'train_ucc_loss': 0.41691, 'train_ucc_acc': 0.90625, 'loss': 0.53598}\n",
            "Step 188900: {'train_ae_loss': 0.65532, 'train_ucc_loss': 0.38537, 'train_ucc_acc': 0.9375, 'loss': 0.52034}\n",
            "Step 188920: {'train_ae_loss': 0.6521, 'train_ucc_loss': 0.3836, 'train_ucc_acc': 0.9375, 'loss': 0.51785}\n",
            "Step 188940: {'train_ae_loss': 0.65576, 'train_ucc_loss': 0.35796, 'train_ucc_acc': 0.96875, 'loss': 0.50686}\n",
            "Step 188960: {'train_ae_loss': 0.65212, 'train_ucc_loss': 0.47717, 'train_ucc_acc': 0.8125, 'loss': 0.56464}\n",
            "Step 188980: {'train_ae_loss': 0.64485, 'train_ucc_loss': 0.45109, 'train_ucc_acc': 0.84375, 'loss': 0.54797}\n",
            "Step 189000: {'train_ae_loss': 0.64547, 'train_ucc_loss': 0.42651, 'train_ucc_acc': 0.875, 'loss': 0.53599}\n",
            "step: 189000,eval_ae_loss: 0.64771,eval_ucc_loss: 0.49413,eval_ucc_acc: 0.81055\n",
            "Step 189020: {'train_ae_loss': 0.65997, 'train_ucc_loss': 0.40627, 'train_ucc_acc': 0.9375, 'loss': 0.53312}\n",
            "Step 189040: {'train_ae_loss': 0.65154, 'train_ucc_loss': 0.46182, 'train_ucc_acc': 0.84375, 'loss': 0.55668}\n",
            "Step 189060: {'train_ae_loss': 0.66193, 'train_ucc_loss': 0.43834, 'train_ucc_acc': 0.875, 'loss': 0.55014}\n",
            "Step 189080: {'train_ae_loss': 0.66759, 'train_ucc_loss': 0.32395, 'train_ucc_acc': 1.0, 'loss': 0.49577}\n",
            "Step 189100: {'train_ae_loss': 0.66127, 'train_ucc_loss': 0.48612, 'train_ucc_acc': 0.84375, 'loss': 0.5737}\n",
            "Step 189120: {'train_ae_loss': 0.65705, 'train_ucc_loss': 0.43803, 'train_ucc_acc': 0.875, 'loss': 0.54754}\n",
            "Step 189140: {'train_ae_loss': 0.65927, 'train_ucc_loss': 0.38222, 'train_ucc_acc': 0.90625, 'loss': 0.52074}\n",
            "Step 189160: {'train_ae_loss': 0.67013, 'train_ucc_loss': 0.46462, 'train_ucc_acc': 0.84375, 'loss': 0.56737}\n",
            "Step 189180: {'train_ae_loss': 0.66172, 'train_ucc_loss': 0.4225, 'train_ucc_acc': 0.875, 'loss': 0.54211}\n",
            "Step 189200: {'train_ae_loss': 0.62921, 'train_ucc_loss': 0.40031, 'train_ucc_acc': 0.90625, 'loss': 0.51476}\n",
            "Step 189220: {'train_ae_loss': 0.66415, 'train_ucc_loss': 0.33637, 'train_ucc_acc': 0.96875, 'loss': 0.50026}\n",
            "Step 189240: {'train_ae_loss': 0.65806, 'train_ucc_loss': 0.42941, 'train_ucc_acc': 0.84375, 'loss': 0.54373}\n",
            "Step 189260: {'train_ae_loss': 0.66323, 'train_ucc_loss': 0.3932, 'train_ucc_acc': 0.9375, 'loss': 0.52822}\n",
            "Step 189280: {'train_ae_loss': 0.66094, 'train_ucc_loss': 0.4138, 'train_ucc_acc': 0.875, 'loss': 0.53737}\n",
            "Step 189300: {'train_ae_loss': 0.64784, 'train_ucc_loss': 0.46479, 'train_ucc_acc': 0.8125, 'loss': 0.55632}\n",
            "Step 189320: {'train_ae_loss': 0.65774, 'train_ucc_loss': 0.45873, 'train_ucc_acc': 0.84375, 'loss': 0.55824}\n",
            "Step 189340: {'train_ae_loss': 0.63713, 'train_ucc_loss': 0.39388, 'train_ucc_acc': 0.90625, 'loss': 0.51551}\n",
            "Step 189360: {'train_ae_loss': 0.64223, 'train_ucc_loss': 0.49457, 'train_ucc_acc': 0.8125, 'loss': 0.5684}\n",
            "Step 189380: {'train_ae_loss': 0.64066, 'train_ucc_loss': 0.42819, 'train_ucc_acc': 0.90625, 'loss': 0.53443}\n",
            "Step 189400: {'train_ae_loss': 0.64529, 'train_ucc_loss': 0.38583, 'train_ucc_acc': 0.90625, 'loss': 0.51556}\n",
            "Step 189420: {'train_ae_loss': 0.6565, 'train_ucc_loss': 0.40975, 'train_ucc_acc': 0.875, 'loss': 0.53312}\n",
            "Step 189440: {'train_ae_loss': 0.65075, 'train_ucc_loss': 0.44899, 'train_ucc_acc': 0.84375, 'loss': 0.54987}\n",
            "Step 189460: {'train_ae_loss': 0.66755, 'train_ucc_loss': 0.41051, 'train_ucc_acc': 0.875, 'loss': 0.53903}\n",
            "Step 189480: {'train_ae_loss': 0.64694, 'train_ucc_loss': 0.44049, 'train_ucc_acc': 0.84375, 'loss': 0.54372}\n",
            "Step 189500: {'train_ae_loss': 0.65377, 'train_ucc_loss': 0.39431, 'train_ucc_acc': 0.90625, 'loss': 0.52404}\n",
            "Step 189520: {'train_ae_loss': 0.66771, 'train_ucc_loss': 0.48362, 'train_ucc_acc': 0.8125, 'loss': 0.57567}\n",
            "Step 189540: {'train_ae_loss': 0.65772, 'train_ucc_loss': 0.41279, 'train_ucc_acc': 0.875, 'loss': 0.53525}\n",
            "Step 189560: {'train_ae_loss': 0.65413, 'train_ucc_loss': 0.40189, 'train_ucc_acc': 0.9375, 'loss': 0.52801}\n",
            "Step 189580: {'train_ae_loss': 0.65063, 'train_ucc_loss': 0.51021, 'train_ucc_acc': 0.8125, 'loss': 0.58042}\n",
            "Step 189600: {'train_ae_loss': 0.66237, 'train_ucc_loss': 0.39328, 'train_ucc_acc': 0.9375, 'loss': 0.52782}\n",
            "Step 189620: {'train_ae_loss': 0.64945, 'train_ucc_loss': 0.46893, 'train_ucc_acc': 0.84375, 'loss': 0.55919}\n",
            "Step 189640: {'train_ae_loss': 0.65462, 'train_ucc_loss': 0.47383, 'train_ucc_acc': 0.84375, 'loss': 0.56423}\n",
            "Step 189660: {'train_ae_loss': 0.65953, 'train_ucc_loss': 0.441, 'train_ucc_acc': 0.875, 'loss': 0.55026}\n",
            "Step 189680: {'train_ae_loss': 0.66102, 'train_ucc_loss': 0.50511, 'train_ucc_acc': 0.8125, 'loss': 0.58306}\n",
            "Step 189700: {'train_ae_loss': 0.65973, 'train_ucc_loss': 0.42053, 'train_ucc_acc': 0.90625, 'loss': 0.54013}\n",
            "Step 189720: {'train_ae_loss': 0.64922, 'train_ucc_loss': 0.41722, 'train_ucc_acc': 0.875, 'loss': 0.53322}\n",
            "Step 189740: {'train_ae_loss': 0.65842, 'train_ucc_loss': 0.41666, 'train_ucc_acc': 0.90625, 'loss': 0.53754}\n",
            "Step 189760: {'train_ae_loss': 0.6625, 'train_ucc_loss': 0.48393, 'train_ucc_acc': 0.8125, 'loss': 0.57322}\n",
            "Step 189780: {'train_ae_loss': 0.66813, 'train_ucc_loss': 0.49484, 'train_ucc_acc': 0.78125, 'loss': 0.58148}\n",
            "Step 189800: {'train_ae_loss': 0.64689, 'train_ucc_loss': 0.51495, 'train_ucc_acc': 0.78125, 'loss': 0.58092}\n",
            "Step 189820: {'train_ae_loss': 0.65668, 'train_ucc_loss': 0.50347, 'train_ucc_acc': 0.8125, 'loss': 0.58007}\n",
            "Step 189840: {'train_ae_loss': 0.65721, 'train_ucc_loss': 0.42371, 'train_ucc_acc': 0.875, 'loss': 0.54046}\n",
            "Step 189860: {'train_ae_loss': 0.67812, 'train_ucc_loss': 0.50869, 'train_ucc_acc': 0.8125, 'loss': 0.59341}\n",
            "Step 189880: {'train_ae_loss': 0.6727, 'train_ucc_loss': 0.45328, 'train_ucc_acc': 0.84375, 'loss': 0.56299}\n",
            "Step 189900: {'train_ae_loss': 0.65172, 'train_ucc_loss': 0.40783, 'train_ucc_acc': 0.90625, 'loss': 0.52977}\n",
            "Step 189920: {'train_ae_loss': 0.6624, 'train_ucc_loss': 0.40401, 'train_ucc_acc': 0.90625, 'loss': 0.5332}\n",
            "Step 189940: {'train_ae_loss': 0.6611, 'train_ucc_loss': 0.45451, 'train_ucc_acc': 0.875, 'loss': 0.5578}\n",
            "Step 189960: {'train_ae_loss': 0.65228, 'train_ucc_loss': 0.46823, 'train_ucc_acc': 0.84375, 'loss': 0.56025}\n",
            "Step 189980: {'train_ae_loss': 0.66785, 'train_ucc_loss': 0.44716, 'train_ucc_acc': 0.84375, 'loss': 0.5575}\n",
            "Step 190000: {'train_ae_loss': 0.66093, 'train_ucc_loss': 0.43788, 'train_ucc_acc': 0.84375, 'loss': 0.5494}\n",
            "step: 190000,eval_ae_loss: 0.64476,eval_ucc_loss: 0.46224,eval_ucc_acc: 0.83496\n",
            "Step 190020: {'train_ae_loss': 0.65622, 'train_ucc_loss': 0.43148, 'train_ucc_acc': 0.84375, 'loss': 0.54385}\n",
            "Step 190040: {'train_ae_loss': 0.65205, 'train_ucc_loss': 0.4502, 'train_ucc_acc': 0.8125, 'loss': 0.55112}\n",
            "Step 190060: {'train_ae_loss': 0.65132, 'train_ucc_loss': 0.51034, 'train_ucc_acc': 0.8125, 'loss': 0.58083}\n",
            "Step 190080: {'train_ae_loss': 0.61654, 'train_ucc_loss': 0.42845, 'train_ucc_acc': 0.875, 'loss': 0.5225}\n",
            "Step 190100: {'train_ae_loss': 0.64042, 'train_ucc_loss': 0.43901, 'train_ucc_acc': 0.84375, 'loss': 0.53972}\n",
            "Step 190120: {'train_ae_loss': 0.63621, 'train_ucc_loss': 0.49182, 'train_ucc_acc': 0.84375, 'loss': 0.56402}\n",
            "Step 190140: {'train_ae_loss': 0.6618, 'train_ucc_loss': 0.43643, 'train_ucc_acc': 0.875, 'loss': 0.54911}\n",
            "Step 190160: {'train_ae_loss': 0.66005, 'train_ucc_loss': 0.43285, 'train_ucc_acc': 0.875, 'loss': 0.54645}\n",
            "Step 190180: {'train_ae_loss': 0.65547, 'train_ucc_loss': 0.49237, 'train_ucc_acc': 0.8125, 'loss': 0.57392}\n",
            "Step 190200: {'train_ae_loss': 0.6647, 'train_ucc_loss': 0.37079, 'train_ucc_acc': 0.9375, 'loss': 0.51774}\n",
            "Step 190220: {'train_ae_loss': 0.66685, 'train_ucc_loss': 0.56841, 'train_ucc_acc': 0.75, 'loss': 0.61763}\n",
            "Step 190240: {'train_ae_loss': 0.64486, 'train_ucc_loss': 0.41058, 'train_ucc_acc': 0.90625, 'loss': 0.52772}\n",
            "Step 190260: {'train_ae_loss': 0.67214, 'train_ucc_loss': 0.58841, 'train_ucc_acc': 0.6875, 'loss': 0.63027}\n",
            "Step 190280: {'train_ae_loss': 0.67477, 'train_ucc_loss': 0.49064, 'train_ucc_acc': 0.8125, 'loss': 0.5827}\n",
            "Step 190300: {'train_ae_loss': 0.66572, 'train_ucc_loss': 0.44213, 'train_ucc_acc': 0.84375, 'loss': 0.55393}\n",
            "Step 190320: {'train_ae_loss': 0.66272, 'train_ucc_loss': 0.43167, 'train_ucc_acc': 0.84375, 'loss': 0.54719}\n",
            "Step 190340: {'train_ae_loss': 0.65063, 'train_ucc_loss': 0.3909, 'train_ucc_acc': 0.9375, 'loss': 0.52077}\n",
            "Step 190360: {'train_ae_loss': 0.65141, 'train_ucc_loss': 0.50117, 'train_ucc_acc': 0.8125, 'loss': 0.57629}\n",
            "Step 190380: {'train_ae_loss': 0.65252, 'train_ucc_loss': 0.47505, 'train_ucc_acc': 0.84375, 'loss': 0.56378}\n",
            "Step 190400: {'train_ae_loss': 0.65164, 'train_ucc_loss': 0.45211, 'train_ucc_acc': 0.875, 'loss': 0.55188}\n",
            "Step 190420: {'train_ae_loss': 0.6655, 'train_ucc_loss': 0.42324, 'train_ucc_acc': 0.84375, 'loss': 0.54437}\n",
            "Step 190440: {'train_ae_loss': 0.66895, 'train_ucc_loss': 0.39276, 'train_ucc_acc': 0.9375, 'loss': 0.53086}\n",
            "Step 190460: {'train_ae_loss': 0.66063, 'train_ucc_loss': 0.44137, 'train_ucc_acc': 0.875, 'loss': 0.551}\n",
            "Step 190480: {'train_ae_loss': 0.65962, 'train_ucc_loss': 0.45532, 'train_ucc_acc': 0.84375, 'loss': 0.55747}\n",
            "Step 190500: {'train_ae_loss': 0.66356, 'train_ucc_loss': 0.4664, 'train_ucc_acc': 0.84375, 'loss': 0.56498}\n",
            "Step 190520: {'train_ae_loss': 0.67019, 'train_ucc_loss': 0.55985, 'train_ucc_acc': 0.75, 'loss': 0.61502}\n",
            "Step 190540: {'train_ae_loss': 0.67555, 'train_ucc_loss': 0.4607, 'train_ucc_acc': 0.84375, 'loss': 0.56812}\n",
            "Step 190560: {'train_ae_loss': 0.64287, 'train_ucc_loss': 0.42489, 'train_ucc_acc': 0.90625, 'loss': 0.53388}\n",
            "Step 190580: {'train_ae_loss': 0.66485, 'train_ucc_loss': 0.37046, 'train_ucc_acc': 0.9375, 'loss': 0.51765}\n",
            "Step 190600: {'train_ae_loss': 0.64229, 'train_ucc_loss': 0.42138, 'train_ucc_acc': 0.90625, 'loss': 0.53183}\n",
            "Step 190620: {'train_ae_loss': 0.66213, 'train_ucc_loss': 0.42415, 'train_ucc_acc': 0.90625, 'loss': 0.54314}\n",
            "Step 190640: {'train_ae_loss': 0.66759, 'train_ucc_loss': 0.50821, 'train_ucc_acc': 0.78125, 'loss': 0.5879}\n",
            "Step 190660: {'train_ae_loss': 0.66743, 'train_ucc_loss': 0.41138, 'train_ucc_acc': 0.90625, 'loss': 0.5394}\n",
            "Step 190680: {'train_ae_loss': 0.65322, 'train_ucc_loss': 0.42083, 'train_ucc_acc': 0.875, 'loss': 0.53703}\n",
            "Step 190700: {'train_ae_loss': 0.65003, 'train_ucc_loss': 0.41179, 'train_ucc_acc': 0.90625, 'loss': 0.53091}\n",
            "Step 190720: {'train_ae_loss': 0.64584, 'train_ucc_loss': 0.46556, 'train_ucc_acc': 0.84375, 'loss': 0.5557}\n",
            "Step 190740: {'train_ae_loss': 0.64016, 'train_ucc_loss': 0.45768, 'train_ucc_acc': 0.84375, 'loss': 0.54892}\n",
            "Step 190760: {'train_ae_loss': 0.6429, 'train_ucc_loss': 0.44617, 'train_ucc_acc': 0.875, 'loss': 0.54453}\n",
            "Step 190780: {'train_ae_loss': 0.66804, 'train_ucc_loss': 0.52164, 'train_ucc_acc': 0.75, 'loss': 0.59484}\n",
            "Step 190800: {'train_ae_loss': 0.65569, 'train_ucc_loss': 0.51391, 'train_ucc_acc': 0.78125, 'loss': 0.5848}\n",
            "Step 190820: {'train_ae_loss': 0.65871, 'train_ucc_loss': 0.46592, 'train_ucc_acc': 0.8125, 'loss': 0.56231}\n",
            "Step 190840: {'train_ae_loss': 0.65999, 'train_ucc_loss': 0.47879, 'train_ucc_acc': 0.84375, 'loss': 0.56939}\n",
            "Step 190860: {'train_ae_loss': 0.65092, 'train_ucc_loss': 0.49783, 'train_ucc_acc': 0.8125, 'loss': 0.57438}\n",
            "Step 190880: {'train_ae_loss': 0.65821, 'train_ucc_loss': 0.49739, 'train_ucc_acc': 0.8125, 'loss': 0.5778}\n",
            "Step 190900: {'train_ae_loss': 0.66696, 'train_ucc_loss': 0.41338, 'train_ucc_acc': 0.90625, 'loss': 0.54017}\n",
            "Step 190920: {'train_ae_loss': 0.64927, 'train_ucc_loss': 0.41239, 'train_ucc_acc': 0.90625, 'loss': 0.53083}\n",
            "Step 190940: {'train_ae_loss': 0.63351, 'train_ucc_loss': 0.45174, 'train_ucc_acc': 0.84375, 'loss': 0.54262}\n",
            "Step 190960: {'train_ae_loss': 0.65981, 'train_ucc_loss': 0.5029, 'train_ucc_acc': 0.78125, 'loss': 0.58135}\n",
            "Step 190980: {'train_ae_loss': 0.66968, 'train_ucc_loss': 0.52958, 'train_ucc_acc': 0.78125, 'loss': 0.59963}\n",
            "Step 191000: {'train_ae_loss': 0.65552, 'train_ucc_loss': 0.46578, 'train_ucc_acc': 0.84375, 'loss': 0.56065}\n",
            "step: 191000,eval_ae_loss: 0.64672,eval_ucc_loss: 0.46381,eval_ucc_acc: 0.84766\n",
            "Step 191020: {'train_ae_loss': 0.66883, 'train_ucc_loss': 0.40499, 'train_ucc_acc': 0.90625, 'loss': 0.53691}\n",
            "Step 191040: {'train_ae_loss': 0.66045, 'train_ucc_loss': 0.49563, 'train_ucc_acc': 0.8125, 'loss': 0.57804}\n",
            "Step 191060: {'train_ae_loss': 0.65817, 'train_ucc_loss': 0.45092, 'train_ucc_acc': 0.875, 'loss': 0.55455}\n",
            "Step 191080: {'train_ae_loss': 0.66017, 'train_ucc_loss': 0.50388, 'train_ucc_acc': 0.8125, 'loss': 0.58203}\n",
            "Step 191100: {'train_ae_loss': 0.66906, 'train_ucc_loss': 0.47572, 'train_ucc_acc': 0.78125, 'loss': 0.57239}\n",
            "Step 191120: {'train_ae_loss': 0.64231, 'train_ucc_loss': 0.53763, 'train_ucc_acc': 0.8125, 'loss': 0.58997}\n",
            "Step 191140: {'train_ae_loss': 0.65549, 'train_ucc_loss': 0.41951, 'train_ucc_acc': 0.90625, 'loss': 0.5375}\n",
            "Step 191160: {'train_ae_loss': 0.64135, 'train_ucc_loss': 0.4679, 'train_ucc_acc': 0.84375, 'loss': 0.55462}\n",
            "Step 191180: {'train_ae_loss': 0.62869, 'train_ucc_loss': 0.46896, 'train_ucc_acc': 0.875, 'loss': 0.54883}\n",
            "Step 191200: {'train_ae_loss': 0.6591, 'train_ucc_loss': 0.42508, 'train_ucc_acc': 0.875, 'loss': 0.54209}\n",
            "Step 191220: {'train_ae_loss': 0.65996, 'train_ucc_loss': 0.56386, 'train_ucc_acc': 0.71875, 'loss': 0.61191}\n",
            "Step 191240: {'train_ae_loss': 0.64613, 'train_ucc_loss': 0.37271, 'train_ucc_acc': 0.9375, 'loss': 0.50942}\n",
            "Step 191260: {'train_ae_loss': 0.65474, 'train_ucc_loss': 0.45239, 'train_ucc_acc': 0.875, 'loss': 0.55357}\n",
            "Step 191280: {'train_ae_loss': 0.64871, 'train_ucc_loss': 0.43274, 'train_ucc_acc': 0.875, 'loss': 0.54072}\n",
            "Step 191300: {'train_ae_loss': 0.65006, 'train_ucc_loss': 0.46375, 'train_ucc_acc': 0.875, 'loss': 0.55691}\n",
            "Step 191320: {'train_ae_loss': 0.65763, 'train_ucc_loss': 0.48556, 'train_ucc_acc': 0.8125, 'loss': 0.5716}\n",
            "Step 191340: {'train_ae_loss': 0.66031, 'train_ucc_loss': 0.48088, 'train_ucc_acc': 0.8125, 'loss': 0.57059}\n",
            "Step 191360: {'train_ae_loss': 0.66179, 'train_ucc_loss': 0.38792, 'train_ucc_acc': 0.9375, 'loss': 0.52486}\n",
            "Step 191380: {'train_ae_loss': 0.66839, 'train_ucc_loss': 0.5601, 'train_ucc_acc': 0.71875, 'loss': 0.61424}\n",
            "Step 191400: {'train_ae_loss': 0.6765, 'train_ucc_loss': 0.36421, 'train_ucc_acc': 0.9375, 'loss': 0.52035}\n",
            "Step 191420: {'train_ae_loss': 0.64231, 'train_ucc_loss': 0.46909, 'train_ucc_acc': 0.84375, 'loss': 0.5557}\n",
            "Step 191440: {'train_ae_loss': 0.65356, 'train_ucc_loss': 0.47743, 'train_ucc_acc': 0.78125, 'loss': 0.56549}\n",
            "Step 191460: {'train_ae_loss': 0.65893, 'train_ucc_loss': 0.47817, 'train_ucc_acc': 0.8125, 'loss': 0.56855}\n",
            "Step 191480: {'train_ae_loss': 0.64379, 'train_ucc_loss': 0.48634, 'train_ucc_acc': 0.8125, 'loss': 0.56507}\n",
            "Step 191500: {'train_ae_loss': 0.66249, 'train_ucc_loss': 0.44676, 'train_ucc_acc': 0.84375, 'loss': 0.55463}\n",
            "Step 191520: {'train_ae_loss': 0.66639, 'train_ucc_loss': 0.39595, 'train_ucc_acc': 0.875, 'loss': 0.53117}\n",
            "Step 191540: {'train_ae_loss': 0.64621, 'train_ucc_loss': 0.46696, 'train_ucc_acc': 0.8125, 'loss': 0.55658}\n",
            "Step 191560: {'train_ae_loss': 0.65607, 'train_ucc_loss': 0.43579, 'train_ucc_acc': 0.84375, 'loss': 0.54593}\n",
            "Step 191580: {'train_ae_loss': 0.668, 'train_ucc_loss': 0.40504, 'train_ucc_acc': 0.9375, 'loss': 0.53652}\n",
            "Step 191600: {'train_ae_loss': 0.65996, 'train_ucc_loss': 0.41791, 'train_ucc_acc': 0.875, 'loss': 0.53893}\n",
            "Step 191620: {'train_ae_loss': 0.65039, 'train_ucc_loss': 0.47033, 'train_ucc_acc': 0.8125, 'loss': 0.56036}\n",
            "Step 191640: {'train_ae_loss': 0.65297, 'train_ucc_loss': 0.46259, 'train_ucc_acc': 0.84375, 'loss': 0.55778}\n",
            "Step 191660: {'train_ae_loss': 0.65699, 'train_ucc_loss': 0.51315, 'train_ucc_acc': 0.78125, 'loss': 0.58507}\n",
            "Step 191680: {'train_ae_loss': 0.65837, 'train_ucc_loss': 0.40351, 'train_ucc_acc': 0.90625, 'loss': 0.53094}\n",
            "Step 191700: {'train_ae_loss': 0.65695, 'train_ucc_loss': 0.42707, 'train_ucc_acc': 0.875, 'loss': 0.54201}\n",
            "Step 191720: {'train_ae_loss': 0.64915, 'train_ucc_loss': 0.35948, 'train_ucc_acc': 0.96875, 'loss': 0.50431}\n",
            "Step 191740: {'train_ae_loss': 0.64662, 'train_ucc_loss': 0.39021, 'train_ucc_acc': 0.90625, 'loss': 0.51842}\n",
            "Step 191760: {'train_ae_loss': 0.67151, 'train_ucc_loss': 0.3866, 'train_ucc_acc': 0.9375, 'loss': 0.52906}\n",
            "Step 191780: {'train_ae_loss': 0.66187, 'train_ucc_loss': 0.40574, 'train_ucc_acc': 0.90625, 'loss': 0.53381}\n",
            "Step 191800: {'train_ae_loss': 0.66446, 'train_ucc_loss': 0.41821, 'train_ucc_acc': 0.90625, 'loss': 0.54134}\n",
            "Step 191820: {'train_ae_loss': 0.64779, 'train_ucc_loss': 0.44484, 'train_ucc_acc': 0.875, 'loss': 0.54632}\n",
            "Step 191840: {'train_ae_loss': 0.65129, 'train_ucc_loss': 0.42084, 'train_ucc_acc': 0.90625, 'loss': 0.53606}\n",
            "Step 191860: {'train_ae_loss': 0.63856, 'train_ucc_loss': 0.4424, 'train_ucc_acc': 0.84375, 'loss': 0.54048}\n",
            "Step 191880: {'train_ae_loss': 0.66359, 'train_ucc_loss': 0.43863, 'train_ucc_acc': 0.84375, 'loss': 0.55111}\n",
            "Step 191900: {'train_ae_loss': 0.65245, 'train_ucc_loss': 0.41815, 'train_ucc_acc': 0.90625, 'loss': 0.5353}\n",
            "Step 191920: {'train_ae_loss': 0.65233, 'train_ucc_loss': 0.56317, 'train_ucc_acc': 0.75, 'loss': 0.60775}\n",
            "Step 191940: {'train_ae_loss': 0.65013, 'train_ucc_loss': 0.52523, 'train_ucc_acc': 0.78125, 'loss': 0.58768}\n",
            "Step 191960: {'train_ae_loss': 0.65083, 'train_ucc_loss': 0.47713, 'train_ucc_acc': 0.84375, 'loss': 0.56398}\n",
            "Step 191980: {'train_ae_loss': 0.65647, 'train_ucc_loss': 0.46699, 'train_ucc_acc': 0.84375, 'loss': 0.56173}\n",
            "Step 192000: {'train_ae_loss': 0.65106, 'train_ucc_loss': 0.38503, 'train_ucc_acc': 0.9375, 'loss': 0.51805}\n",
            "step: 192000,eval_ae_loss: 0.64266,eval_ucc_loss: 0.51929,eval_ucc_acc: 0.78418\n",
            "Step 192020: {'train_ae_loss': 0.6576, 'train_ucc_loss': 0.51339, 'train_ucc_acc': 0.8125, 'loss': 0.5855}\n",
            "Step 192040: {'train_ae_loss': 0.64379, 'train_ucc_loss': 0.50217, 'train_ucc_acc': 0.8125, 'loss': 0.57298}\n",
            "Step 192060: {'train_ae_loss': 0.65039, 'train_ucc_loss': 0.38855, 'train_ucc_acc': 0.9375, 'loss': 0.51947}\n",
            "Step 192080: {'train_ae_loss': 0.68587, 'train_ucc_loss': 0.4137, 'train_ucc_acc': 0.90625, 'loss': 0.54979}\n",
            "Step 192100: {'train_ae_loss': 0.66523, 'train_ucc_loss': 0.36304, 'train_ucc_acc': 0.96875, 'loss': 0.51414}\n",
            "Step 192120: {'train_ae_loss': 0.67664, 'train_ucc_loss': 0.62035, 'train_ucc_acc': 0.6875, 'loss': 0.64849}\n",
            "Step 192140: {'train_ae_loss': 0.65374, 'train_ucc_loss': 0.43589, 'train_ucc_acc': 0.875, 'loss': 0.54482}\n",
            "Step 192160: {'train_ae_loss': 0.63719, 'train_ucc_loss': 0.45941, 'train_ucc_acc': 0.875, 'loss': 0.5483}\n",
            "Step 192180: {'train_ae_loss': 0.64232, 'train_ucc_loss': 0.45319, 'train_ucc_acc': 0.875, 'loss': 0.54775}\n",
            "Step 192200: {'train_ae_loss': 0.66232, 'train_ucc_loss': 0.42474, 'train_ucc_acc': 0.90625, 'loss': 0.54353}\n",
            "Step 192220: {'train_ae_loss': 0.65996, 'train_ucc_loss': 0.4383, 'train_ucc_acc': 0.84375, 'loss': 0.54913}\n",
            "Step 192240: {'train_ae_loss': 0.66725, 'train_ucc_loss': 0.46958, 'train_ucc_acc': 0.84375, 'loss': 0.56842}\n",
            "Step 192260: {'train_ae_loss': 0.65323, 'train_ucc_loss': 0.35099, 'train_ucc_acc': 0.96875, 'loss': 0.50211}\n",
            "Step 192280: {'train_ae_loss': 0.67009, 'train_ucc_loss': 0.45559, 'train_ucc_acc': 0.84375, 'loss': 0.56284}\n",
            "Step 192300: {'train_ae_loss': 0.66937, 'train_ucc_loss': 0.45674, 'train_ucc_acc': 0.84375, 'loss': 0.56305}\n",
            "Step 192320: {'train_ae_loss': 0.64213, 'train_ucc_loss': 0.50204, 'train_ucc_acc': 0.8125, 'loss': 0.57208}\n",
            "Step 192340: {'train_ae_loss': 0.65168, 'train_ucc_loss': 0.51819, 'train_ucc_acc': 0.78125, 'loss': 0.58493}\n",
            "Step 192360: {'train_ae_loss': 0.64886, 'train_ucc_loss': 0.4288, 'train_ucc_acc': 0.875, 'loss': 0.53883}\n",
            "Step 192380: {'train_ae_loss': 0.6634, 'train_ucc_loss': 0.38634, 'train_ucc_acc': 0.9375, 'loss': 0.52487}\n",
            "Step 192400: {'train_ae_loss': 0.66611, 'train_ucc_loss': 0.39436, 'train_ucc_acc': 0.90625, 'loss': 0.53023}\n",
            "Step 192420: {'train_ae_loss': 0.63786, 'train_ucc_loss': 0.40755, 'train_ucc_acc': 0.90625, 'loss': 0.5227}\n",
            "Step 192440: {'train_ae_loss': 0.66395, 'train_ucc_loss': 0.45982, 'train_ucc_acc': 0.84375, 'loss': 0.56188}\n",
            "Step 192460: {'train_ae_loss': 0.65633, 'train_ucc_loss': 0.50481, 'train_ucc_acc': 0.78125, 'loss': 0.58057}\n",
            "Step 192480: {'train_ae_loss': 0.66086, 'train_ucc_loss': 0.42708, 'train_ucc_acc': 0.875, 'loss': 0.54397}\n",
            "Step 192500: {'train_ae_loss': 0.66202, 'train_ucc_loss': 0.48231, 'train_ucc_acc': 0.8125, 'loss': 0.57217}\n",
            "Step 192520: {'train_ae_loss': 0.66822, 'train_ucc_loss': 0.50541, 'train_ucc_acc': 0.75, 'loss': 0.58681}\n",
            "Step 192540: {'train_ae_loss': 0.6465, 'train_ucc_loss': 0.36078, 'train_ucc_acc': 0.9375, 'loss': 0.50364}\n",
            "Step 192560: {'train_ae_loss': 0.65767, 'train_ucc_loss': 0.40924, 'train_ucc_acc': 0.90625, 'loss': 0.53346}\n",
            "Step 192580: {'train_ae_loss': 0.65106, 'train_ucc_loss': 0.39768, 'train_ucc_acc': 0.90625, 'loss': 0.52437}\n",
            "Step 192600: {'train_ae_loss': 0.64791, 'train_ucc_loss': 0.40207, 'train_ucc_acc': 0.90625, 'loss': 0.52499}\n",
            "Step 192620: {'train_ae_loss': 0.66364, 'train_ucc_loss': 0.46934, 'train_ucc_acc': 0.84375, 'loss': 0.56649}\n",
            "Step 192640: {'train_ae_loss': 0.63293, 'train_ucc_loss': 0.52082, 'train_ucc_acc': 0.75, 'loss': 0.57687}\n",
            "Step 192660: {'train_ae_loss': 0.64682, 'train_ucc_loss': 0.52987, 'train_ucc_acc': 0.78125, 'loss': 0.58835}\n",
            "Step 192680: {'train_ae_loss': 0.67248, 'train_ucc_loss': 0.44138, 'train_ucc_acc': 0.875, 'loss': 0.55693}\n",
            "Step 192700: {'train_ae_loss': 0.65998, 'train_ucc_loss': 0.47076, 'train_ucc_acc': 0.8125, 'loss': 0.56537}\n",
            "Step 192720: {'train_ae_loss': 0.64629, 'train_ucc_loss': 0.47095, 'train_ucc_acc': 0.875, 'loss': 0.55862}\n",
            "Step 192740: {'train_ae_loss': 0.65383, 'train_ucc_loss': 0.41575, 'train_ucc_acc': 0.90625, 'loss': 0.53479}\n",
            "Step 192760: {'train_ae_loss': 0.67013, 'train_ucc_loss': 0.37035, 'train_ucc_acc': 0.9375, 'loss': 0.52024}\n",
            "Step 192780: {'train_ae_loss': 0.65694, 'train_ucc_loss': 0.43291, 'train_ucc_acc': 0.875, 'loss': 0.54493}\n",
            "Step 192800: {'train_ae_loss': 0.66913, 'train_ucc_loss': 0.36463, 'train_ucc_acc': 0.9375, 'loss': 0.51688}\n",
            "Step 192820: {'train_ae_loss': 0.66118, 'train_ucc_loss': 0.48801, 'train_ucc_acc': 0.8125, 'loss': 0.57459}\n",
            "Step 192840: {'train_ae_loss': 0.68134, 'train_ucc_loss': 0.39181, 'train_ucc_acc': 0.9375, 'loss': 0.53658}\n",
            "Step 192860: {'train_ae_loss': 0.65498, 'train_ucc_loss': 0.44489, 'train_ucc_acc': 0.875, 'loss': 0.54994}\n",
            "Step 192880: {'train_ae_loss': 0.66049, 'train_ucc_loss': 0.50533, 'train_ucc_acc': 0.8125, 'loss': 0.58291}\n",
            "Step 192900: {'train_ae_loss': 0.66339, 'train_ucc_loss': 0.4768, 'train_ucc_acc': 0.8125, 'loss': 0.57009}\n",
            "Step 192920: {'train_ae_loss': 0.65039, 'train_ucc_loss': 0.46086, 'train_ucc_acc': 0.8125, 'loss': 0.55563}\n",
            "Step 192940: {'train_ae_loss': 0.63827, 'train_ucc_loss': 0.42371, 'train_ucc_acc': 0.90625, 'loss': 0.53099}\n",
            "Step 192960: {'train_ae_loss': 0.65068, 'train_ucc_loss': 0.37169, 'train_ucc_acc': 0.9375, 'loss': 0.51119}\n",
            "Step 192980: {'train_ae_loss': 0.66347, 'train_ucc_loss': 0.39264, 'train_ucc_acc': 0.9375, 'loss': 0.52806}\n",
            "Step 193000: {'train_ae_loss': 0.65507, 'train_ucc_loss': 0.44571, 'train_ucc_acc': 0.875, 'loss': 0.55039}\n",
            "step: 193000,eval_ae_loss: 0.64175,eval_ucc_loss: 0.48889,eval_ucc_acc: 0.81543\n",
            "Step 193020: {'train_ae_loss': 0.65769, 'train_ucc_loss': 0.36191, 'train_ucc_acc': 0.9375, 'loss': 0.5098}\n",
            "Step 193040: {'train_ae_loss': 0.63986, 'train_ucc_loss': 0.49381, 'train_ucc_acc': 0.84375, 'loss': 0.56684}\n",
            "Step 193060: {'train_ae_loss': 0.65477, 'train_ucc_loss': 0.5105, 'train_ucc_acc': 0.8125, 'loss': 0.58263}\n",
            "Step 193080: {'train_ae_loss': 0.65263, 'train_ucc_loss': 0.51282, 'train_ucc_acc': 0.78125, 'loss': 0.58272}\n",
            "Step 193100: {'train_ae_loss': 0.65716, 'train_ucc_loss': 0.36622, 'train_ucc_acc': 0.90625, 'loss': 0.51169}\n",
            "Step 193120: {'train_ae_loss': 0.63456, 'train_ucc_loss': 0.40089, 'train_ucc_acc': 0.875, 'loss': 0.51773}\n",
            "Step 193140: {'train_ae_loss': 0.64381, 'train_ucc_loss': 0.33591, 'train_ucc_acc': 0.96875, 'loss': 0.48986}\n",
            "Step 193160: {'train_ae_loss': 0.65333, 'train_ucc_loss': 0.47413, 'train_ucc_acc': 0.84375, 'loss': 0.56373}\n",
            "Step 193180: {'train_ae_loss': 0.64848, 'train_ucc_loss': 0.48905, 'train_ucc_acc': 0.8125, 'loss': 0.56877}\n",
            "Step 193200: {'train_ae_loss': 0.65538, 'train_ucc_loss': 0.41436, 'train_ucc_acc': 0.90625, 'loss': 0.53487}\n",
            "Step 193220: {'train_ae_loss': 0.65957, 'train_ucc_loss': 0.50505, 'train_ucc_acc': 0.8125, 'loss': 0.58231}\n",
            "Step 193240: {'train_ae_loss': 0.65232, 'train_ucc_loss': 0.37396, 'train_ucc_acc': 0.9375, 'loss': 0.51314}\n",
            "Step 193260: {'train_ae_loss': 0.65267, 'train_ucc_loss': 0.45381, 'train_ucc_acc': 0.84375, 'loss': 0.55324}\n",
            "Step 193280: {'train_ae_loss': 0.65028, 'train_ucc_loss': 0.50988, 'train_ucc_acc': 0.78125, 'loss': 0.58008}\n",
            "Step 193300: {'train_ae_loss': 0.65341, 'train_ucc_loss': 0.39406, 'train_ucc_acc': 0.90625, 'loss': 0.52374}\n",
            "Step 193320: {'train_ae_loss': 0.63222, 'train_ucc_loss': 0.47433, 'train_ucc_acc': 0.8125, 'loss': 0.55327}\n",
            "Step 193340: {'train_ae_loss': 0.65134, 'train_ucc_loss': 0.47888, 'train_ucc_acc': 0.84375, 'loss': 0.56511}\n",
            "Step 193360: {'train_ae_loss': 0.6403, 'train_ucc_loss': 0.46643, 'train_ucc_acc': 0.84375, 'loss': 0.55337}\n",
            "Step 193380: {'train_ae_loss': 0.64248, 'train_ucc_loss': 0.5076, 'train_ucc_acc': 0.8125, 'loss': 0.57504}\n",
            "Step 193400: {'train_ae_loss': 0.64388, 'train_ucc_loss': 0.44558, 'train_ucc_acc': 0.84375, 'loss': 0.54473}\n",
            "Step 193420: {'train_ae_loss': 0.6651, 'train_ucc_loss': 0.51237, 'train_ucc_acc': 0.75, 'loss': 0.58874}\n",
            "Step 193440: {'train_ae_loss': 0.66944, 'train_ucc_loss': 0.4056, 'train_ucc_acc': 0.875, 'loss': 0.53752}\n",
            "Step 193460: {'train_ae_loss': 0.67999, 'train_ucc_loss': 0.4432, 'train_ucc_acc': 0.875, 'loss': 0.5616}\n",
            "Step 193480: {'train_ae_loss': 0.64926, 'train_ucc_loss': 0.39943, 'train_ucc_acc': 0.90625, 'loss': 0.52435}\n",
            "Step 193500: {'train_ae_loss': 0.64584, 'train_ucc_loss': 0.42694, 'train_ucc_acc': 0.875, 'loss': 0.53639}\n",
            "Step 193520: {'train_ae_loss': 0.65277, 'train_ucc_loss': 0.43144, 'train_ucc_acc': 0.90625, 'loss': 0.54211}\n",
            "Step 193540: {'train_ae_loss': 0.65789, 'train_ucc_loss': 0.43755, 'train_ucc_acc': 0.875, 'loss': 0.54772}\n",
            "Step 193560: {'train_ae_loss': 0.66228, 'train_ucc_loss': 0.41685, 'train_ucc_acc': 0.875, 'loss': 0.53956}\n",
            "Step 193580: {'train_ae_loss': 0.64537, 'train_ucc_loss': 0.45456, 'train_ucc_acc': 0.84375, 'loss': 0.54997}\n",
            "Step 193600: {'train_ae_loss': 0.63455, 'train_ucc_loss': 0.51096, 'train_ucc_acc': 0.78125, 'loss': 0.57276}\n",
            "Step 193620: {'train_ae_loss': 0.64399, 'train_ucc_loss': 0.40147, 'train_ucc_acc': 0.90625, 'loss': 0.52273}\n",
            "Step 193640: {'train_ae_loss': 0.66501, 'train_ucc_loss': 0.44352, 'train_ucc_acc': 0.84375, 'loss': 0.55426}\n",
            "Step 193660: {'train_ae_loss': 0.64574, 'train_ucc_loss': 0.46981, 'train_ucc_acc': 0.84375, 'loss': 0.55777}\n",
            "Step 193680: {'train_ae_loss': 0.6662, 'train_ucc_loss': 0.55979, 'train_ucc_acc': 0.71875, 'loss': 0.613}\n",
            "Step 193700: {'train_ae_loss': 0.65566, 'train_ucc_loss': 0.44439, 'train_ucc_acc': 0.875, 'loss': 0.55003}\n",
            "Step 193720: {'train_ae_loss': 0.66855, 'train_ucc_loss': 0.36696, 'train_ucc_acc': 0.96875, 'loss': 0.51776}\n",
            "Step 193740: {'train_ae_loss': 0.67335, 'train_ucc_loss': 0.62041, 'train_ucc_acc': 0.6875, 'loss': 0.64688}\n",
            "Step 193760: {'train_ae_loss': 0.6589, 'train_ucc_loss': 0.41198, 'train_ucc_acc': 0.84375, 'loss': 0.53544}\n",
            "Step 193780: {'train_ae_loss': 0.6607, 'train_ucc_loss': 0.39135, 'train_ucc_acc': 0.9375, 'loss': 0.52602}\n",
            "Step 193800: {'train_ae_loss': 0.65097, 'train_ucc_loss': 0.47538, 'train_ucc_acc': 0.84375, 'loss': 0.56317}\n",
            "Step 193820: {'train_ae_loss': 0.67249, 'train_ucc_loss': 0.39093, 'train_ucc_acc': 0.90625, 'loss': 0.53171}\n",
            "Step 193840: {'train_ae_loss': 0.65433, 'train_ucc_loss': 0.44924, 'train_ucc_acc': 0.875, 'loss': 0.55179}\n",
            "Step 193860: {'train_ae_loss': 0.67376, 'train_ucc_loss': 0.41757, 'train_ucc_acc': 0.90625, 'loss': 0.54567}\n",
            "Step 193880: {'train_ae_loss': 0.66665, 'train_ucc_loss': 0.35294, 'train_ucc_acc': 0.96875, 'loss': 0.5098}\n",
            "Step 193900: {'train_ae_loss': 0.6629, 'train_ucc_loss': 0.45409, 'train_ucc_acc': 0.875, 'loss': 0.55849}\n",
            "Step 193920: {'train_ae_loss': 0.66028, 'train_ucc_loss': 0.41211, 'train_ucc_acc': 0.875, 'loss': 0.53619}\n",
            "Step 193940: {'train_ae_loss': 0.65252, 'train_ucc_loss': 0.45201, 'train_ucc_acc': 0.875, 'loss': 0.55227}\n",
            "Step 193960: {'train_ae_loss': 0.66311, 'train_ucc_loss': 0.51915, 'train_ucc_acc': 0.78125, 'loss': 0.59113}\n",
            "Step 193980: {'train_ae_loss': 0.66469, 'train_ucc_loss': 0.57803, 'train_ucc_acc': 0.71875, 'loss': 0.62136}\n",
            "Step 194000: {'train_ae_loss': 0.65989, 'train_ucc_loss': 0.45375, 'train_ucc_acc': 0.84375, 'loss': 0.55682}\n",
            "step: 194000,eval_ae_loss: 0.64416,eval_ucc_loss: 0.44807,eval_ucc_acc: 0.85449\n",
            "Step 194020: {'train_ae_loss': 0.65581, 'train_ucc_loss': 0.51438, 'train_ucc_acc': 0.78125, 'loss': 0.58509}\n",
            "Step 194040: {'train_ae_loss': 0.64309, 'train_ucc_loss': 0.52824, 'train_ucc_acc': 0.75, 'loss': 0.58566}\n",
            "Step 194060: {'train_ae_loss': 0.6282, 'train_ucc_loss': 0.40566, 'train_ucc_acc': 0.90625, 'loss': 0.51693}\n",
            "Step 194080: {'train_ae_loss': 0.664, 'train_ucc_loss': 0.39361, 'train_ucc_acc': 0.90625, 'loss': 0.52881}\n",
            "Step 194100: {'train_ae_loss': 0.64354, 'train_ucc_loss': 0.42059, 'train_ucc_acc': 0.90625, 'loss': 0.53207}\n",
            "Step 194120: {'train_ae_loss': 0.63565, 'train_ucc_loss': 0.44353, 'train_ucc_acc': 0.875, 'loss': 0.53959}\n",
            "Step 194140: {'train_ae_loss': 0.66275, 'train_ucc_loss': 0.5095, 'train_ucc_acc': 0.78125, 'loss': 0.58613}\n",
            "Step 194160: {'train_ae_loss': 0.63939, 'train_ucc_loss': 0.50869, 'train_ucc_acc': 0.78125, 'loss': 0.57404}\n",
            "Step 194180: {'train_ae_loss': 0.64493, 'train_ucc_loss': 0.38309, 'train_ucc_acc': 0.9375, 'loss': 0.51401}\n",
            "Step 194200: {'train_ae_loss': 0.65003, 'train_ucc_loss': 0.45869, 'train_ucc_acc': 0.84375, 'loss': 0.55436}\n",
            "Step 194220: {'train_ae_loss': 0.63652, 'train_ucc_loss': 0.46968, 'train_ucc_acc': 0.8125, 'loss': 0.5531}\n",
            "Step 194240: {'train_ae_loss': 0.64362, 'train_ucc_loss': 0.40913, 'train_ucc_acc': 0.90625, 'loss': 0.52638}\n",
            "Step 194260: {'train_ae_loss': 0.6498, 'train_ucc_loss': 0.42113, 'train_ucc_acc': 0.90625, 'loss': 0.53546}\n",
            "Step 194280: {'train_ae_loss': 0.64645, 'train_ucc_loss': 0.52832, 'train_ucc_acc': 0.75, 'loss': 0.58738}\n",
            "Step 194300: {'train_ae_loss': 0.63807, 'train_ucc_loss': 0.41882, 'train_ucc_acc': 0.90625, 'loss': 0.52844}\n",
            "Step 194320: {'train_ae_loss': 0.66069, 'train_ucc_loss': 0.473, 'train_ucc_acc': 0.84375, 'loss': 0.56685}\n",
            "Step 194340: {'train_ae_loss': 0.66473, 'train_ucc_loss': 0.52913, 'train_ucc_acc': 0.78125, 'loss': 0.59693}\n",
            "Step 194360: {'train_ae_loss': 0.66738, 'train_ucc_loss': 0.37491, 'train_ucc_acc': 0.9375, 'loss': 0.52114}\n",
            "Step 194380: {'train_ae_loss': 0.65254, 'train_ucc_loss': 0.45389, 'train_ucc_acc': 0.84375, 'loss': 0.55322}\n",
            "Step 194400: {'train_ae_loss': 0.6489, 'train_ucc_loss': 0.50585, 'train_ucc_acc': 0.78125, 'loss': 0.57737}\n",
            "Step 194420: {'train_ae_loss': 0.65448, 'train_ucc_loss': 0.412, 'train_ucc_acc': 0.875, 'loss': 0.53324}\n",
            "Step 194440: {'train_ae_loss': 0.64617, 'train_ucc_loss': 0.36917, 'train_ucc_acc': 0.9375, 'loss': 0.50767}\n",
            "Step 194460: {'train_ae_loss': 0.6518, 'train_ucc_loss': 0.43307, 'train_ucc_acc': 0.875, 'loss': 0.54244}\n",
            "Step 194480: {'train_ae_loss': 0.65437, 'train_ucc_loss': 0.40805, 'train_ucc_acc': 0.90625, 'loss': 0.53121}\n",
            "Step 194500: {'train_ae_loss': 0.63479, 'train_ucc_loss': 0.47226, 'train_ucc_acc': 0.8125, 'loss': 0.55352}\n",
            "Step 194520: {'train_ae_loss': 0.65478, 'train_ucc_loss': 0.35819, 'train_ucc_acc': 0.96875, 'loss': 0.50649}\n",
            "Step 194540: {'train_ae_loss': 0.65905, 'train_ucc_loss': 0.6801, 'train_ucc_acc': 0.59375, 'loss': 0.66958}\n",
            "Step 194560: {'train_ae_loss': 0.6415, 'train_ucc_loss': 0.38594, 'train_ucc_acc': 0.9375, 'loss': 0.51372}\n",
            "Step 194580: {'train_ae_loss': 0.63945, 'train_ucc_loss': 0.4616, 'train_ucc_acc': 0.84375, 'loss': 0.55052}\n",
            "Step 194600: {'train_ae_loss': 0.66152, 'train_ucc_loss': 0.44831, 'train_ucc_acc': 0.84375, 'loss': 0.55491}\n",
            "Step 194620: {'train_ae_loss': 0.65503, 'train_ucc_loss': 0.42203, 'train_ucc_acc': 0.875, 'loss': 0.53853}\n",
            "Step 194640: {'train_ae_loss': 0.66192, 'train_ucc_loss': 0.38804, 'train_ucc_acc': 0.90625, 'loss': 0.52498}\n",
            "Step 194660: {'train_ae_loss': 0.65683, 'train_ucc_loss': 0.43617, 'train_ucc_acc': 0.90625, 'loss': 0.5465}\n",
            "Step 194680: {'train_ae_loss': 0.65651, 'train_ucc_loss': 0.41114, 'train_ucc_acc': 0.90625, 'loss': 0.53382}\n",
            "Step 194700: {'train_ae_loss': 0.66678, 'train_ucc_loss': 0.4753, 'train_ucc_acc': 0.8125, 'loss': 0.57104}\n",
            "Step 194720: {'train_ae_loss': 0.66513, 'train_ucc_loss': 0.3833, 'train_ucc_acc': 0.9375, 'loss': 0.52422}\n",
            "Step 194740: {'train_ae_loss': 0.66006, 'train_ucc_loss': 0.3883, 'train_ucc_acc': 0.9375, 'loss': 0.52418}\n",
            "Step 194760: {'train_ae_loss': 0.65272, 'train_ucc_loss': 0.39898, 'train_ucc_acc': 0.90625, 'loss': 0.52585}\n",
            "Step 194780: {'train_ae_loss': 0.65175, 'train_ucc_loss': 0.48819, 'train_ucc_acc': 0.8125, 'loss': 0.56997}\n",
            "Step 194800: {'train_ae_loss': 0.65875, 'train_ucc_loss': 0.37684, 'train_ucc_acc': 0.9375, 'loss': 0.51779}\n",
            "Step 194820: {'train_ae_loss': 0.65992, 'train_ucc_loss': 0.39263, 'train_ucc_acc': 0.90625, 'loss': 0.52628}\n",
            "Step 194840: {'train_ae_loss': 0.65011, 'train_ucc_loss': 0.4291, 'train_ucc_acc': 0.875, 'loss': 0.5396}\n",
            "Step 194860: {'train_ae_loss': 0.64168, 'train_ucc_loss': 0.47482, 'train_ucc_acc': 0.8125, 'loss': 0.55825}\n",
            "Step 194880: {'train_ae_loss': 0.66697, 'train_ucc_loss': 0.51013, 'train_ucc_acc': 0.78125, 'loss': 0.58855}\n",
            "Step 194900: {'train_ae_loss': 0.66244, 'train_ucc_loss': 0.46792, 'train_ucc_acc': 0.8125, 'loss': 0.56518}\n",
            "Step 194920: {'train_ae_loss': 0.66177, 'train_ucc_loss': 0.41655, 'train_ucc_acc': 0.90625, 'loss': 0.53916}\n",
            "Step 194940: {'train_ae_loss': 0.65445, 'train_ucc_loss': 0.47279, 'train_ucc_acc': 0.8125, 'loss': 0.56362}\n",
            "Step 194960: {'train_ae_loss': 0.643, 'train_ucc_loss': 0.53289, 'train_ucc_acc': 0.78125, 'loss': 0.58795}\n",
            "Step 194980: {'train_ae_loss': 0.65157, 'train_ucc_loss': 0.41587, 'train_ucc_acc': 0.90625, 'loss': 0.53372}\n",
            "Step 195000: {'train_ae_loss': 0.6552, 'train_ucc_loss': 0.45277, 'train_ucc_acc': 0.875, 'loss': 0.55398}\n",
            "step: 195000,eval_ae_loss: 0.64512,eval_ucc_loss: 0.483,eval_ucc_acc: 0.82227\n",
            "Step 195020: {'train_ae_loss': 0.6582, 'train_ucc_loss': 0.38798, 'train_ucc_acc': 0.90625, 'loss': 0.52309}\n",
            "Step 195040: {'train_ae_loss': 0.6671, 'train_ucc_loss': 0.45999, 'train_ucc_acc': 0.84375, 'loss': 0.56355}\n",
            "Step 195060: {'train_ae_loss': 0.66155, 'train_ucc_loss': 0.38403, 'train_ucc_acc': 0.9375, 'loss': 0.52279}\n",
            "Step 195080: {'train_ae_loss': 0.65823, 'train_ucc_loss': 0.43636, 'train_ucc_acc': 0.875, 'loss': 0.5473}\n",
            "Step 195100: {'train_ae_loss': 0.65818, 'train_ucc_loss': 0.45753, 'train_ucc_acc': 0.875, 'loss': 0.55786}\n",
            "Step 195120: {'train_ae_loss': 0.65882, 'train_ucc_loss': 0.42518, 'train_ucc_acc': 0.875, 'loss': 0.542}\n",
            "Step 195140: {'train_ae_loss': 0.6579, 'train_ucc_loss': 0.43356, 'train_ucc_acc': 0.875, 'loss': 0.54573}\n",
            "Step 195160: {'train_ae_loss': 0.66413, 'train_ucc_loss': 0.44122, 'train_ucc_acc': 0.875, 'loss': 0.55267}\n",
            "Step 195180: {'train_ae_loss': 0.65883, 'train_ucc_loss': 0.39089, 'train_ucc_acc': 0.9375, 'loss': 0.52486}\n",
            "Step 195200: {'train_ae_loss': 0.66768, 'train_ucc_loss': 0.40026, 'train_ucc_acc': 0.90625, 'loss': 0.53397}\n",
            "Step 195220: {'train_ae_loss': 0.65869, 'train_ucc_loss': 0.45495, 'train_ucc_acc': 0.84375, 'loss': 0.55682}\n",
            "Step 195240: {'train_ae_loss': 0.66448, 'train_ucc_loss': 0.44218, 'train_ucc_acc': 0.875, 'loss': 0.55333}\n",
            "Step 195260: {'train_ae_loss': 0.64722, 'train_ucc_loss': 0.46917, 'train_ucc_acc': 0.8125, 'loss': 0.55819}\n",
            "Step 195280: {'train_ae_loss': 0.6645, 'train_ucc_loss': 0.40991, 'train_ucc_acc': 0.875, 'loss': 0.5372}\n",
            "Step 195300: {'train_ae_loss': 0.66127, 'train_ucc_loss': 0.51657, 'train_ucc_acc': 0.78125, 'loss': 0.58892}\n",
            "Step 195320: {'train_ae_loss': 0.67418, 'train_ucc_loss': 0.47912, 'train_ucc_acc': 0.8125, 'loss': 0.57665}\n",
            "Step 195340: {'train_ae_loss': 0.67552, 'train_ucc_loss': 0.39278, 'train_ucc_acc': 0.90625, 'loss': 0.53415}\n",
            "Step 195360: {'train_ae_loss': 0.67147, 'train_ucc_loss': 0.41597, 'train_ucc_acc': 0.90625, 'loss': 0.54372}\n",
            "Step 195380: {'train_ae_loss': 0.66483, 'train_ucc_loss': 0.42649, 'train_ucc_acc': 0.875, 'loss': 0.54566}\n",
            "Step 195400: {'train_ae_loss': 0.65795, 'train_ucc_loss': 0.43414, 'train_ucc_acc': 0.84375, 'loss': 0.54605}\n",
            "Step 195420: {'train_ae_loss': 0.65538, 'train_ucc_loss': 0.38365, 'train_ucc_acc': 0.90625, 'loss': 0.51952}\n",
            "Step 195440: {'train_ae_loss': 0.65453, 'train_ucc_loss': 0.39863, 'train_ucc_acc': 0.9375, 'loss': 0.52658}\n",
            "Step 195460: {'train_ae_loss': 0.64933, 'train_ucc_loss': 0.34447, 'train_ucc_acc': 0.96875, 'loss': 0.4969}\n",
            "Step 195480: {'train_ae_loss': 0.65843, 'train_ucc_loss': 0.43772, 'train_ucc_acc': 0.875, 'loss': 0.54808}\n",
            "Step 195500: {'train_ae_loss': 0.64949, 'train_ucc_loss': 0.45157, 'train_ucc_acc': 0.84375, 'loss': 0.55053}\n",
            "Step 195520: {'train_ae_loss': 0.65187, 'train_ucc_loss': 0.44448, 'train_ucc_acc': 0.875, 'loss': 0.54817}\n",
            "Step 195540: {'train_ae_loss': 0.64828, 'train_ucc_loss': 0.40767, 'train_ucc_acc': 0.875, 'loss': 0.52797}\n",
            "Step 195560: {'train_ae_loss': 0.63087, 'train_ucc_loss': 0.3958, 'train_ucc_acc': 0.90625, 'loss': 0.51334}\n",
            "Step 195580: {'train_ae_loss': 0.65531, 'train_ucc_loss': 0.36814, 'train_ucc_acc': 0.9375, 'loss': 0.51173}\n",
            "Step 195600: {'train_ae_loss': 0.66292, 'train_ucc_loss': 0.59666, 'train_ucc_acc': 0.6875, 'loss': 0.62979}\n",
            "Step 195620: {'train_ae_loss': 0.67433, 'train_ucc_loss': 0.43454, 'train_ucc_acc': 0.90625, 'loss': 0.55443}\n",
            "Step 195640: {'train_ae_loss': 0.65112, 'train_ucc_loss': 0.36998, 'train_ucc_acc': 0.9375, 'loss': 0.51055}\n",
            "Step 195660: {'train_ae_loss': 0.6646, 'train_ucc_loss': 0.5314, 'train_ucc_acc': 0.78125, 'loss': 0.598}\n",
            "Step 195680: {'train_ae_loss': 0.64629, 'train_ucc_loss': 0.41054, 'train_ucc_acc': 0.90625, 'loss': 0.52841}\n",
            "Step 195700: {'train_ae_loss': 0.65706, 'train_ucc_loss': 0.37214, 'train_ucc_acc': 0.9375, 'loss': 0.5146}\n",
            "Step 195720: {'train_ae_loss': 0.66703, 'train_ucc_loss': 0.48695, 'train_ucc_acc': 0.8125, 'loss': 0.57699}\n",
            "Step 195740: {'train_ae_loss': 0.64689, 'train_ucc_loss': 0.4451, 'train_ucc_acc': 0.875, 'loss': 0.54599}\n",
            "Step 195760: {'train_ae_loss': 0.65059, 'train_ucc_loss': 0.40167, 'train_ucc_acc': 0.875, 'loss': 0.52613}\n",
            "Step 195780: {'train_ae_loss': 0.66655, 'train_ucc_loss': 0.62922, 'train_ucc_acc': 0.65625, 'loss': 0.64789}\n",
            "Step 195800: {'train_ae_loss': 0.68003, 'train_ucc_loss': 0.36959, 'train_ucc_acc': 0.9375, 'loss': 0.52481}\n",
            "Step 195820: {'train_ae_loss': 0.66052, 'train_ucc_loss': 0.43444, 'train_ucc_acc': 0.84375, 'loss': 0.54748}\n",
            "Step 195840: {'train_ae_loss': 0.6644, 'train_ucc_loss': 0.51456, 'train_ucc_acc': 0.78125, 'loss': 0.58948}\n",
            "Step 195860: {'train_ae_loss': 0.63476, 'train_ucc_loss': 0.4029, 'train_ucc_acc': 0.90625, 'loss': 0.51883}\n",
            "Step 195880: {'train_ae_loss': 0.65737, 'train_ucc_loss': 0.50663, 'train_ucc_acc': 0.8125, 'loss': 0.582}\n",
            "Step 195900: {'train_ae_loss': 0.66009, 'train_ucc_loss': 0.51852, 'train_ucc_acc': 0.75, 'loss': 0.5893}\n",
            "Step 195920: {'train_ae_loss': 0.66919, 'train_ucc_loss': 0.41254, 'train_ucc_acc': 0.90625, 'loss': 0.54087}\n",
            "Step 195940: {'train_ae_loss': 0.65535, 'train_ucc_loss': 0.43667, 'train_ucc_acc': 0.84375, 'loss': 0.54601}\n",
            "Step 195960: {'train_ae_loss': 0.65161, 'train_ucc_loss': 0.37364, 'train_ucc_acc': 0.9375, 'loss': 0.51263}\n",
            "Step 195980: {'train_ae_loss': 0.64155, 'train_ucc_loss': 0.4868, 'train_ucc_acc': 0.8125, 'loss': 0.56418}\n",
            "Step 196000: {'train_ae_loss': 0.6364, 'train_ucc_loss': 0.63893, 'train_ucc_acc': 0.6875, 'loss': 0.63766}\n",
            "step: 196000,eval_ae_loss: 0.64425,eval_ucc_loss: 0.61073,eval_ucc_acc: 0.68555\n",
            "Step 196020: {'train_ae_loss': 0.66389, 'train_ucc_loss': 0.35345, 'train_ucc_acc': 0.96875, 'loss': 0.50867}\n",
            "Step 196040: {'train_ae_loss': 0.66496, 'train_ucc_loss': 0.42331, 'train_ucc_acc': 0.875, 'loss': 0.54414}\n",
            "Step 196060: {'train_ae_loss': 0.67411, 'train_ucc_loss': 0.53562, 'train_ucc_acc': 0.78125, 'loss': 0.60486}\n",
            "Step 196080: {'train_ae_loss': 0.65772, 'train_ucc_loss': 0.46293, 'train_ucc_acc': 0.84375, 'loss': 0.56032}\n",
            "Step 196100: {'train_ae_loss': 0.63911, 'train_ucc_loss': 0.35098, 'train_ucc_acc': 0.9375, 'loss': 0.49505}\n",
            "Step 196120: {'train_ae_loss': 0.65458, 'train_ucc_loss': 0.38915, 'train_ucc_acc': 0.9375, 'loss': 0.52186}\n",
            "Step 196140: {'train_ae_loss': 0.65588, 'train_ucc_loss': 0.41516, 'train_ucc_acc': 0.90625, 'loss': 0.53552}\n",
            "Step 196160: {'train_ae_loss': 0.66615, 'train_ucc_loss': 0.45919, 'train_ucc_acc': 0.84375, 'loss': 0.56267}\n",
            "Step 196180: {'train_ae_loss': 0.65222, 'train_ucc_loss': 0.41521, 'train_ucc_acc': 0.90625, 'loss': 0.53372}\n",
            "Step 196200: {'train_ae_loss': 0.64787, 'train_ucc_loss': 0.43117, 'train_ucc_acc': 0.875, 'loss': 0.53952}\n",
            "Step 196220: {'train_ae_loss': 0.65641, 'train_ucc_loss': 0.38585, 'train_ucc_acc': 0.9375, 'loss': 0.52113}\n",
            "Step 196240: {'train_ae_loss': 0.65534, 'train_ucc_loss': 0.55349, 'train_ucc_acc': 0.71875, 'loss': 0.60441}\n",
            "Step 196260: {'train_ae_loss': 0.67302, 'train_ucc_loss': 0.45391, 'train_ucc_acc': 0.84375, 'loss': 0.56347}\n",
            "Step 196280: {'train_ae_loss': 0.65328, 'train_ucc_loss': 0.40858, 'train_ucc_acc': 0.90625, 'loss': 0.53093}\n",
            "Step 196300: {'train_ae_loss': 0.64367, 'train_ucc_loss': 0.35759, 'train_ucc_acc': 0.96875, 'loss': 0.50063}\n",
            "Step 196320: {'train_ae_loss': 0.65623, 'train_ucc_loss': 0.33927, 'train_ucc_acc': 0.96875, 'loss': 0.49775}\n",
            "Step 196340: {'train_ae_loss': 0.66988, 'train_ucc_loss': 0.41797, 'train_ucc_acc': 0.90625, 'loss': 0.54392}\n",
            "Step 196360: {'train_ae_loss': 0.64901, 'train_ucc_loss': 0.43695, 'train_ucc_acc': 0.84375, 'loss': 0.54298}\n",
            "Step 196380: {'train_ae_loss': 0.66356, 'train_ucc_loss': 0.35771, 'train_ucc_acc': 0.9375, 'loss': 0.51064}\n",
            "Step 196400: {'train_ae_loss': 0.65156, 'train_ucc_loss': 0.43022, 'train_ucc_acc': 0.875, 'loss': 0.54089}\n",
            "Step 196420: {'train_ae_loss': 0.66768, 'train_ucc_loss': 0.38824, 'train_ucc_acc': 0.9375, 'loss': 0.52796}\n",
            "Step 196440: {'train_ae_loss': 0.66588, 'train_ucc_loss': 0.3828, 'train_ucc_acc': 0.9375, 'loss': 0.52434}\n",
            "Step 196460: {'train_ae_loss': 0.65097, 'train_ucc_loss': 0.47865, 'train_ucc_acc': 0.8125, 'loss': 0.56481}\n",
            "Step 196480: {'train_ae_loss': 0.66584, 'train_ucc_loss': 0.38488, 'train_ucc_acc': 0.9375, 'loss': 0.52536}\n",
            "Step 196500: {'train_ae_loss': 0.67607, 'train_ucc_loss': 0.39725, 'train_ucc_acc': 0.9375, 'loss': 0.53666}\n",
            "Step 196520: {'train_ae_loss': 0.65407, 'train_ucc_loss': 0.42506, 'train_ucc_acc': 0.90625, 'loss': 0.53957}\n",
            "Step 196540: {'train_ae_loss': 0.63959, 'train_ucc_loss': 0.48672, 'train_ucc_acc': 0.8125, 'loss': 0.56315}\n",
            "Step 196560: {'train_ae_loss': 0.6623, 'train_ucc_loss': 0.51716, 'train_ucc_acc': 0.78125, 'loss': 0.58973}\n",
            "Step 196580: {'train_ae_loss': 0.64622, 'train_ucc_loss': 0.4362, 'train_ucc_acc': 0.875, 'loss': 0.54121}\n",
            "Step 196600: {'train_ae_loss': 0.65444, 'train_ucc_loss': 0.46361, 'train_ucc_acc': 0.84375, 'loss': 0.55902}\n",
            "Step 196620: {'train_ae_loss': 0.63961, 'train_ucc_loss': 0.46719, 'train_ucc_acc': 0.875, 'loss': 0.5534}\n",
            "Step 196640: {'train_ae_loss': 0.6499, 'train_ucc_loss': 0.41378, 'train_ucc_acc': 0.90625, 'loss': 0.53184}\n",
            "Step 196660: {'train_ae_loss': 0.67055, 'train_ucc_loss': 0.45057, 'train_ucc_acc': 0.875, 'loss': 0.56056}\n",
            "Step 196680: {'train_ae_loss': 0.66545, 'train_ucc_loss': 0.42703, 'train_ucc_acc': 0.875, 'loss': 0.54624}\n",
            "Step 196700: {'train_ae_loss': 0.65932, 'train_ucc_loss': 0.40983, 'train_ucc_acc': 0.90625, 'loss': 0.53457}\n",
            "Step 196720: {'train_ae_loss': 0.65727, 'train_ucc_loss': 0.41473, 'train_ucc_acc': 0.90625, 'loss': 0.536}\n",
            "Step 196740: {'train_ae_loss': 0.65566, 'train_ucc_loss': 0.3826, 'train_ucc_acc': 0.9375, 'loss': 0.51913}\n",
            "Step 196760: {'train_ae_loss': 0.66751, 'train_ucc_loss': 0.39417, 'train_ucc_acc': 0.9375, 'loss': 0.53084}\n",
            "Step 196780: {'train_ae_loss': 0.6554, 'train_ucc_loss': 0.43049, 'train_ucc_acc': 0.84375, 'loss': 0.54294}\n",
            "Step 196800: {'train_ae_loss': 0.65197, 'train_ucc_loss': 0.35804, 'train_ucc_acc': 0.96875, 'loss': 0.505}\n",
            "Step 196820: {'train_ae_loss': 0.64407, 'train_ucc_loss': 0.40918, 'train_ucc_acc': 0.90625, 'loss': 0.52663}\n",
            "Step 196840: {'train_ae_loss': 0.66975, 'train_ucc_loss': 0.45112, 'train_ucc_acc': 0.84375, 'loss': 0.56043}\n",
            "Step 196860: {'train_ae_loss': 0.64459, 'train_ucc_loss': 0.50823, 'train_ucc_acc': 0.8125, 'loss': 0.57641}\n",
            "Step 196880: {'train_ae_loss': 0.64694, 'train_ucc_loss': 0.49359, 'train_ucc_acc': 0.8125, 'loss': 0.57027}\n",
            "Step 196900: {'train_ae_loss': 0.66944, 'train_ucc_loss': 0.39619, 'train_ucc_acc': 0.90625, 'loss': 0.53282}\n",
            "Step 196920: {'train_ae_loss': 0.65077, 'train_ucc_loss': 0.46042, 'train_ucc_acc': 0.84375, 'loss': 0.55559}\n",
            "Step 196940: {'train_ae_loss': 0.66465, 'train_ucc_loss': 0.49737, 'train_ucc_acc': 0.8125, 'loss': 0.58101}\n",
            "Step 196960: {'train_ae_loss': 0.65826, 'train_ucc_loss': 0.41679, 'train_ucc_acc': 0.90625, 'loss': 0.53753}\n",
            "Step 196980: {'train_ae_loss': 0.65382, 'train_ucc_loss': 0.44694, 'train_ucc_acc': 0.875, 'loss': 0.55038}\n",
            "Step 197000: {'train_ae_loss': 0.6639, 'train_ucc_loss': 0.40271, 'train_ucc_acc': 0.9375, 'loss': 0.53331}\n",
            "step: 197000,eval_ae_loss: 0.64472,eval_ucc_loss: 0.46734,eval_ucc_acc: 0.84277\n",
            "Step 197020: {'train_ae_loss': 0.65906, 'train_ucc_loss': 0.53848, 'train_ucc_acc': 0.75, 'loss': 0.59877}\n",
            "Step 197040: {'train_ae_loss': 0.6664, 'train_ucc_loss': 0.45993, 'train_ucc_acc': 0.84375, 'loss': 0.56316}\n",
            "Step 197060: {'train_ae_loss': 0.66481, 'train_ucc_loss': 0.39963, 'train_ucc_acc': 0.90625, 'loss': 0.53222}\n",
            "Step 197080: {'train_ae_loss': 0.65384, 'train_ucc_loss': 0.46633, 'train_ucc_acc': 0.875, 'loss': 0.56008}\n",
            "Step 197100: {'train_ae_loss': 0.64975, 'train_ucc_loss': 0.3988, 'train_ucc_acc': 0.90625, 'loss': 0.52427}\n",
            "Step 197120: {'train_ae_loss': 0.66678, 'train_ucc_loss': 0.46446, 'train_ucc_acc': 0.84375, 'loss': 0.56562}\n",
            "Step 197140: {'train_ae_loss': 0.6669, 'train_ucc_loss': 0.39672, 'train_ucc_acc': 0.90625, 'loss': 0.53181}\n",
            "Step 197160: {'train_ae_loss': 0.65864, 'train_ucc_loss': 0.47273, 'train_ucc_acc': 0.8125, 'loss': 0.56568}\n",
            "Step 197180: {'train_ae_loss': 0.65103, 'train_ucc_loss': 0.44347, 'train_ucc_acc': 0.875, 'loss': 0.54725}\n",
            "Step 197200: {'train_ae_loss': 0.64691, 'train_ucc_loss': 0.42422, 'train_ucc_acc': 0.90625, 'loss': 0.53557}\n",
            "Step 197220: {'train_ae_loss': 0.65614, 'train_ucc_loss': 0.40263, 'train_ucc_acc': 0.875, 'loss': 0.52938}\n",
            "Step 197240: {'train_ae_loss': 0.65677, 'train_ucc_loss': 0.43249, 'train_ucc_acc': 0.875, 'loss': 0.54463}\n",
            "Step 197260: {'train_ae_loss': 0.65299, 'train_ucc_loss': 0.38874, 'train_ucc_acc': 0.90625, 'loss': 0.52087}\n",
            "Step 197280: {'train_ae_loss': 0.66055, 'train_ucc_loss': 0.4312, 'train_ucc_acc': 0.84375, 'loss': 0.54588}\n",
            "Step 197300: {'train_ae_loss': 0.6622, 'train_ucc_loss': 0.41591, 'train_ucc_acc': 0.90625, 'loss': 0.53905}\n",
            "Step 197320: {'train_ae_loss': 0.63823, 'train_ucc_loss': 0.38726, 'train_ucc_acc': 0.90625, 'loss': 0.51275}\n",
            "Step 197340: {'train_ae_loss': 0.6488, 'train_ucc_loss': 0.47954, 'train_ucc_acc': 0.8125, 'loss': 0.56417}\n",
            "Step 197360: {'train_ae_loss': 0.6666, 'train_ucc_loss': 0.51521, 'train_ucc_acc': 0.78125, 'loss': 0.5909}\n",
            "Step 197380: {'train_ae_loss': 0.66082, 'train_ucc_loss': 0.31817, 'train_ucc_acc': 1.0, 'loss': 0.4895}\n",
            "Step 197400: {'train_ae_loss': 0.67519, 'train_ucc_loss': 0.49447, 'train_ucc_acc': 0.8125, 'loss': 0.58483}\n",
            "Step 197420: {'train_ae_loss': 0.65793, 'train_ucc_loss': 0.53375, 'train_ucc_acc': 0.71875, 'loss': 0.59584}\n",
            "Step 197440: {'train_ae_loss': 0.6589, 'train_ucc_loss': 0.47458, 'train_ucc_acc': 0.84375, 'loss': 0.56674}\n",
            "Step 197460: {'train_ae_loss': 0.66049, 'train_ucc_loss': 0.42755, 'train_ucc_acc': 0.875, 'loss': 0.54402}\n",
            "Step 197480: {'train_ae_loss': 0.66041, 'train_ucc_loss': 0.47018, 'train_ucc_acc': 0.84375, 'loss': 0.5653}\n",
            "Step 197500: {'train_ae_loss': 0.65524, 'train_ucc_loss': 0.39317, 'train_ucc_acc': 0.90625, 'loss': 0.52421}\n",
            "Step 197520: {'train_ae_loss': 0.66198, 'train_ucc_loss': 0.55011, 'train_ucc_acc': 0.75, 'loss': 0.60605}\n",
            "Step 197540: {'train_ae_loss': 0.64718, 'train_ucc_loss': 0.45133, 'train_ucc_acc': 0.875, 'loss': 0.54925}\n",
            "Step 197560: {'train_ae_loss': 0.6698, 'train_ucc_loss': 0.41786, 'train_ucc_acc': 0.875, 'loss': 0.54383}\n",
            "Step 197580: {'train_ae_loss': 0.66365, 'train_ucc_loss': 0.41051, 'train_ucc_acc': 0.875, 'loss': 0.53708}\n",
            "Step 197600: {'train_ae_loss': 0.66873, 'train_ucc_loss': 0.52727, 'train_ucc_acc': 0.78125, 'loss': 0.598}\n",
            "Step 197620: {'train_ae_loss': 0.66175, 'train_ucc_loss': 0.46783, 'train_ucc_acc': 0.84375, 'loss': 0.56479}\n",
            "Step 197640: {'train_ae_loss': 0.6563, 'train_ucc_loss': 0.43999, 'train_ucc_acc': 0.875, 'loss': 0.54814}\n",
            "Step 197660: {'train_ae_loss': 0.63681, 'train_ucc_loss': 0.39259, 'train_ucc_acc': 0.9375, 'loss': 0.5147}\n",
            "Step 197680: {'train_ae_loss': 0.66274, 'train_ucc_loss': 0.44424, 'train_ucc_acc': 0.875, 'loss': 0.55349}\n",
            "Step 197700: {'train_ae_loss': 0.66819, 'train_ucc_loss': 0.48839, 'train_ucc_acc': 0.8125, 'loss': 0.57829}\n",
            "Step 197720: {'train_ae_loss': 0.65705, 'train_ucc_loss': 0.40258, 'train_ucc_acc': 0.90625, 'loss': 0.52981}\n",
            "Step 197740: {'train_ae_loss': 0.65517, 'train_ucc_loss': 0.4062, 'train_ucc_acc': 0.90625, 'loss': 0.53069}\n",
            "Step 197760: {'train_ae_loss': 0.67149, 'train_ucc_loss': 0.45978, 'train_ucc_acc': 0.875, 'loss': 0.56563}\n",
            "Step 197780: {'train_ae_loss': 0.65105, 'train_ucc_loss': 0.51172, 'train_ucc_acc': 0.78125, 'loss': 0.58138}\n",
            "Step 197800: {'train_ae_loss': 0.65063, 'train_ucc_loss': 0.51109, 'train_ucc_acc': 0.78125, 'loss': 0.58086}\n",
            "Step 197820: {'train_ae_loss': 0.65571, 'train_ucc_loss': 0.34773, 'train_ucc_acc': 0.96875, 'loss': 0.50172}\n",
            "Step 197840: {'train_ae_loss': 0.67687, 'train_ucc_loss': 0.44759, 'train_ucc_acc': 0.84375, 'loss': 0.56223}\n",
            "Step 197860: {'train_ae_loss': 0.65615, 'train_ucc_loss': 0.4423, 'train_ucc_acc': 0.90625, 'loss': 0.54923}\n",
            "Step 197880: {'train_ae_loss': 0.65678, 'train_ucc_loss': 0.46917, 'train_ucc_acc': 0.84375, 'loss': 0.56298}\n",
            "Step 197900: {'train_ae_loss': 0.64738, 'train_ucc_loss': 0.56228, 'train_ucc_acc': 0.71875, 'loss': 0.60483}\n",
            "Step 197920: {'train_ae_loss': 0.65469, 'train_ucc_loss': 0.36162, 'train_ucc_acc': 0.9375, 'loss': 0.50815}\n",
            "Step 197940: {'train_ae_loss': 0.65195, 'train_ucc_loss': 0.47984, 'train_ucc_acc': 0.8125, 'loss': 0.56589}\n",
            "Step 197960: {'train_ae_loss': 0.67555, 'train_ucc_loss': 0.38353, 'train_ucc_acc': 0.9375, 'loss': 0.52954}\n",
            "Step 197980: {'train_ae_loss': 0.65836, 'train_ucc_loss': 0.45355, 'train_ucc_acc': 0.875, 'loss': 0.55595}\n",
            "Step 198000: {'train_ae_loss': 0.66721, 'train_ucc_loss': 0.44907, 'train_ucc_acc': 0.875, 'loss': 0.55814}\n",
            "step: 198000,eval_ae_loss: 0.65162,eval_ucc_loss: 0.46133,eval_ucc_acc: 0.84766\n",
            "Step 198020: {'train_ae_loss': 0.65123, 'train_ucc_loss': 0.54706, 'train_ucc_acc': 0.78125, 'loss': 0.59914}\n",
            "Step 198040: {'train_ae_loss': 0.67011, 'train_ucc_loss': 0.40627, 'train_ucc_acc': 0.875, 'loss': 0.53819}\n",
            "Step 198060: {'train_ae_loss': 0.66377, 'train_ucc_loss': 0.35012, 'train_ucc_acc': 0.96875, 'loss': 0.50694}\n",
            "Step 198080: {'train_ae_loss': 0.66744, 'train_ucc_loss': 0.43292, 'train_ucc_acc': 0.875, 'loss': 0.55018}\n",
            "Step 198100: {'train_ae_loss': 0.65056, 'train_ucc_loss': 0.40197, 'train_ucc_acc': 0.90625, 'loss': 0.52626}\n",
            "Step 198120: {'train_ae_loss': 0.65794, 'train_ucc_loss': 0.4176, 'train_ucc_acc': 0.90625, 'loss': 0.53777}\n",
            "Step 198140: {'train_ae_loss': 0.65808, 'train_ucc_loss': 0.40113, 'train_ucc_acc': 0.9375, 'loss': 0.52961}\n",
            "Step 198160: {'train_ae_loss': 0.64586, 'train_ucc_loss': 0.41738, 'train_ucc_acc': 0.875, 'loss': 0.53162}\n",
            "Step 198180: {'train_ae_loss': 0.65569, 'train_ucc_loss': 0.43654, 'train_ucc_acc': 0.84375, 'loss': 0.54612}\n",
            "Step 198200: {'train_ae_loss': 0.63962, 'train_ucc_loss': 0.40867, 'train_ucc_acc': 0.90625, 'loss': 0.52414}\n",
            "Step 198220: {'train_ae_loss': 0.64709, 'train_ucc_loss': 0.41984, 'train_ucc_acc': 0.875, 'loss': 0.53346}\n",
            "Step 198240: {'train_ae_loss': 0.65387, 'train_ucc_loss': 0.3778, 'train_ucc_acc': 0.9375, 'loss': 0.51584}\n",
            "Step 198260: {'train_ae_loss': 0.66001, 'train_ucc_loss': 0.39048, 'train_ucc_acc': 0.90625, 'loss': 0.52524}\n",
            "Step 198280: {'train_ae_loss': 0.67303, 'train_ucc_loss': 0.51612, 'train_ucc_acc': 0.78125, 'loss': 0.59458}\n",
            "Step 198300: {'train_ae_loss': 0.65069, 'train_ucc_loss': 0.55177, 'train_ucc_acc': 0.75, 'loss': 0.60123}\n",
            "Step 198320: {'train_ae_loss': 0.65485, 'train_ucc_loss': 0.56774, 'train_ucc_acc': 0.75, 'loss': 0.61129}\n",
            "Step 198340: {'train_ae_loss': 0.64973, 'train_ucc_loss': 0.36797, 'train_ucc_acc': 0.9375, 'loss': 0.50885}\n",
            "Step 198360: {'train_ae_loss': 0.66676, 'train_ucc_loss': 0.45665, 'train_ucc_acc': 0.84375, 'loss': 0.5617}\n",
            "Step 198380: {'train_ae_loss': 0.63798, 'train_ucc_loss': 0.45397, 'train_ucc_acc': 0.84375, 'loss': 0.54598}\n",
            "Step 198400: {'train_ae_loss': 0.64613, 'train_ucc_loss': 0.50416, 'train_ucc_acc': 0.78125, 'loss': 0.57514}\n",
            "Step 198420: {'train_ae_loss': 0.65385, 'train_ucc_loss': 0.4615, 'train_ucc_acc': 0.84375, 'loss': 0.55767}\n",
            "Step 198440: {'train_ae_loss': 0.65429, 'train_ucc_loss': 0.49035, 'train_ucc_acc': 0.8125, 'loss': 0.57232}\n",
            "Step 198460: {'train_ae_loss': 0.66575, 'train_ucc_loss': 0.35869, 'train_ucc_acc': 0.96875, 'loss': 0.51222}\n",
            "Step 198480: {'train_ae_loss': 0.63984, 'train_ucc_loss': 0.40011, 'train_ucc_acc': 0.9375, 'loss': 0.51997}\n",
            "Step 198500: {'train_ae_loss': 0.64465, 'train_ucc_loss': 0.45263, 'train_ucc_acc': 0.84375, 'loss': 0.54864}\n",
            "Step 198520: {'train_ae_loss': 0.6598, 'train_ucc_loss': 0.41202, 'train_ucc_acc': 0.90625, 'loss': 0.53591}\n",
            "Step 198540: {'train_ae_loss': 0.67846, 'train_ucc_loss': 0.393, 'train_ucc_acc': 0.90625, 'loss': 0.53573}\n",
            "Step 198560: {'train_ae_loss': 0.65193, 'train_ucc_loss': 0.43052, 'train_ucc_acc': 0.84375, 'loss': 0.54123}\n",
            "Step 198580: {'train_ae_loss': 0.65347, 'train_ucc_loss': 0.37166, 'train_ucc_acc': 0.9375, 'loss': 0.51257}\n",
            "Step 198600: {'train_ae_loss': 0.65748, 'train_ucc_loss': 0.35216, 'train_ucc_acc': 0.96875, 'loss': 0.50482}\n",
            "Step 198620: {'train_ae_loss': 0.65742, 'train_ucc_loss': 0.38042, 'train_ucc_acc': 0.9375, 'loss': 0.51892}\n",
            "Step 198640: {'train_ae_loss': 0.64279, 'train_ucc_loss': 0.41219, 'train_ucc_acc': 0.90625, 'loss': 0.52749}\n",
            "Step 198660: {'train_ae_loss': 0.64544, 'train_ucc_loss': 0.44806, 'train_ucc_acc': 0.84375, 'loss': 0.54675}\n",
            "Step 198680: {'train_ae_loss': 0.64666, 'train_ucc_loss': 0.41924, 'train_ucc_acc': 0.90625, 'loss': 0.53295}\n",
            "Step 198700: {'train_ae_loss': 0.65555, 'train_ucc_loss': 0.43263, 'train_ucc_acc': 0.90625, 'loss': 0.54409}\n",
            "Step 198720: {'train_ae_loss': 0.65526, 'train_ucc_loss': 0.40553, 'train_ucc_acc': 0.9375, 'loss': 0.5304}\n",
            "Step 198740: {'train_ae_loss': 0.65324, 'train_ucc_loss': 0.40839, 'train_ucc_acc': 0.90625, 'loss': 0.53082}\n",
            "Step 198760: {'train_ae_loss': 0.66437, 'train_ucc_loss': 0.44301, 'train_ucc_acc': 0.875, 'loss': 0.55369}\n",
            "Step 198780: {'train_ae_loss': 0.67065, 'train_ucc_loss': 0.36081, 'train_ucc_acc': 0.96875, 'loss': 0.51573}\n",
            "Step 198800: {'train_ae_loss': 0.63616, 'train_ucc_loss': 0.43843, 'train_ucc_acc': 0.875, 'loss': 0.5373}\n",
            "Step 198820: {'train_ae_loss': 0.64677, 'train_ucc_loss': 0.46954, 'train_ucc_acc': 0.84375, 'loss': 0.55816}\n",
            "Step 198840: {'train_ae_loss': 0.64584, 'train_ucc_loss': 0.56496, 'train_ucc_acc': 0.75, 'loss': 0.6054}\n",
            "Step 198860: {'train_ae_loss': 0.66409, 'train_ucc_loss': 0.40306, 'train_ucc_acc': 0.90625, 'loss': 0.53358}\n",
            "Step 198880: {'train_ae_loss': 0.66213, 'train_ucc_loss': 0.42387, 'train_ucc_acc': 0.875, 'loss': 0.543}\n",
            "Step 198900: {'train_ae_loss': 0.67264, 'train_ucc_loss': 0.40364, 'train_ucc_acc': 0.90625, 'loss': 0.53814}\n",
            "Step 198920: {'train_ae_loss': 0.64899, 'train_ucc_loss': 0.50388, 'train_ucc_acc': 0.78125, 'loss': 0.57644}\n",
            "Step 198940: {'train_ae_loss': 0.65556, 'train_ucc_loss': 0.34247, 'train_ucc_acc': 0.96875, 'loss': 0.49901}\n",
            "Step 198960: {'train_ae_loss': 0.65557, 'train_ucc_loss': 0.44236, 'train_ucc_acc': 0.84375, 'loss': 0.54896}\n",
            "Step 198980: {'train_ae_loss': 0.67499, 'train_ucc_loss': 0.42834, 'train_ucc_acc': 0.90625, 'loss': 0.55167}\n",
            "Step 199000: {'train_ae_loss': 0.65267, 'train_ucc_loss': 0.45203, 'train_ucc_acc': 0.84375, 'loss': 0.55235}\n",
            "step: 199000,eval_ae_loss: 0.65185,eval_ucc_loss: 0.45779,eval_ucc_acc: 0.84766\n",
            "Step 199020: {'train_ae_loss': 0.67653, 'train_ucc_loss': 0.40491, 'train_ucc_acc': 0.90625, 'loss': 0.54072}\n",
            "Step 199040: {'train_ae_loss': 0.63701, 'train_ucc_loss': 0.43313, 'train_ucc_acc': 0.90625, 'loss': 0.53507}\n",
            "Step 199060: {'train_ae_loss': 0.66361, 'train_ucc_loss': 0.46682, 'train_ucc_acc': 0.84375, 'loss': 0.56522}\n",
            "Step 199080: {'train_ae_loss': 0.66772, 'train_ucc_loss': 0.38709, 'train_ucc_acc': 0.90625, 'loss': 0.52741}\n",
            "Step 199100: {'train_ae_loss': 0.67143, 'train_ucc_loss': 0.45555, 'train_ucc_acc': 0.84375, 'loss': 0.56349}\n",
            "Step 199120: {'train_ae_loss': 0.65194, 'train_ucc_loss': 0.39125, 'train_ucc_acc': 0.9375, 'loss': 0.5216}\n",
            "Step 199140: {'train_ae_loss': 0.67024, 'train_ucc_loss': 0.49267, 'train_ucc_acc': 0.8125, 'loss': 0.58145}\n",
            "Step 199160: {'train_ae_loss': 0.66119, 'train_ucc_loss': 0.47041, 'train_ucc_acc': 0.84375, 'loss': 0.5658}\n",
            "Step 199180: {'train_ae_loss': 0.65701, 'train_ucc_loss': 0.4666, 'train_ucc_acc': 0.84375, 'loss': 0.56181}\n",
            "Step 199200: {'train_ae_loss': 0.66084, 'train_ucc_loss': 0.52715, 'train_ucc_acc': 0.78125, 'loss': 0.594}\n",
            "Step 199220: {'train_ae_loss': 0.65087, 'train_ucc_loss': 0.4154, 'train_ucc_acc': 0.875, 'loss': 0.53314}\n",
            "Step 199240: {'train_ae_loss': 0.64797, 'train_ucc_loss': 0.50697, 'train_ucc_acc': 0.8125, 'loss': 0.57747}\n",
            "Step 199260: {'train_ae_loss': 0.65702, 'train_ucc_loss': 0.41161, 'train_ucc_acc': 0.90625, 'loss': 0.53432}\n",
            "Step 199280: {'train_ae_loss': 0.65486, 'train_ucc_loss': 0.51709, 'train_ucc_acc': 0.78125, 'loss': 0.58598}\n",
            "Step 199300: {'train_ae_loss': 0.68628, 'train_ucc_loss': 0.39304, 'train_ucc_acc': 0.9375, 'loss': 0.53966}\n",
            "Step 199320: {'train_ae_loss': 0.66083, 'train_ucc_loss': 0.39476, 'train_ucc_acc': 0.9375, 'loss': 0.52779}\n",
            "Step 199340: {'train_ae_loss': 0.66752, 'train_ucc_loss': 0.62241, 'train_ucc_acc': 0.65625, 'loss': 0.64496}\n",
            "Step 199360: {'train_ae_loss': 0.66701, 'train_ucc_loss': 0.39102, 'train_ucc_acc': 0.90625, 'loss': 0.52902}\n",
            "Step 199380: {'train_ae_loss': 0.64931, 'train_ucc_loss': 0.41728, 'train_ucc_acc': 0.90625, 'loss': 0.5333}\n",
            "Step 199400: {'train_ae_loss': 0.66869, 'train_ucc_loss': 0.51762, 'train_ucc_acc': 0.8125, 'loss': 0.59315}\n",
            "Step 199420: {'train_ae_loss': 0.65947, 'train_ucc_loss': 0.4236, 'train_ucc_acc': 0.875, 'loss': 0.54154}\n",
            "Step 199440: {'train_ae_loss': 0.66545, 'train_ucc_loss': 0.42849, 'train_ucc_acc': 0.90625, 'loss': 0.54697}\n",
            "Step 199460: {'train_ae_loss': 0.66453, 'train_ucc_loss': 0.50156, 'train_ucc_acc': 0.8125, 'loss': 0.58305}\n",
            "Step 199480: {'train_ae_loss': 0.69646, 'train_ucc_loss': 0.38335, 'train_ucc_acc': 0.9375, 'loss': 0.53991}\n",
            "Step 199500: {'train_ae_loss': 0.64173, 'train_ucc_loss': 0.43721, 'train_ucc_acc': 0.875, 'loss': 0.53947}\n",
            "Step 199520: {'train_ae_loss': 0.66181, 'train_ucc_loss': 0.42274, 'train_ucc_acc': 0.90625, 'loss': 0.54227}\n",
            "Step 199540: {'train_ae_loss': 0.6519, 'train_ucc_loss': 0.46499, 'train_ucc_acc': 0.84375, 'loss': 0.55845}\n",
            "Step 199560: {'train_ae_loss': 0.64881, 'train_ucc_loss': 0.42342, 'train_ucc_acc': 0.90625, 'loss': 0.53612}\n",
            "Step 199580: {'train_ae_loss': 0.66599, 'train_ucc_loss': 0.51016, 'train_ucc_acc': 0.75, 'loss': 0.58808}\n",
            "Step 199600: {'train_ae_loss': 0.66006, 'train_ucc_loss': 0.39568, 'train_ucc_acc': 0.9375, 'loss': 0.52787}\n",
            "Step 199620: {'train_ae_loss': 0.66638, 'train_ucc_loss': 0.4407, 'train_ucc_acc': 0.875, 'loss': 0.55354}\n",
            "Step 199640: {'train_ae_loss': 0.66617, 'train_ucc_loss': 0.35136, 'train_ucc_acc': 0.96875, 'loss': 0.50877}\n",
            "Step 199660: {'train_ae_loss': 0.65836, 'train_ucc_loss': 0.4281, 'train_ucc_acc': 0.875, 'loss': 0.54323}\n",
            "Step 199680: {'train_ae_loss': 0.66576, 'train_ucc_loss': 0.35796, 'train_ucc_acc': 0.9375, 'loss': 0.51186}\n",
            "Step 199700: {'train_ae_loss': 0.64617, 'train_ucc_loss': 0.42479, 'train_ucc_acc': 0.875, 'loss': 0.53548}\n",
            "Step 199720: {'train_ae_loss': 0.64954, 'train_ucc_loss': 0.43692, 'train_ucc_acc': 0.90625, 'loss': 0.54323}\n",
            "Step 199740: {'train_ae_loss': 0.65289, 'train_ucc_loss': 0.52259, 'train_ucc_acc': 0.78125, 'loss': 0.58774}\n",
            "Step 199760: {'train_ae_loss': 0.65762, 'train_ucc_loss': 0.58343, 'train_ucc_acc': 0.71875, 'loss': 0.62052}\n",
            "Step 199780: {'train_ae_loss': 0.66715, 'train_ucc_loss': 0.41665, 'train_ucc_acc': 0.90625, 'loss': 0.5419}\n",
            "Step 199800: {'train_ae_loss': 0.64026, 'train_ucc_loss': 0.39721, 'train_ucc_acc': 0.9375, 'loss': 0.51873}\n",
            "Step 199820: {'train_ae_loss': 0.64616, 'train_ucc_loss': 0.51543, 'train_ucc_acc': 0.8125, 'loss': 0.5808}\n",
            "Step 199840: {'train_ae_loss': 0.66262, 'train_ucc_loss': 0.51878, 'train_ucc_acc': 0.78125, 'loss': 0.5907}\n",
            "Step 199860: {'train_ae_loss': 0.64528, 'train_ucc_loss': 0.52339, 'train_ucc_acc': 0.78125, 'loss': 0.58433}\n",
            "Step 199880: {'train_ae_loss': 0.65846, 'train_ucc_loss': 0.46755, 'train_ucc_acc': 0.78125, 'loss': 0.56301}\n",
            "Step 199900: {'train_ae_loss': 0.66068, 'train_ucc_loss': 0.50351, 'train_ucc_acc': 0.8125, 'loss': 0.5821}\n",
            "Step 199920: {'train_ae_loss': 0.66339, 'train_ucc_loss': 0.451, 'train_ucc_acc': 0.84375, 'loss': 0.55719}\n",
            "Step 199940: {'train_ae_loss': 0.64659, 'train_ucc_loss': 0.47034, 'train_ucc_acc': 0.84375, 'loss': 0.55847}\n",
            "Step 199960: {'train_ae_loss': 0.65042, 'train_ucc_loss': 0.44125, 'train_ucc_acc': 0.84375, 'loss': 0.54584}\n",
            "Step 199980: {'train_ae_loss': 0.64337, 'train_ucc_loss': 0.34681, 'train_ucc_acc': 0.96875, 'loss': 0.49509}\n",
            "Step 200000: {'train_ae_loss': 0.64201, 'train_ucc_loss': 0.40729, 'train_ucc_acc': 0.90625, 'loss': 0.52465}\n",
            "step: 200000,eval_ae_loss: 0.64426,eval_ucc_loss: 0.4568,eval_ucc_acc: 0.84961\n",
            "Training finished!!!\n"
          ]
        }
      ],
      "source": [
        "def load_model_and_optimizer(experiment_id, run_id):\n",
        "    model = torch.load(f\"mlruns/{experiment_id}/{run_id}/artifacts/best_model/data/model.pth\", weights_only=False, map_location=\"cuda\")\n",
        "    optimizer = torch.optim.Adam(lr=0.0012, params=model.parameters())\n",
        "    optimizer.load_state_dict(torch.load(f\"mlruns/{experiment_id}/{run_id}/artifacts/optimizer.pt\", weights_only=False, map_location=\"cuda\").state_dict())\n",
        "\n",
        "    with open(f\"mlruns/{experiment_id}/{run_id}/metrics/eval_ucc_acc\") as file:\n",
        "        lines = file.readlines()\n",
        "        step = int(lines[-1].split(\" \")[-1])\n",
        "    return model, optimizer, step\n",
        "\n",
        "def resume_training(run_id):\n",
        "    mlflow.set_tracking_uri(\"mlruns\")\n",
        "    run_name = \"camelyon-ucc-drn-search-init\"\n",
        "    experiment = mlflow.set_experiment(run_name)\n",
        "    experiment_id = experiment.experiment_id\n",
        "    cfg_name = \"train_camelyon_ucc_drn\"\n",
        "    with initialize(version_base=None, config_path=\"../configs\"):\n",
        "        cfg = compose(config_name=cfg_name)\n",
        "\n",
        "    args = cfg.args\n",
        "    model, optimizer, step = load_model_and_optimizer(experiment_id, run_id)\n",
        "    train_loader, val_loader = init_dataloader(args)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
        "    print(device)\n",
        "    print(optimizer)\n",
        "    print(step)\n",
        "    with mlflow.start_run(run_id=run_id, nested=True):\n",
        "        mlflow.set_experiment(experiment_id=experiment_id)\n",
        "        best_acc = train(args, model, optimizer, None,\n",
        "                    train_loader, val_loader, device, step=step)\n",
        "\n",
        "resume_training(\"91929ff20e614b84ae2855d3ee3f565a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFMWHXRKbXWt"
      },
      "outputs": [],
      "source": [
        "mlflow.set_tracking_uri(\"mlruns\")\n",
        "run_name = \"camelyon-ucc-drn\"\n",
        "experiment = mlflow.set_experiment(run_name)\n",
        "experiment_id = experiment.experiment_id\n",
        "cfg_name = \"train_camelyon_ucc_drn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdc_8-oRbYQG"
      },
      "outputs": [],
      "source": [
        "experiment_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i3olSfWXbaAN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "prefix_to_replace = \"/content/gdrive/MyDrive/UCCDRNPytorch/\"\n",
        "prefix_replacement = \"/Users/tanguanyu/UCC-DRN-Pytorch/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "for root, dirs, files in  os.walk(\"mlruns/152105657986962541\"):\n",
        "    for d in dirs:\n",
        "        if d==\"models\":\n",
        "            for rt, ds, _ in os.walk(f\"{root}/models\"):\n",
        "                for d_ in ds:\n",
        "                    with open(f\"{rt}/{d_}/meta.yaml\", \"r\") as file:\n",
        "                        string = file.read()\n",
        "                        string = string.replace(prefix_to_replace, prefix_replacement)\n",
        "                    with open(f\"{rt}/{d_}/meta.yaml\", \"w\") as file:\n",
        "                        file.write(string)\n",
        "                break\n",
        "        else:\n",
        "            with open(f\"{root}/{d}/meta.yaml\", \"r\") as file:\n",
        "                string = file.read()\n",
        "                string = string.replace(prefix_to_replace, prefix_replacement)\n",
        "            with open(f\"{root}/{d}/meta.yaml\", \"w\") as file:\n",
        "                file.write(string)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"artifact_uri: /Users/tanguanyu/UCC-DRN-Pytorch/camelyon/mlruns/152105657986962541/2601d759316f40c78dc0aa8a8f21b5ad/artifacts\\nend_time: 1749875771310\\nentry_point_name: ''\\nexperiment_id: '152105657986962541'\\nlifecycle_stage: active\\nrun_id: 2601d759316f40c78dc0aa8a8f21b5ad\\nrun_name: funny-stork-278\\nsource_name: ''\\nsource_type: 4\\nsource_version: ''\\nstart_time: 1749875771065\\nstatus: 4\\ntags: []\\nuser_id: root\\n\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string.replace(prefix_to_replace, prefix_replacement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"test.yaml\", \"w\") as file:\n",
        "    file.write(string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/tanguanyu/UCC-DRN-Pytorch/camelyon\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "for root, dirs, files in  os.walk(\"mlruns/152105657986962541\"):\n",
        "    for d in dirs:\n",
        "        if d==\"models\":\n",
        "            pass\n",
        "        else:\n",
        "            with open(f\"{root}/{d}/meta.yaml\", \"r\") as file:\n",
        "                obj = yaml.safe_load(file)\n",
        "            if \"run_uuid\" not in obj:\n",
        "                obj[\"run_uuid\"] = obj[\"run_id\"]\n",
        "                with open(f\"{root}/{d}/meta.yaml\", \"w\") as file:\n",
        "                    file.write(yaml.safe_dump(obj))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"artifact_uri: /Users/tanguanyu/UCC-DRN-Pytorch/camelyon/mlruns/152105657986962541/2601d759316f40c78dc0aa8a8f21b5ad/artifacts\\nend_time: 1749875771310\\nentry_point_name: ''\\nexperiment_id: '152105657986962541'\\nlifecycle_stage: active\\nrun_id: 2601d759316f40c78dc0aa8a8f21b5ad\\nrun_name: funny-stork-278\\nsource_name: ''\\nsource_type: 4\\nsource_version: ''\\nstart_time: 1749875771065\\nstatus: 4\\ntags: []\\nuser_id: root\\n\""
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yaml.safe_dump(obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "filess = []\n",
        "for root, dirs, files in  os.walk(\"mlruns/152105657986962541\"):\n",
        "    for d in dirs:\n",
        "        if d==\"models\":\n",
        "            pass\n",
        "        else:\n",
        "            loss_file_path = f\"{root}/{d}/metrics/loss\"\n",
        "            if os.path.exists(loss_file_path):\n",
        "                with open(loss_file_path, \"r\") as file:\n",
        "                    string = file.read()\n",
        "                if len(string)==0:\n",
        "                    filess.append(f\"{root}/{d}\")\n",
        "            else:\n",
        "                filess.append(f\"{root}/{d}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "for f in filess:\n",
        "    shutil.rmtree(f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
